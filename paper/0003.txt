NBER WORKING PAPER SERIES

ERROR COMPONENTS REGRESSION MODELS
AND

THEIR APPLICATION S

Swarnjit S. Arora

Working

Paper No.3

1

COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE

National Bureau of Economic Research, Inc.
575 Technology Square Cambridge, Massachusetts 02139

June 1973

Preliminary: not for quotation
should not be quoted without written permission. This report has not undergone the review accorded official NBER publications; in particular, it has not yet been submitted for approval by the Board of Directors.

limited numbers for comments only. They

NBER working

papers are distributed informally and in

NBER Computer Research Center and University of Wisconsin, Milwnukee. Research supported in part by National ScIence Foundation Grant GJ-l15X2 to the National Bureau of Eeonom c Rcs carob, Trio.

Abstract

In this paper, we have developed an operational method for estimating error components regression models when the variance-covariance matrix of the disturbance terms is unknown. Monte Carlo studies were conducted to compare the relative efficiency of the pooled estimator obtained by this procedure to (a) an ordinary least squares estimator based on data aggregated over time, (b) the covariance estimator,

(c) the ordinary least squares estimator, and (d) a generalized least squares estimator based on a known variancecovariance matrix. For T small, and large p, this estimator definitely performs better than the other estimators which are also based on an estimated value of the variance--covariance matrix of the disturbances. For p small and large T it compares equally well with the other estimators.

Acknowledgments

A major portion of this research was done during the authorts stay, as a Research Fellow, at the NBER Computer Research Center for Economics and Management Science. Special thanks are due to Professors David Beisley, Murray Brown, Paul Holland, and Edwin Kuh for their valuable comments and suggestions. Thanks are also due to Mr. David Jones for programming assistance. The simulation for this paper was done at the NBER Computer Research Center using the TROLL system.

Section 1. Introductjon

Section 2. Estimation of Error Components Regression Models .
Section 3.

.
Contents

1

4

Properties of the Estimators

ii

Section 4. Design of the Experiment arid the Comparative Properties of the Various Estimators

Section 5. Conclusion

18

Appendix A-i. Tables of Mean and Mean Square Error of the Coefficients for Various Estimating Procedures Appendix A-2. Random Number Generating Procedures

19

21

References

23

ERROR COMPONENTS REGRESSION MODELS AND THEIR APPLICATIONS

Section 1

Introduction

In several recent studies, attempts have been made to analyze the problems involved in pooling cross section and
time series data by error components (or variance components)

regression models. These !nodels can be formulated as

(1.1)

+

E6kZk.

+

uit

(i1,2,...,n; tl,2,...,T),
where individual

is an observation on the dependent variable for

in period t. Zkjt is an observation on the kth

independent variable, cD

is

an intercept term, 3k (k1,2,.. ,K-l)

are the fixed but unknown slope coefficients, and u± is an error term. This disturbance term is supposed to represent the net
effect of numerous individually unimportant, but collectively
significant, variables whidh have been omitted from the analysis.

Some of these are specific to the individual and remain invariant over time (say p1); some are specific to the time period but are

--2--

invariant over all individuals (say Xi); and some are specific
to both individual and time (say

In this case we can

write
(1.2)

as
+

X.

+

Mundlak (10) and Hoch (5) analyzed this model, treating p and T n 0 and E A = 0. as unknown parameters and assu4ning E i. t:l L L

i=l

Maddala (9) points out a principal weakness in this approach:
it eliminates a major portion of the variation among both the explained and the explanatory variables when the between individuals and between time periods variation is large. This ap-

proach can also cause a substantial loss in degrees of freedom. An alternative approach is to treat all components as random. This case was analyzed by Wallace and Hussain (1L), Maddala (9), Nerlove (12), and Swamy and the present author (13).1 Under the assumptions of weakly non--stochastic X's and
normally distributed disturbance terms, both approaches yield

asymptotically equivalent estimates with asymptotically equi-

valent variance--covariance matrices. In fact, it can be shown
that there are an infinite number of estimators which have the same asymptotic variance-coVarianCe matrices.2

1.

Whether or not the individual effects may be treated as parameters or random components for the purpose of statistical analysis depends upon the underlying data generating mechanism assumed. For an illuminating discussion of such data generating Tnechanisms, see Nerlove (11) p. 3614.
See Swamy and Arora (13) p. 267.

2.

--.3--

Asymptotic properties, however, are cold comfort to the econometrician for whom the choice of a practical estimator
(and its related small sample properties) is a problem of

crucial importance. Unfortunately, because of mathematical
intractibility, small sample properties are often hard to

obtain theoretically. We therefore employ Monte Carlo experiments to evaluate relative efficiency of the various estimators.

The plan of this paper is as follows: In section 2, a
means of estimating error components regression models is developed for a case when the variance-covariance matrix of

the disturbance term is unknown. We also show the equivalence
of this estimator with an ordinary least squares estimator when inter-individual and inter-temporal variations are zero. In section 3, the asymptotic properties of this estimator are

derived. Section 4 describes the design of the Monte Carlo
experiments and compares the relative efficiency of this estimator with the ordinary least squares estimator, a covariance estimator, an ordinary least squares estimator based on data
aggregated over time and ,a generalized least squares estimator

based on a known variance--covariance matrix of the disturbance

terms. Concluding remarks are presented in section 5. An
efficient way of generating random numbers and independently distributed normal variates is described in an appendix to
this paper.

S

--it--

Section 2

Estimation of Error Components Regression Models

Let us assume that u1
and

+

and the components

are random such that
0

F411

Ep.p.

i

' 10
4

ía2

ifij
ifijandt=t
otherwise.
.
3

Ev1tO
Ev. V.

ía2
I

itjt

0

Let us further assume that p1 and

are independent of each

other. Furthermore T > K and n > K and the variances a2 and

3.

In (13) Swamy and I have analytically shown that the estimator based onthe assumption of both pj and Abeing random is more efficient than either Ehe covariance estimator or the ordinary least squares estimator only if (a) n and T are sufficiently larger than 10, and (b) if the sum of squares due to variation over time exceeds the sum of squares due to remaining variation. If these conditions are not satisfied, random error components model with both components random may give results inferior to other estimators. For a case where either n or T is less than 10, we conjecture that the error components models with a random component (the other component being a parameter or zero) perform better than the model which assumes both Pj and At as random. Here we consider a model with AtA for all t, but we can easily treat all At's as different. A model in this form was also used by Kuh (7), except that he did not assume Pj and Vit are uncorrelated. Hussain (6)

Li..

--5--
2

.

are unknown. For all the nT observations combined we can

write (1.1)
(2.1)

y X + u,
(y11,
'

where y

.

is an nTxl

vector of observations on the dependent variable, X =

'nT'

Z]

is an nTxK matrix of explanatory variables, tnT is a vector of

l's of order nTxl, and Z is an nTK matrix of independent
variables given by

z1 Z211 ... ZK_l,ll
Z =

Z1

Z;lT

ZK_i,1T

Zini Z21 ...
*

Z1flT

Z2flT

ZK_l,flT

is a (K'xl) vector of slope coefficients,
K'

K --

1, and u

(u11, ...,

U1,

... u1,

(, 5')',
...,

uflT)

is an

nTxl vector of disturbance terms. Under the above assumptions,
it is readily verified that
(2.2)

Euu'

2(10

'T

Since the variance--covariance matrix of u is not scalar, ap-

()

treats

a model with nj's a parameters, and At and jt as random. His estimator is identical with the covariance estimator and does not utilize full data information.

--6--

plication of the ordinary least squares procedure might lead
to an inefficient estima.tor of 3.

Let us consider an orthogonal matrix, °T' of order T

such that its first row is equal to i/1T. Let

=

[tT//T,

C] where C1 is a (T-1)xT matrix such that CuT = 0,
--

ll 'T-l' and C1C1

1·TtT/T.

Define the transformations Q1

(ItT/vW) and Q2

(IC1).

By applying the transformation Q1 to all nT observations, we get
(2.3)

y1 X + u1,

where y1 Q1y is an nXl vector of transformed dependent variables, and u1 is an nxl vector of transformed observations. The variance-covariance matrix of u1 is
(2.4)

Eu1u1

Q1EuuQ1

Substituting for Euu' from (2.2) and simplifying we get
(2.5)
2

Eu1u1 = a1
2

2

wherea1 =Ta

+cy

2

Thus the variance-covariance matrix reduces to scalar form, a best linear unbiased estimator of
(2.6)

is an OLS estimator

(1) (X1X1YX1y1.

The subvector of (1) corresponding to the slope coefficient
only is given by
(2.7)

6(1) =

(ZNZ1)ZNy1

,

-7-

where Z1

Q1Z is an nxK matrix, and N

I -

it/n;

a

subscript 1 is attached to

and to differentiate these
and tS to be described

estimators from the other estimators of
later.

The va'jano-covariance matrix of (l) is
(2.8)

V[(l)] =

a12(ZNZ1).

Applying the transformation Q2 we have
(2.9)

(IC1)

to all nT observations

y2

Z26 + u2

where y2

Q2y is an nTxl vector of tr.nsformed observations

on the dependent variable, Z2 is an nT#xK matrix of transformed observations on K independent variables, u2 = Q2u is an nTxl vector of transformed disturbances, and use is also made of the
result Q2inT = 0. The variance-covariance matrix of u2 is

(2.10)

Eu2u; EQ2uuQ2 Q2EuuQ2 ;

= (IflCl)[Q2(IfltTtT)
which can easily be reduced to
covariace matrix of U2
V

+

vIT

IC1)

is

. Thus the variancenT of scalar form. A best linear un-

biased estimator of 6 is the OLS estjm.tor given by

S

--8--'

(2.11)

(2)

--1
(Z2Z2) Z2y2

5

The variance-covariance matrix of (2) is a2(ZZ2)1. Notice
that Q1Q2

0. The rank of Q1 is equal to the rank of

/v'T multiplied by the rank of I because if A and B are any
arbitrary matrices, the rank of (AB) is equal to the rank of A multiplied by the rank of B. Therefore the rank of Q1 + Q2 n + nT n = nT, which is thetotal number of observations.
This indicates that in estimating (2.3) and (2.9) we have used up all the orthogonal linear combinations of the available
observations.

S(l) and S(2) are two uncorrelated estimators

of the same parametric vector and we can pool them in the
following manner.

A

(2.12)

6(0)

rzNz 1

L

21 +

ZZ 11ZNy Zy 2 1 2 2

aJ

2j

I

21 +

La 1

a

where 0 =

[a2

a2] The estimator S(0) is a generalized

least squares estimator of 6. For given values of a12 and a 2,

it is a best linear unbiased estimator. Any other estimator of
6 which is also linear in the vector y and is unbiased, has a variance-covariance matrix which exceeds that of S(0) by a
positive semidefinite matrix.

5.

It can be easily recognized that the estimator 8(1) in (2.6) is an OLS estimator obtained by applying OLS to data agregated over time and multiplied by lIvW; the estimator 6(2) in (2.11) i obtained by applying OLS to nT observations, each observation expressed as a deviation from its time series mean and the overall mean. Please note that there are only n(T-1) independent observations.

An unbiased estimator of
(2.13)

·

.

2.is

.

given by

y1M1y1/(n-K)

where of
is

I -

X1(XX1)X

Also an unbiased estimator

(2.1k) 2 y;M2Y2/(nT'- K')

where I -

z2(z;z2)-1z;,

T' = T -

1,

and K'

K - 1.

An Aiticen es1imtor of the slope coefficients based on the estimated values of

and a2 is given by

(2.l)

=

[zz

+

z2z2]

[Z1NY1 +

Z2Y2]

where Q [2
We can readily show that the estimator 2) as obtained in (2.11)
is equivalent to a covariance estimator (iS)

obtained

by assuming

as fixed parameters. We can also show equivalence of iS(O)
with an ordinary least squares, estimator (iS) when follows:
0 as

An ordinary least squares estimator of the slope coefficients in (1.1) is

6.

A similar pooled estimator of (O) can be obtained if A and Vjt are assumed to beH random. The variance-covarjance matrix of u is giver by aX2(LninIT) + GV2InT . To reduce this to scalar form we consider an orthogonal matrix O [t/v'i, C1]' of order n, and apply transformations (1.n/1/'®IT) and Q2 Qi (C1øITto all the nT observations.

--10--
(2.16)

6

(ZQZ)ZQy ,
'nT --

where

1nT'nT
nT
2

The Aitken estimator 6(0) in (2.15) when a

U
+

o is

(2.17)

() ZNZ1 + Z2Z2][Z1Ny1
= [z(QNQ1
a

+

Q;Q2)Z]--1cZ-(QNQ1
and
a
=

+

Q;Q2)y]
ft
--

Since Q1NQ1

(I

1T'T )

--

nT'nT ______

____ 'T1T)
T

we can easily show that QNQ1 +
proving equality of

'nT
2

o.

nT nT nT

Q , thus

and () when a

U

-11-

.
3

Section

Properties of the Estimators

The estimator (O) of can be written in the generalized
least squares form as follows:
-l
[Z1N
(3.1)
1

NZ1

uz;N
I

Ny1

a1
0

a1

Z2

If W
(3.2)

a

a

A

A. 2

[Z1N, Z2] ,

V

A. 2

--

diag[a1

mT...] and y

[y1N, y2]

the equation (3.1) can be written as
A A
tS(O)

A_i --1 (WV W) W V y
[uN, u;] in (3.2), we get

Substituting y W + e, where e
A A
(3.3)
5(O)

5 + (W'V

A --1

--1 W) W A....] V e

A generalized least squares estimator for a given V, as obtained
in (2.12), can also be expressed as follows:
A

(3.')

6(0) =

6

+ (W

---1 --l-1 V W) W V

If we assume that U's are normally distributed, and since

M1Z1N MX1 [In - X1(XX1)1X]X1
form a2 yM1y1/(n-K) .

0, we an show that the

linear form Z1N1 is distributed independently of the quadratic

Similarly,

we can show that the

linear form Z2u2 is independently distributed of the quadratic

--12--

form that
(3.5)

yM2y2/(nT-K). With these results, we can show

E[()IO] = 6
the expectation of 6 over the distribution of
is 6,

Since
of 6.

i.e., E6

6, this

proves that 6(0)

is

an unbiased estimator

To establish the asymptotic properties of 6(0), we assume that --l X's are weakly non-stochastic and, for a fixed n, urn (nT) Z1NZ1,
T+oo

urn

(nT)-'z;z2 are all finite positive definite matrices.7

For a fixed n, under the above assumptions, we can show that liin --2--2-- n T Z,,uu,Z T (Z1Nu1u1NZ1) 0, thus insuring lim n--2--2 L L.
T+co
T.+co

that plim (nT)ZNu1 0

T÷
.

plim (nT)-iz;u2. Also, plim a12
T÷oo

2

,

plim

T-

"2 c1 =

T÷°°

2

.

Under these conditions, we can easily

show that

(3.6)

plirn /if[() -- 6]
T-,co

0 ,

i.e., 6(e) is a consistent estimator of 6.

7.

The assumption of non-stochastic implies that the time pattern of the variable is bounded by some finite limits, even though it is not necessary for the pattern of the variable to repeat itself. The meaning of non-stochastic X's is simply that the realization of the X's is in accordance with some fixed (albeit unknown) process. Since economic data are stochastic, whichever assumption we adopt about the nature of the fixity of X's, we are sin-- plifying and possibly mis-specifying the model. See also Wallace and Hussain (P4) pp. 55--72.

-13-

Under the above assumptions, we can readily show that V is a ccnsistent estimator for V, and that 6(0), as obtained in
(2.15), is asymptotically equivalent to 6(0), in the sense

that /i[6(0)

6(0)] converges in probability to zero as

T+oo, both coefficient estimators being asymptotically normally

distributed with mean vector 6 and covariance matrix
Since plim
T+oo
=
Ii

(WVW).

2, we can further show that () is also

asymptotically
(3.7)

equivalent to the covariance estimator 6, i.e.,

plim /iTI6(G) -- 61 T+°

0

In fact we can show that there is an infinitely large number of estimators which yield asymptotically equivalent estimates with asymptotically equivalent variance-covariance matrices. Thus asymptotic theory casts relatively little light on the

comparative small sample properties of the estimators. In the
next section, we evaluate relative efficiency of the various estimation procedures by using a Monte Carlo study.

.

--l4--

Section
Design of the Experiment and the Comparative Properties of the Various Estimators The design of the Monte Carlo experiments given here is

similar to that of Nerlove, except that our model contains an
inLercept and we generate random numbers by a slightly different, but more efficient, method.8 Since Nerlove has already

done extensive Monte Carlo studies, we examine intensively

only those cases with large inter-individual heterogeneity and

varying T. The model is given by
(L.l)
=

+

+ 'i

+

'it
+

The explanatory variable, X1, held fixed throughout the
experiment, is generated as follows:

(.2)

X1 =

o.i(t-l)

+

l.05X1±t_i

w,

where w is uniformly distributed in the range from 0 to 2. Initial values of X10 are chosen at random from the uniformly

distributed numbers in the range 0 to 100. To generate nT
values of independent normal variables with zero mean and
N(0,

init variance, n p.'s are first selected with
nT

are then selected with

N(0, cY2), and these are

summed

to

give the

Defining p, the intra-class correla-

8.

This method and a method to convert uniformly distributed random variables to normal, variate is described in Appendix A--2. See also Nerlove (11), pp. 366--371.

--15--

tion coefficient, as p = a 21a2, where

a2 2 + a2

we can

.

write

N(0, pa2) and

v N(0,

(l--p)a2)

Twelve sets of y's were generated for, various combinations of the parameter values
p

0 and 5;

0.5 and 0.8;

0, 0.4 and 0.8; and a2

10. Initially n is set at 25 and

T at 6 .

For each set of parameters, five estimating proce-

dures were examined:

(a) OLS estimator based on data aggregated over time,

(b) (c) (d)

covariance estimator, (2),
ordinary least squares, (s), pooled estimator based on the estimated variance--
covariance matrix, ô(O),

(e) generalized least squares based on known variance-

covariance matrix, (8).
In each experiment, 20 repetitions are performed, from which the mean and the mean square error of the estimated

coefficients are calculated. The entire set of experiments is
ibepeated with T set at 15, giving 480 runs and 24 tables of

9.

These parameter's were selected from the initial set of
parameters c'
0.0, 0.5,

0.1, 5.0 and 10.0; a2 = lOand2O. 0.5, 08; p= 0.0, 0.2, 0.4, 0.6and0.8; For these 150 sets of parameters 2 repetitions were performed. On the basis of mean square error of the estimators in the various estimating procedures only 12 parameter sets were selected for intensive study. The choice of these parameter values may itself cause bias in our results, but the very consistency of the trend strengthens our belief that this is a representative set.

1.0,

.

-16--

mean and mean square error of the coefficients for different

etimating procedures. Table 1 presents the mean and the mean
square error for one such experiment. Results of various other
runs are presented in an appendix to this paper)°

Table 1
Mean and Mean Square Error of the Coefficients for Various Estimating Methods for
10, N
25, T

6, and p =

.8

True Value
OLS--Agg. Covariance
OLS

5
'I.

0

0.5

0

99184 99208 99329

0. 703589

0.499717

7. 725E--05 5. 221E--05
5. 5LiOE--05

0.499688
Li.

0.594746 0.400888 0.386660

0.499714 0.499682

Pooled GLS

5. 00708

2. 791E--05 2. 788E--05

0.499695

From Table 1, we find that the mean values of

and

for all estimating methods are finitely close to the true values, thus demonstrating that all estimators under consideration are

unbiased,

but that the mean square error for the dif-

ferent estimators varies considerably. The mean square error
of OLS--Agg. is about three times as large as that of the

generalized least squares estimator, while those of the covarince estimator and of the ordinary least squares estimator are

10. Mean square error of an estimator
m.s.e.

of 0 is given by

E (0. -- 0) i=l

A

2

--17--

.

only

about twice as large. The mean square error of the

pooled estimator is nearly the same as that of the generalized
least squares.

This is true for all values of p except pO. In this
case, all estimators have mean square error equal to that of the generalized least squares estimator.11 As p increases, sà does the ratio of the mean square error of the OLS estimator tc that of the GLS estimator, but for all values of p the mean square error of the pooled estimator nearly equals the mean

square error of the GLS. Further, for large values of p,
the OLS method gives a serious underestimate of a2, giving low

standard errors of the estimates. In contrast, the standard
errors for the pooled estimator and the GLS are nearly equal. As T increases, the mean square error of the covariance esti-

mator declines, becoming almost equal to that of the pooled
estimator and the GLS estimator.

Hence we see, on the basis of the criterion of minimum
mean square error, that the pooled estimator compares favorably, for all T's and all p's, with all other estimators which do not require a prior knowledge of the variance-covariance

matrix. Furthermore, this estimator shows definite superiority to other estimators for small T's and large p. On the basis of
the criterion of unbiasedness, this compares equally well with
all other estimators.

11. See tables 1 and 2 in the appendix to this paper.

--18--

Section 5

Conclusion In this paper, we have developed an operational method for estimating error components regression models when the variance-covarjance matrix of the disturbance terms is un-

known. Monte Carlo studies were conducted to compare the
relative efficiency of the pooled estimator obtained by this

procedure to (a) an ordinary least squares estimator based on data aggregated over time, (b) the covariance estimator, (c) the ordinary least squares estimator, and Cd) a generalized least squares estimator based on a known variance-

covariance matrix. For T small and large p, this estimator
definitely performs better than the other estimators which

are also based on an estimated value of the variance-covarjance

matrix of the disturbances. For p small and large T it compares equally well with the other estimators. In this instance,
therefore, we are able to give a definite unconditional answer to the question posed to Nerlove's Dodo, "But who has won?"-the pooled estimator, of course!

--19--

.
in.s.e.
0

Appendix A-i
Table 1

Mean and Mean Square Error of the Coefficients for Various Estimating Procedures for
T
6, N

25, and a2 = 10

Method

Mean

Mean

m.s.e.

p=0.O: True Value
OLS----Agg. Covariance

5

0. 80

0

5.20645
5. 14152

0.286728

0.798293

6. 5614E--05

0.806431 0.216076 0.200142 0.216061 0.799298 0.799582
0. 799 297

3. 370E014 5. 342E--05
5. 227E--05
5. 3'42E--05

o LS Pooled GLS

5.12310
5. 114149

p0.4 True Value
OLS--Agg.

5

0

0.80

0

5.34402

0.839164
0.6732143

0.794023
0. 800825
0. 794863

1. 970E--04
2. 19 OE--0 '4

Covarianc e
OLS Pooled
GLS

5.28975 5.18582 5.16231
5

1. 900E--04
9. 10 8E--05

0.544211
0.538673
0

0.796471 0.796834
0. 50

8. 246E--05

p0.8 True Value
OLS---Agg.
Coy ar iance

0

4. 99184

0.70300
0. 594746

0.499717

7. 725E--05 5. 221E--05 5. 540E--05
2. 7 3 1E --05

0.499688 4.99208
5. 00708

o L1S

0.499714

Pooled
GLS

0.400888 0.386660

0.499682
0. 499695

4.99329

2. 788E--05

S

--20--

Table 2
Mean and Mean Square Error of the Coefficients for Various Estimating Procedures for T 25, and 2 = 10 15, N

Method

Mean

m.s.e. ()

Mean

()

m.s.e.

p0.0: True Value
OLS----Agg. Covariance
OLS

5.0

0

0.80

0

5.04207
5.00136 5.00143 5.00150
5.0
14.88203

0.214164
0.0741410

0.800005
0.800616

l.527E--05 1.241E-05
'4.829E--06

0.800355 0.800353 0.800352
0.80

Pooled GLS
p=0.4: True Value

0.074891
0.0714409

14.72lE06 4.827E--06
0

0

OLS----Agg. Covariance

--

1.03832
---

0.800393
0.799514 0.799890

6.816E--05 6.300E-06 l.593E--05 6.210E--06 6.l35E--06
0

OLS Pooled GLS

4.94057

0.346083

4.97141 4.97792
5.0
5.573014

0.217877
0.222983
0

0.799625
0.799570
0.80

p=0.8: True Value
OLS--Agg. Covariance

2.61798

0.793599
0. 799 879

2.O1OE--04
1. 413E--06

OLS

5.15513 4.85477 4.85116

0.639253
0. 3414144

0.797190

3.645E--05
1. 381E--06

Pooled GLS

0.344148

0.799771 0.799802

1.368E--06

--21--

.

Appendix A-2
Random Number Generating Procedures

A desired sequence of random numbers X is obtained by
setting
Xn+1

(aX + c) mod m n

m > -- 0

where a is the multiplier, c is the increment and m is the
modulus, a>0, c>O, m>c, m>a, and m>X0, where X0 is the starting

value. This method is called linear congruential sequence.

When c0, the random generation process is slightly faster,
but the maximum period length (length after which sequence

starts repeating itself) can not be achieved. Nerlove (11)
in order to avoid this problem, suggests mixing two random sequences into a third, so that the third one is extremely

random. We use a method suggested by Maclaren and Marsaglia
as described below.1 A quite random sequence Given methods for generating

two sequences Xn and Y, this method produces a "considerably more random" sequence. We use an auxiliary table V(0), V(l),
.,

V(k-l),

where k is some number chosen for convenience,

usually in the neighborhood of 100. Initially, the V-table
is filled with the first k values of the X-sequence.

1.

See also Knuth (8) pp. 25-31

--22--

Step 1:

[Generate X, Y]

Set X, Y equal to the next

number of the sequence (X Y) respectively.
Step 2:

[Extract j]

Set 5

[kY/rn], where m is the

modulus used in the sequence Y; i.e., 5 is a random
value, 0 < 5 <
Step 3: [Exchange]

k

determined by Y.

Output V(j) and then set V(j) ÷ X.

This method gives an incredibly long period if the periods

of (X) and (Y) are relatively prime; and even if the period
is of no consequence, there is very little relation between

the nearby terms of the sequence. To generate independently
normal variates we follow the Polar Method, which consists of generating two independent random variables (u1 and u2) uni-

formly distributed between zero and one.2 A set of independent
normal variates with mean zero and variance one is obtained by
the transformation
1/2

w1 = (--2 log u1)
1/2 w2 = (--2 log u2)

cos (2iru2)
3

cos (2iru1)

2.

To generate variable uniformly distributed between zero and one, we first generate some random number X between zero and in as described above, and then the fraction u1 = X/m will lie between zero and one. For a comprehensive discussion of this method see Knuth (8) pp. 103-105; also see Nerlove (11), p. 368 footnote 11.

3.

--23--
Ref erences

1.

Anemiya, Takeshi (1971): "The Estimation of the Variances in a Variance-Components Model," International Economic Review, Vol. 12, No. 1, pp. 1--13. Arora, Swarnjit S. (1971): "Error Components and Multipolar Human Flow Models," unpublished Ph.D. dissertation submitted to the State University of New York at Buffalo, September 1971. Balestra, P. and M. Nerlove (1966): "Pooling Cross Section and Time Series Data in the Estimation of a Dynamic Model: The Demand for Natural Gas," Econornetrica, Vol. 34, No. 4, pp. 585--612. Henderson, Charles R., Jr. (1971): "Comment on the Use of Error Components Models in Combining Cross Section with Time Series Data," Econometrica, Vol. 39, No. 2, pp. 397--401.

2.

3.

4.

5.

Hoch, I. (1962): "Estimation of Production Function Parameters Combining Time-Series and Cross-Section Data,'t Econometrica, Vol. 30, No. 1, pp. 34-53.
Hussain, A. (1969): "A Mixed Model for Regression," Biometrika, Vol. 56, pp. 327--36. Kuh, E. (1959): "The Validity of Cross-Sectionally Estimated Behavior Equations in Time Series Application," Econornetrica, Vol. 27, No. 2, pp. 197-214. Knuth, Donald E. (1968): Seminumerical Algorithms--The Art of Computer Progranuning, Vol. 2, Addison-Wesley Publishing Company, Reading, Massachusetts, pp. 25-31 and pp. 103-105. Maddala, G.S. (1971): "The Use of Variance Components Models in Pooling Cross Section and Time Series Data," Econometrica, Vol. 39, No. 2, pp. 341-58.
Behavioral Functions from a Combination of Cross-Section and Time Series Data," pp. 138-66 in Measurement in Economics, C.F. Christ, Ed., Stanford, California, Stanford University Press.

6.

7.

8.

9.

10. Mundlak, Y. (1963): "Estnation of Production and

--24--

11. Nerlove, M. (1971a): "Further Evidence on the Estimation of Dynamic Economic Relations from a Time Series of Cross Section," Econometrica, Vol. 39, No. 2, PP. 359-81.
12.

______ (1971b): "A Note on Error Components Models," Econometrica, Vol. 39, No. 2, pp. 382-96.

13. Swamy, P.A.V.B. and S.S. Arora (1972): "The Exact Finite Sample Properties of the Estimators of Coefficients in the Error Components Regression Models," Econometrica, Vol. 4O, No. 2, pp. 261-75.
l'4. Wallace, T.D. and A. Hussain (1969): "The Use of Error Components Models in Combining Cross Section with Time Series Data," Econometrica, Vol. 37, No. 1, pp. 55-72.

