NBER WORKING PAPER SERIES

WEIGHTED RIDGE REGRESSION:

COMBINING RIDGE AND ROBUST REGRESSION METHODS

Paul W. Holland*

Working Paper No.

11

COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
National Bureau of Economic Research. Inc.
575 Technology Square
02139
Cambridge, Massachusetts

September 1973

Preliminary: not for quotation
NBER working papers are distributed informally and in limited numbers
for comments only. They should not be quoted without written permission.
This
•

report has not undergone the review accorded official NBER

publications; in particular, it has not yet been submitted for approval by
the Board of Directors.
*NBER Computer Research Center. Research supported in part by National
Science Foundation Grant GJ-ll54X2 to the National Bureau of Economic Research, Inc.

Abstract

1)

Gives the formulas for and derivation of ridge regression methods when

there are weights associated with each observation. A Bayesian motivation is
used and various choices of k are discussed. A suggestion is made as to how
to combine ridge regres.sion with robust regression methods.

C)

Contents

1. Introduction

1

2. Ridge Regression When There are Weights

2

3. Motivations and Interpretations

4

Bayesian Background

5

Interpreting Ridge Regression

7

4. The Choice of k
Empirical Bayes Choices of k

8
9

Estimating Optimal k-values

12

Further Study of These Choices of k

14

5. Combining Ridge and Robust Regression Methods

17

References

19

1.

Introduction

We consider here the familiar regression problem specified by

y =

X13

+

c

(1-1)

where y is Nxl, X is Nxp and 13 is pxi. We use the notation

Z

(1-2)

Gau(p, )

to mean that Z has an N-dimensional multivariate Gaussian distribution with
mean vector p

=

E(Z) and covariance matrix

Cov(Z).

In this notation we

assume that

c'' Gau(O , a2 <w) _1)

(1-3)

where( w) denotes a diagonal matrix with the vector w along its main diagonal.
( w> is assumed to be a. known matrix

,

a

and 13 are unknown.

The weighted least squares estimate of 13 is given by
=

(X<w)

x)_lxT <w y

(1-4)

and the weighted least squares "fitted values,"

LS
satisfy

=

BLS

(1-5)

'

the following normal equations:
XT<w) y= XT(w) y

.

..

The problem we wish to attack here is how to improve on
estimator of 13.

Because,

(1-6)
as an

is thebest linear unbiased estimator of 13,

to find an improvement we must consider estimators which are both non-linear

-2-

functions of y and biased. The fundamental work of Stein [1956] and later
that of Baranchik [1970] and Sciove [1968] show that when the number of

regres-

sion parameters is sufficiently large, then uniform imporvements over
are possible using biased, non—linear estimators. The minimum p (not
including the constant term) is p =

3.

The results of Wermuth [1972] show

that the degree of improvement possible increases substantially as the x's
become more multicollinear.

The particular class of estimators we will discuss is a slight extension

of the "ridge regression" estimators developed by Hoer] and Kennard

[1970] and studied by Wermuth [1972], Sciove [1973], Marquardt [1970],
Mayer and Willke [1973].
We use the "weighted least squares" framework because it allows us to

use a suggestion of Tukey [1973] for doing robust regression and in effect to
combine ridge and robust regression methods. This combination is discussed

in Section 6.
2. Ridge Regression When There are

Wei9hts

We now give a prescription for doing ridge regression when there is a
weight, w.,, associated with each observation. The weights are assumed to
be non-negative and they need not sum to unity --

but

they may if that is

convenient. In addition we also assume that we have a "prior mean" for .

This

will be amplified more fully in the next section. We denote the prior

mean for by 5. In the usual case of ridge regression,

is taken as zero,

and the weights, w., are all equal.
In this paper, we always assume there is a constant term in the regression equation, but that this is not reflected in the choice of X-matrix.

Hence we assume that no column of X is constant. The constant term is

-3-

estimated separately from the other regression coefficients via the
following formula
A

0

PA

jj

=S-

(2-1)

where y and x. denote the weighted means of y and he j column of X,
respectively, i.e.,

• w.x.
113

E w.y.

z

Ew.

w.

11

i1

(2-2)

In (2-1), . denotes the estimate of the ji element of which we will
describe shortly.

In the calculation of the regression coefficients, as opposed to
we assume that all variables have had their weighted mean (2-2) subtracted
out,

'=y-

.

The 'weighted" length of ''.

is

given by

s. =1/w. "p..

'i 1

(2-3)

13

(2-4)

thus we give all the x. the same weighted length by setting

X = 7(s)

1

(2-5)

This scaling of the x's implies a rescaling of the 's via

(2-6)

Hence the prior mean

*

=<s)

must be rescaled also,

ó .

(2-7)

-4-

3.
Having properly centered and scaled all variables we may now give the
ridge regression estimates of the rescaled parameter,

* + (X*(w)

=

X + kI)_1X*T<w> (y - X*6*)

The ridge regression estimator of

=

R

)(w) -)

Hoerl

(2-9)

depends on the parameter, k. When k=O, then

no matter what 6 is. When kc, then

values of k, R

(2-8)

.

(rather than *) is given by

6 + T) + k(s2)
We observe that

This is given by

interpolates

=

6.

For intermediate

between these extreme values.

and Kennard [l97 suggest

using several values of k in

a diag-

nostic mode to identify those least-square parameter estimates which might
be improvable. Wermuth [l97 and Sclove [l97 suggest choosing k from the
data and obtaining a single point estimator of

family

rather than a one-parameter

of estimators. In Section 4 we discuss various choices of k that

are data dependent.

In summary, the method of estimation we propose here is as follows.

,

(a) Compute weighted means and subtract them from each variable,

obtaining

#
y and X.

(b) Compute k via one of the methods discussed in Section 4.
(c) Estimate the regression coefficients via equation (2-9),

obtaining R
(d) Estimate the constant term via equation

(2-1) using R

for

the regression coefficients.

3. Motivations and Interpretations
In this section, we shall give the Bayesian motivation for ridge

— 5..

regression

as put forward

in

the previous section. In addition, we show

how ridge regression may be interpreted a a "smooth" selection of variables
method of estimating parameters.

Bayesian Background

We begin with the statement of a useful lemma that allows us to pass
back and forth between conditioning U on V and then V on U when (u,v) has
a multivariate Gaussian distribution.

The back-and-forth lemma: If UJVl% Gau[a + B(V-d), C] and V".'Gau(d, E)
and if C and E are non-singular, then

(a) VU 'v Gau (d +(E_l4BTCB)_1BTC(U.,a), (E_.+BTC_lB))
and (b) UAGau(a, C+BEBT).
The proof of the back-and-forth lemma is a straightforward exercise in

properties of the multivariate Gaussian distribution nd matrix algebra.
Now suppose we consider the Bayesian analysis of the general linear
model given by

yIt3

it', GauN(X, a2 (w> _1)

Gau(6,
Shortly,

A)

(3-1)

(3-2)

we shall specialize A to -r21, but for the moment we consider the

more general setting given in (3-2). Note that we do not give a prior dis-

tribution to 2 in this development. This is to keep the analysis simple.
In all cases we estimate 2 by the weighted residual mean square from the

least squares fit. This is

=

(N-p)1 w.

-

s))2

(33)

-6-

Furthermore, we will often regard 2 as known and equal to the estimated
value, 02.

While this is not the way a full Bayesian analysis would proceed,

it is adequate for our purpose which is to motivate the procedure given in
the previous section.

From the model (3-1) and (3-2) and the back-and-forth lema we may
obtain the posterior distribution of

Theorem 1:

and the marginal distribution of y.

If yJA, GauN(X, 02(w) _1) and 3v Gau (S,A), then

(a) (Posterior distribution of )

IY'-'Gau(6

+ (XT(w) X + 02A_l)_lxT<w> (y -

X),

02(xT <w) X + G21)1)

(b) (Marginal distribution of y)

(w)

y..' GauN(X5, 2

1

+

xixT).

From part (a) of Theorem 1 we see that if the prior covariance matrix

of

is taken as

=

-r21, then except for the replacement ofy by y and X

)

by X* the formula for I3 corresponds to the posterior mean of with
k =

—

.

(3-4)

T2

Suppose we assume that

is given, what conditions on would make the

assumption that

A.Gau(5, T21)

(3—5)

a reasonable one? By scaling the x's as we have, we have made them dimen-

sionless so that the * parameters reflect only the relative slopes of the
regression plane and not merely differences in the units in which the x's

are measured. The assumption (3-5) asserts that the . — ô. behave like a
sample from a Gaussian distribution with unknown variance and zero mean.

This is more plausible for the *'s than the original 's which may have

)

—7—

differing units. Finally, the constant term in a regression is usually of
quite a different character than the regression parameters.

It merely

centers the regression plane to pass through the "middled of the point cloud.
Hence we have centered each variable at its weighted mean and chosen

that the fitted regression plane passes through the point (.,

Thus

is not included in the parameters that have been given

Under assumption (3-5) the posterior distribtion of

so

...,

pri ors.

is

yA/ Gau(S + (XT(w) X + kI)_lXTw)(y - X5), 2(xT(w) X + kI1)

(3-6)

and the marginal distribution of y is

y.i Gau (X5, 2<

-1

w)

÷ T2 xxT)

(3-7)

Interpreeing Ridge Regression

The Bayesian motivation for ridge regression may be satisfactory for
many purposes, but the following interpretation shows that it also has close
ties with regression on principal components.
We may rewrite (2-8) in the following form:
=

5* + (I

+

k(X*T(w) x*))1(

-

(3-8)

where
=
is

(3...9)

'' is

A particularly revealing form of ridge regression appears when we trans-

form to the "principle component axes." The usual orthogonal diagonalization

of X*T(w) X is given by

X*T(w)
where V is pxp

x = v (x)

VT

(3-10)

orthogonal and (X)is the system of eigenvalues of X*T4w) X*.

-8-

3
We

define the "principal component parameters" by

VT*

=

(3-11)

and the corresponding transformed prior mean by

v

(3-12)

One property of least squares is that

1* =

(3-13)

LS

We may define

so that this is also true for ridge regression, i.e.,
=

VT

(3-14)

)

Starting with (3—8) and (3-14) we then obtain

1*

=

v +<.) (*

-

v*)

(3-15)

.

Hence we see that the ridge regression estimators of the principal component

parameters are found by shrinking the least squares estimators of-y towards
'v' by an amount
1

is
Thus

that reflects the size of A.1

relative to k.

If x.
1

large, then

shrunk very little; when A. is small, then it is shrunk a lot.

when S = 0, y may be viewed as a type

nique using

of selection of variables tech-

the principle components as the variables and the size of the

eigenvalues as the selection criterion.
4. The Choice of k
There are two types of choices of k which we shall discuss here. The
first type is in the spirit of empirical Bayes methods because prior param-

eters are estimated from the data. The second type is based on estimates of
certain optimum values of k.

-.9 -

In

all cases 02 is treated as a known constant and set equal to its

estimated value 2 from (3-3). Furthermore, while the theoretical analysis
uses y, in the actual computations the centered values,

are used. This

introduces an error of order N1 into the analysis, but this is more than
overcome by the resulting simplification in the resulting formulas.

Empirical Bayes Choices of k

From (3-7) we have that the marginal distribution of y is given by
1

y' GauN(X*6*, 2 <w)

+ T2X*X*T)

(4-1)

From (4-1) it follows that

(wi) yiv GauN( (wi) X6, o21 + -r2 <w X*X*T(wb> )

(4-2)

and hence that

E[(y -

X)T<w)(y

-

X6)]

=

No2 + i2 trace(X*T(w) X*).

(4..3)

If we let

UT(w)U I

(44)

then (4-3) may be expressed as

E[LIY

-'l]

= N

02 + p T2

(45)

since trace(X*Tw)X*) = p
Therefore an unbiased estimate of T2 is given by

T2

(II'- '
:

-N

G2)/p .

(4-6)

Thus the ratio of 02 to t2 yields a plausible though biased estimate of
k =

02/-r2.

We call this

-10-

p O

k =
a

__

__

y

-

x fl2 - N

(4—7)

-.

Sciove [1973] suggests keeping this estimate of k positive by

replacing N by N-p. This yields

—

k1
a

p 2

(4-8)

IIt;7o 11W - (N_p)a2
Alternatively we might use a "positive part" estimator of the form

ka2 = max(0,

k)

(4-9)

to keep k from being negative.

Another set of empirical Bayes choices of k stem from the following

observation. If c2 is regarded as known, then

is a sufficient statistic

(marginally) for -r2 so' that we may reduce by sufficiency to the marginal
dist1'ribution of

=

(X*"<w) X*)_1X*T(w) yevGau(s*, 2[(x*T(w) X*)1

+ k11]). (4-10)

Equivalently, we may use the marginal distribution of
A

=

A

_ Gau(v*, a2(k +

x))

(4-11)

=((k1 + 1))(*

- *)

(4-12)

If we set

then

5.s'Gau(0, 2I) .

(4—13)

—11—

and we see that

*

p (*1 -

2=

1=1 k

1

+xT1

a2 [chi-square on p d.f.]

(4-14)

Dempster (Wermuth [1972]) suggests settingli *fJ 2 equal to its expected

value and estimating a2 by G2. This yields the following equation for k
(y* 1
i

1

—

2

4-15

k +X
.1

We shall call the solution to (4-15) (if it exists) k. Sclove [1973]
suggests a method that is equivalent to the following observation.

II Qs112 and (N-p) a2 are independent with 2 x and

a2x,_

distributions

respectively. Thus set
2

=1 L9

F

P, N—p

•

(4-16)

(N-p) a2

Sciove then suggest setting F
equal to its expected value, i.e.,
p, N— p

= NNP2

E(F

(if N-p > 3)

This yields the following equation for k

(y_v*)2
2
i

=

2

N

P

k +)I

We shall call the solution to (4-17) (if it exists) k.
If we regard a2 = a2 as known, then the likelihood function for k based
on

is

r

L(y,

k) =

(k1 +

(2ir)P/2 a
1=1

A)exPt

.L

(4-18)
1=1

-12-

Differentiating (4-18) in k yields the following equation for k

(y'-v)2
1
1

1

=

I (k + A)2

(4-19)

I k +

We shall call the solution to (4-19) (if it exists) k.
The expressions, k , k
al
a

a2 kd k$

k

,

,

,

,

and k ,
m

are

all of the empirical

Bayes choices for k that we will consider, except for some minor modifications we shall make later in this section.

Estimating Optimal k-values

Consider the expected squared distance between

and the true param-

eter value * (where expectations are now computed relative to the condition
distribution of y given * and 2) There is a value of k that minimizes
this squared distance, but it depends on the unknown parameters. If we use
the least squares estimates of

in the resulting formula for the optimal

k-value we obtain a data-dependent choice of k that estimates this best

choice of k. This is the spirit in which we present the next two choices
of k. There are actually two meaningful "distances" in this problem and

we consider them in sequence. The first is the simple Euclidean distance
given by
=

Efl -

= EII-

-

=

E

E[(). -

E[(y).

-

(4-20)

But

Ef(y).

= E[d.((y). =d

=

E[v'

+ d.((y*).

*) (*
-

-

v)

-

E((y). *)2 + (l-d.)2(y' - v*)2

-

-13-

where d. =
1

A./(A.
1
1

+ k)

However, because t3ru

Gau(*,

02(x*T(w) X*y1)

we have

Gau(y, 02 (x) _1)

(4-21)

,

and hence

E[(y). -

y*]2

d 2 xT1

+ (l-d1)2(y -

v*)2

Therefore, we have

- *2

EI!

2 d

=

+ (l-d.)2(y' -

v)2.

(4-22)

If we differentiate (4—23) in k to find that value which minimizes the
to

expected squared (Euclidean) distance of

we obtain the following

equation for k (after substituting y for y* and Q2 for o2)

kA.(y 1

1

i

v)2
1

=

A.
1

02

(4—23)

i (x. +

(A. + k)3

We shall denote the solution to (4-23) by

kob.

The other notion of choseness that is relevant to this problem is the
expected squared weighted distance from

R XR toX. We now examine

the result of minimizing this quantity. We have

- X

EIIyR

=

E[(R

-

= E[( - *)TX*T(w x*(
=

=

E(y
E

y*)T<X)(y*

A. E[(). -

- *)

)TT (w)
- *)]

R (4-24)

-14-

Hence we may write

EJ!YR -

where d1 =

A.I(A.

[d

=

XIJ2

02 + (l-d.)2A.(y' -

v*)2]

(4-25)

+ k). Minimizing in k produces the following equation

analogous to (4-23)

kX(y' - v*)2
1
1
1

=

—

i

02 Z

(4-26)

(. + k)

(A. + k)3

We shall denote the solution to (4—26) by

Further

2
1

k0.

-

Study of These Choices of k

We now have eight possible methods for choosing k in ridge regression.
In order to thin down the candidates we begin by considering what they look

like in the important special case when the x's are orthogonal. By this we
mean that A.

1 which implies that

)

x=I.

x*T(w)

(4-27)

When this happens, all of the equations for determining k have easy solutions.
They are given by:

1

-

po2

-1

1

i+kml+kd

II*_*JI2

,N-p
l+k -l _______'N-p-2

—
(428)

429

LS

_1
1+

1

Turning now to k a

p2

=1k0

(4-30)

6*112 + p2

we see that

1)
1+ka

=1—
y - X612

-

A

(N—p)a2

—15—

"i li

But the following identity holds true

2=
y — xoJ' w
i

IV'

F'
•
—

+ ii y
LS

1j2

A

.iSJ1w

2

—

jj

A

= (N-p)a2 + (
LS

(4—31)

flw

- 6)TxT)

X(Ls -

So that we have

+I* - 6*112
J(2 = (N-p)o2 IjLS

(4-32)

1w

and

hence we may express k

a

1
____
l+k = 1-

as

______________________________

— 6*112

II

Similarly we

(4-33)

A

a

have
A
1

1+k

=1 -

(4-34)

______________________________________________

A

- 6*112 ÷ PG2

II LS
and

F.

po2

_______ = 1 — max(l,
1

1 +k

.)

(4-35)

•

lI LS
- 6*112
U

Now

in this case (i.e., Ai

1) we have

A

LS

" Gaup (*, o21)

with 02 an independent, chi-square distributed estimate of the common
variance

2

James and Stein [1961] showed that in this type of situation

A

can

be uniformly improved upon (in the sense of lowering the value of EII* - 3*1L2)
by an estimator of the form

*
A

JS

1

l+k

F'

Ls

- 6*)

(4-36)

—16-

where

is given by

— —cj 0 2

l+kJS

=

I

providing

6%

(

1

*LS _* 2

that p exceeds 2. Further slight improvements can be achieved if

is replaced by

2 and if (l+ky1 is prevented from going negative

N-p+2

by a device like that used in (4-35). However, the bulk of the improvement
stems from the use of the factor (4-37). Comparing the corresponding values
for our proposed choices of k we see that k, kd. and k agree with
except for a Upil replacing the correct 'tp-211. The extra factor in

appears to go in the wrong direction. kob,

k0

all

and k1

k

agree on a

shrinking factor that is too small in general. If we use the value of
to calibrate the performance o1 our choices of k in the orthogonal case,
then we are motivated to alter the definitions of k

a,

when A.

they agree with

will drop k

Ob ,

k

Oy

,

k

al

k

d

and k so that
m

1. Because they fail to agree with

we

and k from further discussion.

,

S

It is easy to change the definition of k so that it agrees with

k5

in the orthogonal case. We shall use

k'a

=

-2

A

a2
(4—38)

-.

y

-

Xó112 - (N-2)

02

It is also obvious how to change (4—15) so that kd agrees with

in

in the orthogonal case. We propose the following simple modification.
Instead of (4—15) use

z

(y' 1

v)2
1

A

= (p—2)

(k1 + AT1)
We shall call the solution to (4—39)

k.

02

(439)

—17—

It is less obvious how to change (4-19) so that km agrees with

in

the orthogonal case. We suggest the following slight change in (4-19),
others might be better.
(.y* -

1

v)2
1

= (p-2)

1

G2 i

(4-40)

k +

' (k1 + Al)2
We shall call the solution to (4-40) k'.

We have now reduced our eight choices of k to 3, k', k' and k'. It
m
a
d
should be understood that none of this applies when p < 2 and that k is never

allowed to be negative for any of these choices. Equations (4-39) and (4-40)
may easily be solved (if solutions exist) by Newton's method starting at
k =

0.

When w.

1 and S =

0,

it is easy to show that (4-39) has a unique

solution if and only if the usual R2 exceeds (p—2)/N. Conditions for the
existence of solutions to (4-40) are similar in spirit bu1 more complicated.

In order to distinguish further between k', k and k' we need compari-

sons of their respective performances. k' is appealing since it does not
require as much work to compute as the other two do.

5.

Combining Ridge and Robust Regression Methods

Ridge regression was invented to deal with the problem of near multi-

collinearity in regression. Another problem that besets the user of
regression methods is outliers and other forms of non-Gaussian errors.
Robust regression methods have been proposed to deal with such problems
(see Huber [1972], Bickel [1973], Andrews [1973] and Tukey [1973] for reviews

of methods and discussions of current research). Considerable attention

-18-

has been given to various versions of Huber's M-estimators. Tukey proposes

iteratively reweighted least squares as a device for computing

using

M-estimators and other related robust estimators. The weights are computed

sequentially from the residuals of the previous iteration. The end product
of Tukey's method is a set of weights w.} such

that the robust estimator

is computed by

= (x (w)
In

view

of

x)_1xT(w) y

(5-1)

the analysis and development given in the previous sections

we propose here to use weighted ridge regression to combine ridge and robust

methods. The specific proposal is to take the weights found for the robust
method and do weighted ridge regression using formula (2-9) and (2-1). The
choice

of k is still problematic but two alternatives present themselves.

(a) Use several k's in the diagnostic mode proposed by Hoerl and

Kennard. This will help identify unstable parameter estimates.

(b) Use k', k or k' computed from the data to obtain point
estimates

of

of . Further work

is necessary to see if these choices

k differ substantially.

The expectation is that a ridgified robust estimator will combine the
benefits of both approaches and be no more difficult to compute than

R

or

separately.

)

-19-.

REFERENCES

Andrews, 0. [1973]. "Some Monte Carlo Results on Robust/Resistant Regression,"
(unpublished manuscript).
Baranchik, A. [1970].
"A family of minimax estimators of the mean of a
multivariate normal distribution," Annals of Math Statist 41, 642—645.

Bickel, P. [1973]. "On some analogues to linear combinations of order
statistics in the linear model ," Annals of Statist 1. 597—616.

Hoerl, A., and R. Kennard [1970]. "Ridge regression. Biased estimation
for nonorthogonal problems," Technometrics 12, 55-68.
Huber, P. [1972]. "Robust statistics: a review," Annals of Math Statist 43,
1041-1067.

James, W., and C. Stein [1961]. "Estimation with Quadratic loss," Proceedings
of the Fourth Berkeley Symposium 1, U. of Calif. Press, 361—379.
Marquardt, D. [1970]. "Generalized inverses, ridge regression, biased linear
estimation, and nonlinear estimation, Technometrics 12, 591-611.
Mayer, L., and T. Wilike. "On biased estimation in linear models,"
Technomety-ics 15, 497-508.

Sciove, S. [1968]. "Improved estimators for coefficients in linear regression,"
JASA 63, 596-606.
Sciove, S. [1973]. "Least squares with random regression coefficient,"
Technical Report 87, Economic series, Stanford University.
Tukey, J. [1973]. "A way forward for robust regression," (unpublished m.s.).
Wermuth, N. [1972]. An Empirical Comparison of Regression Methods.
Unpublished doctoral dissertation, Harvard Univeristy, Department of Statistics.

