NBER WORKLNG PAPER SERIES

GENERAL ALORITHN FOR SIMULTANEOUS
ESTIMATION OF CONSTANT AND RANIX)MLY-VARYING

PARAMETEBS IN LINEAR RELATIONS

Alexander H. SarTis

Working Paper No. 38

CO1FUTER RESEARCH CEN'IER FOR ECONOMECS AND MANAGEMENT SCIENCE

Nà±iotial Bureau of Economic Research, Inc.
575 Technology Square
Caithridge, Massachusetts 02139

-April 1974

Preliminary: not for quotation
NBER wordng papers are distributed informally and in limited
ni.uthers for comments only. They should not be quoted without
written permission.
This report has not undergone the review accorded official NBER

publications; in particular, it has not yet been submitted for
approval by the Board of Directors.

NBER Computer Research Center and Massachusetts Institute of

Technology. Research supported in part by National Science

Foundation Grant GJ-l154X3 to the National Bureau of Economic

Research, Inc.

Abstract
A

/

recursive algorithm for estluating linear models with both constant

and time-varying parameters is derived by rraxiization of a likelihood

function. Recursive formulas are also derived for derivatives of the
likelihood function; the derivatives are needed for numerical evaluation
of some parameters. Smoothing formulas are also derived. The esti—

rnation algorithm is compared with others for similar classes of models.

Contents

1. Introduction

1

2. Problem

Statement

2

3. Maximum Likelihood Estijiation

5

L•

Smoothed

Estimates of

17

5. Conclusion

19

References

20

1. INTRODUCFION

In recent years many economists and statisticians have observed that

the traditional linear model with constant coefficients is inadequate for
the description of several phenomena.

Individual effects across a

population or systematically varying behaviour over time offer two examples.

The literature on stochastic parameters has provided several arguments

justifying the nonconstancy of parameters over different observations,
the reader is ref erred to Rosenberg (1968) and Cooley (1971) among others.

A survey of the existing literature can be found in Rosenberg (197 3a).
The state of the art in estimation techniques for several stochastic
parameter

models has been sununarized in Annals of Economic and Social

Measurements, Vol. 2, No.

(October 1973).

This paper. presents an algorithm for estimating stochastic variations
in a model that is more general than the most general model that has been
analyzed before, that of Rosenberg (197 3b).

In Section 2 the problem that is solved is stated, in Section 3 the
recursive formulas for the maximum likelihood estimators are computed,
Section - L restates some well known results about smoothed estimates, and

Section 5 gives a brief summary of the results.

—2—

2.

PROBLEM STATEMENT

The model

t

that will be analyzed is the following
+

At

(1)

Btt+st

t= 1,2,..., T
where

y - 2 x 1 vector of observations at time t
a- rx

1 vector of constant parameters

1 vector of randomly varying parameters
At,Bt

-

- (2.,xr) and (Qxk) matrices of observations of explanatory
variables at time t.
x 1 gaussian vector of random terms with the following

properties

0 t=1,2,h, T
E(c1!)=
where Q

Q

(2)

ci 1= 1,2,...,T

is a symmetric 2,x9. positive definite matrix,2o is a scale

factor, and ii.. is the Kronecker delta.
The random parameters will be assumed to obey a transition relation
of the Markov kind
=

t—1 +

t-l

where u are normal with

E(ut)

(4)
0 t=l ,

2,...,

The apostrophe (') denotes transposition.

T

(5)

—3—

E(u.u!) =

1]

where

ci2R..
1]

i,j=l,2,... ,T

(6)

R is a syrrmetric kxk positive definite matrix. The initial vector

will be assumed constant but unknown.

A rrodel like this can arise for exaule in the analysis of a crosssection of tine series. Ros:enberg (1973b) analyzed a model like this

with the exception that he did not consider the constant term c. The
recursive formulas that he presented do not hold when such a vector of

constant parameters coexists with the vector of stochastic ones. A
particular case of model (1) with 2..=1, kl and Bt1 was analyzed by Cooley
(1971) and Cooley and Prescott (1973). However, their procedures are not
recursive and can quickly become uimanageable for large sample sizes.

Sarris (1973) has analyzed a particular case with At0 £.=l but his
procedure is also not recursive.
The particular case of AO has also been analyzed extensively in
the engineering literature under the name of iKalman filters (see e.g.
Sage and Melsa (1971)). The difference between those models and the

special case of the one considered here (with Ao) is that in our case

the initial vector

is constant but unknown, instead of having a well

defined prior density. The case of constant l gives rise to what is
called the starting problem which proved quite troublesome until
Rosenberg (1973b) solved it correctly.

We shall assume that the transition matrix 4
of constant and unknown parameters

depends

on a vector,

(whose location in is known).

—4—

Furthermore

Q and

and

meters

0r

R will be assumed to contain vectors of unknown

locations

respectively (with known

in Q arid R)

para-

We shall

define

0 (e
It

o;

,

(7)

op

is important to emphasize the assumptions that Q

and

R are both

positive definite. The most general model one would want to analyze in
this framework would be the following

yt
Yt=
where

+

+

4i

It-i

(8)

ii_

(9)

all the previous assumptions on the

random

terms hold except that

E(p..) =
with

R positive sernidefinite. It is clear that (1) is a special case

of this model. If

is given a proper prior density then the results

of

the Kalman filter

is

constant and unknown, a case

and econometerics,

literature

cover

which

this problem.

However, when

is most prevalent in

the Kalnian algorithm does not hold and

statistics

Posenberg' s

(l973b) algorithm holds only if R is positive definite. The general

problem

of arbitrary positive semidefinite R has not

The

Problem.

as yet been solved.

complete problem is now restated.

Estimate c,$1,ci 0 and $2

t=l,2, :,T.

$,... 'T given the data yt,At,Bt

—5--

where

At+tt+ct
t-l +

(10)

t-1

(11)

3. MAXIMUM LIKELIHOOD ESTIMATION
In this section we shall derive a recursive formula for the likeli-

hood

Let us make the following definitions

function.

p

=

{y±

(12)

{ct,i, a2, e}

(13)

A=
i - {Ai' AL+i'"'A}
j
B

L(p;,4,B)

{B1,B.1,.. . ,B.}

a

Then the

(l'4)

likelihood of given the data
=

is

, B,p)

i

(16)

where

p(y11y10, A, B, p) = p (y1A1,B1,p)

t t

t.-l

3.1 Computation of p(y y1

,

A1, B1,

(17)

p)

In this subsection we shall compute one of the factors of (16).

In the process we shall also derive the Kalman filter for

—6—

t t

t—1

A1, B1,i) =

t t t
p(yI,yt-l,A1,B1,B1,p)

t t t

t-1 t t

y1 ,A1,B1,p) p( y1,A1,B1,i)

t-l t t

(18)

pt Y1 ,A1,B1,i)

Let us

assume

and

t-1 t t

that the

density p( y1 ,A1,B1,p) is normal with mean

-•

covariance matrix

From (10) we obtain

e

(ytt_Btt)Q
(19)

(yt_At _B)}

We thus obtain that the left hand side of (18) is equal to

e-

1

k+

k+2

(2n)—-- c

Q

+

;5

MtIti

(t_t1t_i)Mt_i

By

rearrangement of the

we

obtain the following equality.

ttYt
(Q +

tt

[ (y -A -B )Q
2c

terms

i,nside

(20)

the brackets of the.exponent in (20)

Q'(yt ta_Bt)

+

tt

+

BtMttiB)1

(yt_Ata_Btt

tt i-i

Mtti

i-i

(21)

where

MI4M;t1

+

Bt

BtMtIti

Two

bars

denote

determinant.

[Q+BtMtiiBJ'
(22)

-.7-.

Mtt[Mti tIt-l +

tIti
The

+

t
(23)

i B (Q+BtMtJtiB)'(yt_Ata_BtIti)

formulas (22) - (23) are with

minor variations the ones known

as

the Ka:Lmmn filter in the engineering literature.

It is now easy to see fnm (18) and (21) that
normal with mean

is

and covariance matrix a2Mt!t given in (22) and

(23) respectively. Consequently p(ytIy',A,B,p) is also normal with
mean

and

Aa ÷ BttI

covariance matrix equal to ci

2

(Q+BtMtI

along with the eth'apolation formulas

and

t÷ltt

tJt

Mt÷ilt

Mtit+R

t-l

(2k)

(26)

the initial conditions
(26)
0

3.2

(27)

Computation of the. Likelihood Function

After the results of Subsection 3.1 we can apply (16) to obtain
an expression for the likelihood function. We obtain
L (p;

llp(yJy_l,4,Bt,p)
1

t1 (2n) 2JQ+BtMtltiBj

1

exp {--—.-

(y_A _Btti1)

—8--

(Q+BtNtitiBP' (ytAta_t tIt-1

e -

T

(2)TQ/2T 1tMt ItiB!

_1(Yt_A_BttitiY
2a
(28).

(Q+BtMt!tlB)' (yt_Ata_Bttit_i)}

For maximum likelihood

must

3.3

be maximized

estimation

of p, this quantity or its logarithm

with respect to c,,cr2 and 8.

Estimation of Cr,1,

cY

In this section we present recursive estimators of a and and an
estimator for 02. It can be easily seen from (22) — (27), that M.
,a or cr, and that I3
and a. Let us thus denote 1

does not depend on

sJk

0

slk

sjk

a+

depends linearly on both

(29).

sik

We can irrirnediately see from (26) that p1 o 0

,

0

(30)

0 E

(Note that the 0's. and E's are kxr and kxk matrices respectively).
Using the formulas

p, 0,

and .

(23)

They

and

are the

(2)

we can

obtain

recursive relations for

following.

(31)

't+1lt
®

t+lIt tt

(32)

(33)

—9—

It-1 +Mtlt-1 B

=

tIt
o

—

o

—

These

(Q+BtMtlt_i

(yt_Btptit_i)

MtiiB(Q+BtMtit_1B)' (At+Bto tIt-l
(36).

Mt t_i (Q+BM11Bp BtE tlt-i

formulas are identical to those of Rosenberg (1973b) with the

addition of (32) and (35).

We

rewrite the exponent of the likelihood function in
T

1t_Ata_Btt i-i

(Q+BtMti

(28) as follows

_1c (yt_Ata_Yt it-i

T

iti

—

Bt 0

tt-l a _BtEtItii) (Q+BtMtItiBY'

(yt__BtPtlt_i_Bt 0 tlt-l a_Bttitii)
T

tlt_BtPtIt_1 (Q+BM11Bp yt_BttIt_i
a'FTa

+

i

HTl + 2a'

—

2

+

a T 2i r

where

T

F =
T

E

(At+B i-i' (Q+BtM±iiB

)_l ( A+B

0

(38)

tlt—l)

T

t=l

k-l

(Q+BtMtiiB) BtEtIti

(39)

T
E

t=l (At+Bt 0 tit-i (Q+BtMtIt_i BPBtEt,t_i

(Lo).

—10—

fTF

e

E(A+B

(Q+BtMtiisY' (yt_BtPtit_i) (41).

(42).

b itQ+BtMtIt_lBP' (ytBtt i-i
of the likelihood function with respect

Maximization

to a and

is equivalent to minimization of the quadratic form in (37). We take

the derivatives of that expression and set them equal to zero.
obtain

We

the following two equations:
FTa

+

(43)

=

- hT

= 0

(44).

or

[T

FTGT

(45).

I

I

therefore

a

FT

=
•

C

(46).
bTj

or

(FT_'CY1

(-

(47)

' (- F1

(48).

only thing that needs to be examined is the inveribi1ity of
the matrix that prenultiplies the parameters in (45). We now state and
prove a theorem that will guarantee it. Before we do this we define the
The

following two matrices

—11-

A1

A2
(50)

Ar

B1
B24

X2E

(51)

B342

T-1

BT

the theorem can now be stated.
Theorem 1 .

A sufficient condition for the invertibility of the matrix

w rFT G]

defined in. ('45)

is that

the matrix

I

X

has

full

colunu-i rank.

Proof. Let us assinne that the matrix X [x : x2] has full, column rank.
Then it will be true that the matrix X ' X is positive definite and hence
invertible, arid also that the matrix X'i)X, where i is positive definite,
is also positive definite and hence invertible.
We now consider the original data and substitute

as follows

-12-

tt-1 t-l =
tl
-

+

+

t-2

+

Utl

1.

u.

(52)

jzl

We

then

obtain

A1 ;+B21+c1

y1

A2 a±B21+B+E2

;+

=

or

+ BT

(T_l_l_) +

more' compactly.

y =

Where

y

is

X1;+X21

+

(53).

v

the vector of left hand side

observations

vector of obviously non-spherical residuals. Let

matrix of

the

us denote the covariance

that if the matrix X [x1:
x2]
linear estimates of a and l are

v by V. Then it is well known

is of full column rank, the best
obtained by Aitkents

estimator

(x'v

The same

arid v is

exact

which in this case is

1—1
—1
X)
x'v y

expression for a

lihood function of

y from (53)

(5'i.).

is

obtained if we write

the

like-

and maximize it with respect to; and

-13-

However, this is exactly how we arrived at (5) only via a recursive

Therefore, the normal equations Trust give

formula.

the same

solution

in both cases. We conclude that the matrix W must be equal to the
'—1
product of a nonsingular matrix and X I_i
V X. Hence, since X V X is

invertible we are done.

JI.

The maximum likelihood estimator of a2 is easy to find after the

expressions for a and

are substituted in (28). By differentiating
the likelihood function and setting the derivative equal to zero we obtain:
T

-

a

-_

A

1(yt_Ata

-

Btlltjti_Bt

t

—

(j1h)

0!_ a.BtEtIt j

It_i_Bt e tlt-l

(Q+BtMt jt_iBt)
=

Bp tIt-l'

IFTGT

T11

-l

a_BtE t-

(Q+BtMtItiB)' (YtBttlt_l)] -

iJ
I

I

I'

A

(55)

3.'i. Estimation of 8.:

a2 all depend on the
elements of the vector 0. The log-likelihood function for 0 is found after
substituting out the estimated values of a2 ,cx, andy
It is given by
The estimators of

Mt It-i' a,1, and

the following expression.
log

L

(8; y, 4,

(Q+BtMtitiB)'

B)

- T log

_lL_

tTtIt-l

-

(FT_G)

fT-hi

(_FT)T_

2f (FT_GT G) GTI
log

The

above expression is

IQ+Bt Mtlt

B

(log T + log

+

2+l) (56)

analytically with respect
to 0. Numerical maximization of it will require the derivatives of this
impossible to maximize

expression with respect to the elements of 0. We now proceed to derive

recursive formulas for these derivatives. The procedure will
consider each term of (56) separately and obtain

respect to each component of 0. We
of 0 into un1aown elements of ,

will bear
Q

be to

its derivatives with

in mind the decomposition

and R respectively given in (7).

The quantities whose derivatives will be needed are the following.
+

tt-l'
Pr'om

BtMtiti,

(38)—(42) it can be

Bt, Q+BtMtItThB!, Ft,,GT,fT,hT

seen that

the derivatives of FT,HT,GT,fT

and hT will be determined by the derivatives of H
+

Bt

EtIt_i,

Mt,ti B

So the quantities whose derivatives must be computed are
H

Etiti, Q+BtMtti B and IQ+BtMtIt_iBI

for all 1< t< T
Refering.to
found.

formulas (31) and (3k) the derivative of

ttl

is easily

—15—

tIt

::

+

:::+hlt

+

::tlt_1
ij

MtIti

B (Q+BtMtitiB)' (yt_BtPt it—i

i.j

(Q+BMtit 1BY' Bt

_Mtlti

(57)

Bt
::

(yt_Bttit_i) MtItiB (Q+BM

-

t 1

The matrices

can

(Q+BiiB)'

!tlB' Bt tlt-1 (58)

be computed recursively by differentiating

't1J

(25) and (22).

t÷ilt
30

=

tt

(59)

÷
::tlt

::tlt_1

:tlt_1

÷ Bt

MtltiB (Q+BtMtIt_1BP1 (:q.
1]

(Q+BMiiB)'

B)

—16-

(Yt.Bt1Jtiti)
Mt Iti

-

____
MtitiB (Q+BMi1BY Bttt_l

(60).

is again generated recursively by differentiating
(25) and

r. •
3_I

::t

(22).

A____
Lt

(61).

r..
11

'tt- +

Mt!t_l

•.MIB(Q+BM1B) Bt
-

MtIti

Bt (Q+BtMtt_i BY'(yt_Btpti) —

tlt—1 B(Q+BtMt jt_1BP'
Or..
11

1

MttiB(Q+BtMtItiBP

is recursively

1

Bt

computed by

(25) and

(62).

(22).

The initial conditions for (57)-(62) are
•11

all zero.
From (32), (33), (35) and (36) it can be seen that the derivatives of

°tIti, k- Q+BtMtItiB depend on the derivatives of ft—i and
which we just analyzed.

M
So

the last thing needed to complete the

analysis is

the computation of

the derivatives of the determinant quantity. Let X denote a mm natrix with

rows , x,..., x. Then the following formula can be proved easily by the
definition

of the determinant function (n is a scalar)

—17—

JxH
—

xl

x1

xl

2

(63)

•2
X2

:
:

+

xn
If we denote

the rois of Bt

n
____

+...+

xn
by

1]

b 1=1,. . P and the rows
.

of

Mt jt_i by

j=1,..., k, then the 1th element of BtMtItiB is given by

B M

B} ..
tt!t-lt 1]

E

n1

b. m b

(6'4).

n j

It can thus be seen that the derivative of the determinant depends upon the
derivatives of

These

Mt Iti

which

we analyzed.

recursive formulas

for

the derivatives are

lengthier than the

derivatives. The advantage,

computation

required to compute the numerical

however, is

that we obtain the exact derivatives as opposed to numerical

approximations. The exact tradeoffs must be evaluated through numerical

experiments which will be deferred to a later paper.
SM99JD

The procedure desàribed in the previous section led to rriaximum

likelihod estimates of all the unknown constants i.e.

and 0.

We also obtained the iraxijium likelihood estima.tor of T namely

tIt(1;;

;2

=

(0) +

T!T(0) c +

(66)

-18-

For the solution to be complete we need estimators for

1<t<T

that are

based on all the data from 1 to T and not only up to t as we have obtained.

We thus need "smoothed" estimates of

e. the quantities

as opposed

to "filtered" estimates
What we would like to obtain ideally would be the density

p(2 ,,•.
available

'TIY

). As we formulated the problem we have

T T
2
p(2jy1, 1,a,a ,O)

TT

2

O)
P(2,y 1Ift1 ,a,a
:.;
.

T

2

(66).

,O)

p(y1

For the true values of the unknown conditioning parameters the posterior

density if normal.

We ta]ce the empirical Bayes view and estimate

as

the posterior mode of (66) conditioned upon the estimated values of
2

i,c,Ci and

0.

Sarris (1973) has shown formulas for the posterior mean and covariance

but those formulas are not recursive and

matrix of

practical value.
(1965)

and

limited

Recursive fprmulas have been computed by Pauch et. al.

are repeated

tIT

hence of

here

tJt+MtJt

for completeness.

Mtit t)_1 t+1ITtIt

+

(÷1lTMt+l!t)

MtIT Mt!t

(R+M'Y'

(67)

$

(68)

The procedure stares at time T with TlT and MTIT computed by the
'ormulas

of Section 3.

Another method that could be used would be to consider the density

T 1a2
and maximize it with respect to 1,u,a2,0 as well as

.

The conditional

estimates of 13' conditioned on (3i,ct,G2, 0 would be the same as obtained

above. However, the estimates of a,i,a2,O would not be the same.
method has been used by Bar-Shalom (1973) and
clear at this

Masiello

point how these two different methods

This

(1972). It is not

of estiiiating the

unknown parameters compare.
5. CONCLUSIONS

The contribution of this paper is two fold. First it presents a
recursive solution to the geneml problem of estimating a cothination of
constant and time varying parameters. Then it presents recursive fonnulas
for the derivatives that are undoubtedly required to numerically maximize

the likelihood function with respect to the parameters that appear
nonlinearly.
It is hoped that this type of approach will help the rrore general
problem of estirrating corrinations of time varying and constant parameters
which obey some non-random restrictions.

—20—

REFERENCES

(1) Bar-Shalom, Y., "Optimal Simultaneous State Estimation and Parameter Identification in Linear Discrete-Time Systems,"
IEEE Transactions on Automatic control, Vol. AC-17, No. 3,
June 1973.
(2)

Cooley, T. F.., "Estimation in the Presence of Sequential Parameter
Variation," Unpublished Ph.D. Dissertation, Department of
Economics, University of Pennsylvania, 1971.

(3) Cooley, T.F. and E.C. Prescott, "The Adaptive Regression Model,"
International Economic Review, 114, June 1973.
(4)

Masiello, R.D., "Adaptive 'bdel1ing and Control of Electric Power
Systems," Ph.D. Thesis, Deparbnent of Electrical Engineering,
Massachusetts Institute of Technology, October 1972.

(5) Pauch, H.E., F. Tung, and C.T. Striebel, "Maximum Likelihood Estimates
of Linear Dynamic Systems , Journal of the American Institute

of Aeronautics

and Astronautics, 3, August

1965.

(6)

Rosenberg, B., "Varying-Parameter Estimation," Unpublished Ph.D.
Dissertation, Departmnent of Economics, Harvard University, 1968.

(7)

Rosenberg, B., "A Survey of Stochastic Parameter Regression,"
Annals of Economic and Social Measurement, Vol. 2., NO.4,
October 1973.

(8) Rosenberg, B., "The Analysis of a Cross Section of Time Series by
Stochastically Convergent Parameter Regression," Annals
Economic and Social Measurement, Vol. 2, No.4, October 1973.

of

(9) Sage, A.P. and

J.L.

Communications
(10)

Melsa, Estimation Theory with Applications to
McGraw-Hill,197l.

and Controls New York:

SarTis, A.H., "A Bayesian Approach to Estimation of Time Varying
Regression Coefficients," Annals of Economic and Social
Measurement, Vol. 2, No.4, October 1973.

—

