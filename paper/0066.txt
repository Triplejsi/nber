NBER WORKING PAPER SERIES

DETECTING AND ASSESSING THE PROBLEMS

CAUS BY I'iJLTICOLLINEARfl?:
A USE OF THE SINGULAR-VALUE DECOMPOSITION

D3vid A. Belsley*
Virginia C. 1<lema**

Working Paper No. 66

COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE

National Bureau of Economic Research, Inc.
575 Technology Square
Cambridge, Massachusetts 02139
December

l97L

Preliminary: not for quotation

NBER working papers are distributed infonnally and in limited
niiibers for comments only. They should not be quoted without
written perTIission.

This report has not undergone the review accorded official NBER
publications; in particular, it has not yet been submitted for
approval by the Board of Directors.

*BER Catputer Research Center and Boston College. Research
supported in part by National Science Foundation Grant
to the National Bureau of Economic Research, Inc.

**NBER

GJ-115'4X3

Canputer Research Center. Research supported in part by

National Science Foundation Grant GJ-115'4X3 to the National Bureau
of Economic Research, Inc.

Abstract

This paper presents a means for detecting the presence of nulticollinearity
and for

assessing the damage that such collinearity may cause estimated

coefficients in the standard linear regression model. The means of analysis
is

the singular value decomposition, a nuTkerical analytic device that

directly Exposes th the conditioning of the data matrix X
dependencies that

may

and the

linear

exist among its coluTins. The same infonTation is

employed in the second part

of

the paper to detenriine the extent to .thich

each regression coefficient is being adversely affected by each linear
re3Ation among the colimins of X that lead to its ill conditioning.

Acknowledgments

The authors wish to express their gratitude to Professor Gene Golub of
Stanford University, Professor John Dennis of Cornell, and

Edwin Kuh

the

NBER for many helpful discussions. Moreover, the first author

wishes

to express his gratitude to the Center for Advanced Studies in

of

the Behavorial Sciences at Stanford for the opportunity to initiate his
research

in

this area during his fellowship there.

.

Contents

..1

INTRODUCTION

PART 1. •rI SIULAR-VAllJE DECOMPOSITION AND THE DETECTION OF
LINEAR DEPENDENCIES

....2

1.1 The Singular-Value Decomposition

1.2 The Determination of the Linear Dependencies of X .
1.3

Determination of p (X)

r

1.4 Determining the Structure in the
1.4.1 Defin:isig the Structure
1.4.2

....5
Linear Dependencies of X

Determining the Zeros of G

Appendix to Section 1. Scaling

PART 2. AN ASSESSMENT

•

.

12

•

.

12

•

.

13

•

.

19

OF THE DAMAGE CAUSED BY LINEAR
22

DEPENDENCIES

2.1 The Basic Decomposition of the Variance of lDb

2.2 An Interpretive Consideration: Orthogonality and the Zero
Structure of V
2.2.1 The Zero Structure of V When X has Orthogonal Parts
2.2.2 Near Collinearity Nullified by Near Orthogonality

23

26
27
32
33

2.2.3 An Example

Data

36

2.3.1 At Least Tc.x Variates Must Be Involved

36

2.3.2 Variance Proportions: Necessary but not Sufficient

39

2.3.3 A Suggested Test for Harmful Coilinearity

41

2.3 Assessing the Damage Caused by Collinear

2.3.4

Multicollinearity as a Practical

Problem

PART 3. SOME GENERAL CONSIDERATIONS ON MULTICOLLINEARI'I? AND ITS
CORRECTIONS

3.1 Other Tests for Multicollinearity

42

44
1414

3.1.2 The Determinant

of

3.1.3 Method of Far'rar

.

X'X .

3.1.1 Sir1e Correlations

and Glauber

3.2 Corrective Measures

44
[1.5

45
46

3.2.1 The Introduction of Identifying Infoiiition

46

3.2.2 The Failure of Ridge

47

REFERENCEs

Addenda to bibliography, p. 49
Becker, R., Kaden, N., arid KLema, V., [1974], "The Singular
in Matrix Computation", NBER Working Paper 46.

Value Analysis

G. H., [1969], "Matrix Decomposition and Statistical Calculation",
in R. C. Milton and J. A. Nelder (eds.), Statistical Corxutation,
Academic Press 365-397.

Golub,

Golub, G. H., and Kahan, W., [1965], "Calculating the Singular Values and
Pseudo-Inverse of a Matrix", J. SIAM Numer. Anal., Ser. B. Vol. 2, No. 2,
205—224.
Golub, G. H., and Reinsch, C., [1971], "Singular Value Decomposition and
Least Squares Solutions," in J. H. Wilkinson and C. Reinsch (eds.),
Handbook for Automatic Computation, Volume II: Linear Algebra, Springer
Verlag, 134-151.
Hanson, R. and Lawson, C. L., [1969], "Extensions and Applications of the
Householder Algorithm for Solving Linear Least Squares Problems,"
Mathematics of Computation, vol. 23, no. 1080, 782-812.

Errata

page 6. line 13

line 16
page 7. line 3

triangulation +

triangularization

replace line 16 with from Golub and KaJan (1965)
and Wilkinson (1965) p. 195 illustrate this point.

posses + possesses

INODUCTION

There are three major questions related to the problem of mnJ.±icollinearity:
when does it exist? how much damage has it caused? and what, if anything, can

be done about it? Making use of a technique of numerical analysis, the singularva.lue decomposition, this paper suggests a means for answering the first two of

these questions that is devoid of the ad hoc quality of previous attempts.
Part 1 introduces the concept of the singular-value decomposition and applies it
to the determination of -the existence of linear dependencies among the columns

of any given data matrix X. An Appendix to Part 1 deals with the problems caused

by scalIng of the data matrix. Part 2 addresses the question of assessing the
damege caused by the presence of multicollinearity and applies the mderstanding

gained fran Part 1 toward an answer. Part 3 presents an assessment of several of
the techniques previously advanced in the literati.me fcr diagnosing collinearity

and, additionally, presents a fundamental critique against the use of nonBaysian "ridge regression" as a means of corTecting the problems caused by

collinear data. While some contrived examples are provided for illustration,

a true study of the application of these techniques to economic data will be

the subject of a future paper.

—2—

Part

1. The Singular-Value Decomposition and
The Detection of Linear Dependencies

1.1 The Singular-Value Decoirposition

We learn from the numerical analysts1 that any TcK matrix X, considered

here to be a matrix of T observations of K economic variates, nay be decomposed
as

X UEV'

(1.1)

'K and E is

where U 'U V 'V

diagonal

with non-negative diagonal elenEnts

crk,k_l.K.

.
1 See, for exanpie, Golub (1969), Golub and Reinsch (1970), Hanson and Lawson
2

(1969), and Becker
This decomposition

MINFIT

[Golub and

et al (1974).

is efficiently and stably effected by a piogrern
Reinsch (1970)].

called

In (1) U is DcK, E is KxK and V is KxK. Mteative fonTulations are also
possible and nay prove more suitable to other applications. Hence one may
'IcK

TxT Th.K ThK
Vt

(1

TxK

Txr' ra' ri.K

(1 ib)

x=u

or

have

V
there r p CX). In this latter fonnulation E is

always

la)

of full rank, even if

X is not.

.

—3—

The singular-value deccznposition is closely related to the familar concepts

of eigenvalues and eigenvectors, but its difference frau those concepts is inportant. The non-negative diagonal elements of E are called the singular values of
X, and these are also the non-negative square roots of the eigenvalues of X'X.
This

is readily

seen by noting

X'X

vEu'UV'

(1.2)

VE2V'.

Recalling the orthononility of V, we note that V diagonalizes X' X, and
hence the diagonal elements of E2 must be the eigenvalues of the real synmetric
imatr'ix

X'X.

Equally clear, the orthononnal colimins of V must be the eigenvectors of
X'X, and, as is similarly denonstra.ted, the columns of U must be the eigenvectors of XX'.
The singular -value decouosition does not, however, merely duplicate know-

ledge of the eigensystem of X'X, for the singular value decouosition applies
directly to the data matrix X, and not to the manent matrix X'X. The singular value deccnçosition thus leads to a means of detennining the linear dependencies,
if

any, among the colnris of the data matrix X.

1.2 The Detennination of the Linear Dependencies of X.
Ass.une that X is

orthononral,

and

rank deficient, i.e., p(X) r < K.

hence

There wi.ll, therefore,
nullity

Since U and

V are

necessarily of full rank, we must have p (X) = p (E).

be as meny

of X, and hence

we may

zero

elements along the diagonal of as the

partition the

singular-value decouosition in

—4—

(1.1) as
x
where

uEv'

u

11.o1

is r'w and nonsingular.

After postrrultip1ying (1.3)
x [V1 V2] =
where

(1.3)

V1 is Kxr
V2 is Kx(K-r)

Eu1

by

V and further partitioning we obtain

u2]h1

(1.4)

U1 is Thr'

U2 is Th(K-r).

(1.4) results in the to matrix equations
X V1

(1.5)

U1E11

and

x V2 = 0.

(1.6)

Interest centers on (1.6), for it displays all of the linear dependencies
of X: the Kx(K-r) matrix V2 provides an orthonorinal basis for the nufl space
that is spanned by the columns of X.

Two problems arise in applying the exact algebra leading to (1.6) to real

data. First, how does one determine the rank of X, r, i.e •,

are the zeros
of E discovered? And second, how are the zeros of V2 discovered? Both of these
how

problems arise because computers use finite arithmstic, and only in very special

cases will "true" zeros be calculated as such. There are problems of both rounding er:ror and error in the representation of the data

1. Also sorrtimes called truncation error. However, this tern also applies to
the error introduced by truncating an infinite series after a finite number
of steps, and hence will not be enployed here.

—6--

The importance of the first problem is obvious: only through
determination

of the zeros of E can

we

dericies exist anng the columns of X.

less

correctly assess how many
The

a correct

linear depen-

importance of the second problem is

obvious. But, in general, all elements of V2 will be calculated as non-

zeros, however small some may

be relative to others. Since scaling

of

X will

alter these non-zero elements arbitrarily (a problem that is dealt with in

length

many

in the

appendix to this section), we may

columi-is of X enter

each linear

dependency,

arrive

at the conclusion that

whether or not this is

true.

The economnetrician will rarely be satisfied with such an answer; he would like

to identify the zeros of V2 (or some manipulation of it) so that he can say
which variates do and which variates do not enter into a specific linear
relation. The next two sect ipns deal with these two problems in turn.

1.3 Determination of (X) r
The singular value decomposition presents a means for determining the

rank of the data matrix X. Referring to (1.1) and recalling that U and
V are orthogonal we see that has both the same norm and the same rank

as X. Since

is diagonal, were there no problems of calculation introduced

by the impr'ecis ion of the computer, one need only determine the number of

nonzero elements of E to discover the rank of X. Unfortunately the task

is not quite so sinple, for the nonexact, finite arithmetic necessarily
employed by computers and the problems of rounding error will result in

nonzero elements of E when, under ideal conditions, they should be zero.
it is necessary, therefore, to find a means for determining when an element
of E is "small enough" to be considered zero, and hence evidence of X' s

being rank deficient.

-6--

Proposed Alternatives. The singular value decomposition is useful
in this context of deteniiining rank because it preserves the norm of X

(i.e. column lengths). The singular values are in the same units as the
colins of X, and hence are measurably interpretable. Other suggested
means for detenTlining rank fail on this and other counts

The determinant of the matrix (if square - or X'X if not) clearly

faild, for a small determinant has little to do with the invertability
of a matrix. The matrix CIn has determinant c' which can be made arbitrarily

small, yet it is clear that aI has orthogonal colurrns and is always
invertable for

It is equally infeasible to obtain information on the invertability
(conditioning) of a matrix from the smallness of some of the diagonal elements

of a triangulation of the given matrix. This process is closely related
to the use of the determinant, since the determinant will be the product
of the diagonal elennts of the triangular factorization. Two exanples
from Golub and Reinsch (1970) and Wilkinson (1965) illustmte this point.
Consider
.501

—l

.502

0

—l

.599

—l

.60j

-7..-

and

Each of these matrices will be shown by the singular value decoirosition

to be quite ill-conditioned even though neither posses a snail diagonal

elennt.
The Condition Number. A nans of determining the conditioning of a

matrix that avoids the pitfalls nntioned above is afforded by the singular
value decomposition. The notivation behind this technique derives from a
nore correct nEthod of determining whether an inverse of a given matrix

shall see it is reasonable to consider a matrix to be

"blows up". As

ill-conditioned if its inverse is large in spectral norm' in corrarison
with the spectral norm of the given matrix itself. TO examples aid this

point. Consider first the matrix

A=I

I

[clj.

Clearly as cx -'-

1,

this

matrix tends toward perfectly singularity. Also

the singular values of A are easily shown to be li-ct, and those of A1 are

(l+cx).

Now

as a + 1,

the product I IAI

I

I IAH

=
I

urn (li-ct) (l_ci)Tl explodes,
(2+]

arid hence we conclude the norm of A is large relative

to that of A. A is

ill-conditioned for small cx.

The spectral norm of A (a..), denoted IIAH, is siirply
1]
maximum singular value.

.

, the

—8—

By

way of contrast, consider the matrix, introduced above,

BI[aol
I

There

is some feeling that B becomes ill—conditioned as cx + 0. However,

a and I BI I = a1, and the product I IBI I 1B11 I a = 1 is
B
constant as a + 0. In this case, then, the norm of B' does not blow up
relative to that of B, arid B is well conditioned for all a.
I

I

I

The conditioning of any square matrix can be smmarized, then, by a condition number K (A) defined as the product of the maximal singular value of A

-l . This concept is readily extended
times the maximal singular value of A
to

a rectangular matrix and can be calculated without recourse to the inverse

matrix. From the singular value decomposition of X UEV', it is easily
+
is the generalized
of X is UEV', where
shown that the generalized inverse
inverse of E and is simply E with its nonzero diagonal elements inverted.1

Hence the singular values of X are merely the inverses of those of X, and
the maximal sIngular value of X is the reciprocal of the minimum (nonzero)
singular value of X. We may therefore define the condition number. of X
as K(X) = 1Tax

The Use of The Condition Number in Determining Rank.

We

will now discuss

the sense in which the condition number has meaning as a measure of the ill-

conditioning of a matrix. This will further result

in a

meaningful criterion

for determining when a singular value is small enough (relative to

to

provide evidence of a renk deficiency.

__
1.

See

Golub and Reinsch (1970) or

.
Becker et al. (l97).

—9—

Consider the linear system Xb a, and suppose the data are known

exactly, but

stored in finite

precision. It is shown in Stewart (1973) or

Hanson and Lawson (1969) that a change in the last digit of the elements of

X can result in a change in K (X) times as great in the solution b. That is,
then a change in X in the
if the machine zero is io-10, and K (X) is
tenth decimal place can affect b in the 10—10 x 10L or 10 —6
then,

place. Clearly,

a condition number sufficiently large can wipe out all significance

to a solution to a linear system. Such u1d be the case if K were larger
-than the rd length of the machine.
In a least—squares problem, the solution to X' X bX'y, a similar result
holds, except that now a perturhation in X affects X'X as the square, and we
JTU.lst have the square of the condition number to be like the word length, or,

equivalently, the condition number like the square root of the word length.
Rather generelly, then, in the least-squares context, we would suppose
was less than the
that any singular value, ak which, relative to -the

square root of the machine zero (the reciprocal of the word 1ength-about
2_26 for IBM 360/370 long precision) to be evidence of rank deficiency.

When there is Fuzziness in the Data. The determination of the rank of

the data matrix X is less straightforward when the data are known impreciselywith fuzziness. The analysis of the previous section is based on data known

exactly, and from it we learn that a perturbation in the last digit of the
data's word length can affect digits on the order of K(X) from the
the
K(X)

and

solution for b of a linear system.

is

a

a K(X) of

Thus if

the word length is io8

change in the eighth digit of X can affect

io8

can

last in

remove all significance from b.

b in the 5th digit,

-10-

When the data are fuzzy, further problems are encountered, because

relevant

the

perturbations

word length,

but

possibly much

word length of io8 and
Now

relevant

108x105

in the data now affect, not necessarily the last digit it
a

K Cx)

higher order digits. Suppose again a
up to 1O3.

perturbations of the data as stored in the computer are

1O3 times greater

than perturbations

word length. Hence the solution to the linear

even

are known only

= 1O3 but the data

less precision, and

could1

of the last digit of the

system will

be known with

be affected in the digits on the order of

KCX)X103. In this case that would be io6, leaving

only the first two digits

to be known with any accuracy.

In the least—squares so1utions--as contrasted to the solution-to a
linear system used in the explanation above—the treatment of data fuzziness

is

quite analogous. If the data in X

are exact to, say, l0, then the data

of XIX are exact to 106. A word length of 108 now inplies that perturbations

of the order of 108x106 =

102

are now relavant, and these can in turn be

n.gnified in the least-squares solution by a factor K
number of X'X. Here, this would be C103)2xl02

CX), the

condition

108, and hence the solution

b rry have no definition at all with an 8 digit word length.

1. The word could is used because the figure is an upper bound telling the
worst possible story. It could be better in any given case.

—11-

The preceding leads to the following suggestion for determining when

a singular value is small enough to be considered evidence of rank deficiency
in the data. Let

w be the word

be the fuzziness2 in the data matrix X --

f2. -t1iat of X'X.

when there is fuzziness

length1, and f

Then the foregoing

argues that we must have wf2K2(X) < w if the least squares solution is to
have any meaning (any stable digits) at all. That is we must have K(X) < f.
If the data are known up to 1O3, we can allow X to have K(X) = a max <
a

Hence any ak such that

of rank deficiency.

a

max

<

lO (f) would indicate the possibility

ak

1. w can be measured as lOs", where 2. is the number of digits carried by the
machine.

2. f can be measured as 10h, where h is the number of places known with
exactness.
3. Provided X'X is accumulated in double precesion relative to that of X.

—12—

.

1.4 Determining the Structure of the Linear Dependencies Of X.

1.4.1 Defining the Structure
In this subsection we assume we have already detenmined the rank of

X as described in the previous subsection. Our interest here centers on determining which variates do and which do not enter any specific linear dependency.

It is this information that is meant by the term structure of the linear
dependency. It is not sufficient to examine the zero structure of V2 in (1.6)
to determine the structure of the linear dependencies, for clearly, for any
(k-r)2 nonsingular matrix A, (1.6) becomes

X VA 0,

(1.7)

and we can alter the zero structure of these linear dependencies (given by the

zeros of

that is

the matrix

V2A) arbitrarily. Father we must rework

invariant to linear

transformations.

This

is

(1.6) into

a form

accomplished by partition-

ing (1.6) to produce a "reduced form" as follows:

x V2

where

,

X1 is T x (k-r)

V21 is (k-r) x (k—r)

X2 is T xr

V22 is r x (k-r)

V21 is chosen to be

and

of

[xix2][2]=

0

full rank, such
X1

where C -

nonsingular. Since V2, having orkhononnal

a nonsingular

-XV
VXG
2 22 21

subrratrix must exist. Fran (1.8)

(1.8)

columns,

is

we obtain

(1.9)

—13—

The structure of (1.9) is clearly invariant to linear transformations since
X V2A

0 :Lrrplies cx1x2[] A = 0

tion of the structure of the
the

or X1

-X2V22A A1V = - X2G.

linear dependencies of X therefore

is

The determina-

precisely

determination of the zero structn?e of the matrix G. flom it we learTl which

colunuis of X2 are involved in linear relationships with the variates composing
the co1iiins of X1.

Unfoxkunately we cannot sinply calculate G and look for its zeros, for,

as already mentioned, the finite arit]-tic used in determining V2__ now further
compounded by the calculations determing G as -V22V —will not guarentee that

the zeros of G will indeed be calculated as zero.
l.Li.2 Determining the Zeros of G.

Io methods are suggested here for giving nmrica1 specification to

the zeros of C.' The first is a 1inear-proanming approach, the second a leastsquares approach. Both methods axe based upon the following rationale. Linear

dependencies are exact only in perfect algebra. The econoiztrician has always
sought to extend this concept to one of "near dependency", a notion that has been
more intuitive than rigorous. In the previous section, however, we saw how

"nearness" could be given rianing in a realistic contect both by the natural
fuzziness given by a "n iiine zero", and by the more usually encountered fuzziness

that results from data inaccuracies. This latter concept requires some discussion.
1

authors are greatly indebted to Gene Golub of Stanford University and
John Dennis of Cornell University for their contributiàns to these techniques.
The

ObservatIona1 Equivalence

A published GNP figure of 1.054 triflion dollars is clearly not exact.
Indeed all additional information regarding digits beyond 10 have been surpressed. The datim 1.054 is therefore observationaiLy indistinguishable from

1.0542 or 1.0539. That is ,there is

some region of fuzziness such that,

given

noimai rounding procedures, any data point lying in that region is equally valid

for an entry into X. This concept of truncated data repoxing is quite distinct
from errors in observation. The latter would argue that one might not know for
sure the corl2ectness of the data actually reported. Hence observations error
introduces yet another element of fuzziness into the degree of accuracy with
which one knows one

s data.

In any event

there

is reason to suppose that there exists a matrix E,

determined by the investigator, that puts limits on the accuracy to which he
believes he knows his data. These limits may, for exarrple, take the form that
"coluim 6 of X
I

X - X

E is

equivalence

only up to 10". Hence, any data matrix X such that
This notion of observational
observationally equivalent to

is known

(which could no

doubt also be cast into a statistical framework)

is a data-analytic analogue to the identification problem. Given the fuzziness
in X, any results based on any X observationally equivalent to X iraist also be

indistinguishable within the degree of precision to which the data are known.
Hence the investigator must consider as observationally indistinguishable any
V resulting from the singular value decouosition of any appropriate X = UEV'..
It is this notion of observational equivalence that is exploited to determine the
zeros of G.
1

The notation IXI here is used to mean absolute value of a matrix, not the
determinant.

—15--

Zero Enrichment

Given the data matrix X, we have from (1.9) that
—

XG

0,

(1.10)

and we propose to determine the zero structure of G by determining whether any

of its elements (or specific of its elements) are observationally indistinguishable from (equivalent to) zero. To do thIs we employ a numeric-analytical
analogue to hypothesis testing.' It is proposed that the investigator examine
the G determined by the singular value decomposition of X and specify which of

its elements he has reason to believe to be zero. This may be based upon

a priori considerations of which variates uld not belong in certain linear
dependencies (hence inplying the corresponding elements of C to be zero) or it
may be based on experience he has regarding which values of G that are calculated

to be small numericafly are in fact zero. In any event the matrix C has, as a
rna-ter of hypothesis, certain of its elements made to be zero. The resulting
zero enriched matrix is denoted G. In lx)th of the following procedures a method
is presented to test the hypothesized zero enrichment by determining whether G

is observationally equivalent to G in the sense that G could indeed by calculated

as the G matrix for a data matrix X that is observationally equivalent to X.
Method 1: A Linear -Programming Approach

Let A (S) be a IK matrix to be detennined. X is the given 1d( data
matrix and E is the "limits" matrix defined above. C is the matrix defined in
1Again a statistical fornuilation of this procedure may well be possible, but is not
exaiBined here.

—16—

(1.9)

by the

singular value

decomposition of X and

for which

(1. 10) holds.

Partion A [Lt] to correspond to X1 and X2. G is an hypothesised zeroenriched

rr.trIx

subject to test. We will say that G is observationally equi-

valent to G (arid hence accept the hypothesIsed zero enrichnnt) if there exists

a A [AA] such that G satisfies
(1.11)
and

jAIE,

(1.12)

i.e., if can result from the singular-value decomposition of a data matrix
that is observationally equivalent to X.
The existence of such a A can be established from the feasibility of a
linear progiem. From (1.11) we have
= - (X1

A1 -

-

X2G)

(1.13)

or
AFT

where

Using

—XH

(1.14)

JE1

H1j.
the change of

variable

= A ÷ E,

(1.15)

the problem of finding a A that satisfies (1.14) subject to the inequalities

(1.12) is equivalent to finding the that satisfies
(E-X)H

subject to

(1.16)

-17—

j2E a

(1.17)

The existence of such a ' (q) is clearly established if there exists a
feasible solution to the contrived linear pxoam
mm t
subject

K

EE
tic

, a vector of n ones]

(1.18)

to (1.16 and 1.17).

It is

rth errhasizing that it

to accept the hypothesis of the zero

dnonstrate

the feasibility of the

Is not necessary to solve the 12 (1.18)

erxriched G, rather it is

only required to

program.

Method 2: A Least-Squares (minimum norm) Approach

The 12 given above will, even for moderate sized econcznic problems, be

large. Even the demonstration of a feasible solution could prove costly, and,
hence, a second method appears worthy of consideration.

Our problem is to find a satisfying (1. 14) also obeys the inequalities
Since H in (1.14) necessarily has full rank, we can find all , satisfying this relation without regard to (1.12) (in general there will be an infinity
(1.12).

of them) by considering all
—

XEIFI

(1.19)

where H is any pseudoinverse of H. Among all these solutions, however, is one
with minimum norm (i.e., a E with minimum

), which is found by using the

genere1ized inverse H , i.e.
=

is, of coia'se, no guarantee that £ will satisfy (1.12) in all cases,
there is reason to hope that its property of minimum norm will indeed also
There

but

(1.20)

—18-

result in (1.12) as a px.ctica1 matter. This second method of determining t,,

then, is sufioient brt not necessary to accept the zero ern'ichnnt hypothesis.
That is, a solution to (1.20) that also satisfies (1.12) accepts the observational
equivalence
that

does

does not

quick

of G (the hypothesized zero

enrichment), but

not also satisfj (1.12) does not mean that

exi1

The advantage of this

a

a solution to (1.20)

solution to the 12 (1.18)

technique over the 12

and cheap to employ. If it works, no further effort

doesn't, further

is that

it is

is required. If it

investigation iray be warranted. It will be a metter for

experience to determine just how well this short cut works in practice.

1. We are indebted to our colleague, Paul Holland, for highlighting these points.

—19—

APPENDIX '10 SECUON 1. SCALING

The seemingly elaborate test procedures given in the previous section axe

nDtivated by the fact that the elements of G are scale sensitive and can be

made arbitrarily small sInly by a choice of scale. Detmination of the zero
structure of G, therefore, requires some meaningful (not arbitrary) measure of

snJl, and this measure is afforded by the procedures outlined.

The purpose of this appendix is to deuonslte this probletnful scale

sensitivity.
Let
be

a

X be the

data matrix in

arid let D =

0). Call the scaled data matrix

scaling matrix (all d1

(using

"original units",

the notation of the text)

the SVD of

that of X
A

A A

A A

_i
—X2

arid the

Xi). Now

(1.21)

0.

The reduced fonns corresponding to the original and
—

V22 V21 = X2 G,
A_i

X1

d

is

XUEV', implyingXV2

and

...

X is

XUZV',inplyingXV2=O
and

diag(d1

—X2 V22 V2i

scaled

data are therefore

—i

G —V22 V21

(1.23a)

A A_

(1 23b)

A A

A

X2 G,

G = —V22

Vi

econometrician must insist that the zero structure of G be the same

G, since arbitrary scaling

cannot affect the real

linear dependencies.

as

—20—

We

.

will now show that with exact arithmetic, these zero structures are

indeed the same, but that they can be made to appear different due to finite

arithmetic, hence necessitating the test procedures of Section 1.4.2.
0 we may write

From X V2

X D D1 V
EXD' V 2
2

0.

A

Since

A

have

p CX), the null space of X must

p (X)

hence D1 V2 provides
Hence

(1.24)

a basis

(not

dimension

orhonormal) for the null

space

as X, arid

of 2.

any orrthonounal basis for thIs null space (such as V2) must be a non-

singular transformtian

of D'V2. Let

A

V2ED'V2H

this be

1V21
rD1 o] rv211
orI,I
I
_II
LVzJ

H nonsa.ngular.

(1.25)

D1LV2J

[E

for

the same

1

.

Putting (1.25) into (l.23b) gives

AA

A

A

V22 vJ

—X2

A
= —X2

—

—

—

D V22 H H V2 D
(1.26)

X2 D1 V22 V D1 = X2 D1 G D1

=

Comparing (1.26)

with (1.2 3b) shows

G D' G D1.

1 .It is reacti,ly
normal basis

A
1 lvii
V2 can be any orthof
LJ
that
the null space of X. One

seen

for

(1.27)

froni X

A
[U1 U2]

can therefore

derive

such V2 from V2 by taking the QR decoosition of DV2 =
Q

D'V2R'.

Q

at least one

R to produce

.

—21—

Since D2 and
where

D1 are both diagonal, we

G (g) and G (g).

Hence,

have

0 if and

only if g1

0,

in exact arithmetic scaling does not

change the zero structure of G. However in finite arithmetic it

is

clear that

any nonzero element of G rray be irade as small as desired in G by appropriate

scaling. A nonzero in G nay therefore be a zero in G
limiits of the machine' s

arid

vice versa within the

calculations.

The solution to this problem (that the determination of linear dependencies

nay be scale—affected) is one of numrical analysis. Since there would be no
problem from scaling if we had exact calculations, we should analyze the data
matrix x in units chosen to allow for the rn.mierically most stable calculations

in light of the finite arithmetic. Column equilibration (scaling to produce
roughly equal column lengths) enjoys some usefulness in this

context

.

Conclusions

regarding the zero structure of V2 should be based on a data matrix so scaled.

Then, should the user desire infoniation on a differently scaled matrix, the
aixve detenTdned V2 with the zero structe imposed should provide the basis of

the transfo±'med structure. That is, let X be the data scaled for numerical
accuracy, and let X =

X D be the

data scaled in tenris of the user' s preferences.

Then the zero structure of the G applicable to the data in X is determined by

analysis of (1.23a). Let G be the calculated matrix, and denote G with its
can then be
"zero" elements replaced by exact zeros by G*. Infonnation on
had by the analog to (1.27), namely
G*

Clearly *

will have

1. See Van

(1.28)

D1G*D.
the same

der Sluis

zero structure, invariant to

(1969) and

(1970).

scale.

—22—

Part

2. An Assessment o the Dàe Caused by Linear tpendencies

In this part we address the second njor question set out in the opening
paragraph, namely, how much damage is caused to the regression estiiiiates due to

the presence of linear dependencies (near dependencies) in the data matrix. It

is well known that any such dage manifests itself in mstable regression
coefficients and in inflated sampling variances. But it has not been possible
quickly to deteunine whether the size of any specific sanpling variance was

large because of collinear data or because of inherent noise (arising, for
example, because the given variate does not belong in the hypothesized relation-

ship). The former problem is potentially corctable through additional information that mIght take the form of new noncollinear data, a prior distribution

'or the regssion parameters, or outside estintes for specific coefficients.
The analysis presented here helps to determine whether collinear data is in fact

a cause of inflated sampling variance, and further it helps to highlight which
regression estima,tes are being nDst adversely affected - thereby keying where
corTective measures are nvist profitably employed.
In Section 1, the decomposition of the sampling variance that forms the

basis of the analysis is presented. Section 2 presents a theoretical result that
helps to interpret possible outcomes of the decomposition. Section 3 examines
the procedures suggested in Section 1 for assessing the danage caused to regression
1

estimates from

the use of collinear1 data.

It should be highlighted that the term collinear here means rank deficient

the sense of Part 1 and does not mean the existence of an exact linear
dependency; nor, obviously is it the common but loose usage in econanetrics.
in

—23—

2.1 The Basic Decomposition of the Variance of bb.

The singular value deconposition of a data matrix X, as we saw in Part 1
of

this paper produces a set of singular values that can be associated with

potential linear dependencies in the data. The rd "potential" is used because
(as per SectIon 1.3) it must first be deternined, through machine and data con-

siderations, which sIngular values are small, and for each of these there is a
linear dependency to be identified. As any one singular value, then, gets
there is a near dependency to be associated with that
snail relative to
singular value.

The basis for the analysis presented here is the deconposition of the

variances of the regression coefficients into cononents that are associated
with the singular values of X and hence are directly related to the specific
linear dependencies possesed by X. A derivation of this variance decorrosition
using

eigensysterns of X'X due to Silvey (1969)

we rederive the result

here

using

the

singular-value decompostion to highlight

the correspondence of the components to
linear

dependencies, of X

The

is,

(not

is given in Johnston (1972), but

values,
of the niount matrix X'X).
the singular

and

hence

the

variance-covariance matrix of the least squares estimator b (X'XY'X'y

of course,

Var (b)
where

a(X'X)1

(2.1)

a2 is the corrupn variance of the conponents of

y x $fc. Making use of

K
X

the

singular value

TXK d( d(

UE

VT

with

the T

disturbances c in

decomposition of X

diag (a ..

a1?, and V(v1)

(2.2)

—24—

may rewrite (2.1)

we

as (recalling

.

U' U I)

Var(b)

or,

(2.3)

for the )c-th component

var(bk)

(2. LI.),

a2 E

the

of

square

In the

associated
the other

denominator

one of the singular

dependency of

1.3 how, for each linear

in (2.4),

with a linear

other

dependency

of

a specific

values, a.

X, some a becomes

We recall

small. Since these

then,

a) will be

large

dependency may be

value

to

that an unusually high proportion of

gives evidence that

causing problems.

of var(b)

relative

one or irore coefficients concentrated in components

singular

from Section

things equal, those components

(with small

components. This suggests,

the variance

with

(2.4)

.

it will be noticed, decomposes var(bk) into a sum of components each

containing

are

of b,

the

This suggestion

associated

corresponding linear

is pursued

in Section

2.3 after

some interpretive considerations are developed in Section 2.2.
It is a relatively easy matter

so

the

that

to display

investigator can tell at a glance

these proportions for all

where

var(b)

problems may be arising

Define

v.

K

1kj a

'

1k

kj

k 1 .. K

and

k,j1...K.
Then

all

infonnation

is

sunlii3rized by th tables

(2.5)

—25—

Variance-Components Table

(all entries x 2)
Components of

var(b1)

var(b2)

.

12

r

.

49

.

var(b)
k1

1111

11

.

n

22

2

(2.6a)

C)

0C))

aK
arid

1K

2K

KK

—26—

Variance—proportions table
Coriponents of

var(b1) var(b2)

11

12

21

22

...

var(b<)

•..

(2.6b)
.

I
An example of these tables is given in Sections 2.2.4 and

2.2 An Interpretive Consideration:

2.3.3

Ozkhogonality and the

below.

Zero

Structure of

V.

It will be necessary -to gain much practical experience with the decouçosition
(2.4)

before reasonable

tool. There

is,

guidelines can be established for its use

however, one uiimediate consideration that

can

as

a diagnostic

be given a

are zero, then it makes
v,
no difference to var (1) If the corTespondlng
are very small, i.e •, the
coefficient will be inirmne from collinearity associated with those particular
rigorous

foundation, namely, that If in (2.4) some

singular values. This section examines the conditions under which certain of

the v1 will be zero (or small relative to the corresponding a) and hence
develops conditions under which certain regression coefficients need not be

adversely affected by the presence of multicollinear data. We can anticipate

this result by recalling the well known fact that the addition to a regression

equation of a variate that is orthogonal to all previous variates will not affect
the regression calculations based only on the original variates. Clearly then,
it should also not affect any regressIon calculations to add a set of variates

—27.-

are orthogonal to all previous vai,tes whether or not this additjonal
set itself contains with it a perfectly - colliriear relationship.

that

Indeed, through a series of telescoping theorems of increasing generality,
we arrive at sufficient condition on X (and its singular values) under which

orthogonal partitions of X ixrly specific V..' s to be zero in the singular
value decomposition of X. These are approxinate conditions, then, under which

regression estin.tes may possibly be salvaged even in the presence of strongly

collinear data. Special computational algorithms are required to exploit this
possibility, however, for mDst reession proanis are incapable of dealing with
collinear data no matter how it occurs, and hence can make no attempt to identify
and salvage any coefficients that need not be adversely affected.1

In the rest of this section four theorems axe proved that show the conditions under which orthogonal blocks In the data matrix X imply specific v.. 's
to be zero • 2 The reader not interested In the proofs to these theorems is
advised to read Theorems 2 and L for gist and continue to the next section.

2.2.1 The Zero Structure of V when X hs Orthogonal Parts
Let us begin with a Th}( data matrix X partitioned into two orthogonaJ. blocks X1 ('IK1) and X2 (K2) with X1' X2

the

singular

values

of X by determining them

1A set of calculations

0. In this

separately

case we can

for X1 and X2

determine

Indeedx

that proceed

correctly in the presence of perfectly
collmear data axe given in Belsley (l97'). These algorithms form the basis
of the NBER Computer Research Center's GREMLIN system - a
comprehensive package

for esta.nating sinLiltaneous systems available through the Center' s time sharing

network.

)

21t should be emphasized that these are sufficient, but not necessary conditions.
Indeed there may well be other conditions leading to v. . 's being zero - and
these too would lead to coefficients isolated fran col±?tear relationships.

—28—

.
the SVD of X is

XUEV'

(2.7)

while those of X1 and X2 are
U1 E VI

X1

-

where UU1 VV 'K1 E1 = diag.

matrix
(2.8)

U2

U2U2 - V2V2

EV2

matrix

I< E2

It is clear that the matrix V derived from (2.8)

as

0-1

rv1

(2.9)

I

10

V2

is orthogonal and has the property of diagonalizing X' X

/v'

-

V'(X'X)V (\Q1

'\ Iv

0

o\ /'x1

0

v2J (01

xx2) [o v2)

'\

=

/E 0
(o

z2 j

(2.10)

Hence the matrix
o

-.
=

0

E2)

must be the matrix of singular values of X.
Since these values are unique they mist be the same e1nents as E in

(2.7) - although the order is not unique. We have shown

(2.11)

—29—

Theorem

1.

Let X

determined

0. Then

(X1 X2) with X X2

directly from

the

the

separate SVD

singular

of X.1

values

of X niy

be

U.E.V!, il,2.

3-3-i

This result can be used to show that orthogonality anong sets of columns of X

implies a certain zero structure on the elennts of V in (2.7), and hence on
certain relevant v.. in the numerator of the variance decomposition (2 .14)• We
begin with
Theoren 2.
Let X
X

[X1 X2]

0. Then, if the singular values of

with X' X2

are distinct, the natrix V

ii

in

the SVD of X =

UEV'

has the form [viol
L

where V.
is K.xlK..
1

Proof:
Cdfl

write

The SVD of X. is as in (2.8), and because of Theorem 1, we
E

as

Now

0

(X'X)

(

T<

I = vv

22)
and

V2J

one V that clearly works is

V

-

1

0

ol
V2j

.

But

since

the columns

the V. are the eigenvectors of
the distinctness of the singular
values guarentees the uniqueness of the V1 (up to permutations arid a
of

—30—

multiplier of nDdulus 1). Hence V is unique up to permutations within its

first K1 coluns and its last K2 colunu-is - which clearly will not alter the
zero structure

QED

The condition in Theorem 2 that the singular values be distinct is over.strong for the purpose at hand. Problems in guarenteeing the desired zero struc—
ture occ.xr only when there are multiple roots in camion between E 1and Z2' overlap
of roots. The following exairple demons-b:'ates this. Let

x

[X1X2]

I

Lo
The matrix
1

0

I
o:o
:

r

that x'x = -— --:— . -

[jo ooJ

0 ol

o

V=
o

0

is

--L of
ooj

easily shown to be orthogonal arid

diagonalize

X'X, but it clearly does not

possess the desired zero structure. Even here, however, there
-that

does

is

a V matrix

possess the desired structure, namely V=I, but such a structure

not guarenteed.

is

—31—

If, however, there are multiple roots tha.t do not overlap X1 and X2 (are
not in conun to

and E2) the desired zero structure is assured. This is

seen by assinnIng otherwise, i.e., assume

v—

[vi'

v*1
121

[v vj

in any other orthogonal

x' x v* 2 Since the

V such that

overlap, the non-uniqueness of V

(beyond permutations

and E2 have no

of columns) can occur

only up to linear combinations with its first K1 columns and within its last
K2 columns. Linear combinations aoss these two sets of columns are not

possible. But we already know that["] is a basis for the renge space of the

first

'<1

columns, and

rol

[j

LOJ

a.

basis for the last K2 columns. Hence

any

permis-

sible linear combinations must preserve the zero structure. We have proved
Theorem 3.

If in Theorem 2

and

2

have

no values in connon (however eat the

multiplicities within each), then V in the SVD of X retains the zero structure
shown there.

The assi..uitions behind Theorem 3 are too s-txong, but they nay be weakened

to produce a useful result, nanely.
Theorem 4.

Let X

[x1x2] with XX2 =

(kth element of E2). Then,

0

be the ]<±h singular value of

2k is distinct fixm all other

arid E2), regardless of any other
SVD of X has the property that

and let

multiplicities or overlaps, V =

(in both

(v) in the

—32—

V,K+k_
P
1

i.e •,

for jl, ...,

the ffrst K1 elements of

the K1 +k column

of V

are zero.

Beyond permutations ,the K1+]<±h column of V is uniquely determined up

Proof

to a linear combination of the eigenvectors associated with the value
Since this value is assumed distinct, there is only a one dimensional space
associated with It, and we know that this space is spanned by the K1+kth
column of V =

J

V1 0 ,

which

clearly has the required zero.

L0YzJ
2.2.2 Nearcollinearity Nullified By Near Orthogonality

Theorem 4 has the generality required to analyze the variance
decomposition (2. Li). Let us assume, in the eth'eme, that X has two oikhogonal

parts X1

and X2 and

that X1 is well conditioned but X2 is ill conditioned.

This means that the elements of

are roighly of the same magnitude but that

there are some elements of E2 that are relatively small. Break up the sum

(2.4) into its first K1

terms

and it last K2 terms as

K v2.
var(bk) =

E

_!a

j=l cr2

K1

v2.

1<2 vic,Ki+j

j=l 2 jl
E

isa.
1

J

+

(2.12)

...
2

ill conditioning of X means that some a2 will be small - indeed zero
J
be
Now Theorem 4 guarantees
if X2 Is perfectly collinear. Let thas

The

that for k 1 ...
2

K +p
21

2p

K,., v2

k,K1-I-p

=

0, and hence the term

—33—

for k 1 ...

k 1 ...

K .

K1..

That is, var(bk) is unaffected by near collinearity for

These

estin.tes are salvaged in the presence of collinearity due

to orthogonàlity of Xi from X2. Of eater generality, however, one clearly
need not assume X1 strictly orthogonal to X2. Since the V.. 's are continuous
functions of the óoluiins of X, as the blocks of X become more nearly orthogonal

(their :inner' products get closer to zero) the relevant elements of V also go to

zero in the Limit. Hence some v can be snafl if the data axe pleasantly well
behaved. That Is, the adverse effects of near collinearity in one block of
data, X2 (as measured by sane small Ozj's) can be mitigated in the estimates of

the coefficients corresponding to another block of data, Xi, as these two blocks

are the more nearly orthogonal (as measured by small V]'S, k K1+l ... K).

An Examle

2.2.3

An exanple of the preceding result is useful here. We wifl consider
the matrix
—71.1.

X =

80

14 —69
66 —72
—12
3

18' —56
52

1014

—5'

7614

1528

This matrix, essentially due to Bauer

first

(2.13)

66 —30 '4096
8192
8
—7—13276 —26552

4 —12

column

—112

21:

4 8421 16842

(1971),

has

the

is exactly twice its fourth, and both of these

property

that its

fifth

are orthogonal to the

three columns. That is, X2 is singular and XIX2Z 0.
The preceding theorems tell us the following about the

and V

matrices that result from the singular value deconosition of X: unless there
are mnmltiplicities of roots (which, as a practical matter will occur with

_3L1._

.

probability zero), 1) one of the singular values associated with X2 will be zero

(i.e., within the machine toler8nce of zero), and 2) in V
IYzi

0.

and V21

12
v2j

v

12

0

of the progrmn NINZLT1 to obtain the singular value decomposition

Application

of X results in:
(2 iLl.)

0.54786Ll.D

00

—.625347D

00

0.5556850

00' 0.148362D

—18

—.543l83D

—.835930D

00

0.383313D

00

0.392800D

001 0.2l56l8D

—19

—.470435D

0.3263Ll.2D

—01

0.6797l5D

00

0.732750D

001 0.l58ll3D

—18

—.729449D

—.642653D

—15

—.216297D

—15

0.913326D

—.'t47214D

00

0.894427D

0.321L1.23D

—15

0.108174D

—15

-.456672D

—14' —.894427D

00

—.447214D

I

arid the following diagonal elennts of E
0.170701D

03

a2 = 0.605332D

02

a3 = 0.760190D

01

= 0.36368L1.D

05

0.l3ll59D

—11

a5 =

A glance at V

verifies

(2.15)

that the off-diagonal block pardtions are indeed

small —

all of the magnitude of 10 or smaller - arid well within the effective

zero of

the computational prei

the a associated with x2

1Golub and

is

IBM 67 in

double

Only somewhat less obvious is that one

zero. Actually a5

Reinsch (1970), and

210 n the

2

Becker,

precision.

et a]-.

is of

(1974).

the order

of

iO_11,

and

of

—14

—14

00
00

.

—35—

seem to be non-zero, but the relevant conparison1 is the order of magnitude

would

6
of the scale-free value k , which, in this case, is 10_i

The practical

umax

results are thus in full accord with theory, and we can now exantine the effects

of the perfectly collinear data matrix on the estimated variances of the regression paranters

this

b = (X'X)1X'y.

It is clear that any problem in the calculation of Var(bk) in (2.4) for
particular case will arise because of the very small (1g. However, (15,

small as it is, is several orders of magnitude larger than its corvesponding
v2
v.. for i1, 2, 3. Hence the contributions of the i.5 corronents to calculations

of Var(b1),

Var(b2) and Var(b3) in (2.4)

presence of ptu'e rrnilticollinearity

with

which we can

will not

will

be small. That is,

the

significantly upset the precision

estinate the coefficients of other variates provided

these

other variates are reasonably isolated from the offending collinear variables
through near orthogonality.

To denonstrate this point, we calculate the relative cononents of var( b)
by maans of (2.4).

2

25 V
c E 1j

jl

Var(b*1)

ci (.0010 +

.0107 + .5343

It is clear from (2.4)
by the collinearity,

nanly

+

that
-

2

0.0 + .0017) 10

2

_2

= a2

(.5488 x 10 ). (2.16)

the cononent of var(b) affected adversely

, is

small

2

(.0017 x 10 ) relative

to the total

5

I jT,

Professor Golub shows any k having the property that
where c is the effective machine zero, is considered evidence of rank deficiency.
[Golub arid Reinsch (1970)].

—36—

(.51488 x 102). Indeed, it is only through

the

finite arithmetic of the machine

that this term has any definition, for it, in theory, is an i.n-ideterrnined ratio
of zeros. In practice, there is reason to cast out this component in actual
calculations of var(b).
The preceding is in stark contrast to the calculation of var(b) or var(b),
for these are the variances of coefficients that correspond to variables involved
in the singularity of X. Indeed
5
var(b*)

v2

5j

c2

jl

cy2

(0.0 + 0.0

+ 0.0 + .0000 + 1.1626 x 1023)1.

2

(2.17)

This variance is obviously huge and completely dominated by the last tern

and its role in causing the singularity of X.
2.3 Assessing the Damage Caused by Collinear Data.

2.3.1 At Least Two Variates I&.st Be Involved
The theorems and example of the preceding section help to put
meaning to the variance components and proportions suumarized in tables like

(2.6 a and b). At first it might seem that the concentration of the variance
of any one regression coefficient (var(bK)) in any one of its compoents
kj

(j

problems.

1

k) signals the fact that multicollinearity may be causing
But it is clear from Theorem 14 that if collinearity (ill conditioning)

The difference between 0.0 and .0000 in these expressions is designed to
differentiate between a number within the machine's zero (0.0), and a nonzero
number with highly negative exponent (.0000). The 0.0 's in (2.17), for example,

are of the order of io° ,

while the .0000 is of the order 10'°.

—37—

is causing problems, nre than one variance must be adversely affected by

variance components associated with a single singular value. This is seen
from the following example.

Suppose the data matrix X consists of K mutually orthogonal coli.mnis, and

the singular values satisfy the conditions of Theorem '-I. (as they will with

probability 1). Theorem 4 immediately implies that the V matrix of the singular
valua decomposition of X is of the form'

F"

I

22

0

I

v=

L
Hence

terms in (2.5) will be non-zero, and (2. 6b) will take the form

only the

Proportions in
var
var

(I)

(b1)

'
1

.j
•U)

k

0

1

1

While V has been made diagonal here, Theorem 4 insists only that it have one

non-zero eleiient in each row and column. V is unique only up to column pennuta-

tions and a multiplier of nodules 1. This, of course, does not affect the calculations of (2 . it) or (2.5) since the cr permute in a compensating nnner and
5 are squared arid unique despite the multiplier of modulus 1.
since the

vj

—38—

It is clear that a high
singular

proportion

of each variance associated with a single

value is hardly indicative of multicollinearity, for

the variance

proportions here are for an ideally conditioned, orthogonal data matrix.
Indeed, problems can arise only when a single singular value

is associated

with a large proportion of the variance Df two or more coefficients. This
sisnply reflects the fact that there must be two or more columns of X involved
in any linear dependency.
We know by Theorem

that each of the columns, k, of V

involved in such

a linear dependency must necessarily have a nonzero Vkj associated with the

small singular value

a.

The ratio of these vkj to the small

must, there-

fore, loom large in the calculation of the variances var (bk) by (2J) for
those coefficients corresponding to the collinear (nearly collinear) variates.
If, for example, in a case of K =

5, columns L and

5 are collinear and all

other columns are mutually orthogonal we would expect a variance-proportions

table like (2. 6b) that has the form, say
Proporations in

var var var var var
(b1)
F

•3F

2l0

(b2) (b3) (bk) (b5)

1

0

0

c3Jo

0

1

0

cijo

0

0

1

0

0

0

•9J

Here cL plays a large role in both var(b1) and

var(b5)

—39—

2.3.2 Variance Proportions : Necessary but not Sufficient

We have learned from the foregoing that near collinearity (ill
conditioning) will manifest itself as high proportions for two or more variances
in components associated with a single singular value . Unfortunately, for the

purposes of testing, the converse does not hold; such a pattern of high proportions need not imply the existence of collinearity. Whereas several variances
may have most of their weight in a component associated with the same singular

value, the overall magnitude of the variance may be pleasantly low--near collirt-

earity, if it exists at all, causes no problem. The variance proportions table,
then, is merely a quick means of telling whether collinearity may be problemful,

but once the pattern of high proportions is detected, one must turn to the actual
variance components in Table (2.6a) to tell whether the overall levels are high.
An example will serve to make this clear.

Let us return to the ndified Bauer matrix of Section 2.2.3. This five
column matrix, we recall, has the property that column 4 is exactly twice
column 5, and these two coluna-is are orthogonal to columns 1, 2 and 3. We would

fully expect that the siiall singular value
the linear dependency X1

. 5X5

(

.1312 x 1&)associated with

would daninate several variances--at least

var(b) arid var (b5). The variance proportions table (2 .6b) for the modified
Bauer matrix is given below in Table 1, and a glance at the bottom row verifies

that (35 does indeed account for the entirety of these two variances (the first

three variances are isolated fran this relationship by the orthogonality of the
first three columns of X from the last two).
1. It should be noted in passing that the existence of collinearity in X may
not produce practically hannful problems in estates of a linear model
relating y to X, as in y X+c. Such problems also depend upon the size
(which also enters in Var (b)). This point is dealt with below in
of

greater detail in section 2.3..

—Lo—

TABLE

1

Variance Proportions - Modified Bauer Matrix
Var(b1)

(13

(15

Var(b3)

Var(b)

Var(b5)

.002

Var(b2)
.009

.000

.000

.000

.019

.015

.013

.000

.000

.976

.972

.983

.000

.000

.000

.000

.000

.000

.000

.003

.005

.003

1.000

1.000

A somewhat unexpected pattern, however, is also apparent: The single

singular value
It may well be

(13 accounts for 97%

the case that

a

or more of var(b1), var(b2) and var(b3).

second linear relationship among the cohnns

of X, one associated with (13 is accounting for these high proportLons. But

two facts would tend to discount this possibility. First, the three columns
that

X1, X2 and

orthogonal)
centrated

are

could

be involved in such a relationship1 (X and

reasonably well

conditioned; and

second,

X5

are

in spite of the con-

variance proportions, the overall magnitudes of var(b1), var(b2) and

var (b3) are small. This latter fact is seen fran the actual variance components
for the modified Bauer matrix given in Table

2.

1.

Prom Theorem 1 we know that the singular values for the matrix X1 which is
comprised of the first three columns of the rnodifed Bauer matrix X are pre-

cisely the same as o, 2 and a for the modified Baier 9triX itself.
the condition number of X1 is K(X1)

.171 x 10

.76 xlO
mm

quite low relative to most matrices of economic data.

Hence,
22.5, a number

TABLE 2

Variance

-

Components

Modified Bauer Matrix

xa2
Var (b1)

a

.103 x

a2

.107 x

a3

.534 X

a,

.166 x

a5

.172 x

l0

l0

=.548 x io_2

Sum

Var(b4)

Var (b5)

.366 x

.142 x

.354 x

.401 x

l0

.126 x l0

.128 x

.319 x

.267

io2

.929

.144

io_29

.361 x

i030

.351 x

io_48

.189 x

.151 x

l0

.604 x

10

.129 x

.309 x l0

.465 x

.116

i024

.275 x io_2

.945 io2

.465

.116

io24

10

ia2
i046

Var(b3)

Var(b2)
.240 x

io24

In order to get the actual variances and variance components, each of the
2
figures of Table 2 must be rruitiplied by a , the variance of the error term in

the linear model y X +

e.

But, at least on a relative basis, it is clear

that the high proportions associated with a5 are reflecting massive sizes for
2
24
the order of a x 10 ,

while

var(b4)

and

reflect

smaller variances on the order of a x 10—2 .

figure

var(b5)-on

is small

in fact depends,

tl-se associated with a3

2

Whether

this latter

2
of course, on the size of a

2.3.3 A Suggested Test for Harmful Collinearity
High variance proportions, then, in themselves are not sufficient

to

reveal the existence of harmful ôbllinearity--for, as the preceding example

shows, the high proportions may not be associated with a singular value that

has been determined to be small enough (in the sense of Section 1.3) to indicate

rank deficiency. Such is the case with

the high proportions associated with

a.

5

a ,
arid

however,

has been

determined

to be

associated with

a linear dependency,

its high variance proportions indicate collinearity to be harmful.
It is suggested here, then, that an appropriate means for detecting

harmful

collinearity is the double condition of

1) high variance proportions for two

2) a single singular

value

or

more variances associated with

determined by the methods

of

Section 1.3 to

be small and hence evidence of rank deficiency.

2.3.

Multicollineari-ty as a Practical Problem
Whether

consequence
that

the

variance

multicollinearity turns

out

to be a problem of practical

is a different question from that addressed above. It will be noted

test for harmful collinearity suggested above wholly ignores the error

2, 1

2

a that also enters the relation Var(b) =a (X XY .

terms cancel from the variance proportions

of (2 . 6b),

Indeed,

the

but they are a factor

in each of the entries of (2.6a). It is possible, then, that collinearity

resulting in high variance proportions 4, and indeed high components
2

be rratigated by low a ,

K

for,

can

2

from (2.14) and (2.5), var (bk)

a k where

k j1jk In such a case, the actual variances may be small enough to allow
acceptance of all desired tests of hypothesis, in spite of the fact that the
precision of the least squares estimates would be better in the absence of ill-

conditioned data. In other words, the presence of multicollinearity as determined here, need not be problemful as a practical matter.1 The test suggested
1 Another view of this point is useful. It will be noted that the entire
analysis of collinearity presented here is based on the data matrix X in

the linear regression model y X + c and no where requires knowledge of y.
This is because ill conditioning, and the instability of calculations arid
estimates that result from it, has only to do with X, and one would be
better off with a nicely conditioned X matrix whether or not the ill conditioning is bad enough to cause practical problems. It is the latter
point that depends upon y, for only through the introduction of y can a2
be estimated in order to determine if the overall levels of the estimated
variances are too high for conducting desired hypothesis tests. If they
are, and ill conditioning can be determined as a problem, then corrective
action

is rthwhile.

here, however, highlights when estimated variances are being adversely affected

(whether to a point of being problemful or not), and hence indicates when and
where such variances could be improved should the need arise through the intro-

duàtion of additional inforution that "breaks up" the ill conditioning. This
point will be discussed further in Part 3.

_LL_

Part 3.

Some General Considerations on Multicollinearity

and Its

Corrections

It is not the purpose of this paper to suggest an answer to the third

ques-

tion raised in its introduction: that dealing with corrective measures. However,
some general remarks on multicollinearity and its correction seem called for.
Section 1 of this third part examines other tests for multicollinearity that

have been proposed. Section 2 discusses corrective procedures and presents a fundamental criticism of the use of non—Bayesian ridge regression as a means of correction.

3.1 Other Tests for Multicollinearity

3.1.1 Simple Correlations
The use of simple, pairwise correlations as a means of showing the
presence of multicollinearity has been so basically discredited that it

seems

hardly necessary to mention it. However, the technique appears to flair up anew

with
In

some regularity, and

seems to require constant care to keep it extinguished.

favor of the procedure,it must be said that

the

existence of two

with correlation +1 is a clear indication of multicollinearity and

variates

therefore

it

would seem that "high" correlation would be problemful. But a correlation of .9

need not result in any real problem of estimation. The test is, therefore,
without proper interpretation, for there is no well defined notion of "high".
Conversely,

low correlations are no indication of the absence of multicollinearity,

for three or more variates may be perfectly collinear but have low pairwise
correlations.

Examination of the correlation matrix,

therefore,

worst, erroneous and, at best, misleading information.

offers, at

_Lt 5—

3.1.2

The Determinant

of

X' X

Another discredited test for multicollinearity is the value of
det XtX.

Since X singular inplies det

X'X

0,

the

motivation is clearly that

low det X 'X indicates near singularity. The problem with this notion comes from

the fact that nonsingularity-singularity is not a contini.mi. This is readily
seen by considering the obviously nonsingular nm matrix A aI
the determinant of A (
sufficiently

3 • 1.3

c11) may

be made as small as

snail, but equally clearly A is

for

U>0. Clearly

desired by choosing c

always perfectly invertable.

Method of Farrar and Glauber

Farrar and Glauber (1967) suggest determining the presence of multi-

collinearity based upon a statistical test of the hypothesis that the columns of X

are in fact orthogonal. A rejection of the hypothesis leads to the alternative
hypothesis that the columns of X are nonorthogonal, arid hence collinear. There
are several weaic-iesses with this approach, both theoretical and applied.
1)

Th€. FarTar and Glauber approach is based on the assumption that

the X data resulted fran sane stochastic process whose orthogonality is subject

to test. If the X data are properly assumed as nonstochastic, however, (as they

are in the classicial linear model) the Farrar-Glauber analysis is irrelevant.
2)

does
To

If the X data are

not apply, but it

is

assumed stochastic, the previous consideration

still doubtful

that the Farrar-Glauber technique is

proper.

see this one must realize ti-at multicollinearity is a condition when sdme

linear canbina-tion of the data are observationally indistinguishable from zero,

and as such multicollinearity is seen to be a special case of the identification

problem. As is well )<nown, identification is a problem logically preceding, arid

not a part of, the statistical problem of estimation. Multicollinearity, then,
is not an estimation problem and is not properly treated as such.

3) As a practical matter the test against the null hypothesis of
orthogonality seems to lack power; that is, it indicates nonorthogonality
very often when
with

strong

general

cane

there is no real

t' s).

This

problem (all coefficients are alive,

practica1

inappropriateness of

well arid

problem is not surprising in light of the

the technique. Haitovsky

(1968)

attempts

to over-

this practical problem of Far'rar and Glauber by making the test against the

null hypothesis of singularity. Haitovsky' s procedure, however, falls prey to
the same criticisms advanced above.

3.2 Crrective Measures
3.2.1 The Introduction of Identifying Information

The recognition above that multicollinearii:y is an identification

problem has implications not only for the proper way to test for it, but also
for the proper way to correct it. A multicollinear data set results in an
unidentified equation. As is well known1,

it requires the addition of new,

independent information to identify an unidentified equation. As we shall see
below, the use of ridge

regression

as has been suggested by some fails to add

identifying information and, indeed, fails to remove the estimation problem that

results from colliriear data. Two methods
1

See Fisher (1966).

have been suggested, however, that can

_Ll7_

properly introduce additional information, and hence stand as appropriate correc-

tive measures. These are the time-honored methods of using outside estimates

(such as cathining estimates of coefficients in a time-series equation previously
estimated from cross-sectional data), and the method of using a Bayesian prior

for the coefficients. The former method has the practical weaa'iess that it is

very difficult to find "outside" conditions that are appropriate to obtain
estimates for the given situation. A marginal propensity to consume, for
example, determined from cross-sectional budget studies has dubious relevance
to a time-series estimated consumption function. The second method, proposed

in Zeilner
3.2.2

(1971) and

Learner (1973),

has

much promise.

The Failure of Ridge

Attempts

have been

irade recently to utilize ridge regression to miti-

gate the effects of multicollinearity) Short of a meanìs of combining this
procedure with some method of

bringing

however, this method is doard

in legitimate

information ,2

identifying

to failure--merely substituting

collinearity in

the data for a degenerate distribution of the estimated coefficients.
We begin with the usual normal equations for least squares

X'X

(3.1)

b X'y

and we assume X to be rank

deficient. The suggested ridge solution

an invertable matrix by constructing and solving the
(3.2)

where Q

1

2

(X'X +

is

k)b*

is to create

ridge equation

X'y

some positive definite matrix--often taken as I, and

b*

is the ridge

See, for example, Bushnell and Huettner (1973), Hoerl and Kennard (1970).
Such, for example, as is done by Holland (1973) in which he caribines ridge
with a Bayesian prior.

does exist---arid the

estimator. k arid Q are taken so that (X'X + kQYa

is that b* is

sumption

now solvable and uniquely so as

b (X'X + kQ)X'y

(3.3)

Unfortunately,
Var(b*)

that

pre-

to

amenable

this trick does not solve the problem

is singular, i.e., b* has

proper

niator b from

hypothesis

0 such that Xy

degenerate distribution arid

testing than

since X

is

is

no more

the nonuniquely defined OLS esti-

is rank deficient,

there exists a non-

Hence (3.2) becomes

0.

(X'X + kQ)b*

(3.')

it is readily shown

(3.1).

To see this, note that,
trivial y

a

for

Xy

0

or
(3.5)

C'b*

where

C'

Clearly
samplings.

for

(X'X + kQ)

C depends only on X (k fixed), arid

(3.5)

estimates b*, and
This

0

therefore implies a

renders

fixed

hence

linear

remains

fixed

restriction on

in repeated

the ridge

them degenerately distributed.1

exercise serves to highlight the

point

made above regarding the

identifying information. In multicollinearity,

as

need

strongly as anywhere else,

you cannot get something for nothing. There is something about rnulticollinearity
that brings out the alchemist in econometric:Lans, but there is no way one can

squeeze, stamp or club more out of the data than was there in

the

first place.

If several variates are all giving the same information, yoi cannot make them
speak differently simply by looking at them from a different angle. Only through
the addition of new, independent identifying information can the confounded effects

of collinear data be undone.
1. Again, combining ridge with
this problem.

a Bayesian prior as in

Holland (1973)

solves

—49—

REFERENCES

Beisley, D.A. [1974], "Estimation of Systems of Sinui1taneus Equations, arid
Cont:ationa1 Specifications of GREMLThP', Annals of !oonomic and Social
Measurement, October.

R.C. and D.A. Huettner [1973], "Multicollinearity, Orthogonality
and Ridge Regression Analysis", Unpublished mimso, presented December 1973
Meetings of Econometric Society, N.Y.

Bushnefl,

Farr, D.E. and R.R. Glauber [1967], "Multicollinearity in Regression Analysis:
The Problem Revisited", Review of Economonics and Statistics, February,
pp. 92—107.

Fisher,

F.M. [1969], The Identification Problem.

Haitovsky,

Yoek [1969], "Multicollinearity in Regression Analysis: Comnnt",

Review of

Economics and Statistics,

Hoerl,

November, pp. 486—489.

A.E. and R.W. Kennard [1970a],, "Ridge
for Nonorthogonal Problems", Technome

Regression: Biased Estimation

trios, No. 1,

pp. 55-68.

R. W. Kennaxd [1970b], "Ridge Regression: Applications to
Nonorthogonal Problems", Technometric8, No. , pp. 69-82.

Hoerl, A. E. and

Holland, P.W. 1973], "Weighted Ridge Regression:
Regression Methods", NBER CRC Working Paper No.

Combining Ridge
11.

E.E. [1973], "Multicollinearity: A Bayesian

Leaner,
of Economics and Statistics, August,

and Ibust

Interpretation", Review

pp. 371-380.

Silvey,

of

S.D. [1969], "Multicollinearity and Inrecise Estimation", Journal
the Royal Statistical Society, Series B, Vol. 31, pp. 539—552.

Stewart:, G.W.

[1973], Introduction

to Matrix Conrputations.

Van der Sluis, A. [1969], "Condition, Equilibration and Pivoting in Linear
Algebraic Systems", Numeriache Mathematik, 15., pp. 74-88.

Wilkinson, J.H. [1965], The Algebraic

Eigenvalue Problem.

Zellner, A. [1971], An Introduction to Bayesian Inference in Econometrics.

