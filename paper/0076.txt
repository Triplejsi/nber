NBER WOR1NG PAPER SERIES

ROBUST NONLINEAR REGRESSION
USING THE DOGLEG ALGORITHM

Roy E. We1sch
Richard A. Becker*

brking Paper No. 76

COMPUTER RESEARCH CENTER FOR ECONOCS AND MANAGfl€NT SCIENCE
National Bureau of Economic Research, Inc.
575 Technology Square
Cambridge, Massachuset-ts 02139

March 1975

Preliminary:

NBER

not for

quotation

r]dng papers are dis1'ibuted iriforlly arid in limited

numbers for comments only. They should not be quoted without
written penniss ion.

This report has not undergone the review accorded official NBER

publications; in particular, it has not yet been suhnit-ted for
approval by the Board of Directors.

*NBER Computer Research Center and Massachusetts Institute of
Technology, Sloan School of Management. Research supported in
part by National Science Foundation Grant G.J-1154X3 to the
National Bureau of Economic Research, Inc.

*Bel1

Telephone La.boratories. èsearch supported in part by
Science Foundation Grt GJ_l15LX3 to the National
Bureau of Economic Research, Inc.

National

Abstract

What are the statistical and canputational problems associated with
robust nonlinear regression? This paper presents a number of possible
approaches to these problns and develops a particular algoritl-nn based

on the rk of Powell and Dennis.

Contents

..l

1. Introduction
2.

The Prob1n

1

3. What Can the Average Man t?

Nonlinear Reweighted Least-Squares

1
2

5.

eating Specialized Algorithms
6. Robust Nonlinear Reession
7.

Starting Values

8.

Scale Computation

9.

Confidence Regions

10. Elininating Secor1 Derivatives
11. Examples
12. Concluding Rnarks

13. References .

Tables

Table 1. Marketing !bdel Data

7

Table 2. Marketing bdel Results

8

Table 3.

8

Test Function Results

1)BUS NONLINEAR REGRESSION
USING THE E.OGLB3 ALGORITHM

1. ThTDUCrION

In recent years the concepts of robust estimation
have lead to a rethinking of the ways we fit rredels
to data. Papers by Beaton and Tukey [1974] and
Mdrews [1974] have proposed algorithms for rdust
linear regression using iteratively reweighted
least-squares. This technique has proved to be
quite successful and has considerable intuitive
appeal because of its connection to weighted leastsquares regression.
In late 1973 the authors designed and izi1nented

For nonlinear least-squares (o(t)=t2) we have
always faced the problem of specifying starting
functions such as

values. For robust loss
pc(t) =ç

ct--.

p(t)

tem at the NBER Computer Research Center. It

of requests to provide similar facilities for non-

linear problems. In what follows we discuss
several possible approaches to robust nonlinear
regression, outline a few successful algorithms,
and discuss our experience with them.

that we are interested in fitting the nodel

,... ,O
f(e)(f1(o),.. ,fe (0)),0:(G
shall sek to d
y:{y1,.. .
.

,y)'.

from some start

respect

to 8)

(O) towerd

of

F5(O)
wher.e p(S)

is

} to the data

a local

this by
minintun (with

(y1—f(e)\
il "c\ s

)

(2.1)

or symctric).

also need a way to
haps a better

as

measure

the scale (size is per-

context) of
beginning of the

word in this

the residuals,
r(e):y-f(o), at the
computation
as the computaand, in sane cases, to remeas'e
tion continues. We Tru.ist also choose c, the robustness parameter, or at least provide ways to indicate

it

the effects of changing c.

note that neither (2.2) nor (2.3) have a second
derivative ever#where. There are approxfrations to
these functions which do (e.g. the bisquare of

Beaton and Tukey (1974]) but having a second derivative everywhere has not proved to be practically
the algorithms we shall discuss.
important

for

WMTCANThEAVERAGEt'AND3?

We have found that many people

have .iccess to sorrn

form of general nonlinear optimization program ardf

assumed to satisfy p( t )
p( u
often viewed
a loss function
if Iticlul
(which, in general, need not be independent
i

and is

not only need a starting value but, because
these loss functions are not scale invariant, we
we

3.

/
n

Jtf>

We

2. THE PROBLBI
Assume

(2.3)

2
2c

makes

After sane economists and management scientists
had worked with this macro, we received a number

(2.2)

ft>

c2C1—cos(t/c)3 Itk en

a robust linear regression macro on the TROLL sysuse of reweighted least-squares, iterative
scaling, optional starts including least absolute
residuals, and provides a robust trace of the coefficients as a "robustness parameter" is varied.

Itlc
2

of

or a special routine for nonlinear least-squares.
bst of the researchers interested in robust f itting are not interested in extensively rrcdifying
these programs or writing new ones. So we discuss
first sane approaches to robust nonlinear regression

that allow the use of existing programs.
If we assume that a general nonlinear optimization
rutine is• available then it seems reasonable to

try to estiaatc the scale, s, by making it a part
of the optimization problem,
n
nu.n

' +Slogs

E

8,s i1

fri

direct

(3 1)

analogy to the related meximum J.flelicod

is also imnediatelyclear that this
idea will fail for robust loss functions which are
bounded (such as (2.3)) because s will be forced
to .0. (There is no proper axirrtum likelilood todel
in this case.). However for loss functions of the
problem. It

fonn (2.2)

this

is

not true

and (3.1)

is viable.

in a

The constant S2 can be chosen
variety of ways,
one of which is the following. Differentiating
(3.1) with respect to s and setting it equal
0
we obtain

to

S

(rj(e)r1(e)

n
1

S

(3.2)

We must, of course, specify B and B . If there is
prior information abut the sale thin B1 arid B2
would be taken from that. Otherwise we would
choose B1=S1 and B2

by penalty function considerations such as those discussed by Bard [1974, p.l45].
Our experience with this method is limited and not

wholly satisfactory.

The above tlree methods provide ways for a person

with a general nonlinear optimizer to simply put in
an objective function different fran the one for
least-squares arid use his program as is. We do not
advise

doing this blindly but it generally works
(especially the first two methods). It has drawbacks. It is expensive because another parameter
Cs) must be estimated (with algorithms of order p2)
and the objective function is more complicated and

numerically less pleasing. When choice is available,
it does not seem reasonable to pay so much for the
privilege of iteratively modifying s.
another obvious approach. That is simply
to find an (°) (see section 7) arid then minimize

s

(n-p)_L tp(t) d(t)

Huber arid

scale parameter.

There is

If the residuals were Gaussian then we might try to
choose S1 so that
would be asymptoticially unbiased giving

standardized

where •(t) denotes the
tribution function.

In effect, we have added the negative log likelod
of an inverted gaim prior distribution for the

Gaussian dis-

(2.1) with (°)•

At the end of this computation
a new s, say (1),
computed based on the current
values of 0, then left fixed
a new local
minimusn is found, etc.
less
than, say, ten percent from 50c). This
simple,
but often proved to be as expensive as the earlier
does, however, lead us to consider
ways to hardle iterative scaling without making use

approaches. It

Cl9714] have suggested a related
idea. They propose replacing (3.1) by

n

e,si:1

pct is

available it is

C\ /

algorithms
adapt the iteratively

The

What about the bounded loss function case? It is
natural to consider a penalty function to keep s

For example

n
E
O,s i=1

(r.(e)\

of

F5(e) are

- - J(0)

We have tried (3.1) and (3.3) using a general optimization algorithm to be described later and found
that both work about equally well. Bath can be
implemented very quickly.

mm

gradient and Hessian

(4.1)

(p(t))2/2

and the normal equation for s reduces to the scale
estimate proposed by Huber C1964, 1973]. This idea
also fails for bounded loss functions.

positive.

natural to attempt to

reweighted least-squares idea
mentioned earlier to the nonlinear problem. We now
discuss this method in more detail.

(n—p) I tp(t)—p0(t)d(t)
the Huber type, (2.2), then

tP(t)—p(t)

4. NONLINEAR RIWEIGHI'ED LFAST-SQUARES

For those with special nonlinear least-squares

(

where
S2

until
until s(U changes
is

of the objective function.

Dutter
mi.n

is

s

fr.(o)\ v2f.(e)
(\
)

n

.H5(e) =

il

—pt

+ JT(0)
where J(0)[3f(0)/a9] is the Jacobian Tratrix,
v2f1(e)=t2f1/aekae],
p'(r(0)/s)]and p'T(r(O)/s) is an nxn diagonal matrix.
Now define a weight function

p'(r(0)/s)[p'(r1(o)/s),...,
w(t)p'(t)/t and an

approximate Hessian
+

B1

log

s + B2/s.

(3.4)

(4 .2)

HS (0)

I

-

SLj=

'

w.r.

V2f. +

w

-

—3—
DI
where wj:w(—),w is an nai diagonal matrix,
o" (t) is appvxjmated by w( t). If we have

and

start-

irig values (°) and s:s(O) then the first step of
reweighted least-squares is
+

which

In the

linear

(O) +

case

((O)) g3(e)

It is not at all clear how these changes at each
step will interact with a specialized algorithm
like that of Marquardt. Using this routine with
its

Successful. Direct intervention in the algorithm
is required, it cannot just be called each tine.

(Y—xe°)

The whole procedure can, of course, be iterated.
Except for the fact that p"(t) has been approxinat-

ed by w(t) this is just the first Newton step for
the solution of (2.1) in the linear case. Using

w(t) makes H positive semi-definite and makes the
analogy to weighted least-squares obvious.
A word of caution

so how? Since starting values in non—
linear problems are generally not good, we feel
that w (0) and $ (° are crude and will need itermicn.

special start—up procedures to do each step
after computing new weights will not be very

is

xTx_l xT

along and if

is in order. Even if XX

well
conditioned, XTWX cay be very ill-conditioned arid
is

the first Newton step a very poor one. (This can
which contain all of the information about a parameter or infornation about how to separate the
effects of two carriers.) Even if XTwX is well
happen when there are low weights on observations

behaved at a local minijrn.izn a bad start can lead to

poor results. Often this problem is ignored in the
linear case because of the availability of robust,
scale invariant procedures, such as least absolute
residuals [Bar'rodale and Roberts (1973)] to provide
starting values. AU of the literature about

Clearly such modifications can be made but we show
note that Chantbers [1973, p. 7) indicates that such
iterative procedures nay be inferior to general
Optimization methods.

5.

CREATING SPECIALIZE) ALORITHIIS

If one is willing to

in an

intervene more directly

optimization algorithm then sane special things can
be done to acccrnodate reweighting and scaling. We

shall discuss our efforts
particular algorithm.

in the context of

a

In the past year, work at the NBER Computer Researc:-

Center has created a need for nonlinear optinizaticn
in such diverse areas as full information maxizin.
likelihood estimation, probit analysis, end projection pursuit (Friedman arid Thkey (1974)]. The

first algorithm implemented was
1LEGF, developed
by Chien and Dennis at Cornell. This algcrithm

only requires information about the function F and
is closely related to the MINFA algorithm of Powell
[1970] which, however, requires the gradient as
as F. DCGLF was installed in the NB. TROLL
regression should contain some diagnostic studies of well
system as a function and is riot easily modified
the data matrix to determine potential high leverexcept by experienced progranmers.
age observations. Varying the robustness parameter
c can also be very useful. Ridge regression
In the TROLL system we also had a snbolic differentechniques could also be employed.
tiator arid a proposed way to automatically ccmpile
F ,g, and H into very efficient code suitable for
In the nonlinear case we do not have such good
repeated evaluation. We also have a macro language
techniques for finding starting values and the
that allows a user to glue together various TROLL
first term of the Hessian does not vanish. But
caimarids and functions in a way that cakes it easy
most nonlinear least-squares routines ignore the
to experiment with new algorithms. With these
first term of the Hessian and use a technique like
ideas in mind one of us (RAB) in consultation with
that of Marquardt (1963) to overcome the difficul- John Dennis developed the DOGLDX
algorithm and
ties of auss-Newton steps away from the local
macro.
Since
this
algorithn
formed
the basis for
minimum. Once in a region where the residuals are, further researoh (by REW) on robust nonlinear
hopefully, small the first term of the Hessian can regression, we will describe it in detail.
be more safely ignored. Some work has been done
on the large residual least-squares problem, e.g.
rCGLEXX utilizes a combination of
Dennis (1973]. Robust loss functions help to
and Newton steps in the process of steepest-descent
minimizing a
reduce the size of the first term of the Hessian
function. As long as gradient steps are relatively
because p'(t)l<It for large residuals arid with
large, they are used. However, since gradient steps
(2.3), p'(t) is eventually zero.
tend to perform poorly in valleys, Newton steps are
also
used. Newton steps, however, are of doubtful
With the same caveats we have always had in using
worth when taken from a point far removed fro.-n the
Gauss-Msrquardt nonlinear least-squares routines
minimum. Hence, the algorithm uses a bound on the
for very nonlinear problems, it is reasonable to
maxiznun step size and provides a canpranise 'ogleg"
propose that (2 • 1) be attacked by finding starting
step which combines the gradient and Newton steps.
values
and
forming the weights and
solving the least squares problem with pki y and
As input ECGLEGX requires only the function (all
/T f(e) as data and model, making the obvious
derivatives are computed symbolically), start ing
change if there is a weighted nonlinear leastvalues e (0), an initial radius (R) to provide zn
squares routine available.
upper bound on step size (the default is zero
Newton-Raphson methods applies to this problem such methods are only reasonable in a neighborhood
of a local minirmsn. Good algorithms for robust

We now ask - should we modify w and s as we go

which makes the first step a gradient step), the
maximum number of iterations, and convergcnce tolcr'--

ances for the gradient and relative coefficient diar.c.

If the slope of this line is negative, then A*<0.

Initially the expressions for F, g, and H are
evaluated. H( 0) is then forced to be positive
definite by the use of a Greenstadt modification.

If the slope is positive, A>0. When X'<0 or X2
a step of twice the length of 6 (k) would still have

decreased the value of the objective function if
S(A) really were linear. In these cases R is

This procedure is carried out whenever the second
derivative matrix is reevaluated.

doubled.

At the beginning of each itéDation, there is a test
for convergence using both the gradient and the
relative change in 0 from the previous iteration.

details will not be provided here.

The exact

If OA'<2 one more check is performed. The predicted gradient at
is compared with the actual gradient g(o (k) +6(k))

Assuming that there were no convergence, the algo-

and if

rithm investigates a step in the direction of the
gradient vector. Define

2

I 1g(8(k)+6(k))_g(Ø(k))_M(e(k))6(k)

Ak(6)
where

(a ,b)

.25
denotes

an inner product. The function

Ak(6) then i&a quadratic approximation to F(e()+
based on the gradient

Powell [1970] shows that Ak
gradient

)

vector and the Hessian.
is

minimized along the

direction by a step of length

— ___________________________
—

G

At

(g(0(k)), H(O)g(e))
this point, the step-bound limitation is checked.

If cR then a step in the gradient direction of
length R will be tried. Let 6 represent the

6N' R the Newton step
is attempted. If !<R and I6N I>R a. "dogleg" step
is attempted. The dogleg step 6D is defined as
the point on the line connecting 6, (the gradient
at a distance R from 6(k).
step) arid 6 which

Newton step. If tG<R and

6

the step bound, R, is doubled. In a.ll other eases
the step bound remains the same for the next iteration. Iterations continue until convergence is
reached or the limit on the number of iterations is
exceeded.

used for testing the ideas developed in
section 3. It is an algorithm that invites tinkering (ellipsoids instead of spheres for the step
bounding, quadruple instead of double the radius,
etc.) arid the macro (interpretive) form has permitted this kind of modification, often for specialized
purposes. In particular it permitted us to experiEXDGLEGX was

ment with a number of ideas for robust nonlinear
regression.

is

At this point

let () represent the step that the

algorithm decided to take (gradient, Newton, or
dogleg). If F(0(k)+dOc))<F(0(), the step is set
accepted and we set o(k+1)O(k)+6, otherwise
(k) halve
iteration.

the radius, R, and start a new

of the most powerful features of
volves revision of the step bound R.
One

EOGLEGX in-

If the step

accepted, a test of the approximation Ak(6)
is performed. If the predicted reduction measured
by F(O (k) )_Ak(6 (k)) is more than ten times the
actual reduction, F(0 (k) )—F(O )+SOC)), the radius
is

is halved

and

a new iteration begins.

If this test is passed we perform further checks
to decide if the step bound should be increased.
In order to do this we look at the sca].ar product

S(A)(g(eO)+A6)),6). The term

6. ROBUST NCNLINE.R REGRESSION
Since

EOGLEGX canputes the true Hessian we had r.o

need (at this point) for reweightirig as a way to

solve our problem. We did however have to consider

rescaling arid the LXJGLEGXS macro was developed by
one of us (REW) to accomplish this.

LOGLEOX is complicated by the fact that after it
has found an acceptable step it looks ahead at the
new gradient
bound radius

function.

At this point we compute g(O .+dl'J) so that we
have S(O) and 5(1) available. If we assume S(A)
is linear, these two points define a line and we
let X" be the point where S(A)0 i.e.

—

S(0)

if

to see if it should increase the step
for -the next iteration. The question

we are changing the scale, at what
point in a step do we change it? Our discussion
of this problem is meant to be indicative of the
kind of problems that can arise in modifying nonlinear optimization algorithms for specialized
applications like robust regression.
to compute a scale
In DGLEGXS we use

arises -

0) = median
i

defines a line from (k) in the direction (k)
S(X) measures the expected change in the objective

function starting at the point O(k)+X6(k) and taking a step (k) We would like this change to be
negative, decreasing the value of the objective

2

(Irj(0()I)/.67t45

Sections 7 and 8 contain a discussion of starting
and other ways to compute S. The algorithm
been deter—
proceeds as in EOGLEGX
mined. Still using sP(1 (0(k)+6( /) is evaluated
is an acceptable step.
and checked to see if
If
If the step is rot accepted
accepted we do not yet change s. The test of the

values

yntil 60 s

it is

approxinution A'(6) is performed as before.

Thus in cases where the radius can be reduced we
do not change s before performing these tests.
This costs us an extra evaluation of F (we

shall

—5—

to eventually evaluate it with a new s) but it
is conservative in the sense that chinging s here

have

Cs generally decreases) would cause us to more

often reduce the radius. Reducing the radius is

costly because to increase it again we must compute

It has performed satisfactorily but requires some
form of nonweighted starting scale becaue
is not defined. All of the results reported below

use the MAD scale.

a new g and H, but if a step is not acceptable, no

9. CONFIDENCE REGIONS

new g and H are necessary to reduce the radius.

If the step has

Since

been acepted, we now compute

5(Jc#1) and proceed

increaed

assuming,
was passed

to see if the radius should be
of course, that the test using
(i.e. the radius was not

reduced).

tests for radius increase are thç sane as
before but thern new gradient at 0 (1hj is conputed
with
A number of tests run using s
instead of (k#1) ]-ere gave indication of being
The

there is not yet general agreement about how
to compute covariances for the estinated coe.ffi—
cients in robust linear regression, we cannot hope

to give very definitive results for the r.cnlinear
case. Gross (1973) has proposed a way to find
confidence intervals for robust location estimates.

A partially completed Monte Carlo study by Paul
Holland, David Hoaglin, and Roy Welsch indicates
that a reasonable covariance estimate for robust
linear regression would be

better or worse, but were irnich sore expensive

since the gradient had to be evaluated twice (with
s (k) and s (k+2)). The next iteration begins using

w. r?(0)(XTwX)
1

n—p i1 1

00C42) and (h1

7. STAR'rflG

where

VALUES

How to start a robust nonlinear regression is not
an easy problem. A scale free start would be nice
but least-squares is the only readily available

one and, of course, requires a start itself.

an L ,lcpc2, start would work, but we have
not tried it. We could also linearize the
the supplied starting values and then
problem
use least absolute residuals to get a revised

the w. are the weights used to obtain 0 in
the final ieration of reweighted least-squares.
The associated t-statistics would probably be
based on an equivalent number of degrees of freedom

like.1

wi-p.

An obvious extension

(Perhaps

at

.

start.

We have often found that the original
values specified by an intelligent uodel builder
can be used directly
robust loss function

starting

in a
c chosen so that the asnptotic efficiency at
the Gaussian is say, .8, i.e.

with

2

o(t) d(t)]
I [p(t)]2 d(t)

jL

w1 r(0)

to the nonlinear problem
(JTWJ)l

(9.2)

It is useful to see what type of covariar.ce forsla
arises if we attack the nonlinear problem directly.
To do this we follow Bard (1974, p. 176) and argue
that we want to examine the effect on the solution

O of perturbations in the residuals at e'. Eard

j

gives the approximate covariance (in our

(See Huber (1973) for a discussion of asymptotic
efficiency.) For (2.2) this means c is about .67
and for ç2.3) about 1. Too low a value of c can
throw away a lot of data (low weights) if the
start is poor and too high a c does not downweight
large residuals enough. We see little reason to
perform a least-squares analysis first, although

we may want to do this at sane point in studying

the data.

8. SCALE COMPUTATION

We have used a median absolute deviation (LAD)

scaling adjusted so that it will be unbiased for
Caussian

is

where J and w were used to obtain 0. This, of
course, has been used in nost weighted nonlinear
least-squares programs where the weights are
assumed to be fixed.

as

independent

(9.1)

residuals. In order to allow

j

v0

notation)

(9.3)

where V is the "coyarianc&' matrix of the residuals
at ea a&L we have replaced p" by w. One estirrute
of Vr would be r(0)rT(0*). Various other formulas

arc possible and some have been explored by Thkey
(1973). Until sore information is available, we
prefer to take the approach that (9.3) is ccnditior.ed

on the weights, set Vr2w1

n

E w.r.(O*)
i=l
11

and estimate 2 by
(9.4)

for a rwe asyrs.etric set of residuals, reduce the
In cases where we ignore the first term of the
"granularity" of the median, and reiove from the
then reduce to (9.2). We have
scale computation very large residuals we also tried Hessian, (9.3) would
mainly relied on (9.2) especially because robust
loss functions tend to reduce the size of the first
i:11 1
term of the Hessian (cf. section 4).
)

w.r?o0

ii

1

—6—

10. ININATn.G SECOND DIVATIVES
Computing the exact Hessian is expensive even in

sophisticated systems. After the above algorithms

were developed using DOGLEGX as a base we replaced

the exact Hessian by JrwJ, i.e. we used a type of
reweighted least-squares within the context of the
IX)GLEGX algorithm with scaling. (Call this
algorithm IX)GLflW.)

However, as one might expect, this modified algo-.
rithrn does not work well on some types of highly
nonlinear problems. A compromje algorithm
(DOGLH) is now being tested by Dennis and Welsch.
In it, each of the two parts of the Hessian is

treated separately. The second part is always
catuted exactly (except for the fact that w
replaces p"). The first part is approxijtated and

updated using methods developed by Broyden Csee

Dennis,:(1973)Jto update the entire Hessian in
general optimization algorithms. This can be
accomplished in a way that keeps the Hessian
positive definite, removing the need for the
'eenstadt modification in IY)GLEGX.

of standard problems, but we present here an
example from marketing which arose in joint work

is

SALES(t) :

We note that y is highly sensitive to changes in c
and further investigation is called for, including,
perhaps, a change in model formulation. The leastsquares results are not listed because the algorithn
forced y to infinity (machine overflow) in that case.
A more detailed discussion of the mode], is contained
in Little and Welsch (1975].
In order to show how the IX)GLEGXS algorithm per—
formed on synthetic data we used the function
(see

Chambers (1973)]

y:e

—01x

—6,x

+erDor
where 01 arid 02 had tnie values of 1 and 10, ten
—e

observations were taken for x.l(.l)l, and the error'
was contarithiated ussian with 75% fran J(0,.l) arid
25% from N(0,l). The convergence criterion consisted
of having the length of the gradient less than .1 and

All canputatior5 i'e started at Oo and 02:0. The
results are listed in Table 3.
12. CONCUJDThG RU'tR1

we are trying to

We hope that the above discussion will stimulate
R0. STREND(t). PROM.MOD(t). ADV.MOD(t)

srRnlD(t)

SESON(t).TREND(t)

PROM.ZD(t) =

i+B1.PROM(t)_B2.ppjM(t_)

a.i

ADV.MOD(t) =

i=i
with

I

[Cl+ 1+ (K ADV(t—i+1) )Tj

K .0041
: .88

.32
.22

2

3.

function

Barrodale,

I. arid F.D.K. Roberts (1973). An

SIAN J.

Numer. Anal. 10, 839-848.

4. Beaton, A.E. arid J.W. Tukey (1974). The Fitting
of Power Series, Meaning Polynomials,
on Band-Spectroscopic Data. Techriometric 16,

Illustrated

.2

147—192.

the first twenty-fota'

of type (2.3)

Nonlinear Parameter Estj'riation.

Improved Algorithm for Discrete L1 Approd.mation.

estimate

observations of the data in Table 1.

Using the loss

Bard, Y. (1974).

Academic Press, New York.

Bi=i

Al]. estimation was dome on

1. Andrews, D.F. (1974). A Robust Method for
Multiple Linear Regression. Technometrics 16,
2.

C2 : .24

B2 =

13. RflICES
.523—531.

and starting values of what we want to
R0=538

statisticians to consider' the types of algorithms
they would like to see developed for a flexible nonlinear fitting package which would include robust
loss functions. We also hope that numerical analysts
will consider the problems that arise in this area,
including large residuals, weights, and the role of
special parameters such as scale.

C2(K.ADV(t—j+].))
—

.46
a2

and JX)GLEGXS

we started the series of computations with c:l using the

wi-p) and the

.001.

The above algorithms have been tested on a number'

calibrate

the "corrected" degrees of freedom (
regular degrees of freedom (n-p).

the maximum relative coefficient change less than

II. ALS
with John Little. The model

the final value of the (adjusted) MD scale (s),
the weighted least-squares scale (ws) as given in
(9.4), the number of evaluations of F g, and H,

given starting values, and then used the

results at c:l to start the computation for c:.8
and c:i.5 corresponding to asymptotic efficiencies
of

about 50 percent and 95 percent at the Cussian.
The standard errors (using 9.2) are given below the
coefficient estimates in Table 2. Also
are

listed

5.

Chambers, J.M. (1973). Fittin Nonlinear Models:
Numerical Techniques. Biometrika 60 1-13.

6.

Dennis, J.E. (1973). Some Canputaticnal
Techniques for the Nonlinear Least Squares
Problem, in Bryne and Hall, eds. Numerical
Solutions of Systems of Nonlinear A]gbraic
Academic Press, New York, 157-183.

uations.

7. Cross, A.M. (1973). A Robust Confidence 1ntcrvl
for Location for Symnctric Long-tailed Distribu-

tions. Proc. Nat. Aced. Scj. 70, 1995—1997

—7—

8.

Friedman, J.H. and J.W. Thkey (1974). A
Projection Pursuit Algorithm for Exploratory
Data Analysis. IEEE Transactions on Computers
881—890.

TABLE 1
MAR1ING MODEL DATA

2i.!
9.

Huber, P.J. (1964). Robust Estimation of a
Location Parameter. Ann. Math. Statist. 35,

ROW
1
2
3

73—101.

10.

Huber, P.J. (1973). Robust Regression:
Asymptotics, Conjectures, arid bnte Carlo.
Annals of Statistics 1, 799—821.

11. Iluber, P.J. and R. Dutter (1974). Numerical
Solution of Robust Regression Problems.
Research Report No. 3. Fachgruppe Fuer
Statistik.

'4

6
7

.

12. Little, J.D.C.

and R.E. Welsch (1975). Robust
Calibration of Nonlinear Marketing ?'bdels.
Unpublished manuscript. Sloan School of
Management, M.I.T. Cambridge, Mass.

-

SALES
677.475
407.716

PROM
0.

750.12
118.44

676.695

0.

507.60

418.784

1.01505

0.

529.228
960.094

0.
1.

90.24
81.78
902.4(

1.03375
1.13322

1.

ADV

STREND
0.95285

0.999951
1. 01806

450.273

0.

98.70

1.10791

8
9

508.651
872.330

0.
1.

177.66
454.02

10

354.248
406.859

0.
0.

1.08965
1.09695

14.10
45.12

1.05676

403.507

0.

177.66

0.66

0.83928

673.500

397.62

0.99085

0.34

115.62

0.

1.03967

138.18

1.05525

0.

129.72

1.05826

11
12
13
14
15

0.897005

13. }tarquardt, D.W. (1963). An Algorithm for Least—
Squares Estimation of Nonlinear Parameters.

16

483.164
518.784
437.880

17

554.404

0.

394.80

Soc. Indust. Appi. Math. 11, 431—441.

1.09082

18
19
20

861.341

1.

640.14

1.17766

468.277

0.

149.46

1.15123

568.979

0.

200.22

800.701
404.365

1.
0.

1.13209

21
22

239.70
0.001

1.13955
1.09768

23

'418.706

0.

81.78

24

394.388

0.

25
26
27

903.819
391.426

1.
0.

530.16
121.26

488.230

1.02885
1.07939

0.

290.46

1.09545

28

522.915

0.

974.980

1.09846

29

1.

177.66

679.62

1.13210

30
31

450.896

0.

245.34

589.634

0.

1.22210

104.34

32

1.19455

561.029

0.

592.673
896.882
379.735
414.965

1.17453

33

0.

112.80

J.

14.

Powell, M.J.D. (1970). A New Algorithm for
Unconstrained Optimization. J.B. Rosen,

O.L. Mangasarian, and K. Ritter Eds., Nonlinear
Prograinning, Academic Press, New York,
TROLL Experimental Programs, Robust arid Ridge
Regression, NBER Computer Research Center
31—65.

15.

Documentation Series D0070.

16.

Tukey, J.W. (1973). A Way Forward for Robust
Regression. Unpublished menrandum., Bell

Laratories

(Murray Hill).

34
35
36

1.
0.
0.

0.001

U5.62
121.26
138.18
5.64

0.931605

0.87156

1.18215
1.13860
0.966205
0.90384

—8—

TABLE 2
MKEI'ING MODEL RESULTS

.8

C

1.

1.29

B2

R0

1.5

(1.23)

.96
(1.34)

6.81
(9.88)

.44
(.06)

.53
(.07)

.48
(.06)

.22

.21
(0'3)

(.04)

(.03)

'491.

'499.

.23

514.

S

'40.8

48.3

39.6

us

27.2

37.5

'44.5

13.

24.

16.

#GH

13.

20.

16.

c.d.f.

12.5

14.6

15.6

d.f.

18.

18.

18.

TABLE 3
TEST flJNCTION RESULTS

c

.8

1.

1.5

IS

2.16

1

.75
(08)

(.09)

.84
(.19)

(1.19)

82

8.17
(1.17)

8.02
(2.65)

6.78
(2.09)

(28.85)

O

.76

18.49

s

.15

.15

.21

.52

ws

.06

.07

.114

.95

#GH

12.

18.

11.

16.

12.

17.

10.

16.

c.d.f.

3.6

3.8

4.68

8.

d.f.

8.

8.

8.

8.

