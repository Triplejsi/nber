NBER RKtNG PAPER SERIES

CERTAIN ASPECTS OF GENERPLIZED
BOX—JENICtNS MODELS

Richard

W.

Hill*

Wor]dng Paper No. 82

COMPUTER RESERCH CENTER FOR ECONOMICS AND MANAGflIENT SCIENCE

National Bureau of Economic Research, Inc.
575 Technology Square

Canbridge, Massachusetts

02139

May 1975

Preliminary:

not

for quotation

are dis'ibuted

NBER working papers
camients only. They should

This

report has not

not be

informally arid in limited rnmibers
quoted without written permission.

undergone the review accorded official
yet

in particular, it has not

for

NBER publications;

been suthiitted or apoval by the

Board of

Directors.

*

NBER

Canpute.r Research Center. Research supported in part by National Science
the National. Bureau of Economic Research, Inc.

Foundation &'ant GJ.fl5L1X3 to

Abs1act
We define a class of models that are generalizations of regression models
arid moving average-autoregressive tine series models. Then we investigate
the asympotic and canputatiorial properties of the maximum likelihood

estimator, with numerical examples. The main conclusion is that care must
be excercised when using simple approxinations to the covariarice matrix

of the estimates.

.

.

Contents

I. The Model

1

II. Estimation

5

III. The Generalized Box—Jenkins Stup

9

IV. Special Results

20

V. Approximations to the Covariance Matrix

25

VI. I&unerical Considerations
References

31

.

32

The main

aspects of

purpose of

Box-Jenkins

models:

specifically

tia1 nthods arid approximations to
We begin,

certain

the paper is to examine

however, by introducing

I.

we will examine conputa-

asymptotic

covariance matrices.

a nore general setup.

I} MJ]JEL

We will work with the following model.

Let 8 be a kxl vector

of paraJrters, m a twice differentiable function m: Rl<Z÷t, so that

m(B) is an nxl vector. V(o) is an nm synmetrip positive definite
matrix,

whose elennts are a function of the pxl vector,

0.

Our nodel is

Y m(8) +
where
SO that

c,

(1—1)

c 'bN(O, cy2v(8)),

if
V(o)

=

cv½e)J

V(0) E

[V(e)JT,

N (0, a2I).

(1—2)

For exarple if V(0) I, we have the usual nc)nlinear model, arid

m(8)
Y-X8

then we

N(0,

a2In)

if

have

which is the usual

linear regression model.

—2—

For
of

convenience, we put f( B)

residuals. We let y (),

our applications

values,

the combined parameter vector.

we will find that

naller than n, so that

V(e)

so that f(8) is the nxl vector

Y-m( 8),

In

the dimension of e, is much

p,

is unknown only up to few parameter

which we wish to estimate. For example, if Y were a zero

mean time

series, we

could take f(8) =

o

0

1

0

Y, and

perhaps

assume

v(e)

o ...

o

This

0

e

.

1

is a one parameter model, in which we are trying to estimate the
uncor-

correlation between Y and Yi1, assuming that Y1
related

for

t>2. The usual Box-Jenkins models described in Box and

Jenkins (1970) are special cases of (1-1). In fact, they can
be written as

"

Yi

ai—a

Cj1Cj

4'b si—b'

(1—3)

where

Cl "'
In

our

n

N(0, c2) variables.

notation

P(p)Y

T(4) c,

••'

"a '

(1Li.)

=

'

—3—

1

P(p)

a—1 a—2
a—1

0

0

o

o

0

0

0

o

0

1

0

0

1

0

1/

and

_q1

T() =

(1—6)

0

a-1 a—2

0

a-1

0

0

0

0

0

1

Letjg

e

(p1, ••.,

a' j' '

nd

so that the

th 1fl()
fld T()

(1-8)

.ns

fltdels

fldeed

(1-1),

We wjfl let P(p)

V(o) given

defj by the abo

Cases

es.

.

—5—

II. 'ESTIMATION

Since we have not asszned that f(6) is linear, and V(o) is not

necessarily linear in 0, we are not in an exponential family, so

the

theory of sufficiency is not applicable. We resort to the
principle of mathiium likelihood. We can only observe the nxl vector

Y, so we need the likelihood in terms of Y:

L(f,6,8,a)

C

det(V(0))

exp

where C is a constant (see Pao (1965)
For

all our applications

we will

fT(6)V;l(O)
f(6) ]

[

(2-1)

,

section 8a.4).

have det(V(0))

1 (see 1-5

and 1-6), so we irrmediately siirlify things by assuming that

det( (0)) =

1

for

Hence

log

L(f,6,0,ci)

all values of 0.

j. fT(6) V(0) f(6) -

-

(2-2)

n

log a + C

.

(2-3)

2a2

To maximize this we differentiate and set the derivatives to 0.
(Recall that f( 6)

Y-m( 6), so for each 6, f( 6) is observable.)

D log L

1

fT(6) V1 (o)

f(6) —

=0

or
fT(6) V(o) f(6) = n a2
Hence

fT(s) V(e) f(s) ,

(2-4)

—6—

and

we can treat a2 as a constant throughout the rest of the discussion.

Note that we are row trying to mirthnjze
fT(8) V1() f(S).
We write f( 5)

(V) for convenience.

f)T V1 (0)

(f1,

Then

BlogL

—

-1

B

j

2a2

-—

(E
2a2

-

—1

ij

a2

BlogL =

1

2a2

m

2a2

-

[

Bm
(E
..
13

J

5i

V'f.)
i

af()
B

f)

af.1 v1Jf.+f.v'J—l])
1

(E

ii

—1

3'

jj

)T V1(0) f(S)

..ij f.vf)
1

_

f.1 BO f.)
3
m

-1 fT(5) BV(5)
m
2a2

.

—7—

So

the k+p nornaJ. equations are

vce f(s) =

0

J
(2—5)

m

Note how the first equation imposes the usual least squares condition:

residuals orthogonal (in the right ntric) to the "data", represented
here by the first derivative matrix.
The second equation is also an orthogonality condition, albeit

somewhat less obvious: as we shall see later, for certain special

cases this condition becanes nre explicit.
To solve these equations we propose to use some variant of Newton's

nethod, so we compute the second derivatives. Omitting the details we get
_ci2

2 log L

—a2

D2 log L =

fT()

3v1(e)

i8m

-2a2

and

V(e)

L

fT(s)

+

V(e) f(s)

f(s)

(2—6)

,

(2—7)

2V(e)

Newton's method is
y

(i+l) -- y (j)

—l
(i) )
(y

—H

G(y

(1) )

—8—

where

H

/ff,]T V'f + f"

Vf

[f,]T[V_l]tf\

I

[f,]T

[lJ, f

fT[V_l]flf

and

)
(2—8)

fT

The primes denoting the appropriate derivatives. (Note that the factor
is cmitted from the lower right hand corner of H, because it is omitted
in the lower half of C). The Fisher information matrix 1(y) is

na2

T vi f

[f]T Vf' +

1
(2—9)

[. —— [f?]T [V1]'f

1 T[V
—1"
—f
] f
a2
2

.

Holland (1973) described a method for carrying out the expectation in
2

Since a is considered fixed, we treat it as a constant.Then

2—9.

[ft]T Vf' +

E

[ft]T

V_if]

1
Th tft]T
a
since f'

[fIt]T V_i [Ef]

f' +

since f(s)

=

[f,]T Vf',

() m' () was assumed fixed;

E [2 [f]T [V_1]?f]
E [2

2

fT

1

—2E

2a

[f?]T [v1]'

[Ef]

0,

Y— m() N (0,a2V(O)) , by 1—1

[\fll]fl fj
trace

-l
—

2

trace

2a

[fT{v_i],tfl =

2a

2

[E [ fT

E

trace

[V]"f] ]

.

- 8a

1
2a

trace CEIrr1]"

2

2

-

ff]]
V

trace

2

2a

trace [(V*I"

E[ffT]]

trace [vCv]"]

2a

So we have

L [fI]T v1 f'

0

(2—10

ICy)

—1
0

1I2trace(V0

thder suitable regularity conditions, it
(y-y) Nk+P(O
We

can

)

be shown that

f1(1)).

will assi.mte that (2-U) holds.
We now specialize to a subset of (1-1) for which the expressions

(2-7) are easy to canpute.

(2-11:

—9—

III.

THE NERALJD BOX-JENKENS SETUP

We restrict ourselves to the subset of (1-1) for which

V(@) P(p)T()
so

that

=

(3-1)

[pJ(p) T()]T

T($) p(p)

where T, P are given by

(3—2)

(1-5) and

(1-6).

Note that det (T( )) det (P( )) 1,

so that det (V( 0))

as required by (2-2).
We will use the following lenurias:
Lemria 1

Let A a), B( a) be any non-singular matrices whose elements

are a function of a scalar a. Then

i)

}—

ii)

A(a) = -A(ct) }-A(a) A(a)
A(ct) B(a) +

A(a) B(a)

A(c*)

B(a)

Proof:
1)

I A(a)

A(a)

So
E

3

kj

O--- kaik(a)a
3

k

a(a) a kj
(cx) +

3

kj

—a

(a) a(a))

Hence
o

-- A(a)

A(a) +

A(a)

f- A(a)

.

— 10

-

and

hA(cL) —A(c)

ii)

follows similarly

A matrix A

Definition

can

A

f-A(c) A1(ct)

is said

to be Column Thiarigular if A

be written as

a1
a

0

0

0

a

0

0

a3

a2

a1

a
n

an-i

a

Lenifra 2 If A and

n-2

0

a

B are arbitrary columi triangular matrices, then

1) AB is column triangular

ii)

A

is

column triangular (if it exists)

iii) AB = BA
'l'
iv) Trace (AB)

il ia .

n—i bn—i

v) If furthermore the entries of A are a function of the
scalar CL,then - is column triangular.

— 11 —

Proof:

Let a1, ...,

i)
where
The

I

a1

and b1, ...,
Z A Bkj

(AB)
{k : 1 <

b detenine A and B respectively.
ai_k+l bk...j+l

i-k+l <n and 1<k-j+i

rerige in the surnni.tion can

-i

be

<n}.

rewritten as

<-k <n-i-i

<k<n+j-1
or

fi>k> 1+1-n

I

),j<k<n+j-i
=

kj a±_k÷l bk_j+l
0 whenever j

>i

Hence

(AB)..

and

(AB)1,+ kj+Z aik+i bk_j+l
j<k—9<i—2

S

a±_k+l bk_j_÷i

1—.Q

Za.
-. i—m—2+i bm—j+l

m-j

=(AB)

which establishes coiiniui triangularity.

.

— 12

ii)
so

its

—

Note that a cohmn triangular natrix is lower triangular,

inverse can be computed column by column by simple forward

substitution. Letting

fi if i=j

6.. =
1J

We have

6..

'

A

-

i-i
k—

1•
A.
A'
:ik

A..
11

for i1, ..., j—i

0

j elerient

where A1 is the
But

A

..., n

for ij,

of A1.

a.k+l
i—i

so

A' =

1j

k
.a.
A
i—k+l
kj

,

...,

a1
1—1

A1,j =

6±,j+i

k=j+i

a±_k+l

Ak,]
,

i=j+l,

...,

1

hence

Ai+l,j+i

tsi_+1,j+l

kj+l
a1

ai+l_k+l

A

. -.
, 1—],

...,

- 13 —

a

ES.. —

A'1

and

A1,i

i+1—k

kj

I=j, ...,

n—i

.

a1

Since this formula aflows us
for i=j,

Ai+l,j+1

..., n-i and we Jow that A1+i1 0 for 1=1, ...,j—l,by

comparison with the
1j, ...,

to successively compute the elements

n-i,

for A1

formula

which

establishes

we see that

A1'1

A1 for

triangularity.

column

a11 bkj+l

i+j-k, so that

by setting &

iv)

T

CAB)..
'

(BA)..

=

E b.÷1
k

i+j-2

1

k1 A B = k=i a1

Thace (ABT) =

ak+l bjk+l

i!l k=1

n I
=

E

E akb-

1:1 k1

nn-k
k1 i0 a]b]
n

n
(n-k+1) abk
k=i

.

k+1 bjk÷l

i-i

ia11 b.1

v) obvious.

QED

— lL -

Note

that P( p) and T( ) are both column triangular, so from the

above lemna, we never need to actually carry around P and T, but only

their first columns. This allows us to sinlify things considerebly.
Specifically (omitting the arguments)

v1 = CTP] T1P
/[TP ff]T TP f

so

(3-3)

G

[(TP)' f]T (TP) f

C(T_1P)t]T T1P + [T_1P]T (TP)'

since

fT (V)' f is a scalar.

and

Also H:

(TPf' ]T

T1Pf' +

[T1Pf' T TPf

[(TP)f' ]T (T1P) 'f +

[(TP) ?f?

]T (TP)f
(3_14)

[(Tp)f,JT

(TP)'f +

since

Further

r(TP)tf,]T

(TTP)f

[(TP),fJT (TP)'f +

[(TP)?,f]T (T)f

(V)" [(TP)?t]T T1P + [(TP)tJT (TP)'

sinplification is çossible noting that
.L... (TP)

T
a

p1

+

T1 L. T1
ap1

1

T1
(3—5)

.L(T'P)

P =

-T LI T1P

1 T2P

-

- 15

3 2(f1-p)

-

T

3

1

pip

32P

—l

3p.

3p1

.

=0

3pipj

(3—6)

32(flp) -

- = -T1 3TT-13P_ 3T !I!L T2
34
pj

____

2(T1p)

3T3f'T-lp 3T

TT1P) =

(-

T

•P

ai4j

-

3
'

T T1

T T1 T1

34.3.

P = 2

-h

J

h

T3

P

A similar simplification occurs for the information matrix.
Holland (1973) suggests

tmce (vc

a method

=

trace

for showing that:
(3-7)

[TP)9O')

1•

((T_1P)

S

To simplify this canputation we will write V=QQT, V :Q_TQ_l, where

QT is co1.min triangular (leiin-ia 2 will be used in succeeding computations).
It is easy to show that
32 V_i

= 2V1 3V V1
302,

Hence trace (

av V—l -

V1

30

3ocfl

:),

trace

0:

(av v1
302,

= trace

m
[(QT
1
r3
DQ QT + Q• QT QTQ1
- trace
80
m
m
L

c

DO

m v)
QT + Q

Q_TQ_l
302, /

V-i

32V

DO

3em

J

- ra( t:om ri)
/ QTQ11

+

.

- 16

- trace r

i\rn

30
2.

92Q
30 0

L

T -T
!Q-1 +QJ+
/

-1 3QT Q_T

90

(

trace

—

-

QT +

Q

0m

+

+

m

302. 90m

30

2,

Q 92QT

302.-rn
0

)

Now Q is lower triangular, with 1 on it 's main diagonal.

QTQl

So

is lower triangular with U on it mein diagonal. It follows
that any product of lower triangular matrices involving
will

have U trace. Similarly for

and

for

m

matrices involving —

and

3QT

•

30

products of upper triangular

Using

this fact we obtain

rn

(

trace

+

= ce (3Q Ql 3QT Q-T

tm

2.

trace

( 30

90

\

trace (AT)

—

rn

2v1

Q_T) -

for any matrix

302, ,)

trace

(Q

(p_a.

\ Ti

w)iI
mj

Q is coluirn triangu1r).

°m Q_T)

A, so we finally get

trace[( 3QQ) (
9Q

1rQ_1

L
(because

'\
0
)
t.rn

+

)

302,

trace (A) ,

trace ( V 30

trace

Q_T

( Ql •Q

trace

But

Ql

+

I

90rn

Q) T]

- 17

Now having estb1ished

_ (PT)

k

—

3-7 we note

=

T +

.

that

P1

-—-

T

PT +

So

(TP) —p--- (PT)

-T

PT = - -- P1
(3—8)

(f1P)

(PT) T1 LI. = LI T1

.

— 18

Expressions (3—5), (3—6), (3—7) and (3—8), together with lemma 2,
perndt

efficient computation of the expressions (2-8) required for

the computation of ewt's step (2—7). First we notice that
and

.—j merely shift the coluims of A down by j

j zezos to the top of A.

Furthermüre, since Af

A,

places, arid append

is a vector if A

is a matrix, and P and T are ]x)th coliru-i triangular, we see that
we will never need to actually compute any riai matrices (since we
need only compute expressions of the form Av, where v is a vector,

and A is cohuiu-i triangular, so this computation can be done trivially
witi-out expanding A into an nxn matrix). Specifically, we carl break
the computatioi down as follows:
Let

A T1P
1)

Compute and store Af (requires n cells)

2)

Compute and store Af' arid A' f (requires n x (k+p) cells)

2.1) The gradient G is now given by computing

EAf,]T [

and

[Atf]T [hf]

2.2) Compute and store the ttxTxt, matrix, ttiat is

]T ,
[Af]T [t]

[Af'

[A'

[AtfJT [A'f]

:(requires (p+k) x (p+k) cells)
3)

Compute arid add to the matrix computed in 2.2 the

nonlinear corction terms due to the second

derivatives, that is

— 19

[Atft]T [Af]
[A,,f]T

(requires only n x 1 cells of temporary storage).
This gives
Of

us the Hessian H.

course, the computation of' and At! must be further broken

down into special cases, depending on whether A = T1P or
or

just T1

P. This is done using specializations of (3-5) and (3-6). Note

that in both steps 2 and 3 we have only had to compute products of
the form Qv, where Q
pointed

out previously, this can easily be done given only the first

colunri of Q. Steps

inner

is column triangular, and v is a vector. As

2.1, 2.2, and 3 also require

the computation of

products; again, this can be done easily with no need for

additional

storage. In fact,

the entire algorithm given above does

not use sifficant1y irore storage than that

required for an ordinary
the special forms of the matrices involved,

regression, and, thanks to
the required derivatives are

computed fairly efficiently.

A similar

algorithm works for the infonmation matrix 1(y); the upper left k x k

corner is given by [Aft]T [aft] so we need only worry about the

lower right p x p corner. Using (3-7) and part 4 of lenma 2, we see
that it suffices to compute the p matrices given in formula (3-8), and

then to compute the trace of all cross produots. Again, note that
colunin triangularity allows us to compute only the first columns of

matrices, so we need only n cells for each matrix, rather than n2.

In addition to the above, several interesting special results
can be gleaned easily

the simplified forms (3-5), (3-6), (3-7),
and (3-8). They are discussed in the next section.
from

— 20 —

IV.
We

SPECIAL

specialize to m()

0,

RESUL

so we have a pure Box-Jenkins

iiüdel,

f(8) = Y.

1) If T() I, so that we have a pure autoregressive
ndel, then defining Y 0 for i< 0 we have
[y]T py
n

p
—

i1 1

kj+l

n

E 'kk

Hence

p.

E

i1 1 k=j+l

kj+l —J

(4—1)

—1

n

p
Z

k•
Yk—j
.Yk—i

we want

kj+l k-j

k- •••

kj+l k-j k-p

k=j+l

k-j k1
(L._ 2)

for j1,
Equation

...,

p

(4-1) is i1minating: it shows that (at least for

this case) the second half of equations (2-5) reduce to an orthogonality
condition: residuals orthogonal to the "data", where now the data

is Y, rather than X as usual. This is reasonable because in this
type of nx,del, Y acts like X
(1-1)

and

(3-1)

our

'"N

rxodel is

in

the usual setup. In fact, recalling

now

(0, a2 p_l{p_lJT)

— 21

That

—

is
PY "

N (0, a2 I),

or, expanding the product PY,

P1Y1 +

C

= lk-1
Y

n

where

C

"N

+

pk-p

+

pY
in-i+...+pY
pn-p +n
(0, a2 I)

.

Clearly this is foniialiy identical to the usual linea± regression model
Y

X8+C, where

here
0

0

Yl

0

Y

n-i

So indeed LY-X ]Tx =

n-2

0 is equivalent to ('i-i).

— 22 —

The usual method for estimating
solve

the

Yule-Walker equations (see

In our notation these
n
E

pure autoregressive models is
Box and Jenkins (1970)

3.2.2).

equations are:

n

(Y
kj
- p1

y . -7) ('i'k-7)
k—j

kj+1
___________

.

(Y-7

-

n

+...

( 72
ki k

k=1

n
z

+p

to

"
p

-i

(yi-J
. -?)(Y-7)

,

n

j = 1, ..., p

(4....3)

(Y—Yy
k1
E

Eliminating the common term

Y=0

for 1<0, we see that

substituted for Y. This

2
k= 1

(4-3)

nJces

,

and

recalling that in (4-2)

differs from (4-2) only

in that 0 is

sense, because the assumption m(13)E0

implies EY=0.

So we see that

in

this

then

our method

of estimation reduces to the usual one

simple case. If we assume m()

the two

estimation

methods are

(a

similar,

scalar), so

that EY,

but not identical, since

estimate simultaneously with p, rather than merely setting Y.
(In practice, usually is close to 7).

we

— 23 —

ii) IfT()IandpO,then
n—i

0

0

.

.

.

0

o

n—2

0

.

.

.

0

o

0

n—3

.

.

0

0.

0

.

1(e)

n-p

This is a perfectly sensible answer, since we see from the above
is essentially based on n-j
equations that the estirrte for

observations. In per'ticular, for p1
n

i2E Y.1 Y.i—i
n
E Y.1

i2
iii)

P\jrthernore, from fonTlulas (3-7)

and

(3-8)

we see that

if

either P( p) El or T( p) El, so that we have only 's or only p 'S

to

estimate,

the value

of 1(y) will depend only on the value of the

+ or p vector, and not on whether or not it i a c

vector.

vector or

a p

That is, I (+ ) I(p) whenever + p and, respectively, P( p ) 1

or T(+)I.
This result is
variance

for

rather surprising:

it says that the asymptotic

the p's is the same as that for the 4 's if only p ?s or

s are present, even though they

represent quite different itodels:

—

—

One

is

••• -pY1

—

'v

N(O, a2)

The other is
—

f)Cj)

where
c1,

C

i.i.d.

N(O, a2)

iv) If p, then I() is singular,

(_2

)

.

since

it has the fonTi

This means that the paremeters are not estimable, and

this is reasonable since our nodel is n
Y ".. N(O, a2 I)
and

many choices of p

arid

will give us this nodel.

— 25 —

V.

We

that if

•

APPROXIMAflONS TO THE COVARIANCE MAThIX

have assumed that I(y)

1(y), and

in

fact Rao

(1965)

shows

is the distribution function of y arid Gn is the distribution function of a randan variable distributed MC 0, I ( y)), then
F

1m IFn_Gl

By the strong law

1(y)

H(y) -

(Note that it

is

consistency we also have

of large numbers, and

not true

since EH(y)

0,

that H(y)

--

1(y),

E ICy) -

in

fact H(y)

need not

converge to 1(y), as we will see later.)
On the basis of this
a

A

H( y)

result,

it

has

been

suggested that we

use

rather than 1(y) as an estimate of 1(y). We point out some

disadvantages

to this approach.

1) Suppose that f( ) Y, and that p 4, so that 1(y)
is singular. 11(y) is not necessarily singular; in fact, let p

= 0,

and p2. Then

n

Y_
i

i=2 1
•

•

'

i=2

H(y) =

n

,

E Y.
1

•
i=2

n

n

,,

fl
E
•

Y.
Y.
i—2i

n
.

i=2

,,

.

i=3

fl
+

•

y
i—2i

E Y.
Y.
i—2i

— 26

ii) Let f(8)

Y—8, K

I, p

1, T(4)

1.

1,

Then
Y -Y

in

n

H(O)

Y-Y
in

i2

(n

.

1)

I

Whereas

I(O)

n—i

o

)
The

form for H( 0) is nest

easily

derived

i

by observing that

n

2log L(f, 8,

p)

So
—

alogL

)

2

2

[(Y1-8)

(yi—i..)]

—

[(y.—B)
1

2

logL

—

-

)

(yi—i

P(Y1_f8)]

[—l+p]

n

E (Y.
i—i_8)2

2

2logL

1 +

8
log

[(y

Z

— (Y1—8) + 2

8

2

(Y1-8)2

n

log L

+

here

[—l+] [—l+p] = N

—

2(N—i)

p

+ (N—i)

2
p

2

L-

n

n
[-l+p] (Y1_18) +

8

2

z (Y11—8)

(Y—8)
1

n

n

n

=

[(Y1-8)

—

—

p Z (Y11—8)

(Y—8)
n

in

=Y—Y

—

2

- p(Y118)]

(-1)

n
+ p
(Y±—8)

(Y±i_8)

— 27 —

Clearly

H(O) does not converge in probability

to

1(0); however, under

the assumption p0
N(O, 2) , and

2

Y.
i-l
1=2
:

X

n—i

so we see that

Y-Y
1 n

0 ()

pn

n
and

-n+l)

1=2

In

:i-l

o

()

.

this case, however, 1(0) is the correct answer, so we see that

H(0)

is not as good.

There

is

another

approximation which
[f?]T Vf'

is clearly superior to H(y):

0

(5—1)

H2(y)
0

fT[V_l],If

This is obtained by eliminating those corionents in (2-8) whose

expectation is obviously 0. For the example we have

n
I

0

i_21)

— 28 —

which

is still not as good as 1(0). H2 also still suffers from

disadvantage 1) above; in fact, the lower z'ight corner of H2 is

identical to that of H.
We conclude that the variance of the 0's Bo enkins' parameters)
should not

be

estimated fran

H( y),

but from ICy),

differ significantly: a numerical example

We generated Y by taking 100
bution, so that Y "..

m(8)

N( 0,1).

both P and

average,

the two

can

follows.

from

a normal (0,1) distri-

Then we fit the rxdel (1-1) with

8, where 8 is a scalar and

order noving

points

since

o=

(), so

that we fit a first

first order autoregressive process. (I. e •,

T are present, but each depends

only

on one parameter.)

We found

.13536
—.36

—.3125

.009328

.000476

.000826

.3'47967

.345833
-

0

.009319

0

3.1134

I_1(y)

350404

3.16488
3. 22633

Since admissibility requires I

ans that p and
It

4,

are

I . 1,

I

I < 1, this last expression

essentially inestimable.

is to be noted that the

large observed variances

for p

and

— 29 —

4

are not accidental: if we had found p

-.36,

=

-.36, then

1(y) would have been singular, and the variances would have been

infinite. In fact, if we fix p at -.36 and vary , we get a sirDoth
progression from reasonable variance estimates to absurdly large ones.
Estimated

variance of q)

.0787

0

—.2

.33

—.3

2.06

One might conclude from this example that the estiniated variances

given by H(y) are absurd.
In this context Wall (1973) has suggested looking at the estimated

correlation matrix for p and 4), This is

for H1 () and

.99

(i
1

04,)
for I(y)

indicates at once that the estimates for p and 4) are unreliable,
since they are so highly correlated. We could also look at the condition
number for the covariance matrix of p and 4). For H1 the eigenvalu
This

are .0033505, .695021, the condition number 207; for I

6.33525

and l,I+lL. The condition numbers for the correlation matrices are 208
for

and 1,l7 for I. So we see that in fact the estimated

covariance matrix is nearly singular, for H1 as well as i_l; this

.

— 30

—

indicates that the parameters are "nearly inestimable". That is, we

can reasonably conjecture that the estimated variances given by
are much too small.

This example points out that blind acceptance of variances

estimated from H1, without examination of correlation coefficients,
eigenvalues or condition numbers, can be quite misleading for this class
of problems.

— 31

VT.

NUMERICAL CONSIDERATIONS

Assiining that m( )

if T()
is

-

not very nonlinear, one would expect that

is

I, Newton's method would work well, since then the model

almost linear: if m( ) is linear the nonlinearity is

the presence of products

when T(q) is present

the

p 3• This is in fact the case. However,
model is strongly nonlinear,

would expect, straight Newton's
Various

caused by

method

and, as

one

does not work very well.

schemes to insure convergence have been found to help:

these are all based on the principle that the objective function
fTV_lf should not be allowed to increase from one iteration to the

next. If the
it

step based

on

Newton's method would cause an increase,

is not taken, but a step based on some sort of gradient meti-od

is taken instead. The specific algorithm that was found to be most
effective is a derivative based modification of Powell's (1970)
dog-leg,

which was suggested by John

Dennis.

Even with this method, Iwever, we have encountered models where
G was not zero, yet H1@ was.

This means the algorithm got stuck in

a valley or "rut", even though a minimum had not been found. The

only way out would be to start again with a different initial guess.
A further problem for which we have no solution is that not all
values of U are allowable. The admissibility condition given by Box
and Jenkins (1970, pp. 5+ and 67)

is rather messy to compute, so we

athiissibility of the estimated U. As a
we may occasionally return ridiculous estimates. In

do not attempt to verify
consequence

general, as pointed out by Box and Jenkins, great care should be
exercised when fitting this sort of model.

— 32 —

Finally a few words on

initial

guesses. The following seemed

to work reasonably well.

1) Fit f( ) by ordinary nonlinear least squares. Let r
2)

f( ).

Fit the Box-Jenkins model for P(p) only to r. Let these
new residuals be r2.

T() only to r2.
initial values for the full

3) Fit the Box-Jenkins model for
14)

Use

the estimated , p,

as

model.

RERENCES

Box, G.E. P. and Jenkins,

G.M. (1970), Time Series Analysis,

San ancisco, California.

Holden-Day,

personal citunication.
J .0. (1970), A New Algorithm for Unconsained Op€niization,
Prograiruiiing (Rosen, Mangasarian, Ritter, editors), Academic

Holland, P.W. (1973),
Fbwell, M .
Nonlinear

Press.

C.R. (1965), Linear Statistical Inference
John Wiley and Sons, New York, New York.

Rao,

Wall, K. (1973), personal

communcation.

and Its Applications,

