NBER WORICfl PAPER SERIES

ROBUST LINE ESTATION WTI'H
ERRORS IN BOTH VARIABLES

Michael L. Brown

Working Paper No. 83

COMPUTER RESEAjCJ-j CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE

National Bureau of Econcmic Research, Inc.
575 Technology Square
Cambridge, Massachuset-ts 02139

May 1975

Preliminary: not for quotation
NBER working papers are distributed infonially and in limited numbers for
coninents only. They should not be quoted without written permission.
This report has not undergone the review accorded official NBER publications;
in particular, it has not yet been sU]iTlitted for approval by the Board of
Directors.
*

Canputer Research Center. Research supported in part by National Science
Fourxlation Grant GJ-U54X3 to the National Bureau of Econaiiic Research, Inc.
NBER

Abstract

The estimator holding the cen-tral place in the theory of the multivariate
"errors-in--the--variables" (EV) model results frcm performing orthogonal

re'ess ion on variables rescaled according to the covariance matrix of

the errors [7]. Our first principal finding, via Monte Carlo on the
univariate model, essentially relegates this estimator to use only in large
samples on very well-behaved data, i.e., with no trace of outlier contamina-

tion. A modification, requiring a robust preliminary slope, is proposed

that essentially sets out the generalization to EV of the w-estimator

in regression. It is d-nonstrated that the modification is robust to outlier
contamination even in small samples, given a sufficiently good preliminary

estimator. A candidate for a preliminary slope estimator based on the data
is proposed arid its performance under simulation examined. Least-absolute

residuals estimation in EV is cited as an alternative candidate.

Contents

1.

the Robust Regression

Motivation from

2. The Simplest EV Model
2.1 Classical assumptions for the univariate nde1

2.2
2.3

3.

distributions
Identifiability ob1em in classical EV
Contaminated error

1

.2
.2
.3
.3

Form of ML Estinator for Classical EV

4. 3"IL as the "Best" ML/LS Estintor

5. Monte Carlo Results for

5.1

6

L

Specification

5.2 CaiTnents

on

Table 1

6. "Robustizing" 3'1L : w—Estimation

6.1 Introduction to w-Estimation in Regression

6.2 Proposed

w-Estinlator

9a

in EV

10

6.3 ASimplification
6.4 Details of the Intplanentation

12
.

12

6.5 CcmnentsonTabJ.e2
6.6 Influence of sample size n

7. Data-based

Preliminary Estimators

7.1 Apreliminaryslope
7.2 CamnentsonTable3

15

7.3 LAR estimation on EV

15

BIBLIO

17

1. Motivation from

the

Robust Regression

[3] addressed the question of generelizing robust etima.tes 5f
location parameter to the problen of estimating robustly the coefficients
Huber

a

{ •J }"

in a multivaria-te regression model

jl jij +
with

contaminated errors v. He asserts

squares theory...

the matrix

X=

i

that "[in] the classical

(1)
least

(X.) is thought to derive from a fixed

and rigorous mathematical model. In statistics, it is more customary to

treat the coefficients

as independent variables, possibly also subject

—2—

to errors.
procedures

to nothing is known about how to robustize regression
with respect to errors in the
(underlines ours). The
Next

underlined statement is the problem we consider.
2. The Simplest EV Model
2.1 Classical assumptions for the univariate model
We set p=l in (1) and, to quantify the phrase "errors in the

we introduce an error u into X, so that we no longer observe X directly

but rather x.,1 where
x.1 X.+u.,
1
1

(2)

To fill out the EV model specification, we assume

y.
Y.+v.
1
:i
1
with

the "true" values exactly linearly related:
Y. =

1

(For

(3)

x.1

.

('iV)

the moment we assume a constant term a equal to 0 for simplicity.)

Further,
E(v.) = 0

E(u1) = 0

(5a)

(5b)

.

cov(u1,v1)

0

.

(6)

We suppose

X1 (a "structural relation") and

N(0,a)

.

—3—

cov(X1,v1)

0

(7a)

cov(X,u)

0

(7b)

(Note: Our discussion applies equally well to the "functional relation",

detenministic

where the X.'s arise in same

of

fashion rather than

as

realizations

a randan variable X.)

Our given data consist of the n observations
1

with

il

successive observations taken

(8)

,

independently.

Notice that xEX(uEO) is the case of regression with a line through

the

origin.

22 Contaminated error distributions
We define a contaminated EV model with u and v samples from the

contaminated noriial distribution. This means that

u

is drawn

fran N(0 ,a) with probability (l_y) (9a)

and fran N( 0 ,h) with

where h >>

2.3

and

probability

similarly for v.

Identifiability problem in classical EV

Lindley

[5] and

of gaussian errors,

others

have pointed out that, in the classical case

yU

h

.00

.00

=

the ML estniators of the parameters ,
Kendall and

error

Stuart []

0

(lOa)

0

(lOb)

U

a,

c cannot all be consistent.

"we must make an assumption about

observe that

the

variances.... [Assuming]
known

is the classical means of resolving the unidentifiability problem."
Throughout,

we assume
X

(11)

is known.

3. Form of ML Esthnator for

Classical

EV

is convenfLent to derive the ML estimator of ,

It

BML

say, for the

functional relation; its form remains unchanged for the structural relation.
The likelihood function is
-

ex{_ 2a
allows

(x—X±)2 } , which

2(

(12)

us to characterize the ML estimator of as

-

;x1...,Xn1ilL v'X

[(Yç-X)

+

(x.-X.)2
1 1

}

}

,

(13)

—5—

where the symbol to the left of the braced quantity means "that value
is a minimum, for
of for which the braced function of ( )<
<
<-<X<
,n." We see that transforming

y

, i1,.

.

to y/v' and to /v'X makes the effective A equal to 1, so we assume

A=l

(lu)

from now on without loss of generality.
{ }

Setting

0 where "

}" is

the braced quantity from the

xi

min condition yields
A

x.+(BML)y.
1
1
1

Setting

}

(15)

l+(BNL)2

0 fran the min condition implies that

nA

X.y.

E

(16)

flA
E X.

il 1

where

of (16) is itself a function of BIlL. Thus BIlL is of precisely the

same form as BMLR, the ML estiirator in regression, except that in place

of the known

in regression are estimates X. (Note: Because the various

approaches to the identifiability problem in EV are distinguished essentially

by how

they

bear the

obtain the X, it is in our

view

unfortunate

that the

name incidental parameters.)

"Unwrapping" the iiilicit characterization (16) yields

in EV

—6—

12
2

(17)

S12

where

s11

inE 2

il

inE

s12

22

xy1

s22 E

i=1

-

in y.
2
E

i=1

S11

BML is formed by mininiizirig the sum of squared-"residuals" taken perpendicular

to the estimated line. I . e.,

BML is the orthogonal regression estintor

on rescaled variables; see e.g., Malinvaud [7]
L.•

chaps. 1, 10.

___________
BIVIL

as the "Best" ML/LS Estimator

.

The classical ML estima-tors that apply throughout the range of nde1
parameters are those which assume ]<riowledge of
'

say);

'

say);

or

or
a2

x4

BIlL

u
or

both or a and

CJç

,'

say).

See Madansky

[6].

.

—7--

Kendall

and Stuart sunirnarize a result of Birch that justifies calling

BNL the "best" of , ' BML. Birch demonstrated that
u,v

except under condit±ons (the violation of certain inequalities relating
sample product-moments to model parameters) which Kendall arid Stuart state

"seem unlikely [to hold] in practice". While we regard this last claim as
perhaps

of

a bit optimistic (because the violation occurred

the time in our

about 5-10%

sinuilations), this fact nevertheless allows us to understand

why BML is at least as good an estimate

of ,

uniformly over the range of model

parameters, as the better of the other two ML estimates which each require
knowledge of exactly one of

or

Our s:iinulations confirmed this

property when both u and v are normally distributed.

Madansky gives a remarkable survey of the history of this estimator,
asserting that the form of BML "has appeared iridependen-tly innumerable times

with the earliest appearance in 1879.. .".

It is also of interest to recall Malinvaud's laudatory remarks about BML.
He derives an approximation to L' s asymptotic variance which, he says,
"allows us to verify that, in the case [of one X-variate

--- M.L.B.]

probably generally, the weighted re'ession has good asymptotic
at least when the dispersion of the errors is small relative to
true

and

efficiency
that

of

the

variables, and when the errors are normally distributed". Comparing

this variance with the minimum variance lower bound, he concludes that

—8—

"the

asymptotic efficiency is very

Later: "[these]

near 1 jf [11C

01'S]

small".

results.., apply only to the asymptotic distribution of the

[estimator BML]. Unfortunately there seems to exist

no

study of the properties

of this [estimator] for finite samples."

5. Monte Carlo Results for BML
5,]. Speciflcatjon

We consider first the performance of the ML/LS estimator BML under

contamination;

for comparison we include

n
BMLR =

x1y
i-i
r
E

x.

1=1
BMLR is the

1

usual ML/LS estimator that

free (i.e. u

0, x

(18)

2

would

be appropriate if

x were

err-

X).

We choose model parameters which, except for the contamination in y

only, are smetric in x and y:
n = 20;l00
= 1.

8

=
2 =

U

1.
0.502

0.50

u
cS

.00

u =.0

11.

.05 h varying

Note that this implies A = 1.
We are thus

of BML.

considering both

the small-sample and the large-sample performance

When n 20, e.g., an average of (.05) (20)

1 out of 20 observations is

drawn from N (0 ,h) and the rest from N (0, .502). Notice that h 0.50 corresponds

to classical EV.

.

—9—

Table 1.

y, O.05;81.

100 replications

n

MSE(BNLR)

2O

0.50
1.00
1.25
1.50

20

20
20

20

20
20
20

3.0

10.0

100
100

1.50
3.0

100

10.0

MSE(BML)
.0391

.01.99

.0501
.0506
.0513
.0533
.0602
.2120
.0395
.01401

.0580

.0I1.7L1.

.0559
.06914

1261
2.1.1.50
21.1.6..

.0119
.105
26.9

5.2 Corrvnents on Table 1

is in our view

to overstate the gravity
of the results of Table 1, which tables the quantity
It

difficult

- and

ne irony!

-

100

MSE(.)

At

iJ..

(.

l.)2

n2O we see that when just one observation i,s drawn from N(0,1.252) instead

of from N( 0, . 502),

BML is a poorer estimate than BMLR, which i-res the error

in x! Contamination this light would almost always be indistinguishable f±anpure

gaussian

has

sampling,

By

the tine h becomes

acquired outrageously heavy

(See the hlO entries.)
is still more than small

"very noticeably" large, BML's distribution

tails, while BMLR has kept

transition at n100 occurs between h1 .5 and 3., which
enough to go unnoticed in a real sampling situation.
The

BML' s performance is thus so shockingly poor as to make

appropriate for the regression model, BNLR --

are

relatively stable.

whose

the

MIlLS estiiator

non-robustness properties

now notorious -- look almost well-behaved by comparison! For this reason,
we come to the ironic conclusion that those investigators who have "looked the
by

other way" when the possibility arose of error in the independent variable in

- 9a

-

their "regression" irdels, arid used BMLR "by default", instead of BML which
requires knowledge of X, probably made the better choice. But this is a
choice

between Scylla and

Charybdis; the ne'ct section proposes a way out

of this strait.

6. "Robustizing" BML w-Estation
6.1

Introduction to w-Estirnation in Regression (R)
V

The w-estarnator in R, BWR, with preLuxanary slope ,

mease s of residuals, and

"psi-function"

BWR =

mm1

is

ip,

.

robust

scale-

defined as

i

. (y.—X.)2]
1

i=l yi

,

(19)

where

X.

y1 —

is the residual fran the preliinary

yi

(20)

slope

',

and

iP(1/Y)

(21)

r .15

yi y

is a weight superposed onto the i' residual serving to "damp the influence of"

the th point on BWR -to the ectent that it is diagnosed as an outlier. Notice
that when i, (r) r, BWRBMLR. See

Beaton

and Thkey [ 2

]

and Andrews et.

al. [ii.

S

— 10

6. 2

Prposed w-Estiinator

We

-

in EV

propose the following natural generalization BW

BW involves two weights .,
outlying

coordinate"

EV

in EN, $

:

EV.

superposed onto

the ML/LS

esthiiator

s with the meanings as in R, we set

and

•

min

+

to

intended to "weight-down an

of the th point,

for p, ,
BW

. each

of BWR

il

(22)

V

wXl.

(x.—X.Y]
11

where

I

V

1

1

Xl

are the

(23b)

1

two EV residuals associated with v arid

1,

yl

V

u respectively and

V

rIyi 7

-

(23a)

1

)X(rXl./sX
rXl./sX

(24

(2tb)

— 11

-

are the two weights. Notice that we now require a preliminary estimate
X. for each X. as well aa
1
1
Th.irther

I,

V

on we

discuss

$ for 8.

the some data-based possibilities for choosing

$andX1.
Taking the (n+1) derivatives of the weighted min condition (22)
yields, after simplifying, this 6th degree equation for BW:

ii

x
E Ck.w
1 yi[2
xi

i=l

—

2

Xl.wyi.y.
1

+ (w

i.

Xl yi.x.y.(BW)2)}
1 1

0

2
- v2
w .x.).BW
Xl 1
(25)

.

where

A

k

V
Ew
+

(26)

.

— 12 —

6.3

A Simplification
Choosing X1 to have the ML/LS fonn (15) as a function of

8

may verify

we

easily

that

r
I,

Thus {r . }
x

and
-,

scale

yi}, for this

{r .

V

choice of X, yield the same measures of

we have

s, whence (assuming

Xl

(27)

—$r .

(28)

yl

Substituting this coninon weight ( say) into the min condition (22)
shows that

BW

has the

form (17)

t

12

of BML except

that

1

E w.x.y.

(29)

n i::1

replaces s12 and corresponding weighted moments t11, t22 replace

6.

S22.

Details of the Implementation

We have used

(l_(.)2)2 if() c
ff() >c
with c

5.0.

(30)

— 13 —

This

p-function,

scale is due

due to

Tukey,

is known as

the "bisquare". Our robust

to Harnpel:

median

s({r1})

{(r. - median {r.})}
J

(31)

j

In order to separate the issues of preliminary and final robust estimators,

we set
(32)

for the present w-simulations. Thus BW in our simulations takes "one
step away from the true s", and these results indicate the character of
performances to be expected with a good data-based preliminary estiimtot'
V
later remarks.

. See

We also exhibit the performances of the regression w-estimator BWR,
which is (19) with x in place of X.

Table 2. Same 100 samples as for Table 1

n
20
20
20
20
20
20

20
100
100
100
100

NSE(BWR)

MSE(BW)

.02714

.0211

.0277
.0277
.0277
.0273
.0271
.0317

.02140

.0211
.0213
.0215

.003142

2.0
3.0

10.0

.0223,

0.50
1.0
1.25
1.50
2.0
3.0

10.0
1.50

.0257
.0263
.0250
.02'#8

.0225

.00363
.00361
.00322

— 14

6.5

Coimierits

—

on Table 2

Table 2 exhibits the MSE 's for 8W arid for BWR corresponding to the

situations of Table 1. Besides the artifactitious "superefficiency"
induced by
we see

assuming

8

(so that MSE ( BWR) <MSE ( BML) when li is

that MSE (BW) remains

below

MSE(BWR)

superposing the w-weighting allows the
operate

6.6

in all cases.

error-in-x

near 0.50),

In other words,

correction using

A to

on an effectively uncontaminated sample.
Influence

of sample

size n

The ratio
MSE (BW)

MSE(BWR)

decreases as n increases for fixed y, h. This accords with our expectation,
for as n increases both variances decrease like 1/n but bias-squared approaches
a non-zero limit in the case of BWR and zero in the case of 8W because of BW' s
bias-correction

using A.

7. Data-based Preliminary Estimators
7.1 A preliminary slope
V
One straightforward method of obtaining a 8 for
good preliminary regression

and

estimators,

EV might

be to

combine

in the mariner that

BNL combines BMLF.. and

2

E

ii

..

(34)

n
x.y.

i1
L

We

have simulated
BL

t +(sgn(BYX)) •

+A

(35a)

where
[BRXY

—

]

(35b)

— 15

—

with BYX the "robust line" ("medians-of-thirds grouping" estimator) of
Thkey C

],

8

and

BRXY the reciprocal of the same estimator corresponding

to regression of x on y.
Table 3.

r

MSE(BYX) MSE(BRXY)

(Y,h)

20

50
20

50

20
50

MSE(BL)

(.0,c)
(.0,0)

(.G,0)
(.0,0)

.061499

.3777

.1359

.05143

.1327

.0260

(.0,0)
(.0,0)

(.05,3)
(.05,10)

.06514

.5822
.6985

.1921
.1637

(.05,10)
(.05,10)

(.0,0)
(.0,0)

.1037
.1026

.1421414

.16014
.01417

.0599

.1123

7.2 Coinents on Table 3
Either a small sample or contamination in y renders' BRXY suffiOiently
unstable as to make MSE (BL) >MSE (BYX).

But for moderately large n arid

contamination in x only, the estimator EL with A-correction improves on
BYX, the "regression" slope. Notice that contamination in x alleviates

some of

BRXY's

7,3

One

is

overestimating

bias at

LAR estimation in EV

of

the best-}c-iown proposals for a preliminary

estimator inregress±on

the LAR (least-absolute--residuals) slope

-

$min-1 i:l
.Z y.—BX.
1
1

fv

V.

The EV generalization requires B, X1,..., Xn jointly to minimize

il
Z

y.1

—

)<•

1

I + I x.1

—

X.}
1

.

- 16

—

solution (C. MUows, 1973 personal carrnunication) is 8 to minimize the
smaller of
,
I
I
iii I

The

c8 x

In classical EV, we may remark that this essentially c'nputes the LAR estimator

of y on x or of x on y according as which of x or y respectively has the smaller
"noise-to-signal ratio". We conj ecture that this estimator would be very
satisfactory in, arid only in, "large" samples in contaminated EV.

- 17

-

BIBLIOGRAPHY

[1]

Andrews, et. al., Robust Estimates of Location: Survey and Advances,
Princeton University Press (Princeton, New Jersey), 1972.

[2] Beaton, A.E. arid J.W. Tu3cey, "The Fitting of Power Series, Meaning
Polynomials, Illustrated on Bandspectroscopic Data," Technometrics,
Vol. 16, No. 2, May 1971+, pp. 1l+7_192.

[3] Huber,

P.,

"The 1972 Wald Lecture. Robust Statistics: A Review,"

Ann. Math. Stat., Vol. '43, No. 14,
['4]

1972,

pp. 10'-l.l—1067.

Kendall,

M. G. and Stuart, A., The Advanced Theory of Statistics, Vol. 2,
3rd ed., Griffin (London), 1973.

[5] Lindley, D. V., "Regression Lines and the Linear Functional Relationship,"
Jour. Royal Stat. Soc., Supp., 9, 19147, 219-21+14.

[6] Madarisky, A., "The Fitting of Straight Lines When Both Variables are
Error," Jour. Amer. Statist. Assn., 51+, March 1959,
Subject
pp. 173—205.

to

[7] Malinvaud, E., Statistical Methods of Econometrics, 2nd ed., American
Elsevier Pub. Co. (New York), 1970.
[8] Tukey, J., Exploratory Data Analysis, 1iited preLiminary edn., Vol. I,
Addison-Wesley Pub. Co. (Reading, Mass.) 1972.

