NBER WOR1NG PAPER SERIES

The Maxirrnm Likelihx)d and the Nonlinear
TI-ree-Stage Least Squares Estimator

in the General Nonlinear Simultaneous
tuation Model
Take shi P2fleIPiya*

Working Paper No. 90

p

COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE

National Bureau of Econic Research, Inc.
575 Technology Square

Cambridge, Massachusetts 02139
June 1975

Preliminary: not for quotation
NBER working papers are distributed informally and in limited
numbers for connents only. Tl-y should not be quoted without
written permi ssion.
This report has not undergone the review accorded official NBER

publications; in particular, it has not yet been su]initted for
approval by the Board of Directors.

*Stariford University and NBER Computer Research Center
Research supported by NSF Grant DCR 70_03L56 A04 to

the National Bureau of Economic Research, Inc.

Abstract

The consistency and the asymptotic normality of the

maximum likelihood estintor in the general nonlinear
simultaneous equation model are proved. It is shown that
the proof depends on the assumption of normality unlike in

the linear simultaneous equation model. It is proved that
the maximum likelihood estimator is asymptotically more

efficient than the nonlinear three-stage least squares

estimator if the specification is correct, However, the

latter has

the advantage of being

consistent

even when the

normality assumption is removed. Hausrnan' s instrumental-variable-

interpretation

the

of the maximum likelihood estimator is

extended to

general nonlinear siiailtaneous equation liDdel.

Acknowledgement

The author had stimulating discussions with David Belsley,
Michio

Hatanaka,

and Jerry Hausman.

I

Contents

1. Inoduction
2. Model
3. Mxirnum Likelil-ood Estimator

tj

p

Iterative Meti-ods

13

5. Nonlinear Three-Stage Least Squares Estimator

17

6. Conclusions

24

References

25

Appendix

Al

1. Introduction
In this paper we obtain the asymptotic properties of the maximum

likelihood estimator in the general nonlinear simultaneous equation
model and compares them with those of the nonlinear three-stage least

squares estimator. The main results of the paper are the following:
1) The proof of the consistency and the asymptotic normality
of the maximum likelihood estimator in the general nonlinear
simultaneous equation model crucially depends on the

assumption of normality of the error term unlike in the
linear case.

2) All the th±od-order derivatives can be asymptotically

ignored either in the iterative method for obtaining
the maxihjrn likelihood
of

estimator or in the computation

the asymptotic variarlce-covariance matrix.

3) The maximim likelihood estimator is asymptotically more

efficient than the nonlinear three-stage least squares
estimator.
L)

Hausmari's

iteration method for the computation of the

maximum likelihood estimator in the linear case

(see

Hausman

Unlike in the

[1975]) is generalized to the nonlinear case

linear case,

it does not produce

an asymptotically

efficient second-round estimator even if the initial estimator
is consistent, but, like in the linear case, it illustrates

the

similarity and

and

the nonlinear three-stage least squares estimator.

the difference between the maximum likelihood

—2—

2.

Model
We will consider the nonlinear simultaneous equation model defined

by the following system of n equations:

f1(y,x,a1) u., i1,2,.. . ,n

where

(2.1)

is a n-dimensional vector of endogenous variables, x a

vector of exogenous variables, and

is a vector of unl<nown parameters.

Not all of the elements of vectors

and x may actually appear in the

arguments of each

Define a n-dimensional vector u as (uit ,u2,

.

Then we assume {ut} is independently and identically distributed as
rruitivariate N(O,2). We assume that there are no constraints among CLs, but
the results we subsequently obtain are not affected by the removal of

this assumption as we will show at the end of Section 5. We assume either

that f1 defines a one-to-one mapping between y and Ut or that the researcher

can apriori specify a prticular root of y for a given value of u so
that

the density of

can

be obtained by the usual way as the product

of the Jacobian arid the density of u. Finally we assume that all the partial
derivatives of f1 with respect to

and y that appear in equation (3.5)

T
in Section 3 exist and are continuous and that -4 and

where f

, are

f f'

t t

norisingular, These assumptions enable us to

define the maximum likelihood estimator. The other conditions needed
for the consistency and asymptotic normality of the maximum likelihood
estimator are given in Section 3.

—3—

P

3.

Maximum Likelihood Estimator
Because

logarithmic

of the basic assumptions of Section 2, we can write the

likelihood function as
L*

-

T

+

log

E

t=l

where we defined

.

,f)'

log

I

f-4i I Yt

.

t

T
E

ti

f

(3.1)

Equating the partial derivatives

of L* with respect to to Zero, we obtain

Eftt
f'

a——T

(32)

T

where we will abbreviate E as E from now on. Putting (3.2) into
t=1

(3.1)

we obtain the concentrated likelihood function

L

E log I

II I -

We define a vector

log

IT

.

E

(3.3)

and a matrix

We will write the partial derivatives of L using these symbols below.

To avoid the excessive subscripts, we will omit the subscript t from f,

y, u, and g whenever they appear inside the suiunation. We have

E
act.

1

where we used

ag.

au

of

3g1
3u.

1

=

- T E g. f'(Eff'):'
1

ag. 1fi'- and. wrote (

—4
I—,ay

(3.4)

)

1

for

the 1th column

the inverse of the matrix within the bracket. We have

—Lb—

E

1]

- T Z g.. f'(Eff')T1
1

3u.
1

ij

ag.

-

J

- T

(Eff') E g1 g

1]

1

(3.5)

(ff')1
I

E fg

E g.
f,(EffYa
1

fg!

+ T E g1 f'cEff'):'
J

+

T(Eff)

: [!1

where we used ::

of

element

1

for

the 1, th

the inverse of the matrix within the bracket.

define

We

and wrote C )

I

the maximum

likelihood

estimator of a as a root

of

0. Given assumptions A through E in the appendix,

equation

one of the roots is consistent and if we denote the consistent root by
I.'

a

,

we have

plim

.

(3.6)

The proof is given In the appendix. The above result is of course not
a surprising

one.

Our

main reason for writing down the assumptions

explicitly Is that checking some of these conditions, especially B and

E,

is instructive in our model: It will show that the consistency proof

depends crucially on the normaility assumption and

involving

g

in (i.l..5) can

asymptotically

that the

terms

be ignored. Also, it will aid

I

—5—

us later when we compare the maximum likelthood estimator with the nonlinear

three-stage least squares estimator.
We will consider each of the assumptions in the appendix arid

indicate what conditions on the function f are implied by each. We
will not make a great effort to find the minimum set of assumptions needed

on f since that is not likely to be a useful exercise. As it was stated
earlier, assumptions B and E are most interesting to verify and we

will

devote

most of our time on

their

verification. But since assumption

C requires the greatest number of conditions on f, we will

set

state a sweeping

of conditions on f to make assumption C satisfied. After this is done,

only a small number of additional conditions is needed to satisfy the

remaining four assumptions. Thus we assume
Condition_1. The probability limit of T1 times every surimation that
occurs

in

of T1

tines its expectation. Moreover, the convergence is uniform in

the right-hand side of (3.5) is finite and is equal to the limit

a neighborhood of c.

In addition, pm T1 ff' is

nonsingular.

Note that the uniform boundedness of the third-order derivatives may

be

substituted for assumption C.

Before proceeding further, we will prove the following important lenm
which will be frequently used.
Lenua. Suppose u1, u2,. . .u are jointly normal with mean 0 and h(u1 ,u2,.. .u)

is such that E h and EDu.
— are finite. Then, iu1
1

where

a11 is the covariance between u1 and u..

i-i E

a11
Du.
1

—6—

Proof. Replaceu.inhwith — u1+w. for i2,...,nandtreathas

a
w

function of u1, w2, w3,..., w. Then, Ehu1 EE hu1 where
(w2,. ..

,w).

But using integration by parrts we have

Ehui

I

hu1

du1
(3.7)

2

- a1 [h]00 +

where

200dh
4

du1

is the density of NC 0 ,a). But the first term

side of (3.7) is zero because Eh is finite. Note

of the rht-hand

p—
du1 il u1 a2
I

Therefore,

taking the expectation of both sides of (3.7) with

respect to w, we get the desired result.

Now we will consider assumption B. Using (3L) we have

-I-

-k-

1T ct1
0

- g.1

u'

J

where a' is the th column of Q. We irranediately see that the mean
of the first term of the right-hand side of (3.8) is zero since g1
satisfies the condition of the lenana because of condition 1. Also
using the leimia we have

(3.8)

—7—
ag.

plim -

E

E

urn

g u'

(3.9)

E •-4 c

u} satisfies the conditions for a law of large numbers

since

because

of conditon 1. Therefore, denoting the equivalence of the limit

distribution by the symbol j, we have

- il

'

+

i2

(3.10)

where

lag.

p.ii

E !—-.

p. = lim

EF

L'i

i2

Written
ag.

and

thus,

is

ag.
u

.1

- g.1

—-— E (uu' -

a certain essential

it is clear that

(3.11)

u' a'l

c?)a' .

(3.12)

boundedness of g1

sufficient to let (3.10) follow a central limit theorem.

For exanle, the following condition is certainly sufficient:

3

Condition 2. Egl and E

are uniformly bounded
it

where

I

g.itand

it
it

ag.

au.

are evaluated at c.o

for all t,

—8—

Next we

of T1

will verify assumption E,

I

probability limit

Ta)dng the

times (3.5) evaluated at c, we have

plim

jr

T'

- pun T

+

+

plan T-l

plim T1 E [:
g•
j

---J-

1

-

- g1

u' o]

o plim T
(3.13)

ji'
g.1 ua
a

a pun T1 E g1 u' •

•

.
—l
plan T

• plim

ug.

T1

J

ug

I
Because of condition 1, we can replace plim in the right-hand side of
(3.13) with him E. But, then, the first term drops out provided g.
1]

satisfies

—

the condition of the leia.

ijt

So we impose

Condition_3. E g.. is finite, where g.. is evaluated at a

Thus,

either in performing

estinator
one

ijt

0
Newton's iteration to obtain the rnaximiur likelihood

or in Obtaining its asymptotic variance-covariance matrix

need not compute g...

Also we can apply the result of the lemma to

each term involving the product of u and g1. Thus we have

I

—9—

act.aco

1

. a.

- urn T1 E E —i

1

I

ct0

-

Urn T1 E E g1

urn T1 E E

+

(3.1L)

urn T

•

a Liiii T1

+

must

compare the

urn T

above with

ag.

E E au
—4

1

1EE—au.
1

au5

We

g!

ag.

plim T1

•

1L I
EL

—
I
act1

ag!

Urn T1 E E au

1

I

-- I

jIo

We ijiipose

Condition .

E

ag1

it

j(

I

I°J

[3u.

Then,

fite.

1

g'

by the repeated application of the lemma, we have

[

r ag.
E'

au. - g. u'

--

t

-

L

all

J

I

from

(3.11)

J

u' a]

.
ag.
ag.
1—--—--u'a1—o
Bu.
au au

ag.1

Therefore,

- g'

ag

au.

1

and (3.15) we have

g

]

g

(3.15)

— 10 —

Em E p.1 p.1 Em T1 E

9g.

-3g!

(3.16)

urn T1

+

E g g!

We have

E (uu' —

where

Y

o

(uu' —

)lJ

+ 9,.

(3.17)

is a n-dimensional vector with 1 in the th place and

0 elsewhere. Therefore, from (3.12) and

(3.17)

we have

[Continued on page U]

I

— 11

-

Urn E p2 p.2 a' Em T1 E E

•

Em T1 E E
(3.18)

ag.

+ Em T

E

au.

Em

•

TEE

J

ag!

au.1

By the application of the leiima we have

E

rag.

.1
-

g1 u' a' a (uu' J

(3.19)

ag.

ag.

-a'E-—au'
Therefore from

(3.11),

(3.12), and

-

Urn E p11 p.2

au.
1
J

(3.19)

a'

urn

we have

T1

E E

•

lin T1

E E
(3.20)

-

Em

T1

E E

•

Urn

T1 E E

Similarly we have

•

E(uu'

- Q) a'

rag.

- a1

-

g

u'

a'
2 E •cj•ii

£. E

a'
.

(3.21)

— 12

—

I
Therefore we have

liin E p.2 p1 urn E p.1 p2 .
Finally, assumption
and (3.22).
leaves

This

(3.22)

E follows from (3.10), (3, l'4), (3 .16), (3 .18), (3.20),

assiunptions A and D.

Assumption A requires only one

additional condition:
Condition

5.

plim T1 E log I

ITI

I

exists

in a neighborhood of

a0.

As we will show in Section 5, assumption D is implied by
af.

3f.

Condition_6. him T E

E

is finite and nonsingular

for every i.

To sum

up,

conditions 1 through 6 imply assumptions A through E in the

appendix.

Note that the proof of both consistency and asymptotic normality
crucially depends on the normality assumption unlike in the linear case
where the maximum likelihood estiiitor can be
for general specifications

usefulness

shown to

easily

on the error term, This

be consistent

fact increases

the

of such an estimator as the nonlinear two-stage or three-stage

least squares estimator which has been shown to be consistent for general
specifications on the error term.

- 13

4.

—

Iterative Methods

Consider the class of gradient nthods of iteration defined by
A

'S

2
Ct1

where

a1 is an initial estimator and A is some matrix which may be stochastic.

.

Using a Taylor expansion of

around

ct, the true value, we have

Ct1

fran (4.1)

V (a2-a0)

-

A

+

[i

-A

1o

(4.2)

A

*

where cz lies between c and a0. Suppose that a1 is a consistent estima.tor

P

(ct1-ct0)

from (4.2) that

the

asymptotic distribution of the second-round estimator

does not depend

upon

of

such that

if and

has a proper

limit

distribution. It is apparent

the asymptotic distribution of the

first-round estimator

only if

plirn T A = plim

T'

2

(4.3)

.
a0

Moreover, it is apparent fran (4.2) that in this case the limit distribution

of ,7'

(ct2-ct0)

is the same as that of the maximum li]elihood estimator. We

will call the gradient method satisfying (4.3)

the efficient Newton

iteration.

— 14 —

Next consider the iteration that can be derived from

the

equation

obtained by putting (3.4) equal to zero. We can rewrite the equation as

r1

g.

---- • F'

-

G!

LT

1

-l F'F).

F(T

0

(4.'.i)

j

where F' is the nxT na-trix whose i, tth element is f±(y,x,c±) and

is
•

•

the matrix

—

whose tthcolumn is ___________

cti

1

1

.

Define

E —4- • F7

— T

(4.5)

and

0
o

.

.

0

Gt
2

0

Also define f as the (nxT)-dinensional vector obtained by stacking the

columns of F. Then, all the n equations in (4.4) for i1,2,...,n

can

be

combined as

G(cr'®I) f

0

(4.7)

— 15 -

where we have written Q for f1F'F. Expanding f(a1) in a Taylor series

nd ct we

finally obtain the iteration

a2

a1 —

['(c'Qi) GY' G'(cr'QI) f

(L.8)

where
0

.

.

0

o

(g)
o

arid

G

n

every variable in the right-hand side of (.8) is evaluated at

ct. Equation (. 8) is the generalization of the formula expounded by
Hausmari E1975] for the linear case.

class

Note that (-k8) belongs to the

of iteration defined by (.1) with A [G'(QØI)Gi

By the application of the lenuiia we can easily show

plim T'

G(c2I) G - a'

lirn T1 E E g1g
(L.10)

+ a' lim T-1

ag.

E

•

. lijn T-1

E

— 16 —

By comparing

(3.14)

with (4.10) we see that condition (4.3) is violated.

Thus we conclude that the asymptotic distribution of the second-round
estimator in this iteration depends on the asymptottic distribution of the
initial

estimator

and

is not

asymptotically efficient. Note that

the

result is not changed if [Gi(Q'®I)G] is used instead because its
probability limit can
the linear

case

be shown to be equal to (4.10). Note also that

the sum

of

the first term and the third

right-hand side of (3.14) is zero

so

term of

in

the

that conditon (4.3) gets satisfied.

Although (4.8) nay not be a good method of iteration, it does serve
a useful pedagogical purpose as Hausman's

a

certain similarity between the maximim

three-stage least squares estimator.

linear case

does, for it demonstrates

likelihood estimator

and the

nonlinear

— 17 —

5.

NonLinear Three-Stage Least Squares Estintor

Jorgenson arid Laffont [l97Ll] defined the nonlinear three-stage

least squares esthnator (henceforth to be abbreviated as NL3S) arid proved

its consistency and asymptotitc noniiality, extending the result of Amemiya
[1974]

obtained

for the nonlinear two-stage least squares estimator.

They defined the NL3S as the value of a that minimizes

f(a)'[c21Q X(x'x) X']f(a)

(5.1)

A

where

c

is

some consistent est:iinate of 2 and X

is

a matrix of exogenous

variables which may not coincide with the exogenous variables that appear

originally in the arguments of f. Its asymptotic variarice-covariance
matrix is given by

-i-i
X(X'X)

p1iinT

In

x'] aoj

.

(5.2)

this paper we will define the NL3S more generally as the value of

a that minimizes f' A f where A could take any

one

of the following three

forms:

A1 =

A

2

—
—

A S1(SS1Y' S A4
A
A

,

(5.3)

_-

2'2 2' 2

(5.4)

— 18 —

and

A3 =

A s3Y-

s3(S3

S

A

(5.5)

where S1, S2, and S3 are matrices of at least asymptotically
nonstochas-tic

variables and

= Q

A

0 I.

The asymptotic variarice-covariance

matrix is given by

r

plim T1 --

L

All

the

A
a0

three formulations are

1-i
b-.,

(5.6)

.

aoj

equivalent

in the sense that

can be made equal by appropriately choosing

A1, A2, and

A3

Si, S2, and

S3. If we take

X0. .0

123

Ox
:

0

X

all the three are reduced to the Jorgenson-Laffont NL3S. It is apparent
from (5.6)

that

for all A1, 11,

2, 3, its

lower bound is

equal

to

— 19

r
L

—

limTE (crl®
A

,

The

lower-bound is attained when S1

arid

S3 = E

that appears

, where

in E .-,

(5.7)

I)

E-

A

,

S

E

2

we are iixlicitly assuming that the a

must be estimated consistently. We will call the

resulting NL3S estimator where any of these optimal S's is used as the
best nonlinear three-stage least squares estimator (abbreviated as BNL3S).
af
is usually difficult
This is often not a practical estimator because E

to obtain in explicit form, but the consideration of BML3S is theoretically
useful as it provides something to aim at.

One can also attain the lower bound (5.7) using the Jorgenson-Laffont
NL3S, but that is possible if and only if the space spanned by the column
vectors of X contains the union of the spaces spanned by the column

f.

vectors of

E

i

a

for i

1, 2,.. .

,n.

This necessitates including many

columns in X, which is likely to increase the finite sample variance

the

estimator although it has no affect asymptotically. This is the

disadvantage
of

of

of the Jorgenson-Laffont definition compared to the

definition

this paper.
We will next show that the BNL3S is asymptotically less efficient

than

the

maximum likelihood estixtor. Using the lemma

we

have

— 20 —

—
- g.1 U
U.
1

.

(5.8)

1a1'uE}

EfL
Similarly

aEg;Eg

a1

we have

g.

EEg..ua1[

=-a

-g u?a

Eg Eg' .

(5.9)

We have

E(E g1

u' a' a1 u E

a'1

g1)

E g. E g'

ii

(5.10)

.

We obviously have

ENuu' -

Q)a' ai' u

E g.]

0

u' a' a1(uu' -

Q)]

(5.11)

and

ECE g1

0

(5.12)

.

Therefore, from (3.10), (3.11), (3.12), and. (5.8) through

him E(p.1 +

p12

- —-— E g. •

(5.12)

j2/

u' a')(p' + pt
j1

we have

-

——a u E

g4)
J

(5.13)

limT_1E
I

L

'I

iI

I-a 1imTEg1Eg'

1

- 21

term of

The first

right-hand side

the

—

of (5.13) is the ij

th block

of the inverse of the asyirptotic variarice-covariance matrix of the maximum

likelihood estimator

and

evident

But the matrix whose

from (5.7).

left-hand

the second term is that of the BNL3S as it is
i-j

side of (5.13) is clearly nonnegative definite. Moreover,

since the matrix is nonzero with probability
that

the

th block is given in the
one in

general, we conclude

BNL3S is asymptotically less efficient than

the

rrximum

likelihood estimator.

Although the NL3S is asymptotically less efficient than the maximum
likelihood estimator, it

is

more

robust

against non-normality because it

is

consistent provided the error term has mean zero and certain higher-order
finite moments whereas the concistency of the maximum likelihood estimator

in the nonlinear model depends crucially on the normality assumption as
we have seen in Section 3 above.

A necessary arid sufficient condition for the matrix to be inverted

in

(5.7) to be nonsingular is easily seen to be condition 6

of Section 3. In the linear case this condition implies the usual rank

condition
case

of identifiability for each equation. However in

the above condition is likely to be met

even

the nonlinear

if all the exogenous

variables appear in each f1 provided f1 is sufficiently nonlinear.
Because of (5.13), condition 6 implies assumption D of the appendix.

The Gauss-Newton iteration to obtain the

a2

a1 —

[G'

(cr1-s I)

BNL3S can be written as

GJ G' (clØ I) f

(5.15)

— 22

—

I

where

G'
0
1

.

.

0

(5.16)

o

o

n

arid

1

E G!

(5.17)

1

Equation (5.15) differs from (Li..8) only in the respective "instrumental
variables" used defined by (5.17) and (Lf.5)

respectively.

Intuitively

speaking, . catches more of the essentially nonstochastic part

of

than G. does. Note that by a Taylor expansion we have

u

Cut) g (0) +

But (14.5)

can be written

as

(ut) g + T

The

1

au . ut

____

similarity between (5.18) and (5.19)
as

in 1(o)

(5.19)

.

provides some justification of

the alternative instrumental variable. The
must

(5.18)

.

that appears

be consistently estimated. The resulting NL3S is

I

— 23 —

asymptotically

less efficient than

the

BNL3S but is much nore practical.

An even nore practical choice of the instniment is to use

where;
linear

A
be

is

calculated simply as the predictor of y obtained by the

least

squares regression of y on all the exogenous variables.

definite comparison between this choice and the use

of g (0) can not

easily made.

So far in this paper we have assumed that there are no constraints

removal of this assrtion, however, causes no difficult
problem. If there are constx.ints anong ct.js, we can express each a1
among als.

The

parametically as

a1() where the number

those in c (aj c,. . . , cc1)'. Thus,

of

one can

elennts in

simply premultiply the

of the asymptotic variance—covariance matrix

estimator

or the NL3S by

results of the

paper hold.

is fewer than

of

inverse

the maximum likelihood

and postmultiply by

.

Hence,

all the

-2

6.

-

Conclusions

We have proved that the maximum

likelihood

estimator is asymptotically

more efficient than the nonlinear three-stage least squares estimator.
However we have also shown that

the

consistency of the maximum

likelihood

estimator depends on the assumption of normality whereas that of the

nonlinear three-stage least squares is not. This fact increase the

attractiveness of the latter. The following are some important topics
for further research:
1)

Evaluate

the degree of the relative inefficiency of the best

nonlinear three-stage least squares estimator as compared to
the maximum likelihood in specific models.
2)

Evaluate the degree of the
of

realtive inefficiency of several versions

the computationally practical nonlinear three-stage least

squares estimator as compared to the best nonlinear three-stage

least squares estimator in specific models

3) Is there an estiirtor, possibly even better than the best
nonlinear three-stage least squares estimator, which is
computationally simpler than the maximum likelihhod estimator?

Can that estimator remain consistent when the normality
assumption

is removed?

— 25

—

References

1.
2.

Amemiya, T. [1974], uThe Nonlinear Two-Stage Least-Squares
Est:ijiiator,' Journal of Econometrics, Vol. 2, 105-110.

Hausman, J. [1975], 'An Instrumental Variable Approach to
Full Inform-tion Estimators for Linear and Non-Linear Econometric
Models , r Econometrica, forthcoming.

3. Jorgenson, D.W., and J. Laffont [1974], "Efficient Estimation

of Nonlinear Siuuiltaneous Equations with Additive Disturbances,"
Annals of Economic and Soc ialMeasurenien-t, Vol. 3, 615-640.

p

-Al-

APPENDIX

Assumpitons
We nake

-the following assumptions in additon to the basic assumptions

of the model stated in Section 2.

A.

plirn

B.

—--

C.

plim

-N

/T

and

D.

(a) exists in a neighborhood of

f

_____

in T1 E

exists

'

in a neighborhood of ct0

the convergence is uniform in the neighborhood.

plim T'

definite.

is negative

a0
—
E.

- pin T'

.

him T1 E

a0

a0

a0

Theorem. Under the basic assumptions of the model stated in Section 2 and
assumptions A through E above, a root of the equation

and the consistent root a

satisfies

(a-a0)

O

is consistent

pin T [a
Nf_

ao])

-A2-

Proof.

Expanding T1

L(ct) in a Taylor series around

the

true

value

we have

T1 (a)

+ f1 (a

f'

a0)
(A.l)

-l
aa
2

1
2

o

(aa)
o
aT

where

lies between a and a(). Taking the probability limit of both sides

of (A.l) and using assumptions A, B, and C, we have

plim T1 L(a) plim T1 L(a0)
(A.2)
2

1
+-(a-a) plT
.

Since

aT

is continuous by a basic assumption stated in Section 2,

assumption C implies that plim T1

is

continuous in a neighborhood of

a0. Therefore, by CD), the second tern of the right-hand side of (A.1)
—l
is negative for all a in a neighborhood of a0. Therefore, pin T L(a)
attains a local rrximum at a0. This implies that a root of equation

0 is consistent. The asmptotic normality follows easily from
assumptions B through E using the Taylor expansion

I

-A3-

+
c.

a2i

** (c

—

c)

cx

where a is the consistent root

and

lies

between a

noting the left-hand side of (A. 3) is zero by

and
A

of a.

the definition

(A.3)

