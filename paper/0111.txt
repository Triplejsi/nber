NB WOR1G PAP. SIES

CONFIDENCE REGIONS FOR ROBUST REGRESSION

y E. We1sch
Working Paper No. 111

National Bureau of Econic Research, Inc.
575 Techno1or Square
Cambridge, Massachusetts 02139

November, 1975

Pre1sninary: Not

NBER working

Quotation

are distributed infonilly and in limited number for
They should not be quoted without written permission.

papers

cmnts only.
This

for

report has not undergone the review accorded official NBER publications;

in particular, it has not yet been submitted for approval by the Board of
Directors.
Bureau of Economic Research, Inc., and Massachusetts Institute of
Technology. Research supported in part by National Science Foundation (kant
No.

GJl151.X3.

Abstract

This paper describes the results of a Monte Carlo study of certain aspects

of robust re'ession confidence region estimation for linear rrdels with
one, five, and seven parameters. One—step sine estimators Cc

l.2) were

used with design matrices consisting of short-tailed, Gaussian, and longtailed columns. The samples were generated from a variety of contaminated

Gaussian distributions.
A number of proposals for covariance matrices were tried, including
forms derived from asymptotic considerations and from weighted-least
squares with data dependent weights. Comparisons with: the Monte Carlo

"truth" were made using generalized eigenvalues. In order to measure
efficiency and compute approxizxte t-values, linear combinations of parameters

corresponding to the largest eigenvalues of the "truth" were examined.
For design matrices with columns of rrcdest kurtosis, the covariance

estimators all give reasonable results and, after adjusting for asymptotic
bias, some useful approxizrate t-values can be obtained. This implies that
the standard weighted least-squares output using data-dependent weights need

only be modified slightly to give useful robust confidence intervals.
When design matrix kurtosis is high and severe contamination is present

in the data, these simple approximations are not adequate.

TABLE OF CONTENTS

1. INTRODUCTION

1

2. THE REGRESSION ESTfl4ATOR

1

3. COVARIANCE ESTfl'ATORS

1

4.

3

THE

X—I"IA.TRIX

Table 4—1 Final Standardized Coliixris of
5. TUE MONTE

VDATA1

CARLA)

3

6. LOCATION

4

Table 6—1 ACIL (xl000)

4

Table6—2 t

14

Table 6-3 Adjusbint

Factor

Table 6—4 Adjusted t

5
5

5

7. REGRESSION RESULTS
Exhibit

7-1

H(xlOO) for (XTWX)_l

5

Exhibit

7-2 D(xlOO) for (XTWX)1

6

Exhibit 7-3

Median

H(xlOO) for Adjusted Covariances

Exhibit 7-4 ACIL (xlOO) for ?ssion
Exhibit
8.

3

7-5

CONCLUSIONS

Adjusted

t*

6
6
6
7

ACOWLGU1ENTS

7

REFERTCES

7

—1—

1. ThTRODUCION

in

In the past few years, a number of ways have been
proposed to perform robust regression. Perhaps

the simplest to impleirnt is called iteratively
reweighted least-squares. Initial values for the
coefficients are found, a scale for the residuals
from this start is computed, and then a set of
weights is detenidned by using a weight function
applied to the scaled residuals. The weight function usually gives a value near one for small
residuals, and near zero (or equal to zero) for
large residuals. The weights are then used as if

Therefore, all one needs is a device to compute

the start, scale, and weights and the rest of the

computation (the hard part) can be done with a
standard WLS routine or by multiplying each observation by the square root of the corresponding
weight and using an LS routine.

Naturally, the question arises -

it may not be wise to use a one—step estimator.
-X
For an initial scale, we form r

arid

compute
=

.6745

The weights are found from the function

w

the
output from the WLS routine to find confidence
regions for the regression parameters? This

more readily usable. One purpose of our study of
robust regression was to see if it would be feasible to use WIS output in a simple way.
This

paper is organized as follzs: section two

discusses the robust estimator (weight function)
we used, section three covers the covariance formulas, section four examines the desigo CX] matrix
we used, and section five sumiarizes the parameters
and distributions used in the Monte Carlo. In
section six, we discuss our results for the location case. The seventh section covers our work on
the regression problem and in the last section, we
try to give some advice and indicate where we

think we stand at this point.

2. THE REGRESSION ESTIMATOR

We want to estimate the parameter vector in the
model

vector, X is an
n x p design matrix, arid e is an n x 1 random vecwhose ccordinates are independent and identically distributed symmetric random variables.
tor

Only one

family

of

estimators

is used,

rnator (called LAR):

Lo

itt
itt

C

(2.1)

> 7T C

where t is replaced by the scaled residual, ri/S.
Often w (t) is approximated by Cl _(.)2]2 which is

the bisquare weight function. In this study we

used c 1.42 (or 2.' .f the .6745 in s is omitted). This choice or c corresponds to about 96
percent asymptotic efficiency for the Gaussian
error imdel.
Once we

have found the weights, our estiate is;
-

-lT

(X

)

X

(2.2)

W is an n x n diagonal matrix of the weights.
For reference purposes, we also computed the standard least-squares estimate
The weight function for least-squares is, of course, w (t) 1.
Some properties of SIN1 are explored in C 7].
where

.

COVARIANCE ESTIMATORS

In order to constrict confidence regions, we
liice to estiiMte the covariarice matrix for
A WLS program automatically gives us

(W) w2

the one-

step sine M-estisnators, which have been found to
have good robustness properties in a number of

previous studies. As starting values for this
estimate, we used the least absolute residual esti-

tO

., Jsin (tic)
- t/c

3.

is the n x 1 response

11

(t,

+

where y

(largest n—p+l elements of rj }.

median

This ncdified median absolute deviations scale
CHMAD] is discussed in C 7] arid is especially
designed for an LAR start.

can we use

would make life simpler and make robust regression

5:].

Several efficient algorithms exist for finding
this estimate. (See C 3] for an exa'r'.ic.) If
only least-squares is available as a tort, then

they were the weights for a weighted least-squares
(WLS) regression. The above process can, of

course, be iterated.

.

x..

. ic].

BIAR:min fl

where r: as

(xT) -l

Viewing the

a measure of "degrees of freedom"

would

(3.1)

of weights
(note that the
sum

—2—

weight function has been standardized to unity at
the origin), led us to propose

0.

det (A—AB) =

(3.7)

are known as gtnara1izad
of onsidering uot
we followed a suggostion of key [13]

The solutions of (3.7)

eigenvalues. Instead

(3.2)

wi'i2 (XTWX)

(WW)

and

and computed

i1wi_p

H' £

p=1 A

as an easily computed alternative.
Asymptotic considerations (see £ 9])
a good choice might be

P5
2n

(A)

ialy that

r•

2 — (XTx)_l

(3.3)

i

(......_ A.)

2

(3.2)

The uantity, H, is not scale invariant. In order
to measure difference in shape we followed a
suggestion of Paul Holland and put A er 3 in

correlation form. Since correlation matrices are
not invariant (i.e., do not remain correlation
matrices) under changes in the basis of the Xspace, this approach is very X dependent. H does,
however, use more than one parameter to remove
"scale".

where (r) r w(r). Mother alternative that we

began to use after a part of the study was completed combined pert of (W) with part of (A) to get
(AW)

()2

A one parameter scale—free approach would be
to consider the mirthnujn over d of H2 ( d) where

(3.9)

H2(d)

(3.)

2(r (XTO_l

not do this for the full covariance matrix
but we did use it for the diagonal elements (the
We did

2

variances). We computed
Many other forms have been proposed in C 9]

a1fb

and

[13].

d=

What is the truth to which we sluld compare these
foii:? There is no agreed upon answer, but we

il

chose the unconditional covariance matrix

•L
i1

(1.10)

2

A
D

where R is the number of samples used in the Monte
Carlo. We omphasize the word unconditional because a

covariance matrix proportional to (XTWX)_l

involves the weights and is clearly conditional on
the data.
How can we measure the difference between, say,

(3.1) and (3.5)? Basically, we are seeking confidence intervals for linear combinations of the
If the true covariance matrix
parameters,
is A, then .QTAC is the variance of this linear
combinatioriT ince we did not want to consider,
at this point, specific linear combinations We

chose to examine for an estiwated covariance
matrix, B,
T —T/2

t.

—1/2

.

B

'

2, and

1

Id::

2

v1

(3.11)

is, of cot'se, invariant under a. change of
basis.
H was tried on the full covariance matrix after
some adjusthent factors, suggested by Cohn
Mallows Cli] were used to correct for asymptotic
bias.
The above measures could be termed "diaostic"
and are useful in finding which classes of coven—
ance estimate seem reasonable. We would still
need, however, to consider the "scale" of the
covariance matrix and find something like tstatistics. There are two kinds of scaler multipliers of (XTXY or (XWXY1, those that depend

(3.6)

——

where u

2

1/2 denotes one of the stan—

den ways to find a se;uare root of a positive
definite matrix. The right-hand side of (3.6) is
equivalent to finding the largest value of A in

the data and those That do not. The later
kind are very difficult to separate from tstatistics and can make it hard to develop useful
approxisite t tables.

on

We can choose among data dependent scalars by

looking at confidence region hsizeT. In the

—3—

TABLE

14—1

FINAL, STANDARDIZED COUJMNS OF VDATA1

COL5

COL6

COL].

COL2

COL3

1

0.2712

0.2712

0.01453

0.0257

—0.3880

2

0.2712

0.1627

0.1092

—0.1268

—0.0509

0.01470

0.0963

0.01143

0.0682

ROW

COL14

3

0.2712

0.05142

0.14513

14

0.2712

—0.05142

—0.1605

0.2977

—0.1065

0.0225

5

0.2712

—0.1627

0.22142

—0.3618

0.21463

0.3193

6

0.2712

—0.2712

0.0107

0.12146

—0.08114

0.01461

7

0.1627

—0.2712

0.1937

0.1006

—0.0373

0.0583

8

0.05142

—0.2712

—0.21435

0.3205

—0.1373

0.014014

9

—0.05142

—0.2712

—0.009'4

—0.0852

0.0228

10

—0.1627

—0.2712

0.1382

0.14631

—0.0630

—3.0112

11

—0.2712

—0.2712

0.0956

0.09814

—0.01489

0.0388

0.0597

—0.1136

—0.0732

0.0327

12

—0.2712

—0.1627

13

—0.2712

—O.05142

—0.0613

—0.1263

0.0914l4

0.0303

114

—0.2712

0.05142

0.1282

0.0598

—0.0680

0.0691

15

—0.2712

0.1627

—0.0966

—0.0085

0.1387

—0.0672

16

—0.2712

0.2712

—0.1060

—0.3819

—0.13140

0.0559

17

—0.1627

0.2712

0.2013

0.01145

—0.0290

0.0966

18

—0.05142

0.2712

—0.143214

—0.2083

—0.1520

—0.9198

19

0.05142

0.2712

0.09114

0.08140

—0.01417

—0.0620

20

0.1627

0.2712

—0.51486

0.051414

0.8917

0.0833

•

location case this can be accomplished by cornputii average confidence interval lungth once a t—
statistic has been determined.

In the regression case measuring "size" is nre
complicated. Here we chose to consid& specific
1irar tem tior of -the parmeteru that the
problem could be reduced to considering tstatistics and average confidence interval lengths.

The linear combination we chose was the eigenvector
of the largest eigenvalue of A, the Monte Carlo

"truth". This corresponds tothe linear combinamis
tion giving the largest variance,
means, ot course, that 9- changes with the sampling
situation.
All of the X-rnatrices used in our Monte Carlo
study vere derived in various ways from a basic
20 x 6 matrix, Vrt'TA1. First we describe the
constructi:- of VDATA1 and then indicate how the
X-rnat'ices ised in the study are derived from it.
6 colurns of VUTAl were divided into 3

colrnm was standardized to have mean zero and unit
sum of squares. In Table 14-1 we give the final,
standardized colusms of VDATA1. Further details
about VDATAJ. can be found in C 7].
The two X-matrices we used were formed by append-

ing a colus7l of ones to the

first

14 columns of

to the number of
fit-ted parameters) and to all of VDATA1 to get V7.
Thus V5 could be considered to be well-behaved
while V7 contains outliers.
VDATA]. to get VS (where 5 refers

14. THE X—MATRIX

The

1

6 were chosen to be r
inc1epndent bivrizite variabicu vitil
independent Cauchy samples of st- were dra
and then the largest observatics c:h sample
were reduced in ms.gnitude until they contributed
80% ar 5% of the total s.zn of squ:res of their
coluzt, respectively.
After the 6 cohsrus of VDATA1 were selected each

Colunns S and

groups

of 2. Colurnz I and 2 were chosen so that their
scatter plot forms a perfect square centered about

the cri i. Thus the first two coltrrs correspond

to va i:lc like those in a designed experinent.

Colurrr 3 d 14 were chosen to be roughly independent biveritte Gaussian.
-

5. THE MONTE CPdLO

The Monte Carlo work was done in two parts. The

first dealt with the location problem for samples
of size 10, 20, and 60 using SThIl. Swindling
techniques, such as those described in [12] and
C

1J,

were used

th'oughout.

The Gaussian and the

slash (see C 1]) were the error distributions.
The slash distribution has the vcrv long tails of
a Cauchy distribution, but with a Gaussian-like
center.

—4—

The second part using VS and V7

relied

ACIL residuaJ.+coLsin fit (Cr).

on samples

generated by Richard Hill and Paul Holland for
their related study C 7] where it was felt that
Monte Carlo swindling would not be worth the cost.
Unfortunately, this meant we could not use swindling for out study, except in the Gaussian case.

The standard errors

ACIL (xl000)

1— a +

a k2.

invariant in the sense that if the vector of
X " for
observed values y is transformed o
then 3 is transformed to
+ 8'. Here a.].].
some
of the Monte Carlo results were computed with the
true values of i set equal to 0.

In all cases the nurrer of Monte Carlo samples
was 500. Standard errors were computed directly
where 5
except in the case of the
blocks of size 100 were used to obtain an estimate
of standard error.

t-statistes

6.
The

LOCATION

location problem with X equal to a colunn of

point of the empirical distribution of the t—
statistics, and a measure of sampling error. We

will study t

in detail later.

analyzed by using medians
umn into:

to decompose each col-

23'43

SE

8

3L

0

CF 1673

39LL

90

SE

23

n 60
G
W
WV

1

0

0

—2

-l

2

S'
0

CF 538

1231

2

10

in all cases, (W) is worse than (A),
but probably not enough to warrant writing special
programs to compute (A). Except for two cases
(WV) is better than (A), but the gain is within a
single standard error.
We see that

To make these confidence procedures work, we need

to find a t-like value that is independent of the
underlying distribution, since we do not Iciow
what that distribution is. Our t" values are listed in Exhibit 6-2. The standard errors listed are
the median of the colusri standard errors.

In this case, the t' value at the Gaussian is al-

ways larger than that for slash, so we will focus
our attention on the Gaussian values.
TABLE 6—2

tA

nlO
G

W
WV
A
SE

n20
G

S

S

3.21

2.88

W

2.53

2.149

2.93
2.71
.09

2.53

WV

2.39

2.23

2.143

A

2.21

2.13

.03

.014

.06

SE

n60

a measure of

numbers 'times 1000) are their standard errors are
displayed in Exhibit 6—1 as tables that have been

991

0

WV
A

efficiency, we computed the average
length of the intervals [ACIL] that resulted when
was used to form confidence intervals. These

As

CF

—60

ones provided a basic staring point for our study.

Location has been extensively studied by Gross C 5]
who found that an estimator similar to SIN1 per—
formed very well when used with the (A) variance
to construct confidence intervals. Of course, we
were mainly interested in seeing if (W) would
also work, since it comes naturally from weighted
least-squares.
For each sampling situation and variance estinator
we computed a value, t", corresponding to the .975

0

—5

CE

Both of the estimators LAR and SIN1 are regression

_!4

WV

A

where

—9

57

5

Thus

g()

S

10
0

W

V

A

ei

0

S

G

1u21

.

n20

nlO

where0<a<l,andl<k<. Thesearedenoted
COke so that CG3.l indicates a choice of g where
k 3 and a = .1. In all cases the scale of f(u)
was selected so that the errors had unit variance.
f(u)

bottom

TABLE 6-1

The error distributions, f(S), for the regression
case are a simple 2-parameter family of a mixture
of a N(0,l) density with a N(0,k2) density. This
is given by

g(u) _L I(l_a)e_u/2 +

(SF) displayed at the

of the table are the median of the standard errors
of the ACILs in the respective colunn.

The regression results are therefore less precise
than we would have liiced. For details on the
swindles used in the Gaussian case see C 8],

r

(6.1)

.

W
WV

A
SE

G

S

2.30
2.20
2.05
.01

2.27
2.05

1.95

.oa_

—5—

This leaves us with a table of t nwnbers that we
could use with fovmu].a (W) and feel reasonably

sure of getting 95% confidence intervals of ncder-

ately high efficiency for distributions in the
"neiborhood" of the Gaussian.

We had originally hoped to base the degrees of
n

freedom on E w.-1, i.e., to have a conditional

il 1

degrees of freedom formula. This has not worked

well, giving t-numbers too small at the Gau.on
and too large at the slash. A close look at our
tables of t for (W) shows that it is not easily
related to the standard t-tables. However the t'
tables for (A) seem to be approxinable by the
standard t on (n-l) /2 degrees of freedom. (Gross
[ 5] also noticed this.)

19 and 59 degrees of freedom, so we were not
expecting and did not get the pleasant —'c-suits

obtained in the location case. On the other hand,
we see many regressions n with 2 and 3 degrees of freedan per parameter and we felt it was
necessary to gather soma infoziration about these
cases.
We first computed H [see (3.8)], comparing the
Monte Carlo th to (XTXY1 and (XTWX)_l where
all matrices have been t in corr1ion form.
The results are contained in Exhibit 7-1 where
we have included the midspread (interquortile
range) of the H values for (XTWX)_l and also the

values of H obtained when th andard least-

squares Monte Carlo truth is compared to (XTX)_l
——
in correlation farm (LS).

Mallows [11] had proposed a form of asymptotic
adj ustoent for covariance fcrmlas that would

DJIBIT 7—1

renove any asymptotic bias relative to the correct
asymptotic forrnla, (A). For example, the adjustment factor for (W) is:

H(xJ.00) for (XTWX)
VS (LS:lO)

E(w(Z))

W E2p'(Z))
where Z is N(0,l)
rnent

t

since we want to do the

for the Gaussian case. To adjust

(XTx) -1

adjust—
we use

(jJ')l/2 Table 6—3 lists these t adjust!Ent

factors which have been computed by using the
bisquare approximation to the sine weight function
with b .72 (or l.5r).

MS

Mjustnent Factor
.89

.93

WW
AW

l.0L

A

1.00

degrees of freedom approximation.
TABLE 6-

Adjusted t
W

WW

A

t

3.25

10.1

10.25

10

20

60

2.86
2.73
2.71

2.25
2.22

2.78

2.26

2.05
2.05
2.05
2.05

2.21

L2
7. REGRESSION RESULTS

For the regression problem we only used one sample
size, 20, and the matrices VS and V7, giving 15
arid 13 degrees of freedom or 3 degrees of freedom

per praneier for V5 and aobut 2 degrees of freedom per parsmater for V7. For location we had 9,

8

12

17

21

42

12

18

23

32

53

8

10

11

17

19

(XTX)_1 28

28

32

108

814

27

31

36

119

2

7

8

16

100
17

MS

}thinit 6-4 contains the adjusted Gaussian t
volues. W sce that the values are much nore comparable and that
is a useful, but not perfect

n

3.1

V7 (LS23)

TABLE 6-3

W

G

E(Zi(Z))

These results seem to indicate that (xTX)_l is a
bet-tar choice than (XTWX) wost of the time. In
other words, for overalT shape, it may not pay to
use a forn conditional on the data. On the other

hand, we have been comparing both forms to the unconditional Monte Carlo truth. We do rioL yet ce
how to do these comparisons in a conditional way.

Next we looked at D [see (3.ll):1 and found a similar story (Exhibit 7-2) although less pronounced.
The worst cases occurred, as we might expect, for
V7 and for CG1O.1 and CG1O.25. We are still
puzzled by the fact that for V7, CG1O.l is worse
than CG1Q.25. Similar results show up in later

tables.

At this point we decided to look at six covariance
formulas - the four discussed in section three and
(W') which is (W) but using (XTX) instead of
(XTWX)_l and (riM') which rrodifies (WW) in a simi-

lar way. The asymptotic adjusthent factors developed in section six were applied (the factors for
Wt arid WW' are .85 and .89) but this tire we used
AF and not
since we. were looking at vari-

ances and not t-statistics.

H, as defined in (3.8), was then computed for all
these cases; the results form Exhibit 7-3. The

—6—

ttBIT 7—2

eigenvectors discussed in section three. As in
the location case we computed a t for each
situation and then the average cc fiünce interval
length. The ACIL results are listed in Exhibit

D(x100) for (XTWX)_1
VS (LS=5)

MS

DiB 7—4

10.1 10.25

3.1

3.25

7

9

12

16

21

9

U

114

19

2

5

5

7

28
11

G

(XTxY1

7—4.

ACIL (xlOO) for Regression
V5 (LS536)

V7 (LS8)

(XTx) 9

W

NED

9

12
15

MS

2

5

12
16

91
98

57
66

6

8

9

14

2

5

1

—l

1

1

—l

—6

0

—5

—l

1

3

—4

—1

0

566

481

472

217

220

(LS=864)

V7

G

0

3.1
5

NW

1

0

AW

5
0

ww'

A
CF
SE

3.25
1

10.1 10.25
5

—3

—3

—30

1

6

1

—2

7

—1

2

17..

0

0

—3

—8

—20

2

—1

3

—2

19

60

78

100

29'4

88

130

119

l])4
152

—1

20

12

381

V7 (LS—614)
0

1

NW

—3

—2

1

—17

—76

AW

4

8

15

51

1414

W,

0

-3

—S

—17

—12

5

7

...49

—100

4

3

9

18

11+

75
155

102

113
227

456
600

1052

W

NW'

A
CF

SE

4

2114

w

21

6

26

14

—9

NW

—4

—2

5

3

0

AW

—5

2

314

5

—12

—6

—2

0

984

826

812

464

522

A

1

CF

of the standard errors for each column, SE,
show that these n.rrers are quite variable and
comparisons will be difficult. Generally speaking (NW) and (NW') perfono well, confinning soma

beyond (W) or perhaps (NW).

Finally we computed adjusted t*_valucs, as in the

location case. These numbers are listed in
GS, the values obtained by swindling rather than
Exhibit 7-5 where we have added a new column for

simple sampling. Since the maximum swindle gain
occurs at the Gaussian, we may not have lost too

ff BIT 7-5
Adjusted te
V5

W

GS

3.1

3.25

10.]. 10.25

2.32

2.35
2.30
2.38

2.38
2.36
2.33

2.53

2.149

2.41
2.47

2.141

2.37
2.30
2.25
2.22

2.25

2.35

2.33
2.48
2.23

.04

.06

.10

.14

.10

2.58
2.48
2.53

3,614

3.39
3.31
3.13
2.97
.08

NW

2.31

AW

2.26
2.21

A
SE

(W') a.-id (WV') because (XTX) is not available
when computing SIN1 using weighted least-squares
The form (A) was cr'ried along as a benchmark.
We now u-'ied to measure efficiency by using the

(t7=2.37)

G

.10

of our earlier results. It also appears to be

the case thr fonos involving cxTxYl perform a
littic better, which agrees with Exhibits 7—1
and 7-2. We decided not to continue to look at

7

The average standard error in this table is about
10 so -that it is hard to make any real distinctions. Generally, (AW) seems to perform best.
As far as efficiency goes we see no reason to go

573

- medians

10.1 10.25

—l

Median H(xlOO) fox' P4justed Covariances
VS (LS57)

3.25

14

A

E)IBIT 7—3

3.1

—3

Nw

CF

C

V7

(t6=2.45)

2.44

2.55

AW

2.62
2.61
2.53

2.37
2.53

A

2.144

2.55

.13

.014

2.59
2.46
2.39
.03

w

SE

2.141

3.53
3.52
3.28

.13

.28

—7—

naich by our inability

bers

to swindle all of these num-

EFERflES

(see section three).

No longer can we say that the values at the Gaussian

are the largest, so perhaps adjustttent at the

[1] Andrews, D.F., Bickel, P.J., Hainpel, F.R.,
Huber, P .J., Rogers, W.H. and Tukey, J . W.

of Location,

(1972). Robust E'stimates
Princeton University Press,

Gaussian is suspect. However, for VS we would feel
reasonably happy with t7 2.37 as an approximate

t-value for SIN1 regression, especially for (M).

[2) Andrews, D.F. (197'+). A Robust Method
Ftiltiple Linear
16, 523—531.

this breaks down for V7 and CG1O .1 and

Clearly

CgiO.25 where t6

2.45 is

not

adequate. Severe

contamination coupled with a distorted X-matrix
(high kurtosis) has diminished our hopes for sinpie approximations.
8. CONCLUSIONS
Strictly interpreted our results only apply to the
specific situations and X-matrices examined in
this study. However, we would like to generalize

somewhat. Hill and Holland [7] have shown that
SIN1 is a reasonably good robust regression estimator. We feel that the usual output fran the one-.
step of weighted least-squares can be used for

inference providing the t-statistic is found using
[fl2.] degrees of freedom and then divided by the

adjusthent factor (.89). If the X-matrix is really
unusual. then caution is advised.

examining what happens

839—8'+8.

['+] tknby, L. and Larsen, W.A.

(197'+). Robust
Regression Estimators Compared Via Monte Carlo.
Bell Laboratories, 'I11—74—1215—26/121'4—18.

[5] Gross, A.M. (197'+). Confidence Interval

Robustness

results

[6] Gross, A.M. (1975). Confidence Intervals
for Bisquare Regression Estim-te. Bell
Laboratories Technical Memorandum.

[7]

n
il 1

or Ejq'. work? For the abnonnal X-matrix

1=1
problem

we siay have to examine the proposal of
Mallows [10 J which attemDts to smooth the X-matrix

and reduce its kurtosis.

Finally, there is the question of how these results
generalize to the F—statistics associated with more
complex simultaneous confidence regions.

Massachusetts.

[8] Holland, P.W. (1975). A Variance

Technique

Reduction

for Monte Carlo Studies of Robust

Confidence Intervals. Unpublished

Mcnrandum.

[9]

doesn't sons condi ional degrees of freedom forym.ila
E

Holland, P.W. (197'+). A
Monte Carlo Study of Io Robust Alternatives
to Least Squares Regression Estiri'ation. NBER
Computer Research Center Working Paper No. 58,
National Bureau of Economic Research, Inc.,
Hill, R.W. and

Cambridge,

when n'+0. Why ['] for degrees of freedan? Why
n

with Long-Tailed, Syrrmetric Dis-

tributions. Submitted to JASA.

Regression

to our regression

P.J.

(1973). Robust Regression:
Huber,
Asymptotics, Conjectures and Monte Carlo.
Annals of Statietias 1, 799—821.

[10] Mallows, C.L. (1973). InfliTience Functioiis.

Talk

at NBER Conference on Robust Regression,

21 June 1973.
[ll]

(1973). On Some Topics in
Robustness. Paper delivered at the Eastern

Mallows, C.L.

Regional ASA Meeting, University of Rochester.
[12]

Relies, D.A. (1970). Variance Reduction
Techniques for Monte Carlo Sampling from

Student Distributions, Technometrics,

ACOWLG'TS
The author has benefitted greatly from the advice
and assistance of Paul Holland, David Hoaglin,
John Tukey, Alan Cross, Richard Hill, Stanley
Wasserman,

arid Sheila Howard.

This work has been supported by National Scionce
Foundation G'ant GJ-ll5X3 to the National Bureau

of Economic searth.

for

Regression. Tachnometrics

tion.

of this instability.

We have left many questions unanswered. We are

N.J.

[3] Barrodale, I. and F.D.K. Roberts (1973). Art
Improved Algorithm for Discrete L1 ApproximaSIAM Journal of Nwnerical Analysis 10,

We have most often used the above results in a
diagnostic way. The least-squares and SIN1 regression results are both obtained and then compared

in various ways. If the confidence intervals are
radically different (or test results reversed) we
explore further, attempting to diagnose the cause

Princeton,

12

'+99—515.

[13] Tukey, J.W. (1973). A Way Forward for Robust
Regression. Unpublished Memorandum.

