NB. WORICNG PAP. SERIES

ROSEPACK

Rank Degeneracy arid

tocument No.
LEast

Suares Problems

Gene Golub*
Stan.for University

Virginia Ktema**
National Bureau of Economic Research
G. W. Stewart4
University of Maryland
Working Paper No. 165

Computer Research Center for Economics and Management Science
National Bureau of Economic Research, Inc.
575 Technology Square
Cambridge, Massachusetts 02139

February 1977

NBER working papers are distributed infonTally arid in limited
nmibers for cments only. They should not be quoted without

written

pennission.

*

Supported

in

part by Contract No. Anriy DAHC04-75-G-0185

NSF DCR75—131497.
Supported in part by the National Science Foundation under
Contract No. IXR-75-08802.
Supported in part by the Office of Naval Research under
Contract No. N0001'4-76-C-0391.

1. Introduction
In this paper we shall be concerned with the following problem.

Let A be an m x n matrix with m n, and suppose that A
a sense to be made precise later) a matrix B

is near (in

whose rank is less than

n. Can one find a set of linearly independent col.mins of A that span
a good approximation to the column space of B?
The solution of this problem is important in a number of applica-

tions. In this paper we shall be chiefly interested in the case where

the columns of A represent factors or carriers in a linear model
which is to be fit to a vector of observations b. In some such applica-

tions, where the elements of A can be specified exactly (e.g. the
analysis of variance), the presence of rank degeneracy in A can be
dealt with by explicit mathematical formulas and causes no essential

difficulties. In other applications, however, the presence of degeneracy
is not at all obvious, and the failure to detect it can result in meaning-

less results or even the catastrophic failure of the numerical algorithms
being used to solve the problem.

The organization of this paper is the following. In the next section we shall give a precise definition of approximate degeneracy in terms

of the singular value decomposition of A. In Section 3 we shall show
that

under

that

is

certain conditions there is associated with A a subspace

insensitive to how it is approximated by various choices of the

columns of A, and in Section 4 we shall apply this result to the solution
of the least squares problem. Sections 5, 6, and 7 will be concerned with
algorithms for selecting a basis for the stable subspace from among the

-2columns of A.
The ideas underlying our approach are by no means new. We use the

singular values of the matrix A to detect degeneracy and the singular

vectors of A to rectify it. The squares of the singular values are
the eigenvalues of the

correlation matrix ATA, and the right

singular vectors are the eigenvectors of ATA, that is the principal
components of the problem. The use of principal components to eliminate
colinearities has been proposed in the literature (e.g. see [4,9,16,17]).

This paper extends these proposals in two ways. First we prove theorems
that express quantitatively the results of deciding that certain columns

of A can be ignored. Second we describe in detail how existing computational techniques can be used to realize our methods.

A word on notation is appropriate here. We have assumed a linear
model of the form b = Ax + e, where b is an rn-vector of observations

and x is an n-vector of parameters. This is in contrast to the usual
statistical notation in which the model is written in the form y = X3 +

where y is an n-vector of observations and

is a p-vector of parameters.

The reason for this is that we wish to draw on a body of theorems and
algorithms from numerical linear algebra that have traditionally been

couched in the first notation. We feel that this dichotomy in notation between statisticians and numerical analysts has hindered communication

between the two groups. Perhaps a partial solution to this problem is the
occasional appearance of notation from numerical analysis in statistical
journals and vice versa, so that each group may have a chance to learn the
other's notation.

-3norms. The first is the
for an n-vector x by

Throughout this paper we shall use
Euclidean vector norm

II

112 defined
IIxfl

and its

subordinate matrix

The second

= i'l x

norm

defined by

hAil2

= sup llAxil2.

Xlj2l

is the Frobenius matrix
=
IiAll

Both

two

norm

defined for the m x n matrix A by

i=l j=l a..

these norms are consistent in the sense that

(p =
iABii

hiAiliiBhl

2,F)

whenever the product AB is defined. They are also unitarily invariant;

that is if U and V are orthogonal matrices then

iIAII =

=

IIUTAII

iJAVii

(p =

2,F).

For more on these matrix norms see [14].

2. Rank Degeneracy
The usual mathematical notion of rank is not very useful when the

matrices in question are not known exactly. For example, suppose that A
is an m x

n matrix that was originally of rank r < n but whose elements

have been perturbed by some small errors (e.g. rounding or measurement

errors). It is extremely unlikely that these errors will conspire to

keep the rank of A exactly equal to r; indeed iihat is most likely is

-4that the perturbed matrix will have full rank

nearness of A to a matrix

of

defective rank

n.

Nonetheless, the

will

often cause it to

behave erratically when it is subjected to statistical and numerical
algorithnis.

One way of circumventing the difficulties of the mathematical

definition of rank is to specify a tolerance and say that A is numerically defective in rank

if

to within that tolerance it is near a defective

matrix. Specifically we might say that A has c-rank r with respect
to the norm IIH

if
r =

(2.1)

inf

e}.

{rank(B): A-BfJ

However, this definition has the defect that a slight increase in c

can decrease the numerical rank. What is needed is an upper bound on
the values of c for which the numerical rank remains at least equal to
r. Such a number is provided by any number

a

(2.2)

Accordingly

< 6 5 sup

{:

IIA-Bli

6 satisfying

n

rank(B)

>

r}.

we make the following definition.

Definition 2.1. A matrix A has numerical rank (6,c,r) with
respect to the norm HI

if

6,c, and r satisfy (2.1) and (2.2).

When the norm in definition 2.1 is either the 2-norm or the Frobenius
norm, the problem of determining the numerical rank

solved

in terms

of

a matrix can be

of the singular value decomposition of the matrix. This

decomposition, which has many
the following theorem.

applications

(e.g. see {7]), is described in

-5Theorem 2.2. Let A be an m x n matrix with m

n. Then there

is an orthogonal matrix U or order m and an orthogonal matrix V

of order n such that
T
UAV=

(2.3)

0

where

z

=

diag(a1,a2,...,a)

and

?a O.
For
numbers

proofs of this theorem and the results cited below see [14]. The

a2.. . , a, which are unique, are called the singular values

of A. The columns u1,u2,. •

of U are called the left singular

vectors of A, and the columns v1,v2,...,v are called the right singular

vectors of A. The matrix A

> 0 =

(2.4)

in

has rank r if and only if

which case the vectors

form an orthonormal basis for the

u1,u2,.

column space of A (hereafter denoted by R(A)).
It is the intimate relation of the singular values of a matrix to
its spectral and Frobenius norms that

enables

us to characterize numeri-

cal rank in terms of singular values. Specifically the spectral norm of

A is given by the expression.
=
IIA2

C•l.

-6Moreover, if

are the singular values of B = A + E, then

2

1,2,... ,n).

(i =
hEll2

In view of (2.4) this implies

that

inf

(2.5)

ar+1'

hlA-B112 =

rank(B)Sr
arid

this

infijnum is actually attained for the matrix

B

defined by

fz'\ T

B=U(

(2.6)
where

' = diag(a1,a2,..

,o 0,... , 0).

Likewise
2
IAIIF =

2

2

OI + a2

+

+

2

and

inf
rank(B)Sr

a

r+1

hIA_BIIF

+

... + a

fl

The infimum is attained for the matrix B defined by (2.6).
Using these facts we can characterize the notion of numerical rank.

In the following theorem we use the notation rank (5er) to mean numerical rank with respect to the norm IHI.

Theorem 2.3. Let a1

...

a be the singular values of A.

Then A has numerical rank (o,c,r)2 if arid only if

(2.7)

-7Also A has numerical rank (5, c,r)F if and only if
2

ar+
Proof.

2

r+l+ •••

+ n5
2

>C2

2

r+l+
2

+

2

We prove the result for the spectral norm, the proof for

the Frobenius norm being similar. First suppose that (2.7) holds. Then
by (2.5) if JIB-All2 < 5 we must have rank (B) >

r.

satis-

Consequently a

fies (2.2). This also shows that

mm

r.

{rank(B): JIB-All

But the matrix B of (2.6) is of rank r and satisfies IIA-B112
Hence

c.

satisfies (2.1).

Conversely, suppose 5,, and
by (2.5), 5

Also

&

°ri

B of rank r satisfying IIA-Bli <

r

satisfy (2.1) and (2.2). Then

for if not by (2.1) there is a matrix

r+l'

which contradicts (2.S).a

Because of the simplicity of the characterization (2.7) we shall

restrict ourselves to rank defectiveness measured in terms of the spectral
norm.

We shall need two other facts about singular values in the sequel.
First define

(2.8)

inf(A) =

inf

IIAxII2.

Then

thf(A) =

where

is the smallest singular value of A. Second, let X and Y

be any matrices with orthonormal columns and let

>

k be

-8T

the smgular values of C = X AY. Then

(i =

(2.9)

1,2,... ,k)

and

k-il °n-ii

(2.10)

(i =

1,2,... ,k).

3. The a-Section of R(A)

Having confirmed that a matrix A has numerical rank (o,a,r)2
with r <

n,

one must decide what to do about it. If the singular value

decomposition has been computed as a preliminary to determining the

numerical rank, one solution naturally presents itself. This is to work

with the matrix B defined by (2.6). Because B has an explicit representation in terms of Z', the usual difficulties associated with zero singular

values can be avoided. Moreover, the solution so obtained is the exact
solution of a small perturbation of A.
However, this solution has the important defect that it does not

reduce the size of the problem. For example, if the problem at hand is
to approximate a vector of observations b, the procedure sketched above
will express the approximation as a linear combination of all the columns of

A, even though some of them are clearly redundant. What is needed is a

device for selecting a set of r linearly independent columns of A.
In Sections 5 and 6 we shall discuss numerical techniques for actually

making such a selection. In this section and the next we shall concern
ourselves with the question of when making such a selection is sensible.

The main difficulty is that there are many different sets of r

-9linearly independent columns of the matrix A, and not all these
sets may be suitable for the problem at hand. For example, if the

is again that of approximating a vector of observations b,
then for each set of columns we shall attempt to find a vector in

problem

the subspace spanned by the columns that is in some sense a best

approximation to b. Now if
widely

the

subspace determined by a set varies

from set to set, then our approximation to b will not be sta-

ble. Therefore, we turn to the problem of determining when these
subspaces are stable.

We shall attack the problem by comparing the subspaces with a
particular subspace
Let

that

is determined by the singular value decomposition.

A have numerical rank (6, s,r). Let the matrix U in (2.3) be

partitioned

in the form
U =

(U,I),

where U has the r columns u1,u2,. .. ,ur.

Then we shall call R(U)

the e-section of R(A). Note that the s-section of (A) is precisely the
column space of the

matrix

B defined in (2.6).

We shall compare subspaces in terms of the difference of the ortho-

let

gonal proj ect ions upon them. Specifically for any matrix

X

denote the orthogonal projection onto R(X). Then for two

subspaces

R(X)

and R(Y) we shall measure the distance between them by IIPx-Py112 (for

the

various geometric interpretations of this number, which is related

to canonical correlations and the angle between subspaces, see [1,2,13]).
It

is known

that if Y has orthonormal columns and

has orthonormal

- 10

-

columns spanning the orthogonal complement of R(X), then

'xY112 —

(3.1)

1k Y112.

The selection of r columns a. ,a. ,. ..

11

A =

(a1,a2,.

..

,a)

,a.

from the matrix

12

has the following matrix interpretation. Let W

be the n x r matrix formed by taking columns

•

from the n x n

identity matrix. Then it is easily verified that (a. ,a. ,...

,a.

)

= AW.

11 12

T

Of course W W =1, so that W has orthononnal columns, and this is all
that is needed for the following comparison theorem.

Theorem 3.1. Let A have numerical rank (5,c,r)2 and let U
be defined as above. Let W be an n x r matrix with orthonormal columns
and suppose that

inf(AW) > 0,

(3.2)

where inf(X) is defined by (2.8). Then

IIPuII2

(3.3)

Proof.

The matrix WTATAW is positive definite and hence has a

nonsingular positive definite square root. Set Y = AW(WTATAW)l/2.

It

is easily verified that Y has orthonormal columns spanning R(AW).
Moreover, from (3. 2)

ITATAl/2I =

(3.4)

The matrix ii

also

has orthonormal columns, and they span the orthogonal

- 11

-

complement of R(Ue)• It follows from

(2.3)

that

IIUAI2

(3.E)

Hence from (3.1), (3.4), and (3.5)
C

1'AW

2=

JAW TATAWT) - 1/2112
!IUTAII2IIWIJ2IIOTATAWYZII2

Theorem 3.1 has the follow:ing interpretation. The number

'y

measures

the linear independence of the coltmns of AW. If it is small compared to
IIAWI! then the columns of AW themselves must be nearly dependent. Thus

Theorem 3.1 says that if we can isolate a set of r columns of A that
are strongly independent, then the space spanned by them must be a good

approximation to the c-section R(U).
However, there are limits to how far we can go with this process.

By (2.8) the number y
cal rank c

satisfies

cYr

',

and by the definition of numeri-

r+l• Consequently, the best ratio we can obtain in (3.3) is

C•r+l/(:7r. Thus the theorem is not very meaningful unless there is a well

defined gap between or+l and ar. One cure for this problem is to increase c in an attempt to find a gap; however, such a gap need not exist
(e.g. suppose

=

/2

(i =

1,2,... ,n-1)).

What to do when the matrix

A exhibits a gradual rather than a precipitous decline into degeneracy
is a

difficult problem, whose solution must almost certainly depend on

additional

infoniat ion.

- 12

-

A second difficulty is that it may be impossible to obtain the
ideal ratio because in practice we must restrict our choice of W to
columns

of

A.

of the identity matrix; i.e.

we must choose from among columns

That this is a real possibility is shown by the following example.

Example 3.2.

components.

Let e denote the vector (1,1,... ,1)T

with n

The matrix

T

An =1 n -

e' (n)
e
n

has singular values 1,1,. ..,l,0, so that it has numerical rank (1,0,n-l)2.

Thus we should like to remove a single column of A to obtain an approximation to the 0-section of A. Owing to syunnetry, it does not matter which
column we remove. If we remove the last one, the resulting matrix
has the form

A'=E
n
n
where E

e(n)
e

(1)T

n

consists of the first n-i columns of the identity matrix.

Thus

A'

=

ir

[(e1
TL\o /
1

1

/ e'1

niT \n-l
from which it follows that

"

(e(

- 13 e (n-i)

T2

A'

A

and

y= inf(A1!) :-i

It should be observed that the factor

n2 exhibited in the

example is not extremely small. For n = 25 it is only 1/5. Unfortunately

no lower bound on y

is

known, although with the computational algorithms

to be described in Sections 5 and 6 it is easy enough to check the computed value.

A final problem associated with Theorem 3.1 is that it is not

invariant under scaling. By scaling we mean the multiplicative scaling

of rows and columns of A and not additive scaling such as the subtraction of means or a time factor from the columns of A (this latter
scaling can be handled by including the factors explicitly in the model).

Since by multiplying a column by a sufficiently small constant one can
produce as small a singular value as one desires without essentially altering the

model, Theorem

3.1 can

be coaxed

into detecting degeneracies that

are not really there. This means that one

must

look outside

the hypo-

theses of Theorem 3.1 for a natural scaling. While we are suspicious of
pat scaling strategies, we think that the following criterion is reason-

able for many applications. Specifically, the rows and columns of A
should be scaled so that the errors in the individual elements of A are

- 14

-

as nearly as possible equal. This scaling has also been proposed in
[4], and an efficient algorithm for accomplishing it is described in
[5].

The rationale for this scaling is the following. From the definition of the singular value decomposition it follows that

Av =

au.

(i =

1,2,... ,n).

Now if we imagine that our matrix is in error and that our tnie matrix
is A +

E, then
(A+E)v.1

(3.6)

a.u. +

11

Ev..
3.

If we have balanced our matrix as suggested above, then all of the elements
of E are roughly the same size, and Ev11f2

IEII. Thus if

hEll2,

equation (3.6) says that up to error v is a null vector of A + E, and
the matrix is degenerate.

We recognize that this scaling criterion raises as many questions as

it answers. An important one is what to do when such scaling cannot be
achieved. Another question is raised by the observation that in regression row scaling is equivalent to weighting observations, which amounts

to changing the model.* Is this justified simply to make Theorem 3.1
meaningful? Although this question has no easy answer, we should like to
point out that it may be appropriate to use one scaling to eliminate

colinearities in A and another for subsequent regressions.
We are indebted to John Chambers and Roy Welsh for pointing this out.

- 15

In
Theorem

the next

section

-

we are going

to

examine

the

implications

of

3.1 for the linear least squares problem in which a vector of

observations b is optimally approximated in the 2-norm by linear coinbina-

tions of the columns of A:

bAx.
In

sane applications the 2-norm is not the best possible choice, and one

may

wish

to minimize

p (b-Ax),

where p is

a function that may

not

even

be a norm. For example, in robust regression one approach is to minimize
a function that

may reduce the

influence of wild points. We shall not pur-

sue this subject here; but we believe that

Theorem 3.1

has

important

impli-

cations for these problems. Namely, if we are searching for an approxi-

mation to b in g(A), we cannot expect the solution to be well determined

unless R(A) itself is. Theorem

3.1

provides a theoretical basis for

finding stable subspaces of R(A); however, specific theorems xmist wait

development of a good perturbation theory for
other than the 2-norm.
the

approximation in norms

4. The Linear Least Squares Problem
In

this section we shall consider the linear least squares problem

(4.1)

It

minimize J[b-AxII.

is well known

if and

only

that this

problem always has a solution, which is unique

if A is of full column rank. At the solution, the residual

vector

r=b-Ax

- 16 -

is the proj ection of b onto the orthogonal comp1nent of R(A).
When A has rnmierical rank (5, c,r)2, the solution to (4.1) may
be large, and some of the individual components of the solution will

certainly
small

have large variances. If

a stable solution can

be

the ratio c/S is sufficiently

computed by restricting oneself to the

c-section of A. Computationally this can be done as follows. Define

UC

and

C

as in Section

3, and

further define

V = (v1,v2,. ..

V)

V

=

(vr+i,.

,v)

and
=

Then the matrix

diag(a1,a2,.

.

. ,aj,

=

diag(a1...

B of (2.6) is given by
B=

U

2 vT.
ccc

Moreover the vector

x
c

is

=VZUTb
cc c

the unique solution of the problem of minimizing

Ib-Bx!12

that

is

of minimum

2-norn. It

rc

is easily seen that

=b-Ax c

=b-Bx.c

- 17 As we indicated

in

the last section, this solution is not entirely

satisfactory, since it involves all the columns of A, whereas we might

hope

to obtain a satisfactory representation of b in terms of r

suitably chosen columns; that is with a model having only r carriers.

It

is a consequence of Theorem

pendent

3.1 that any

set

of r reasonably inde-

columns will do, although in practice additional considerations

may make some choices preferable to others.

Theorem 4.1. Assuming the notation and hypothesis of Theorem 3.1,

let x and rg be defined as above. Let

be the solution of the

linear least squares problem

minimize

and

let

2

IIb-AWy1i2

r1q be the residual

r=

b -

AWy,.

Then

Ir-rW2

Proof. By the properties of the least squares residual

r =

(IPu)b

and rW =

(I-P)b.

Ir- rWII2
Theorem

4.1

=

Hence

(Pu-PAw)bIl2

IJbj2.a

partially answers a question raised by Hotelling [10];

namely if carriers are chosen to eliminate dependencies, what guarantees

- 18

that

-

one such set will not fit b better than

another?

The answer is

that if there is a well defined gap between 6 and e, then any set of
r strongly independent columns will give approximately the same residual. However, there remains the possibility that by including more
columns

of A a considerably smaller residual could be obtained. We

stress that such a solution cannot be very stable. By (2.8) any matrix
consisting
less

than

of more than r

,

or equal to

columns of A must have a singular value

and

it

follows from the perturbation theory

for the least squares problem [15] that the solution must be sensitive

to perturbations in A and b. (Another way of seeing this is to note
is a lower bound for II(ATA) 1112, so that the solution must have

that

a large covariance matrix.)

However, one might be willing to put up with the instabilities in the

solution provided it gives a good approximation to b. We shall now show
that any solution that substantially reduces the residual over r

is not

only unstable, it is also large.

Theorem 4.2. Let r be defined as above. Given the vector x, let
r =

b

-

Ax.

If

IrII2

> 11r112, then
II r

2

1

r

1x112

Proof. Let z =

Tx

and let

UTb =

(c

l

2

- 19
where
c =

c

n-vector. Then

is an

T T T

(c,c)

conformally

with

-

if we partition z =

T T T
(z ,z ) and

the previous partitions of U, V, and

Z, we have
2
11r112

T

= IIU (bAWTx)II
=

/z\
I
(
I JIc\
\d/

J2

\O, 112

=

lIc-2zII + IldII

=

lic -z z

2

+ fl -z z

+ IldIl V

Consequently

2>

(4.2)
Now

the

- £112
+ IIdfl.
c 2

11r112

vector y =

vT e is

given by

I Z1c
yc =

so that
hr

(4.3)

C

II = hIhh + hIdhI.

From (4.2)

VjrfJ

- hIdhI
2

Icc-zCC
z 112

*

hhchI2 - Ix 1111211
eZ e2

hIhI - £JIZ 112•

- 20

-

Hence

1k

112

lIll2

lCl2-/rIl- lid il

and fran (4.3)

lxii 2

/r,I2-ild

lrl

- 4i?2-!ldll

- 11r112

must
The theorem shows that even a slight decrease in the residual
result in a great increase in the size of the solution.

It is hardly

necessary to add that a large solution is seldom acceptable in practice:
it must have high variance, and it may be physically meaningless.

The results of this section have implications for a common practice
in data analysis, namely that of fitting a large number of subsets of the

columns of A in an attempt to obtain a good fit with fewer than the full
complement of columns (for example, see [6]). We have, in effect, shown
that if the ratio

is

reasonable, this procedure is not likely to be

very productive. Any set of r independent columns will give about the
same residual, and any larger set that significantly reduces the residual

must produce an unacceptably large solution. There are, however, two cases

where this procedure might be of some help. First when it is hoped that

fewer than r columns can produce a good fit, and second when the c-6
ratio is not very small. An approach to the second problem that uses the
singular value decomposition of the augmented matrix (A,b)
in [9] and [16,17].

is described

- 21
5.

Extraction of Independent

-

Columns: the

QR Factorization

We now turn to the problem of extracting a set of numerically inde-

pendent columns. The first method we shall consider is based on the QR
factorization of the matrix

Specifically, if A is an m x n matrix

A.

with m n, then A can be written in

the form

A = QR,

has orthonormal columns (QTQ_1) and R

where Q

If A has

full

column rank, then the factorization is unique up to the

signs of the Lolumns

be noted that

is upper triangular.

the

of Q

and

columns of Q

the

corresponding rows of R. It should

form an orthonormal basis for R (A).

A knowledge of the QR factorization of A enables one to solve the

least squares problem (4.1). Specifically, any solution x of (4.1)
must satisfy the equation

Rx = QTb,

which

can be

easily solved since R is upper triangular. Moreover, since

ATA = RTR, we have
(ATA)l =

R1RT

so that one can use the matrix R in the factorization to estimate the
covariance

matrix of

the solution.

An especially desirable feature

of

the QR factorization is that it

can be used to solve a truncated least squares problem in which only an

- 22

initial

-

set of columns are fit. If A denotes the matrix consisting

of the first r columns of A and R11 denotes the leading principal
submatrix of order r of R then
AIr =

(5.1)

QIrRi

Since Rir is upper triangular and Q" has orthonormal columns,
equation (5.1) gives the QR factorization of A and can be used as
described above to solve least squares problems involving AIr.
The basis for using the QR factorization to extract a linearly

independent set of columns from the matrix A is contained in the
following theorem.

Theorem 5.1. Let the QR factorization of A be partitioned in the
form

/R1l R12
(A1,A2) =

0

where A1,Q1 E xr and R11 E Rr<r
IIR22I2 = c

If

< 6 =
inf(R11),

then A has rank (6,c,r)2. Moreover,

inf(A1 •)

Proof.

values

of

Because the columns of Q

= 6.

are orthonormal, the singular

A and of R are the same. Now 6 is the r-th singular

value of R11, and hence by (2.9) 6 is less than or equal to the r-th

- 23
singular
Thus

A

value of A; i.e. °r >
has rank (8, ,

r).

-

o• Likewise

from (2.10), e

c7r+l.

Moreover, since Q1 has orthonormal columns,
=

inf(A1)

inf(Q1R11)

=

inf(R11)

=

The application of this theorem is obvious. If, after having
computed the QR factorization of A, we encounter a small matrix R22
and a matrix

with a suitably large infinuin, then the columns of

A1 span a good approxination to the €-section of A.

Because

of (5.1), we have at hand the QR factorization of A1 and can proceed
immediately to the solution of least squares problems involving A1.

There remain two problems. First how can one insure that the first r
columns of A are linearly independent, and second how can one estiiiate
inf(R11)?

The solution to the first problem depends on the method by which the
QR factorization is computed. Probably the best numerical algorithm is
one based on Householder transformations in which the QR factorizations

A1k = Qik.J
K are

computed successively for k =

1,2,... ,n

(e.g. see [14]).

At the k-th step, just before Qik and R are computed, there is the
possibility of replacing the k-th column of A by one of the columns
ak+],ak+2,..

If the column that maximizes the (k,k) - element of

R is chosen to replace ak, then there will be a tendency for independent columns to be processed first, leaving the dependent columns at the
end of

is

the matrix. An ALGOL

program incorporating

this "column pivoting"

given in [3] and a FORTRAN program is given in (11].

-24Once
estimate

a satisfactory QR decomposition has been calculated,

we can

ItR22fI2 by the bound

I!R22112

5

v11R22JJ1IIR22IL,

where
= max
j

E x..
i

J

and

IX__ =max
•

Z

13
•

x..I.
13

one can estimate inf(R11) by computing R1 (an easy task
since R11 is upper triangular) and using the relations
Likewise

inf(R )

=

-1-1

1

___________
11

The procedure sketched
that

1 11

above is completely reliable in the sense

it cannot fool one into thinking a set of dependent co1tns are

independent. However, it can fail to obtain a set of linearly independent columns, as the following example shows.

Example
for n = 5:

5.2.

Let

be the

matrix

of

order n illustrated below

- 25

Letting x =

-

1

-i//2•

-i/v

-l/

o

l/v7

-l/v

-l/ -l/

o

0

1/vs

o

0

0

l/v

o

0

0

0

-i/v'

-'/14

(l,v'2/2,v'/4,/4/8,...

it is easily verified that

Ax
=2%
nfl
where eT

(1,1,... ,l). Thus

A

has the approximate null vector x

and must have nearly dependent columns.
zation of

However, computing the QR factori-

even with column pivoting, leaves A undisturbed. Since

no element of A is very small, we shall have R22 void; i.e. no dependent colunm will be found.
It should be observed that

in

the above example there is no danger

of the degeneracy in An going undetected. Since R22 is void, R =
and

any attempt

to estimate inf(R11) will reveal the degeneracy.

It may be objected that the matrix A in Example 5.2 shows an
obvious sign of degeneracy; viz, its determinant

(n!)2 goes rapidly

to zero with increasing n. However, the matrx

lAI

obtained from

by taking the absolute value of its elements, has the same determinant

yet its columns are strongly independent. Thus the example confirms a
fact well Iciown to practical computers: the value of a determinant is
worthless as an indication of singularity.

A

- 26
6.

Extraction of Independent Columns: the Singular Value Decomposition

When

the singular value decomposition of A has been computed (an

ALGOL program is given in [8] and
way of

the

-

a

FORTRAN program in [11]), a different

selecting independent columns is available. The method is based on

following theorem.

Theorem 6.1. Let A have the singular value decomposition

fz

T

UAV=(

Let V be partitioned in the form

ci

v= (

ci.

v

a2

where V1 is r x

r,

and let A be partitioned in the form

A =

(A1,A2),

where A1 has r columns. Let 6 =
= 6

°r'

e =

ar+l

and

inf(V1).

Then A has numerical rank (6,c,r)2 and

(6.1)

inf(A1)

Proof.

The fact that A has

ijTunediately from Theorem

write

2.3.

y.

numerical

rank (5,c,r)2 follows

To establish (6.1), observe that if we

- 27 AV =

= (S1,S2)

where S1 has r columns, then SS2 = 0.
1

T
Since S1S2

T

AT

id

ci

Now since A =

svT, we have

= 0,

in.f(A1)

inf(S1VT1)

=

inf(S1)inf(VT1)

ar inf(V ) =
ci

As with the QR factorization, Theorem 6.1 provides us with a way of

detennining when an initial set of r columns of A are independent.
Since an initial set may be degenerate, we must adopt some kind of inter-

change strategy to bring an independent set of columns into the initial

positions. If P is any pennutation matrix, then

T

U

so

T

fz\

(AP)(P V) = I

that in the singular value decomposition an interchange of columns of

A corresponds to an interchange of the corresponding rows of V. This

suggests that we exchange rows of V until inf(V1) becomes acceptably

large. One way of accomplishing this is to start with the r x n matrix
=

CVT1,VT2)

and
of

compute its QR factorization with column pivoting to force a set

independent columns into the first r positions. Alternatively one

- 28
could

-

apply an algorithm such as Gaussian elimination with complete

pivoting to V (e.g. see [14]).
If either of the above suggestions is followed, the final matrix

V1 will be upper triangular, and its infimum can be bounded by the method
suggested in the last section.

If r is small,

significant savings can

be obtained by observing

that the singular values in [0,1) of V1 and 2 are the same (see
the appendix of [15] for a proof). Thus one can start with the smaller

matrix

V' = (\?T1,T2)

(6.2)

and use the QR factorization with column pivoting to determine the

dependent columns of A. Note that when r =

n-l

the column to be stricken

T

corresponds to the largest element of the row vector V2.
The question of whether to use the QR factorization or the singular

value decomposition is primarily one of computational efficiency. Although
Example 5.2 shows that the QR factorization can fail to isolate a set of
independent columns in a case where the singular value decomposition does,
this is an unusual phenomenon (see Example 7. 2) and in most cases the QR

factorization with column pivoting is effective in locating independent

columns. When m is not too much greater than n, the calculation of the
singular value decomposition is considerably more expensive than the
calculation of the QR factorization, and it is more efficient to stick with
the latter, if possible.

- 29 When m >>

The

matrix R

n,

we can begin by computing the QR factorization of A.

has the

same singular values as A, and

indeed if

UTiw =

(63)

is the singular value decomposition of R, then V
right

is the matrix of

singular vectors of A. Since R is an n x n matrix, the reduc-

tion (6.3)
putation

is computationally

far less expensive than the initial com-

of R, and there seems to be no reason not to use the singular

value decomposition.

7. Examples
In this section we shall give some examples illustrating the pre-

ceding material. The numerical computations were done in double precision
on an IBM 360; i.e. to about sixteen decimal

digits.

Example 7.1. This example has been deliberately chosen to be un-

complicated. For fixed

n, let
H

where eT =

(1,1,... ,l).

i

=

n

-

eeT,
n

It is easily verified that Hn is orthogonal.

Let

Z =

diag(1,l,l,1,1,0,0,0,0,0)

and
A =

H50

(z)

H10.

- 30

-

Then A has five nonzero singular values equal

to

unity

and

five zero

singular values, and thus it should have five linearly independent
columns.

The singular values of A were computed to be l,l,l,l,l,.35xl0,
0,0,0,0, so that A can be regarded as having rank (l,c,S) where
e =

io16.

The pivoting strategy described in Section 6 was used to

isolate a set of five linearly independent columns. These turned out to

be columns 1,2,4,5, and 9. The associated matrix V1 had an infijnum
of .45 which is very close to the optimal value of unity. As a final

check, we compute 'UAW11' where W =

(e1,e2,e4,e5,e9)

is the matrix

that selects the independent columns from A (cf. Theorem 3.1). The
result is

UAW2 =

.37 x

which shows that columns 1,2,4,5, and 9 of the matrix A almost exactly
span the c-section of A.
The QR factorization with column pivoting that is described in Sec-

tion 5 was also applied to A. The pivot columns and
S
4
2

.89
.86

3

.81
.71

6

.44

1

.45 )(

7
8

0

9
10

0
0

10
.13 x 10

-

their

norms were

- 31

-

If the gap is taken to lie after the fifth vector we have
=
inf(R,11)

1,

1R22112 =

.20 x io16

Thus the QR factorization exhibits the same sharp gap as the singular
value decomposition. However, the five columns 2,3,4,5, and 6 designated as independent are different from those chosen by means of the

singular value decomposition. Nonetheless, for W =

(e2,e3,e4,e5,e6)

we have

=

UAW1

so that this choice of columns

x

is

as good as the one predicted by the

singular value decomposition.

using the 1- and -nonns is

Incidentally the esti.mate of 1R22112

=

.94

x

which is not a gross overestimate.

Example

7.2.

This is the matrix A25 of Example

values of this matrix

5.2.

The singular

are
,a24=.31,cr25=.77 x

Again

there

(.3l,,24)
vector
right

is a well defined gap, and we may

where c =

which can be

l0'.
found

This time

by looking for

singular vector v25

equation

(6.2)). This

there

take

A to have rank

is only a single dependent

the largest component of the

corresponding

to c5 (cf. the coments at

component, . 75, is the first, which indicates

- 32

that

-

column one should be discarded. For this selection we have

'UAW'12 = .49 x lO.
In principle, the QR factorization should fail to isolate a depen-

dent column of A25. However, because the elements of A25 were
entered with rounding error, the pivot order with column norms turned

out to be

1.0

1

.98
.88

25

6

:37

24
2

-

.15 x 10

This again gives a well defined gap and indicates that column 2 should

be thrown out (the second component of v25 is .53 so that also from the
point of view of the singular value decomposition the second column is
a candidate for rej ection). For this subspace we have
=

.11 x io6.

UCAW
Thus the QR factorization gives only slightly worse results than the
singular value decomposition, in spite of the fact that the example was
concocted to make the QR decomposition fail.

Example 7.3. To show that our theory may be of some use even where
there is not a sharply defined gap in the singular values, we consider the
Langley test data [12], which has frequently been cited in the literature.

- 33

-

Since it is a common practice to subtract means from raw data, we have
included a column of ones in the model. Specifically the columns of

A are as follows:
1 --

ones

2 - - GNP Implicit Price Deflator, 1954 - 100
3 - - GNP
4 -5 -

Unemployment

- Size

of armed forces

6 --

Noninstitutional

7 --

Time

population

14 years old

(years)

The scaling of this data will critically affect our results. For the
purposes of this experiment we assume that columns two through six are

known to about three significant figures. Accordingly each of these
columns was multiplied by a factor that made its mean equal to 500.

The column of ones is known exactly and by the equal error scaling

criterion ought to be scaled by a factor of infinity. As an approximation we took the scaling factor to be 1010.

The column of years can be treated in two ways. First the errors in
the time of measurement can be attributed to the column itself, which

would result in the column being assigned a low accuracy. However, we
observe that any constant bias in the time of measurement is accounted
for by the column of ones, and any other errors can be attributed to the

measured data. Consequently we have preferred to regard the years as
known exactly and scale the seventh column by iol0.

The

singular values of the matrix

thus scaled

are

- 34

-

.78

x iol4

.94

io8

.58 x 1O3
.26 x 1O3

.26 x io2
io2

.22

.51 x 101

Since the error in A is of order unity, the last singular value must
be regarded as pure noise, and we may take A to have rank (22,5.1,6)2.
The largest component of the seventh singular vector is the sixth and has
a. value of .90. When the sixth column is removed from the matrix, the

resulting subspace compares with U51 as follows:
=

.12.

'U51AW112
The relatively poor determination of the 5.1-section of A suggests

not much useful information can be obtained from a least squares

that

fit, even when the sixth column is ignored. The next gap that presents

itself is between the
as

fourth and fifth singular values. If we

regard A

having rank (260,26,4)2 and use the pivoting strategy of Section 6 to

isolate a set of four independent columns, we choose columns 1,4,5, and 7

with

inf(V1)
For

.991.

this choice of columns
=

'IJ26OPAW11

0.011,

- 35

-

a far more satisfactory result.
If the QR factorization is applied to A, there results the following sequence of pivot

columns and

norms:

7

.78 x io14

1

.94X108

5

.47 x 1O3
.31 x 1O3
.24 x 102
.21 x io2

4
2

3
6

.57x101

This agrees completely with the results from the singular value decomposi-

tion. Either one or three columns should be discarded, and columns 6, 2,
and 3, in that order, are candidates.

Although these results indicate that columns 2, 3, and 6 should be
discarded from the model, they are not conclusive, since there may be
other sets containing some of these columns that give a satisfactory

approximation to the 260-section of A. However, a singular value decomposition of the matrix consisting of columns 1,2,3,6, and 7 gives the singular
values
.78 x

io14

.94
io2
.25 io2
.10 x io2

.50 x

which

shows that none of these columns

sion in the model.

is

a really good

candidate for

inclu-

- 36

-

up: if the raw Longley data is taken to be accurate to three
significant figures, if years are assumed to be exact, and if means are
To sum

subtracted from the columns, then the column corresponding to noniristitu-

tional population is redundant, and
inplicit price deflator and

the

the

columns corresponding to the GM'

GNP are so nearly redundant that their

inclusion in the model will affect the stability of the residuals from any
regressions.

- 37

-

References
1. S. N. Afriat, Orthogonal and oblique projectors and the
tics of pairs of vector spaces, Proc. Cambridge Philos. Soc. 53
(1957) 800-816.

2. A Bjork

and G. H. Golub, Numerical methods for computing angles between
linear subspaces, Math. Comp. 27 (1973) 579-594.

3. P. Businger and G. H. Golub, Linear least squares solutions by
holder transformations, Numer. Math. 1 (1965) 269-276.
4. J. M. Chambers, Stabilizing linear regression against observational
error in independent variates, unpublished manuscript, Bell
Laboratories, vkirray Hill, New Jersey (1972).
S. A. R. Curtis and J. K. Reid, On the automatic scaling of matrices
for Gaussian elimination, J. Inst. Math. Appi. 10 (1972) 118-124.
6.

C. Daniel and F. S. Wood, Fitting Equations to Data, Wiley, New
York (1971).

7.

G. H. Golub, Least squares, singular values, and matrix
tions, Aplikace Matheinatiky 13 (1968) 44-51.

8.

_________ and C. Reinsch, Singular value decomposition and least
squares solution, Nuiner. Math. 14 (1970) 403-420.

9. D. M. Hawkins, On the investigation of alternative regressions by
principal component analysis, Appi. Statist. 22 (1973) 275-286.
10. H. Hotelling, The relations of the newer multivariate statistical
methods to factor analysis, Brit. J. Statist. Psychol. 10 (1957)

—

69-79.

11.

C. L. Lawson and R. J. Hanson, Solving Least Squares Problems,
Prentice-Hall, Englewood Cliffs, New Jersey (1974).

12. J. W. Longley, An appraisal of least squares programs for the electronic
computer from the point of view of the user, J. Amer. Statist.
Assoc. 62 (1967) 819-841.
13. G. W. Stewart, Error and perturbation bounds for subspaces associated
with certain eigenvalue problems, SIAM Rev. 15 (1973) 727-764.

14. ____________,

Introduction

New York (1973).

to Matrix Computations, Academic Press,

- 38
15.

-

_____________, On the perturbation of pseudo-inverses,
tions and linear least squares problems, to appear SIAM Rev.

16. J. T. Webster, R. F. Gunst, and R. L. Mason, Latent root regression
analysis, Technoinetrics 16 (1974) 513-522.

17. _____________, A comparison of least squares and

latent

sion estimators, Technometrics 18 (1976) 75-83.

root

