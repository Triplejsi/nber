NBER WORKING PAPER SERIES

SOLVING SYSTEMS OF NONLINEAR EQUATIONS BY

BROYDEN'S METHOD WITH PROJECTED UPDATES

David M. Gay*
Robert B. Schnabel**

Working Paper No. 169

COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
National Bureau of Economic Research, Inc.
575 Technology Square
Cambridge, Massachusetts 02139

March 1977
Preliminary

NBER working papers are distributed informally and in limited
numbers.
This report has not undergone the review accorded official NBER
publications; in particular, it has not yet been submitted for
approval by the Board of Directors.
*NBER Cc'mputer Research Center. Research conducted in part during
a visit to the Atomic Energy Research Establishment, Harwell,
England, and supported in part by National Science Foundation
Grant MCS76-00324 to the National Bureau of Economic Research, Inc.
**Computer Science Dept., Cornell University. Research conducted in
part during a visit to the Atomic Energy ResearCh Establishment,
ilarwell, England, and supported in part by a National Science
Foundation Graduate Fellowship.

Abstract
We introduce a modification of Broyden's method for finding

a zero of n nonlinear equations in n unknowns when analytic
derivatives are not available. The method retains the local
Q—superlinear convergence of Broyden's method and has the

addi-

tional property that if any or all of the equations are linear,
it locates a zero of these equations in n+1 or fewer iterations.

Limited computational experience suggests that our modification
often improves upon Eroyden's method.

—ii—

CONTENTS

Section

Page

1.

Introduction

1

2 .

The Ne'q Method

5

3.

Behavior on Linear or Partly Linear Problems.... 17

4.

Local Q—Superlinear Convergence on Nonlinear
Problems

25

5.

Computational Results

34

6.

Summary and Conclusions

39

7.

References

40

1. Introduction
This paper is concerri& with solving the prob1n

given a differentiable F
(1.1)

find x' c
when

such that F(x*) =

0

derivatives of F axe either inconvenient or very costly to compute.

We denote the n component functions of F by

i1,...,n
af.

(x).

and the Jacobian matrix of F at x by F'(x), F'(x) =
When

F' (x) is cheaply available, a leading method for the solution

of (1.1) is Newton's method, which produces a series of approximations
{x1 ,x2,. . .

} to x by starting from approximation x0 and using the

foxrctula

X11

x1

—

(1.2)

F' (x)F(x) .

F is nonsingular and Lipschitz continous at x' arid x0 is
sufficiently close to x*, then the algorithm converges Q-quadratically to
2
- x*
c I lxi — x*
i.e., there exists a constant C such that
for all i and sanevector norm 1.11 (c.f. 59.1 of [Ortega & Rheinholdt,
If

—

I

I

1970]). If F is linear with nonsingular Jacobian matrix, then x1 = x.
When F' (x) is not readily available, an obvious strategy is to replace

F' (xi) in (1.2) by an approximation
iteration

B. This leads to the irodified Newton

—2—

x.i+1=x.1 -BF(x.)
1
1

(1.3a)

B÷i = tJ(B.)

(1.3b)

where U is sara update formula that uses current information ahout F.
[1965] introduced a family of update formulae U known as

Broyden

quasi-Newton updates. He also proposed the particular update used in
"Broyden' s method", which we consider in ITore detail below. If

x0 is

- F' (x0) is sufficiently

sufficiently close to x, the matrix norm of

snail and several reasonable conditions on F are met, then Broyden 's

method converges Q-superlinearly to x i.e.,
Hx

Hx
linear

_x*j

=

0

[Broyden, Dennis &

_x*I

ré, 1973]. However for

F, convergence may take as many as 2n steps—and B - F' (x*)

may have rank n—i (see [Gay, 1977]).

In this paper, we introduce a new method of form (1.3) using an update

(1. 3b) which is different fran but related to Broyden' s update. Our new
method is still locally Q—superlinearly convergent under the conditions for

which Broydeth method is. It has the additional property that if F is
] inear with nonsingular Jacobian matrix, then x

=x

for some i < n+l, and

if k+1 iterations are required ,then 3k+1 - F' (x*) has rank n-k.
Initial tests show our method to be somewhat superior in performance to
Broyden's method.

—3—

The

basic idea behind our new method is related to one originally proxDsed

by Garcia-Palamares [1973]. Davidon [1975] used. this idea independently
in deriving a new method for the unconstrained minimization problem,

XEn '

n

mm f(x)
Davidon

also ncdified an existing update formula to produce a quasi-Newton

method which does not use exact line searches but is exact on cuadratic
problems.

has

This new method has been an irrprovement in practice. While it

not yet been shown to retain the local superlinear convergence of lie

method it rrcclifiecl, Schnabel (1977] uses the techniques of this paper to show

that a very similar rrcdification retains Q-superlinear convergence as well

as the

proper-ties of Davidon's [1975] method.

In

Section 2

we briefly describe Broyden' s method and the iTrtant

features of quasi-Newton methods. We then introduce our new

algorithm

in

to forms: Algorithm I, a simplified version which is sufficient to discuss its basic and linear

properties,

and Algorithm II, the version used in

practice and to prove local superl±near convergence. We also derive the
basic properties of our method which we will use in subsequent sections.

In

Section 3

we discuss the

behavior of our

algorithm

on

linear problems.

if any or all of the equations f. are linear, then our new
algorithm will find a zero of these equations in n+l or fewer iterations.
We show that

We also discuss the effect of a certain restart procedure on our algorithm.
In Section 4 we show that our new method is locally Q-superlinearly con-

vergent on a wide class of problems. We discuss our computational results in

Section 5 and untrarize our results in Section 6.

—4—

Ilenceforth,

I

2 1/2
v.)
(IIvII=(z
i=1

matrix norm, while

I I

I

I

F

=(

will denote the 2.2 vector
for v=(v1'.. ,v'1)T

1=1 j=1

or the ccrrescring

'will denote the Frcbenius matrix norm:

nfl 21/2
in. .)
E

norm

for i = (rn.)

—5—

2. The New Method

asi-Newton methods are often damped: they take the form

x÷1

where

=

x

=

U(B)

—

(2.la)

F(x)

(2.lb) -

> 0 is chosen to pratte convergence from

the damping factor

starting points x0 which may lie outside the region of convergence of
the corresponding direct prediction method (1.3). When it leads to a

"successful" step, e.g. reduction of I F

, the

choice

1 is

usually preferred.
Broyden' s ("good") method is a method of form (2 .1), using the update
equation
=

B

+

Bs)

s

T

=

x1

—

F(x÷1)

x1

(2.3a)

,

- F(x)

(2.3b)

.

Because of equation (2.2), B1 satisfies B÷1 bc1 =

for 1l

,

(2.2)

where

Si

Si = Ax =
=

(y1 —

F' (x÷1) tax.

AF ,

we

.

Since

expect that B1 resth1es

—6—

F' (x1+i) in the direction

of

our

last

step. Since we have no other

infoirnation which uld help apçroxirnate F' (x.1),
to change B— which hopefully approximates
possible

consisnt with B+i . =

it is reasonable

'(xi) —as

little

as

This suggests the rank one

change

3i+1 =

B1

v

for any vector

which yields Broyden' s

+

(y1 -

B.

VT

s1)

such that vT

1

(2.4)

,

s o. The choice V

method, minimizes

S

the £2 or Froberilus norm (the

Z2 norm of the e1ents) of (B+i - B) over all possibilities (2.4) [Dennis
and

)

1977].

Broyden defined quasi—Newton methods to be those of form (1.3)

which satisfy the "quasiNsc.n' equation,
B÷1 s = y1

(2.5)

,

in their attanpt to build Jacobian approximations. Broyden'S

method,

with

choice of A1 in (2.1 a) ,has been the rrost successful quasi-Newton
method for solving systEns of nonlinear equations.
intelligent

It is interesting to canpare Newton' s and Broyden' s methods on
lineer prob1ns where F (x) = AX + b and A is nonsingular. Whereas

= x* for i 1, Broyden 's method
may require 2n direct prediction (A = 1) steps to produce the cact
Newton' s method (1.2) yields
x1

—7--

[Gay, 1977). In part this is because B. may never equal A, even
though F' (x) = A for all x1. We can easily see why this may be so. After
one iteration we will have s0 = y (= A S for a linear problem); after
the next iteration we will have 132 S1 =
(= A s1), but not in general
solution

y1

B2 S0

about

=

y0.

A; but in doing so we

destroy other

through previous iterations.

Therefore

iteration x1= x. - B1 F (xi)
might

seem

irost

current

good information

information

about A learned

we will never have B A, SO the

necessary.

the receeding analysis, we
equation which, while giving B.1 s =
<

13i+1 our

may take twice as many steps to converge as

From

j

into

At each step we introduce

i and B

s=

Note

are

interested in finding an update
also retains B÷1

s = y. whenever

however that for any formula of form (2.4),

B.1 S1 = y1; we can retain old information by our choice of
if
v1
B. s =
and v' s = 0, then •÷l s. =
These considerations lead to

v

our n algorithm, given

y.

in

simplified form as Algorithm I below.

We choose our update at each iteration to be the B11 which rniniraizes
the Frobenius
norm of B.1 - B aring all 1+l satisfying

B.1 s =
that

and

(B1 —

B1)

s. =

0

for all j

<

i. In Theo:r 2.1 we show

the unique solution to this problem is given by update (2.4) with

v the

projection of Si perpendicular to all the Si 's, < i. The proof is similar
to Dennis and vbrts [1977] proof that Broyden's method is the least-change
update arrong all B11 satisfying B1

s = y.

—8—

ar s, y be In-zero vectors C

Thrn 2.1 Let B

an in dimensional subspace of :IR ,
either the L2 or the Frobenius nonn, a

with Bg y.
Then for

I I

Let

II

Z

be

in

< n.

solution to
rain {B — B ItBS = y,

(B — B)z=O for all

z

C

Z}

is
A

BB+

(y-Bs)v

vs
T

where v is the ort1gorial projection of s onto the ort1gonal canp1nt

of Z, i.e.,

VS

in STZ.1
T

i=1 z

1

with (z11.. Z) an ortrgonal basis for Z. The solution is unique in
the Frobenius norm.
Proof:
Let S={BBsy, (B-B) z0 ZCZ}. Naw Bs=y;

ansince T=o for i=1,...,m, vT=o forall zCZ.
Thus B C S.
Now

consider any B c S. Since y 35

A

B—B=

—

vT

sv
s

(2.6)

—9—

Define

m Ti

d=

Z

z. =

'!'

1

11

i=1 zz.

d = o. mus
so ( - B) s

=

v.

Since d c

Since

( - B) v.

=

s -

(

- B)

Z and v is perpendicular to

z = 0 for all z c Z, (B -

Therefore (B - B) = (fl-B)

T

so for

B)d = 0,
N the

v.&.v

or Frobex-ijus

Z,

norm,
T

lB

- IIN lll ll - B

- BuN

'S

Thus B is a solution to (2.

norm because the
strictly convex

AlgorithTl

is the uniciue

function 5: JR

over

-

JR

solution

given by

all B in the convex

in the Frobenius

(B) =

-

is

set S. •

I

Let x

0

For

). It

JRfl, B
0c

F

+

JR11

be given.

i=0, 1, 2,...
=
c1ose nonzero s. E JRfl (likely s.
1

1

x.i+1 = x.1 + s.1

If F(x.i+l
y. =
1

)

=0

-x n F(x))
(2.7a)

then stop

F(x.i+l) - F(x.)
1

(2.7b)

—10—

i—i

Q=

E

j=O

S.1 =
B

Alçcrithn

S.

1

A

S. S.
AT
S Si

(2.7c)

C).s.

2.7)

—

—j
i+l

11

÷

.s.)
11

(y.——

AT S.
S.
1
1

s.1

unsuitable for crtuter irLplerrentatiorl

i is

for

several

reasons—— rrost inirtaritly; if i > n, then s. will be zero vector. However,

it is sufficient for deriving the basic properties of our algorithu (for
general functions F) in Theor 2,2 below; and is also sufficient for discussing the bebavior

We

of our

rr

y0,. .. , y },

Define

,s. as

..

each

P,

P,B

{s0,... ,s },

is

problns in

Section 3.

E a.b. , a, b
i=l 1 1

TheorEn 2.2 Given

at

on linear

use tJ'ie tation <a,b> to denote the scalar product

ab =

Then

algorithn

{B01.

..

, B1 }

F

P -P

,

let the sequences

be generated by Algorithm I.

in Algorithm I and let

y

=

y - BQs1

j

= 0,. . .

i.

iteration i, if se;. . . ,s. are linearly independent, then 3i+l

well defined and

— 11 —

= 0

Sk>
<S.,

S> =

0

A

k =

0,...,

i-i

(2.8a)

k =

0,...,

i-i

(2.8b)

A

A
= <s,,
<s.,
S.>
1
1

11

Proof

B÷1

Sk =

B.1

Sk =

s.>

(2.8c)

(2.9d)

k = 0,.

It is straightforward to prove

.,

i

(2.8 a-d) by induction. In

view of (2.8a) and (2.7e), it suffices to consider
Using (2. 7e), (2. 8c), and

the

A

definition of

- B. s.)
1 1

S.

+ (y.1

- B. Q. s.) - B.
1

A

A

1

3.

Tira
ort1gonaj. to

2.2

for

shows

in (2.8e).

k =

111
A

1

0.1 s.)
1

A

- B. s. =

y.

i.

that we

are

selecting s

all previous steps s, j <i,

information contributed

(s. -

1 11 1

= B.11
S. + y.
holds

i

we find

+ (yj

S.

= B.

so that (2. 8e)

k=

A

= B. S.
i+l 1
3. 1

B.

(2. 8e)

.

in

Algorithm I to be

so that we do not disturb

by previous quasi-Newton equations. The equations

(2.8e) can be thought of at each iteration as the part of the quasi-

Newton equation giving information in the subspace where previous
iterations gave none.

-

— 12 —

Note that

if B.

and B1 are nonsinguJ.ar, then (2.7e)
(s. —

B.1
11

Therefore

= B. + 1
1

is equivalent

to

T B.1
B.1
y)
1
].
1
1
AT

s

(2.9)

B y

if B is nonsingular and <sj,

—1

0 for 0

B1 y>

< j < i,

then

B1 exists, i.e., B+i iS flOflSinU.lr.
now state, in general form, the version of our new algorithm
which is used in practice and in proving local Q-superliflear converWe

of s orthocJO1l
gence. it recognizes that, in general1 the projection
vector for sane
to the subspace spanned by So,... ,s1im.ist be the zero
=
if
i < n. The algorit1Et therefore "restarts" by setting s s

''

every n steps).

is too small canpared to Si (which rrn.ist happen at least
vectors s,
ThecrEn 2.2 is still valid if we consider only the

since the last restart. Since the version of TheorEn 2.2
stated as
applicable to Algorithm ii is needed in Section 4, it is
TheorEn 2.2.
TheorEn 2.3. The anitted proof is a1irst identical to that of

generated

Because of the restart iteria,

s

is alcays strongly linearly indepen-

dent of all s 's since the last restart.
Algorithm II

Let

X0 £

,

i=

0,

B0

F

c>

0,

't >

1

Set
For

1, 2,...

Choose nonzero s

(likely s =

—X

B1 F (xi))

be given.

—13—

x1 = x.

+ s.

If I

y. =

—

F(x.1)
i—i

1 11

s. — Q. s.J

Theorn

2.3

Given x C

IR,

(y.1

=

arid

S1

else (. = S.

B.1 = B. +

(2.lOc)

Sj

S.
j

(s =

(2.lOb)

S.

S.

IIs.II —
> -r
1

then

stop

F(x.)

E

J=.i—i

C then

<

J

Q. =

1±

(2.lOa)

(2.i')d)

i)

— Q. S. and 9...
—

B.

=

,rr

11

s.) s.

(2.lOe)

F

B0 c

C

> 0,

-r >

1,

let the

. ''÷l be generated by Algorithm
II. Define
.. . ,} as in Algorithm II; let = if = and
{,
y s s
=
sequences {s,... ,s1}, Cy,. . .

y.

y.

s ,
1

13. Q.

... ,s

are

s

,y.},

otherwise, j

linearly

<s.,

= 0,. . ,i.

independent,

=

0

<s., Sk> = 0

{B,. .

.

Then

at each

iteration i,

B1 is well defined,

and

k = L,...,i—1

(2.lla)

2..,.. . ,i—l

(2.llb)

k =

— 14 —

A

A

A

<s.,
1

s.>
1

(2.llc)

= <s., s.>

11

k = Zr,..

B1 Sk =

ic

B.1 5k =
IIsII <

=

j

(2. lid)

2.... ,i

(2. lie)

ilisHi

(2.lif)

(2.llg)

finally note that the

We

nonlinear systans of

entire

equations

subject of quasi - Newton methods for

can be approached by directly fozning

approxitnations H to F' (xi) -1, the inverse of the Jacobian matrix of F at x.
In this case we require H1 y = s and can achieve this through the
rark -one

update

=

H+

T
(s.1 - H.1 y.)
1 w.1

(2.12)

w.
1 y.1

y o.

we have already seen
such that
for any vector w. c
fran (2.9) that if B. is non-singular, Broyden' s update sitriply corresponds

to

w1

-r s

= B.

in (2.12).

The choice of w
(H+i -

H.)

is w =

in

(2.1 Z which mi.nimizes the Frobenius norm of
The

also proposed by Broyden and is

because it doesn't perform as

quasi-Newton method using this update was
saTetimes

called "Broyden' s

bad

rrethxl",

well as Broyden's method (update (2.4)) in

— 15

practice. However, it

—

also been dconstrat1 by Broyden, Dennis, and
Ibr [1973 to have local superlinear convergence under reasonable assurrtions
has

on F.
Similarly, we can propose a1gorit1ns I' and II', which update
approximations H1 to F' (x1) , arid choose w in (2.12) to be the
projection

y 's.

of y. orthogonal to (sorr of) the previous

instance, Algorithm II' would

only

For

require replacing (2. lOc—e) with

i—I
=
1

I

i+l

'j Y

j=9.i—i

If

H.

E

II I >

y y
— Q4'

TI

(y.

y and 9..

else

(y. =

y

= H.
1.

and we can prove the sane

Q.'

y

= i)
and

(2.13b)

=

- H.

(2.13c)

Y

we can prove theorems analagous to
convergence

results for linear

F as are proven in Sections 3 and

fact, the proofs of Section 3 are

asse B11

—

I

1 11y 1

+ (s.

Using Algorithms I' or II'

functions

yI

then

yi

nonlinear

(2.13a)

then

2.2 and

2.3;

and general

4.

(As a matter

a bit nicer as they never need

non-singular). We have tested hoth algorithms II and

II'

of

—16—

In practice, and have fotrid that AlgoritlTn II appears irore likely to

nverge than II'.

—17—

3. Behavior on Linear or Partly Linear Problems
In this section we examine the behavior of our algorithm on

systems of n equations in n unknowns, some or all of which
are linear. We find that our algorithm will always locate a
zero of whichever of the equations are linear in n+l or fewer

iterations. This property is not shared by Broyden's method.
Theorems 3.1 and 3.2 examine the behavior of Algorithm I on

a corp1etely linear system. In reality we woulc not expect to
use our algorithm to solve linear equations. However, it is
possible that near a solution, a system of nonlinear equations

may be almost linear--and these theorens then tell us what sort
of behavior to expect.

Theorem 3.1 shows that if Algorithm I is applied to
F(x) = Ax + b, A nonsingular, then x will equal x =
i <

n+l;

and if ri+1 iterations are required, then

-Ab

B = A.

for some

Follow-

ing Powell [1976),however, we are really more interested in
Theorem 3.2, which shows what happens if we do a restart wiile

solving a linear system of equations. This is likely to be the
case if we enter a linear region after the algorithm starts.
Theorem 3.2 shows that we still require at most n+2 iterations

to firu , but Example 3.3 shows that B+l may not equal A.
Theorems 3.4 and 3.5 examine the behavior of Algorithm I

when some but not necessarily all of the component functions of

F are linear. This may be the most important case in section
3, as partly linear systems do arise in practice; they may also

approximate the behavior of a nonlinear system near a solution.

—18—

Theorem 3.4 shows that our method will locate a zero of the
linear components in n+l or fewer iterations—-and if n+l itera-

tions are required, then B will also agree with the Jacobian
on the rows corresponding to the linear equations.

rratrix

Theorem 3.5

shows that in this case, subsequent updates by any rank-one
formula (2.4) will not disturb the correct linear information
= 1

and as long as we take quasi-Newton steps of length one (X1

in (2.la))

,

we will only visit points at which the linear coin-

ponents are zero.

Theorems 3.1, 3.2, and 3.4 are stated for simplicity for

Algorithm I. They are also true for Algorithm II, ihich we

use, as long as the algorithm doesn't restart prematurely
(i.e., IIjI < Hs Q.s.H in (2.lOcl) when i — 9i—1 <

really

—

T is

Since

set significantly larger than 1 in practice, we often expect our

theorems to hold for Algorithm II. The conclusions of Theorem
3.5 do not depend on which of the two algorithms we are using.
by

We denote the subspace spanned by vectors V1
,vk]; and the column space of matrix M
(v1

,V
by

C(M).

Theorem 3.1 Let A
F (x) = Ax

any x C
pendent,

+ b:

-

flXfl be non-singular; b C
]R

and E C

.

Consider

Algorithm I acting on F, starting from

,s_1 are ]irearly inde-

If s0

then B = A; and if Sn =

-B1

F(x) then F(Xn+i)

= 0.

ISk_1 are linearly indepen-

Moreover, if for some k < n, s

dent, Bk1 exists and k1 F(xk) c [s
Bk1 F(xk), then F(xk+l) =

and

0.

lSk_lI1 arid if

—19—

If s,... ,s1 are linearly independent, then by Theorem

Proof.:

2.2, Ens. =

y,

we have B S. = A
fll
If

O,...,n—l. Since y = F(x.1) — F(x.) = A
s., i = O,...,n—l, so that B = A.
1

i =

fl

5'••'5k1 are linearly independent, then by the same

reasoning as above,B.s. = A s., i =

11

Sk =

Si,

3k

F(xk÷l) =

F(xk)

1

O,...,k-l.

Thus if

[s...,skl], then Bk 5k = A

F(xk) + A Sk = F(xk) + Bk Sk =

F(x)

Sk .

Therefore

+ Bk [_Bk1 F(xk)]

0.

I
From the proof of Theorem 3.1, we see that if ALjorithni II i
acting on a linear problem, then after n—rn iterations in which

s,.. .,s1 are linearly independent and no restarts have occurred,
Em will agree with A in n-rn directions-—i.e., (A have rank in.

Bn_rn)

will

It is possible——especially if we have entered a

linear region after we began--that we will then do a restart:
set

=
n-rn

s

n—rn

and 2.

n-rn

= n—rn.

Following Powell [1976], we

wonder if the information from these n—rn iterations is of help.

In Theorem 3.2 we show that. it is: using quasi-Newton steps (1.3a),

we require at most m+2 additional iterations, or a total of n+2, to

locate the zero of F. Our conclusions are not as general as

Powell's for Davidon's [1975] new unconstrained optimization
algorithm, as they do not allow for subsequent restarts or completely general steps; however, our conditions should mirror the

behavior of Algorithm II in practice. Also, in our case Example
3.3 shows that the full m+2 iterations may be required and that
Bm+i
may still not equal A.

—20—

Theorem 3.2 Let A
Consider

b

,

c , and F(x) = Ax

Algorithm I taited from

and B0 C nxn

x0 C

is

singular with rank (A - B0) = n > i. suppose s.
Si = —X.

B.1 F(x) and if s. '

nonsingular.

+

+ b:

(s0,...,S_i]r

selected by

assume B1 is

Then there exists j < in + 1 such that

and if X = 1, then F(x±1) = 0.

Si C (s01...1s_1);

Proof: We first show that for any update of form (2.4)-—one of
for some

which is used by Algorithlr. I-— that s e Es0

j < m + 1. We accomplish this by showing by induction that if

'. ,s_1

are linearly independent, then

Si C [s0,

(s —

CCI -

B1y)c

(3.1)

BQ1A)1

C (I — BQ1A)

(3.2)

•

For i = 0, (3.1) is trivi.1ly true, and
—

Bol

y0 = S0

-

(I

A S0 =

B01

1

Xk+l

Sk+l —

—l

3k+l

F(xk+l) = Bk+l
=

By Theorem 2.2, B,,+l1Yk
Bk+l

-l —
— B

Bk+11 F(xk) =

k

3k

BQ1A) s0 C C(I

0,... ,k.

Assume (3.1-2) true for i =
-

-

$3,;

1

-

BQ1A).

Then

(F(xk) +

and using the inverse form of (2.4)

(Sk - Bk
1 +
T

F(x1j +

-1

T

-1

'k Vk Bk
-1

Vk Bk

- Bk1

<Vkl Bk1 F(xk)>
-l

<Vkl Bk

k>

have

—21—

Since Bk

F(xk) =

-

Ski we have

E

so by the induction hypothesis (3.1-2) for

Sk+l c [5, C(I -

E1A)J,

plete the induction,

(sk -

[Ski

Bk'

k,

which shows (3.1) for i = k+1.

Sk+l -

k'

To corn-

=

Bk+,yk+

k (s.-B.-1y.)v. TB. —1
—'B° +
T3 -1
j0
,
L
J
3
I

I

=

—
—

s

k+1

Sk÷l

k

-1

-

B0

Since

k+1 + E0 (Si -

Sk+l -

(s -

B01

'k+1 =

(I

<vj, B.
-

B1A)

C (I-B0A) for j

C

>

<v., B.-

-l

<

Sk+l and

k by the induction

hypothesis, we see that (3.2) holds for i = k+1.
Because the subspace [s0, C (I -

B01A)]

has dimension at

most m+1, we must have S. C Is01.. .s1] for some j < m+l. Now
=
B.
= 0,...j-3. by Theorem 2.2; and
B s1 = A s, i = 0,
,j—l since F is linear. Therefore B S. = A
and

i

F(x.1) =
Example

F(x.)

3.3.

s,

+ A S. =

s =

and

.

x

Let F(x)

Algorithm I, with

F(xi) + B. (_B' F(x.)] =

-B1

(F'(x)

0

11.
o

•iio...o

:

L)

1...10....01
rn

n-rn

1

I). Consider

F(x.) started from x0 = (1

10

B=

0.

—22—

with

1 < m <

requires

rank (I -

s0 =

n.

Then ran] (I-B0) = m.

full jn+2 iterations to reach x = 0, and

B1)

= 1.

The intermediate values are:

(—1,0,.. .10,

—1)

=

,0)T

1/2
.

1

Sm

=

=

—

= l,...,m—1

j

j = i,... ,r

,

0

—1/2

.

I n—rn—i

..

'•l

x(j1) '—v_S---1
i

0

lx(n—m—1)

(0,,•••,_1,21•,2)T
Sm

s. = s.,

:

•

1/2

S0

.

.•
.
.
(n—j ) .

'j-•l

=

,

=

00mx(n—m—l)

0

lx(j—l)

s

,

11)'P

10
B

T

o,

Si

x

Algorithm I then

(so+...+srn...i)

=

,

.

=

im

1/2
=

(01_1,•••,_1)T

(1101•••,01_2,••,_2,_1)T

Bl= B + 4(nm)20'1'••• ,-1)
S+i = (011)T; X+2 = 0; and
=
(I
S+i (—1/2 — t,0,...,0,2t,...,2t, 1/2 + t),
Bm+i)
t =-1/(4(n—m)-2). •
Therefore

—

We

where

now consider the case when some but not necessarily all

of the component functions of F are linear. For ease of notation we assume that the first in component function of F are
linear--however the positioning of tb€ linear functions has

—23--

no bearing on the algorithm or the proof. The Jacobian of F
will therefore be constant in its first m rows, and we will de-

note our Jacobian approdn'ations B by [
D.

c.
)

1

R(nm)xn

,C

c

Theorem 3.4 Let A c mxn 1 < m < n; b
EF (x)l

F(x)=
F2(x)j
Consider Algorithm I acting on F, starting from any

B e

.

If

and

for some k < n, s01... FSk_l are linearly ince-

pendent, Bk' exists and Bk1 F(xk) c [soI...lskl], then the
choice Sk =

3k

F(xk) leads to F1 (xk÷1) = 0.

Furthermore if

s,... ,s1 are linearly independent, then C = A.
Proof: Suppose s,... ,5ki are linearly independent and Bk1

y,

exists. By Theorem 2.2, Bk s =

0 < i < k-i. Since the

first m components of y are F1(x÷1) —

F1(x)

= A s, while the

first m components of Bk s equal Ck s, we have Ck s1 A s,
o < i < k-i. In particular,
k = n then this irrplies C = A. More-

if

over, if Bk F(Yk)

[s,... ,sk_1] (which will necessarily hold
F (Xk)l then this implies

for some k < n) and Sk =

Ck Sk = A Sk; because Ck Bk1 =
+ A Sk =
Fl(xk+l) =

Fl(xk)

=

Fl(xk)

(I

Fl(xk)

-

Fl(xk)

= 0

0mx(n-m)' we thus have
-

Ck Bk1 F(xk)

—24—

Theorem 3.5 Let A, b, F, F1, F2 be defined as in Theorem 34.
If Ck = A and Bk+l is defined by (2.4) for any value of Sk (and
any Vk such that <Vkl Sk>

0), then Ck+l = A.

F(xk) or Fl(xk) = 0 and Sk
Bk
then Fl(xk+l) = 0. I
either Sk =

Furthermore, if

Ak Bk1 F(xk),

Theorem 3.5 shows that once we have correctly obtained •the
linear part of the Jacobian as Theorem 3.4 shows we are likely

to do in n iterations, then our quasi-Newton algorithm will not
disturb this information; and whenever we take a quasi-Newton
step of length one, which in practice we

usually

do on our final

iterations, we will locate a zero of the linear functions.

—25—

4. Local Q—Superlinear Convergence on Nonlinear Problems
In this section we show, subject to reasonable conditions
on the function F : ]R" - :iRrl, that if x is close enough to x
and if

is close enough in norm to F' (x*) [or F' (x0)], then

the sequence of xi's generated by Algorithm II with

converges Q—superlinearly to x.

s = —B11F(x)

Our proof leans heavily

on the local superlinear convergence proof of Broyden, Dennis,
and More [1973] for Broyden's method; and On the work of Dennis
and Nor& [1974] characterizing superlinear convergence.

In Theorem 4.2, we give a general condition under which a
quasi-Newton algorithm of form (2.1) with steplength one will

achieve linear convergence. This theorem amounts to Theorem 3.2
in Broyden, Dennis, Mor [1973] extended to updates using infor-

mation from previous iterations. Lemmas 4.3 and 4.4 show that

the update of gorithm II satisfies the conditions of Theorem
4.2 along with some further conditions. Using this we show in
Theorem 4.5 that Algorithm II achieves local Q-superlinear con-

vergence. We first state a simple lemma which we will use
several times; its proof follows immediately from §3.2.5 of
[Ortega & Rheinboldt, 1970].

Lemma 4.1 Let F: rI +

be ciifferertialle in the open convex

set D, and suppose for some x* c D and p > 0,

F'(x) -

FI(x*)II

<

K

Ix

1< >

0

that

- x*HP

(4.1)

Then foru, veD,
I

F(v)

-

F(u)

F'

<

(x*) (v-u)

KI Iv—uI Imax {

v_x*I

Iu_x*I P}

• (4.2)

—26—

-

Theorem 4.2 Let F :

be differentiable in the open

convex set D, and assume for some x c D and P > 0, K > 0
(4.1) holds, where F(x*) = 0 and F' (x*) is nonsingular.

J =

F'(x*).

that

Let

Consider sequences Cx0, x11...} of points in

and {, '••• of

nonsingular matrices which satisfy

Xk+l_XkkF(Xk)
and

'1k+l —

IIF

HF

Hk

.

+ c max flIXk+l -

x* I1. ..,

I

lIXk
k =

0,1,...,

j

0.

<

for some fixed

x*It°}

0, '1ere x.J = x 0 for

0 and q >
—

—>

—

(4.4)

Then for each r c (0,1), there are positive constants

e(r), d(r) such that if x0 —

x*II <

(r)

and lIE0

— JHF < 5(r),

then the sequence x,x1,...} is well-defined and converges to x
with

I jX - x*l
for all k > 0.

Furthermore, {I

I <

ri

Xk

IBkI } and Cj

- x*l

I

tkt

are uni-

forinly bounded.

The proof is so similar to that of Theorem 3.2 of (Broyden,

Dennis, & Mor, 1973] that we omit it. I

In Lemma 4.3 we show that for Sj y. defined in Algorithm II,
asymptotically I Iy —

F'

(x*) sJ I is small relative to I IsI

This is the key to proving in Lemma 4.4 that the update of Algorithni II satisfies equation (4.4) of Theorem 4.2.

—27—

Lemma 4.3 Let F

IRn be differentiable in the open

convex set D and assume for some x c D and p >

0,

K > 0 that

(4.1) holds, where F(x*) = 0 and J E F' (x*) is non-singular.

Consider the sequences {x

,x11. .

.} of

points in JR' and {B,B11...}

of nonsingular matrices in )R generated from (x0,B)
A
by Algoritjt II with S. = -B. —l F(x.) for all i. Let S.

be defined

as in Algorithm II and y. as in Theorem 2.3. Then

ili

—

j

i—i

i—Z..—l

max {f

Jx I

1

max {1, T

<

—

x*f

}

lxi

,...,

2

si

1<
I

— x*J

l, I Ix÷1 —

Proof. The proof is by induction. For i = 0,
=

- J

0,so

Lemma 4.1 with v =
Since

=

0

JI=
x1,

u =

jy

- J
sI J,

x.

=

which

S0

*f

P}

and

is < KJ sf fIn by

by Algorithm II.

£k, then 'k =

'S

=

Ski and

Lemma 4.1, so we are done. If k >

0,... ,k-i. For i = k,
- Ski < KI
'k
'k' then

YkJSk_YkEkksk_Jsk+
=

-

J

if

ISki Ink by

J

S})

-

(Bk

-

Sk

j) k

k—i
E

(YkJsk)

Sk
A

<s.,s >

j

(Bk_J)s.J

k

A

<sj,sj>

= (y
k

—

J

<.,s >

k—i

s )

k

—E
-,

(.

k

J

—

A

(4.5a)

(4.5b)

Thus for i = 0 (4.5) is true,

Now assume (4.5) holds for i =

k =

i , where

m

A

<s. s.>

j'j

—28—

the last equation following from 3k

in

=

s

Theorem 2.3.

Therefore
I

—

— J S1

S]j

fact that

—

induction hypothesis 4.5, (2.llf), and the

m., 1=

1k - J

Li Sj11 IlSklI/ISII.

j-Lk

4.1,

Thususing Lemma

+

ii

the definition of

S)J Xflk

+ KJ Ski

i9

h—i
+

j—2.

E

we have

—l

s.

j—L

K

2

T

is Hrn

''sill
1+

K ISki Imk{1 +

-

<K

=

rr.,

kt mk

TkLkl

k-i

(2T)k

E

}

k—i

1 +

E

29'k}

1k-2.k-l 2 k-2.k

which proves (4.5) for i = k and comp1tes the induction. I

Lemma 4.4 Let all the conditions of Lemma 4.3 hold. Then

I

IB÷ —

J. <

JB. —

/1 — ®.2 +

'F

(2t)1

K rn.1

(4.6a)

where
-

o

1

=
1

1

J)

- II F

1

H.ll
1

(4.6b)

—29--

Proof:

Using the definitions of s and y along with the equa-

i

•>
<i.,
1
1

tion <s., S.> =
1

from Theorem 2.3, we find
T

(y. - B. s.)
1

11

B.
1+1=B.+

=E.1 +

AT S.
S.
1
1

1

B.
-J
1+1

= (B.
1

ii

S.

S.

1

—

J)

—

AT

I

A

S.'
iJ

S.

1

--

A
I

, 'F

lB+1 —

(B. -

—

1

S.

J)

1

—

I

+

(9

1

1

J s.) s.
1
1

and

AT
AS.
S.
1

1

AT

AT
S.
S.)
1
1

- J

S. S.

11

+v± S.TA

ATA
S.
S.

1

1

-

AT

A

—

AT
S.

1
S.

1

-

There fore

(. - B.
1
1

1

S.
1

F

(4.7)
nxn
and
BrOyden, Dennis, and Mor (1973] show that for E C m

UC

I

r T1

uu
u uJ F

-

HE

—

S.
s.2. AT
1

2
- ____
HEuH

= lIEu 2

Thus

2

Hul

F

A

J)

-

E

A
AT S.
S.
1

H2

-

=

P

(B

- J)

lB. -

JI

1

2.

A

2

F
(4. 8a)

Secondly

A Til
S. H
(y.1 — J s.)
1 II
1
A

A

S.

1

<max {l, T

S.1
i—..—l
1

H
ii

IIj J lI
—

=

S.

F
F

(4.8b)

A

1

Ils.!I
———
2
I

II

1i

m. < (2T)

1—

n-i

Km.

from (2.11 f—g) and Lemma 4,3. Combining (4.7—8) gives (4.6). I
Lemma 4.4 shows that Algorithm II satisfies the conditions

of Theorem 4.2 and is locally linearly convergent for any
r

(0 1). The extra power supp1i by the /1 -

term in equa-

tion (4.6) enables us to prove local Q-superlinear convergence.

e

-

Theorem 4.5 Let F :

differentiable in the open

convex set D, and assume for some x c D and p > 0, K > 0, that
(4.1) holds, where F(x*) = 0 and J

F' (x*) is nonsingular.

I
Consider the sequence {x0, B0, x1, E,, x2, 2' •
flXfl, generatec fron (x0,E0) by Algorithm II with

s1 =

E

-B. F(x.) for all i. Then there exist , >0 such that for
—

-

and

< S, {x} converges Q-super
'F
linearly to x and CiIBJ }, C k1' are bounded.
<

Proof: The linear convergence of Algorithm II, and houndedness

of CIIB.II}, {lIBII}1 follow Theorem 4.2 and Lemma 4.4. The

term (2T)'K in (4.6) corresponds to a in (4.4).
We turn now to the superlinear convergence of Algorithm II.
From Lemma 4.4 we have

IIB1

.

— JHF
—

e.
1

If lirn inf {

=

H B.

-

J)

-

+ a

m1 where (4.9a)

.

JIIF

'F

/i- G.2

—

(4.9b)

Ilsill

= 0, then Corollary 3.3 of Broyden,

Dennis and More [lS73] shows that Algorithm II is Q-superlinearly
convergent.

—31—

Now suppose urn inf { lB1 -

i

> 0. From the linear

'F

convergence of Algorithm II we know urn m1 = 0.
=

must therefore have lim

urn

(B. 1

J)

By (4.9) we

0, i.e.,
s•

= 0

(4.10)

.

lklJ
Now Theorem 2.2 of Dennis and More [1974] shows, under the conditions of Theorem 4.5, that if Algorithm II is linearly convergent, then
— J)
urn ll(B.
1

1

=

0

(4.11)

I lsI I

is a sufficicr't (and necessary) condition for local Q—superlirear

convergence of the algorithm. Therefore it only remains to show
that (4.10) implies (4.11).

Let
i-l

Q. =
1

S1

=

(I

-

E

2T S.
j=i.13
S.
J

Q) s. Now

so that

,

=1

1 -

because (I -

zero orthogonal projection matrix, so

1

-

J)

1

11s4

IiII

is a non-

I IsH and

II(B.1 - J)

-

Q)

1

(4.12)

H

By the triangle inequality,
II (B —

J)

IISjIJ

SjI

—

(B

—

J)

IlSjlI

+

I (B —

J)

Q s

IISjII
(4.13)

—32—

As i+, the first term on the right hand side of (4.13) approaches zero due to (4.10), (4.12). For the second term on the
right side of (4.13), Theorem 2.3 and Lemma 4.3 show
i—i

I (B — J)

Sj1

=

—

I

s>/<

J)

>I I

j=zi

i—i

=

1Z

) <i., s.>/<,

—J

(

i-i

rnax{1,t

—

j-.-1
1

}2

j-i.

1K

>I I

IIs.II

IIH

• Is.

1

rn.

J

Because IsII/IIII < T (by (2.llf)) and rn < rn.1, jZ,...,i—1
with i-2.. < n (by (2.ilg)), we thus have

i—2.—1 i—i

H (B

—

3)

. KI Isj rnj.1

sj I

5.

XI IsI

< iI
—

T

1

T

j—..1

i—Z.—1 i—i..1
1
2

rnj....1

n—2 2n—i

s.1

2

m.

i—i

Hence
I!

lint

(s
I

I (B

sdj
—

j)

HH

< K n—2 2n—i rn1 ,

I

=n

,

so

(4.14)

-33—

since im m. = 0.
1

Therefore (4.10) and (4.12—14) imply (4.11)

is true, which completes the proof of local Q—superlinear convergence of Algorithm II.

It should be noted that the techniques of this section apply

equally well to an algorithm identical to II except restarting
whenever i —

R..l

> t, t < n (or I Is.I I/I lsI I

> T). Such an

algorithm would not be exact on linear problems, however. Another interesting algorithm covered by the techniques of this
section is one setting
<S.

S. = 5. — S.
1

i—i ,

S.>
1

i—l<s.
i1,s.
i—l >

1

at each iteration. Such an algorithm would preserve the current
and most recent quasi—Newton equation at each step, and can be
shown by the techniques of this section to be Q-superlinearly

convergent without restarts. We have not tested this algorithm.
Finally, the techniques of this section would also apply to

an algorithm which set s ecual to the projection of s orthogonal to the previous t si's, t <n,suhject to the strong linear
independence of

.

. ,s. as in Algorithm II. Such an algorithm

would require no restarts arid would be exact for linear problems
if b =

n.

It would be fairly easy to implement (in 0(n2) house-

keeping operations per step) using Powell's [1968] orthogonalization scheme.

—34—

5. Computational Results
We have implemented Algorithms II and II', with some modifica-

tions,and tested them on several problems. In Step (2.lOa) we

choose s =

-.X

B.1 F(x.), where ).

is

determined by the scheme

described in rBroyden, 1965] with the added restriction that
I Isj I < 1 (except as otherwise noted). Instead of storing

3l•

we actually store and update H =

Rather than compute Q

explicitly by frmu1a (2.lOc), we use ajpropriate Householder
transformations to express in product form an orthogonal matrix

P such that

=

p

,

whence

=

s —

L

n—i+2._1

Our implementation includes the option suggested above of restart.xszj
whenever i-L.

try

>t, where t -< n is fixed. For t = 1 this lets us

Broyden's original methods on the test problems.

Test Problems

The test problems we used include the following; we write x1
1
th component of x =
for the i—
(x ,.

.. ,x

T
)

c

Prob1 3. [Frown, 1969, p. 567]: n = 5.

i
n

f. (x) = —(n+l) + 2x1
1

+

j=1
j

n

f (x) =
n
=

(.5

—1

+

11

x,

1 <

i

<

n—i

X

j=l
T
5)

;

* (1 11)

T

—35.-

2.

n =

Problem 2 [Brown, 1969, p. 567] :

i2 —x2 —1.
(x)
2

= (x 1 —2) + (x 2

-

"Chebyqd'

j=1

0

polynomial,
T1() =

2

[Fletcher,

1
/ T()d - inE

f(x) =

2

— 1

(.1,2); x (1.06735, 139228)T

x0 =

Problem 3 -

.5)

—

l965,y. 36] : n =

2,3,4,5,6,7,9.

T(xJ), where T.1 is the

Chebyshev

transformed to the interval [0,1], i.e. T0()
—

1,

—

T1() = 2(2—1)T()

T.1()

for i >

1,

1.

Note

that

10 if i isodd

1

/ T.1 ()d =

x = j/(n+l),

j

1 <

<

l-l/(i2-l)
n;

if is is even.

the components

of a solution are any

permutation of the abscissae for the Chebyshev quadrature rule of
order n.

None of the variations of Broyden's method which we tried
solved this problem for r=9, so we omit the results of these runs.

Problem 4 (Brown and Conte, 19671: n = 2
f1(x) =

f2(x)
x0

=

1

sin (x12
x )

= (1 —

(63)T.

)

-

x2

-

--

[exp(2x1) —

x =

(5)T

1

x

e] +

— 2ex1

—36—

Problem 5 (Erown arid Gearhart, 1971, p. 341): n = 3.

22
(x12
) + 2(x

f1(x) =

f2(x)

)

(x22
) + x3 -

+
(x 12
)

=

f3(x) = (x11)
=

(1,

—4

+ (2x2

8

+ (x-5)2

-

= (0, /•

.7,

6)T

Problem 6 [Deist and Sefor, l962j_ ri =
f(x)

=

6
E

j=1

j1

1'• '6 = io2
x0 =

1 <

cot $.x,
1

(75,

- 4.

6

i

6,

<

where

(2.249, 2.166, 2.083, 2, 1.918, 1.833)

75,• ,75); x

(121.850, 114.161,

93.6483,

62.3186, 41.3219, 30•5027)T
Problem 7 [Broyden, 1965)

n =

5,

(.5x—3)x1 + 2x2

f(x)
f(x)

For n = 5,
and

xr1

= (—1,

x0

x*

for n =

10,

—

1.

+ (.5x1—3)x1 +

=

=

10.

(—.

+

(.5x-3)x -

—i,...,—1)

2xj1

—

1, 2 <

n—i

1.

T

968354, —1.18696, —1.14848, —.958989, _594159)T

x (1.0301l,

—1.31044,

—1.37992, —1.39071,

-1.37963, -1.34993, -1.29066, -1.17748,
—.975O1.

5657)T

—37—

We ran our tests in double precision on the IBM 370/168 at

Cornell University. Table I below gives the results of some of

these tests. 1'Probiern ct. means probiemc with n =

For each test problem we report both the actual nuriber of
function evaluations needed to achieve IFI

< 1010 and a

normalized number of function evaluations obtained by dividing
the actual number by the minimum of the three numbers for that

problem (and rounding to two decimal places). Although Algorithm
II sometimes fares worse then Broyden's good method, the means
of the normalized numbers show that Algorithm II with T = 10
averaged about 10% fewer function. evaluations than Broyden's

good method on these test. prc1lems. The choice T = 10 worked
considerably better than T = 100 in Algorithm II, suggesting
that a reasonably small value of T, such as 10, may be best.

We ran several other tests, whici we shall not report in

detail. True to its name, for example, Broyden's bad method
failed six times as often as his good method. Algorithm II'
with -r = 10 failed on 5 of the 15 test runs; with T = 100 it

failed on only 3, but fared rather worse than Broyden's good
method with respect to mean normalized function evaluations.

We tried a hybrid between Algorithms II and II' whose average
behavior for T = 10 was as good as that of Algorithm II. The
hybrid applies the projections of Algorithm II' to the inverse
form of Broyden's good method, so that
(I -

Q)H. T

A

S. and the choice

=

y

y -

is

replaced by
A

is replaced by y =

T

H

Si.

—38—

Total Function
Evaluations

Normalized Function
Evaluations

P robl em

'hhi

1.5

31

27

28

ii.

10

ltD

3.2

9

9

3.3

13

Broyden' S

3.5
3.6

Algorithm II

3royden' S

r=lO T100

"Good"
__________
1.15

Algorithm II
T = 10 T = 100
1.00

1.04

1.10

1.00

1.00

9

1.00

1.00

1.00

11

13

1.18

1.00

1.18

19

23

23

1.00

1.21

1.21

20

24

23

1.00

1.20

1.15

26

33

--

1.00

1.27

(31)1

—
—

3.7

45

35

36

1.29

1.00

1.03,

4.2

12

10

10

1.20

1.00

1.00

5.3

15

1.00

--

--

532

16

1.00

1.00

2.14

1.00

2.07

(28)1(28)1
15

15

1.07
-

6.6

62

29

60
—

—

6.62

32

28

57

1.14

1.00

2.04

7.5

13

13

13

1.00

1.00

1.00

7.10

21

20

20

1.05

1.00

1.00

Mean

1.17

1.03

1.21

Table I:

Function
Evaluations
Required to
Achieve
I

Notes:

<

1010

Std.Dev

.29

Failures

1

.074
1

.37
1,

1. Eroyden's [1965] quadratic interpolation technique
failed to reduce IFI i in 10 function evaluations.
The number reported is the total number of function
evaluations at the time of failure.
2.
was allowed to incr3ase as much as twofold
I IFI
(per step) and a maximum steplength of 10 rather
than 1 was allowed.
3. A maximum steplength
of 10 rather than 1
sj
was allowed.

—39—

6.

Summary and Conclusions

We have introduced some new quasi-Newton algorithms for

solving systems of n non-linear equations in n unknowns. These
methods are modifications of 'Broyden's good method" and "Broy—

den's bad method' (Broyden [1965]). They retain the local Q—
superlinear convergence of the unmodified methods and have the
additional property that if any of the equations are linear,
then the methods locate a zero of these equations in n+1 or
fewer iterations.

(We have only proven these properties in this

paper for the modified Broyden's good method, but virtually the
same proofs go through for the modified bad method.)

Our computational results suggest that our modified form of
Broyden's good method performs better, on the average, than the

original form. We think our new method should be further tested
and possibly considered as a replacement for the conventional
Broyden's method in existing subroutines.

Acknowledgement
We are grateful to Professor 14.J.D. Powell for helpful discussions and advice.

—40—

7.

References

Brown, K.I4. (1969), "A Quadratically Convergent Newton-Like
Method Based Upon Gaussian Elimination," SIAM J. Numer.
Anal. 6, pp. 560—569.
Brown, 1CM., & Conte, S.D. (1967), "The Solution of
Simultaneous
Nonlinear Equations," Proc. 22nd National Conference of the
ACM, Thompson nook Co., Washington, D.C., pp. 111-114.

Brown, I.N., & Gearhart, W.B. (1971), "Deflation Techniques for
the Calculation of Further Solutions of a Nonlinear ystcm,"
Nurer. Math. 16, pp. 334—342.
Broyden, C.G. (1965), "A Class of Methods for Solving Nonlinear
Simultaneous Equations," Math. Comput. 19, pp. 577-593.

Broyden, C.G.; Dennis, J.E.; & Mor, J.J. (1973), "On the Local
and Superlinear Convergence of Quasi-Newton Methods," J.
Inst. Math. Appi. 12, pp. 223—245.
Davidop, tT.C. (1975), "Optimally Conditioned Optimization Algorithins cithout Line Searches," Math. PrograInxruiii 9, pp. 1-30.

Deist, F.H.; & Sefor, L. (1967), 'Solution of Systems of Nonlinear Equations by Parameter Variation," Comput. J. 10,
pp. 78—82.
Dennis, J.E., Jr.; & More, J.J. (1974), "A Characterization of
Superljnear Convergence and Its Application to Quasi-Newton
Methods," Math. Comput. 28, pp. 549-560.

Dennis, J.E., Jr.; & More, J.J. (1977), "Quasi-Newton Methods,
Motivation and Theory," SIAM Rev. 19, pp. 46-89.

Fletcher, R. (1965), "Function Minimization Without Evaluating
Derivatives; a Review," Comput. J. 8, pp. 33-41.

Garca-palomar U.M. (1973), "Superlinearly Convergent QuasiNewton Methods for Nonlinear

Programming," Ph.D. disserta-

tion, University of t'7isconsjn.

Gay, D.M. (1977), "Convergence Properties of Eroyden-rype Methods
on Linear Systems of Equations," in preparation.
Ortega, J.M.; & Rhejnbcldt, W.C. (1970), Iterative Solution of
Nonlinear Equations in Several Variables, Academic Press,
New York.

—41—

Powell, M.J.D. (1968), "On the Calculation of Orthogonal Vectors,"
Comput. J. 11, PP. 302-304.
Powell, M.J.D. (1976), "Quadratic Termination Properties of
Davidon's New Variable Metric Algorithm," Math. Programn-iing,
(to appear).

Schnabel, R.B. (1977), Ph.D. Thesis, Computer Science Dept.,
Cornell University.

