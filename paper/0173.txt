LINEAR REGRESSION DIAGNOSTICS

Roy E. Welsch
and

Edwin Kuh

Massachusetts Institute of Technology
and

NBER Computer Research Center

Working Paper No. 173

PRELIMINARY DRAFT

Revised

We are

25 March 1977

indebted to the National Science

research under

Grant

this

Foundation for supporting
SOC76-]j311 to the NEER Computer Research Center

.
ABSTRACT

This paper attempts to provide the user of linear multiple regression
with a battery of diagnostic tools to deteniuine which, if any, data points
have high leverage or influence on the estiiiation process and how these

possibly discrepant data points differ from the patterns set by the rrajori-ty
of the data. The point of view taken is that when diagnostics indicate the
presence of ariomolous data, the choice is open as to whether these data are

in fact

unusual and helpful, or possibly harmful and thus in need of modifica-

tions or deletion.

The methodology developed depends on differences, derivatives, and

decompositions of basic regression statistics. There is also a discussion of
how these techniques can be used with robust and ridge estimators. An example
is given showing the use of diagnostic methods in the estimation of a crosscouriUy

savings rate model.

ACKNOWLEDGE!EJ'JT5

The authors would like to

Hoaglin,

Frank Hampel, Richard

Doug Martin, and Fred Schweppe.

acknowledge helpful conversations with David

Hill,

David

Andrews, Jim

Franc, Cohn Mallows,

S
TABLE OF CONTENTS
Page

1. INTRODUCTION

.

1.1 General Goals
1.2

Regression Diagnostics arid
Perturbations

1

1
Model Input
3

1.3 Modeling Research Aims and Diagnostics .
1.3.1 Leverage and Disparate Data

6
6
7
8

1.3.2 Collinearity

1.3.3 Regression Parameter Variability in Time
1.14 Notation
2. LEVERAGE POINTS AND DISPARATE DATA

9

.

10

2.1 Introduction

•

10

2.2 Residual Diagnostics
2.3 The Hat Matrix

• 14

2.14 Row Deletion Diagnostics

• 23

2.5 Regression Statistics

•

26

•

28

•

31

•

2.6 Influence and Variance Decomposition
2.7 More Than One Row at a Time
2.8 Interface with Robust and Ridge Regression

18

• 33

2.9 An Example: An Inter-Country Life Cycle Savings
Function

2.9.1 Residuals
2.9.2 Leverage and Diagonal Hat
2.9.3 Coefficient Perturbation

36
38

Matrix Entries

Variation in Coefficient Standard Errors
2.9.5 Change in Fit
2.9.6 A Provisional Sununary
2.9.7 One Further Step
2.9.4

2.10

38
•

.

•

.

•

.

•

•

•

.

Final Conunents

REFERENCES

. 39
. 39
. 40
. 41
. 42
143

•

.

414

Appendix 1. BASIC DIFFERENCE FORMULAS

•

.

Appendix 2. DIFFERENTIATION FORMULAS

•

.

Appendix 3. 'IHEOREMS ON THE HAT MATRIX

•

.

Appendix 4. EiIBITS FOR SECTION 2.9

•

•

. A1.1
. A2.1
. A3.1
. A4.1

.
1. INTRODUCTION
1.1 General Goals
Economists and

other model

builders have responded willingly to major

opportunities that have appeared in the past two decades deirand for policy

the

guidance and forecasts

business, and
purely intellectual goal of advancing the state of knowledge through
from government and

model development. The fundamental enabling condition has

to

a rapidly growing

been

the ability

produce more intricate models at decreasing unit cost because of advances

in computer technology. A large econometric
twenty

only

equations:

model twenty

years ago had

today a large model has a thousand equations. It is not

larger mode's, but also larger data sets and more sophisticated

functional

forms and estimators

that have burgeoned.

The transition from slider'ule and desk calculator to the large scale
digital computer has happened with startling speed. The benefits have, in

our opinion, been notable and at times exciting we know a great deal more
about the economy and can provide more intelligent guidance as a direct

result of increased computational power. At the same time, there are
hidden costs of current approaches to quantitative economic research via
computer which ought to be recognized.

One major cost is that, today, the

researcher is

a great deal further

away from data than he was, perforce, in the heyday of the desk calculator.

If there are a great many equations to estimate or thousands of observations
for a few equations, there is a natural tendency to use the computer for what

it does well: process data. A tape arrives and after a frustrating day or
two is accessible by a computer program (often a regression package, plain or

—2—

fancy). Then estimation

and hypothesis testing get underway until some

satisfactory conclusion is obtained.

It is not misguided

nostalgia to

point out that it was more likely, with the more labor intensive technology
of the past, for the researcher to uncover peculiarities in the data.

Nor

do we counsel a return to the golden past. What concerns

us is that the
valuable and

is

Tsomethingt

which has been lost in modern practice is

not recoverable from standard regression statistics.

major objective is to suggest procedures that exploit computer

Our first

brawn in new ways that will permit us to get closer to the character
of the data and its relation to hypothesized and estimated models.

There is the related

issue

of reliability. Our ability to crunch

quantities of numbers at low cost makes it feasible to iterate
times with a given body of data until the estimated model meets

large

many

widely accepted performance criteria in terms of statistical measures
such as t statistics, Durhin-Watson statistics and multiple correlations,
along with theoretically approved coefficient signs and magnitudes.

The iterative process is not what the statistical theory employed was
originally all about, so that it behooves us to consider alternative
ways of assessing reliability, which is a second major objective of this
paper.

Another aspect of reliability is associated with questions of distance

from the data that were mentioned at the outset. Specifically, the
closer one is to the data, the more likely it is that oddities in the
data will he uncovered or failure of the model arid data to conform with

each other will be discernible, so that reliability can be increased

when the

researcher has

more

intimate

contact with the data. At the same

.

.

—3—

time,

this posses a dilejmia, since the researcher may

prone to devise theories from data.

then

be excessively

This temptation, often referred to as

mining, should be restrained. One sort of insurance against

data

mining

is to be a strict Baysian arid

combining prior and

posterior

should be tested - repeatedly

thus

data

be guided by sensible rules for

information. Alteniatively the model

if possible - on bodies of data unavailable

at the time. Being a strict Baysian is not always practical nor is it
deemed to be universally desirable. As a general rule then, the most
practical safeguard lies with replication using previously unavailable data.
1.2 Pegression Diagnostics arid Model Input Perturbations
This paper presents a different approach to the analysis of linear

regression. While we will sometimes use classical procedures, the
principal novelty is greater emphasis on new diagnostic techniques.
These procedures sometimes lack rigorous theoretical support,
hut possess a decided advantage in that they will serve as yet unmet

needs of applied research. A significant aspect of our approach is
the development of a comprehensive set of diagnostics.

An important underlying concept is
inputs

that of

regression model
and examining the model output response. We view model inputs broadly

to include data,

perturbing

be estimated), error models and estimation
assumptions, functional forr and a data ordering in time or space or
parameters (to

over other characteristics. Outputs include fitted values of the
dependent

variable, estimated parameter values, residuals and functions

of these (R2, standard errors, autocorrelations, etc.).

We plan to

develop various types of input perturbations that will

reveal

where model outputs are unusually sensitive. Perturbations can take the

.
form of differentiation or differencing, deletion (of data), or a
change in estimation or error
The

model

first approach to perturbation is "differentiation" (in a

broad sense) of output processes
order

of

assumptions.

with respect to input processes, in

to find a rate of change. This will provide a first order measure

how output is influenced by input; differences would he substituted

for derivatives in discrete cases. If the rate of change is large, it
can be a sign of potential trouble. Generally, one would like to have
small input perturbations lead

to small output deformations. We would

also use this idea to see how big a perturbation can be before everything
breaks down. Of course, a "good' model is generally responsive to anticipated
changes in input.

For example, one could 'differentiate" the model with respect to its

parameters to ascertain output sensitivity to small changes in the
parameters. (We

could,

for example, evaluate this parameter sensitivity

function at the estimated parameter values.) This might indicate some
of

the more critical parameters in

the model that deserve further

analysis.
A second procedure is to perturb the input data by deleting or

altering one data point and observe changes

in the

outputs. More generai]y

we can remove random groups of data points or, for time series, sequences

of data points. This is one way to search for parameter instability
over time. By deleting individual data points or collections of points
one can observe whether or not subsets of the data exert unusual influence

on the outputs. In particular,

it is

possible to establish if a minority

of the data behave differently from the majority of the data. The concept

—5—

S
of discrepant behavior by a minority of the data is basic to the diagnostic
view elaborated in this paper.
The

in

error

the

least

third approach will be to examine output sensitivity to changes
model.

Instead of using least squares, estimators such as

absolute residuals would be applied which impute less influence

to large residuals. A more promising alternative for diagnostic purposes
is the Huher type error model [1]. Varying a parameter in the Huher

model

provides a. way to examine sensitivity to charges in the error assumptions.

This area is related to recent research in robust statistics[17].
Another aspect of changed error assumptions is specific to time

series. Practicing econometricians are well aware that parameter estimates

change when the sample period is altered. While this might only
reflect expected sampling fluctuations, the possibility exists that the
population parameters are truly variable and should be modeled as a random

process. It is also possible that the population parameters are stable
hut mispecification causes sample estimates to behave as if they were a

random process. In either case explicit estimation methods for random
parameters based on the Kaimnan filter might reveal parameter instability

of interest from a diagnostic point of view.
Thile classical statistical methods in most social

the

sample as a given and

more

then

science

contexts treat

derive tests about model adequacy, we take the

eclec-tric position that diagnostics might reveal weaknesses in the data,

the model or both. Several diagnostic procedures, for example, are designed to
reveal unusual rows or

S

has no formal

outliers in the data matrix

distribution properties.

which by assumption

If a suspect data row has been

located, the investigator faces several choices. One common practice is

—6—

.
to introduce a di..mnny variable, especially when subsequent examination
reveals that an "unusual" situation could have generated that data row.
Alternatively the model

may

be respecified in a more

complex way. Of

the suspicious row might simply be deleted or modified if found
to be in error • In surrnary, the diagnostic approach leaves open the
course

question of whether the model, the data or both should be modified.

In some instances described later on, one might discover a discrepant

row and decide to retain it, while at the same time having acquired

a more complete understanding of the statistical est5ites relative
to the data.
1.3 Modeling
We

Research Aims and

Diagnostics

reiterate here several principal objectives that diagnostics can

serve, from the modeler's perspective, in obtaining a clearer understanding
of regression beyond those obtainable from standard procedures. Some of

these are of recent origin or are relatively neglected and ought to be
more heavily emphasized. The three main modeling goals are detection

of disparate data seents, collinearity, and temporally unstable regression
parameters. It will become clear as this paper proceeds that overlaps
exist among detection procedures,

1.3.1

Leverage and

Disparate Data

The first goal is the detection of data points that have disproportionate
weight,

either because error distributions are

poorly behaved or

because

the explanatory variables have (multivariate) outliers. In either case

regression
on

statistics, coefficients in particular,

may be

heavily dependent

subsets of the data. (This draft is principally concerned with these

—7—

S
aspects of diagnosis; the other topics are of equal importance. At this
stage

of our research we are coming to a better understanding of the

scope of regression diagnostics and
of others in describing these other

we

shall rely heavily on the work

methods.)

1.3.2 Collinearity
While

exact linear dependencies are rare among explanatory variables

apart from incorrect problem formulation, the occurance of near dependencies

arises (all too)

frequently in practice. While some collinearity can be

moderated by appropriate rescaling, in many

instances

ill-conditioning

remains. There are two separate issues, diagnosis and

our

to

main purpose is

treatment.

Since,

diagnosis, we are not presently concerned with what

do about it, except to note that the

more

collinear the data, the

more prior information needs to be incorporated.

Collinearity diagnosis is experimental too, but the most satisfactory
treabnent

we

know of has

been

proposed by David Beisley [2],

who builds

By exploiting a technique of numerical
value decomposition, it is possible to

on earlier work of Silvey [3].

analysts

called the singular

obtain an

index of ill-conditioning and relate this to a decomposition

of the estimated coefficient variances. This relation enables the
investigator to locate which columns of the explanatory variable matrix,

associated with the index of collinearity, contribute strongly to each
coefficient variance. By thus joining Silvey's decomposition of the
covariance matrix to numerical measures of ill-conditioning, economists
now have an experimental diagncstic tool that

enables

an assessment of which

columns of the data matrix are prime sources of degradation in estimated

coefficient variances.

—8—

1.3.3

S

Variability in Time

Regression Parameter

third major goal is the detection of systematic parameter variation
in time. Many statistical models assume that there exist constant but
A

unobservable

parameters to be estimated. In practice, econometricians

often find this assumption invalid, Suspicions that there are more than
one set of population parameters can be aroused for a large number of

reasons: the occurance of an external shock that might be expected to
modify behavior significantly (a war, hyperinflation, price-wage controls,

etc.) is one possibility. Another is that a poorly specified relation might
exclude important variables which change abruptly. There is always the

possibility that aggregation weights [] may
introduce variability in macro

parameters

change

over time and

even when micro

thereby

parameters

are stahl e.

An argument has been made by Lucas [23] that anticipated changes in government

policy

will cause modifications in underlying behavior. Finally the parameters

may follow a random process and thus be inherently variable. When discrete
changes in parameters are suspected, and the sub-divisions of data where this

occurs is identifiable from outside information, the analysis of covariance in
the form discussed in Gregory Chow [5] or Franklin Fisher [6] is an appropriate
diagnostic that has been

frequently applied. When the break point

of points have

to be estimated, maximum likelihood estimators proposed by Quandt and Goldfeld
.7][8] are available.

An alternative diagnostic

procedure has

recently been suggested by

Brown, Durhin and Fvans [9]. They have designed two test statistics with
a time series orientation.

From a regression formed by cumulatively adding

new observations to an initial
predictions are

generated.

subset

of the data, one-step ahead

Both the associated cumulated recursive lebidual s

—9—

and their stuns of squares have well-behaved distributions on the null
hypothesis of parameter constancy.

1.

Notation
We use the following notation:

Population Pegression

Estimated Regression

YX+r
Y : nxl column

vector

for dependent variable

same

X : nxp matrix of explanatory variables
pxl

same

cohutu vector of regression coefficients

nxl column error vector

Additional notation

row

error

of

r : residual vector

I

same

X matrix

variance

: estte of

I

s2 estinated error variance
estimated with

s

i

row

of

data matrix

Y vector deleted.

Other

notation

is either obvious or will be introduced in a specific

context not so obviously tied to the generic regression model.

and

I

— 10 —

2.

LEVERAGE POINTS AND DISPARATh DATA

2,1 Introduction
At this stage in the development of diagnostic regression procedures,

we

turn

to

analysis of the structure of the X matrix through

rows. In the usual case, the X's are assumed to

perturbation of

be a matrix of fixed numbers

and the matrix to have full column rank. Otherwise, statistical theory
suggests we ought to have little interest in the X matrix, except when
exper:imental design considerations enter. In actual practice, researchers

pay

a great deal of attention to explanatory variables, especially in

investigatory

stages.

initial

Even when data are experimentally generated,

peculiarities in the data can impact subsequent analysis, but when data
are non-experimental, the possibilities for unusual data to influence
estimation is typically greater,

To be more precise, one is often concerned that subsets of the data,
i.e., one or more rows of the X matrix and associated Y's might have a
disproportionate influence on the estimated parameters or predictions.
If, for example, the task at hand is estimating the mean arid standard

deviation

of a univariate distribution,

exploration of the

data will

often

reveal outliers, skewness or multimodal distributions. Any one of

these

might cast suspicion on the data or the appropriateness of the

mean and

standard

deviation as measures of location and variability.

The original model may also be

original

questioned and transformations

of the

data consistent with an alternative model may be suggested, for

instance. In the more

complicated

multiple regression context, it is corrTnon

practice to look at the univariate distribution of each column of X as well
as Y, to see if any oddities (outliers or

its

gaps) strike the eye. Scatter

— 11

diagrams

-

are also examined. While there are clear benefits from sorting

out peculiar observations in this way, diagnostics of this type cannot
detect multivariate discrepant observations. That wea)-iess is what we
hope to remedy.

The benefits from isolating sub-sets of the data that might disproportionately impact the estimated parameters are clear, but the sources of
discrepancy are

diverse.

First, there is the inevitable occurance of

improperly recorded data, either at the source or in transcription to
computer readable form, Second, observational errors are often inherent
in the data. While more appropriate estimation procedures than least squares
ought to be used, the diagnostics we propose below may reveal the unsuspected

existance or severity of observational errors. Third, outlying data points
may contain valuable information that will improve estimation efficiency.
We all seek the "crucial experiment", which may provide indispensible
information and its counterpart can be incorporated in non-experimental

data. Even in this situation, however, it is

extreme

constructive to isolate

points that indicate how much the parameter estimates lean on these

desirable data. Fourth, patterns may emerge from the data that lead to
a reconsideration and alteration of the initial model in lieu of suppressing
or modifying the anomolous data.
Before describing multivaria-te diagnostics, a brief two dimensional

graphic preview will indicate what sort of interesting situations might

be subject to detection. We begin by an examination of Figure 1,

which

portrays the ideal null case of uniformly distributed and, to avoid statistical

connotations, what might be called evenly distributed X. If the variance of

X is small, estimates of will be unreliable, hut in these circumstances

— 12

standard

.

—

test statistics contain the necessary information,

In Figure 2, the point o is anomolous, but since it occurs near the
mean of X, no adverse leverage effects are inflicted on the slope estimate
although the intercept will be affected. The source of this discrepant
observation might be in X, Y or c, If the latter, it could be indicative
of heteroscedas-ticity or thick-tailed error distributions; clearly more

such points are needed to analyze those problems further, but isolating
the single point is constructive.

Figure 3 illustrates an instance of leverage where a gap arises

between the main body of data and the outlier, While it constitutes a
disproportionate amount of weight in the determination of ,

it might

be that benign third source of leverage mentioned above which supplies

crucially useful information. Figure

is a more troublesome configuration

that can arise in practice. In this situation the estted regression
slope is almost wholly determined by the extreme point. In its absence,
the slope might be almost any-thing. Unless the extreme point is a crucial
and valid piece of evidence (which of course depends on the research
context), the researcher is likely to be highly suspicious of the estimate.
Given the gap and configuration of the main body of data, the estiiite
surely has less than n-2 degrees of freedom; in fact it might appear that
there are effectively two data points altogether, not n.

Finally, the leverage displayed in Figure 5 is a potential source of
concern since o and/or • will heavily influence

but differently than the

remaining data. Here is a case where deletion of data, perhaps less
drastic downweighting, or model rformu1 ation is clearly

indicated.

— 13

—

Plots for Alternative Configurations of 1)ta

yi

0

x.1

x

Figure 1

x1

Figure 2

0

y-.

0

yi

/

x.1

Figure 3

Figure 4

0

y.

x.1
Figure

5

x.1

— lL

2.2

.

—

Residual Diaiostics

Traditionally the examination of functions of the residuals,
y. -

y., and especially

large residuals, has

indications of suspect data that
results.

been

used to provide

in turn may unduly affect

regression

It is best to have a scalar covariance matrix, so that

detection of heteroscedasticity or autocorrelation (and later on, eliminating
them) is desirable.
Approximate normality is another desirable property in terms of estimation

efficiency and the ability to test hypotheses. Harmful departures from nonility
include pronounced skewness, multiple modes arid thick-tailed error distributions.
Even moderate departures from normality can noticeably impair estimation

efficiency. At the same time, large outliers in error space will often be
associated with modest—sized residuals in least squares estimates since the
squared error criterion heavily weights extreme values.

It will often be difficult in practice to distinguish between
heteroscedasticity and thick—tailed error distributions; to observe the
former, a number of dependent variable values must be associated with

(at least) several given configurations of explanatory variables. Otherwise,
a few large residual outliers could have been generated by a thick-tailed

error distribution or franents from a heteroscedastic distribution.
Relevant diagnostics have three aspects, two of which examine the

residuals and the third involving a change in error distribution assumptions.

The first is simply a frequency distribution of the residuals. If there
is evident visual skewness, multiple rrdes or a heavy tailed distribution,
the graph will prove infonmative. It is interesting to note that econaiiists
often look at time plots of residuals but seldczn at their frequency distrikution.

-15-

The second is the normal probability plot, which displays the cumulative normal distribution as a straight line whose slope measures the standard

deviation

arid whose intercept reflects the mean. Thus departures from

normality of the cumulative residual plot will show up in noticeable departures

straight line. Outliers will appear inunediately at either end of
cumulative distribution.
from a

Finally, Denby and

[17] and

Mallows

the estimated coefficients and

Welsch

the

[18] have suggested plotting

as the error

or, equivalently,
as the loss function (negative logarithm of the density) is changed. One

family of loss functions

has

residuals

been suggested by Huber [1];

ItIc

It?
S

p(t)

density

_c2

ItI>c

which goes from least-squares (cx) to least absolute residuals (c0). This
approach is attractive because of its relation to robust estimation [1], but
requires considerable computation.

For diagnostic use the residuals can be modified in ways that will

enhance our ability to detect problem data. We first

note that the

do not have equal variances because if we let H X(XTX)1XT, then
E[(YY)(YY)T] E[(IH)YYT(I_H)T]

(I-H) E(YYT)(I_H)

since (I-H)2

I—H and (I-H)X

a2(I-H)

0. (See Theil [10] and Hoaglin and Welsch [13]

for a more detailed discussion.) Thus

var

where

h1 is the

(r.)

2

(ib)

diagonal element of H.

(2.2.1)

-16-

Consequently a number of authors 111] have suggested that instead

of

studying r., we should use the standardized residuals

r./s
where S2

is

(2.2.2)

the estimated error variance.

and ask

For diagnostic purposes we might want to go further

the size of the residual corresponding to y when data point i has
been omitted from the fit, since this corresponds to a simple
about

pertutation of the data. That is, we base the fit on the remaining
n—i data points and
then predict the value for y1. This residual is
yl -

and

x

has been studied in a different context by Allen [12].

Similarly

s2.
i" fit, and the
(1)is the estimated error variance for the "not
_______________________
standard deviation of

is estimated by 5(i)/i + x1(X1)X(1))x

We now define the studentized residual:

r
-

(i)

I+X

-

i (1) (1)

(2.2.4)

.

-l
i

Since the numerator and denominator in (2.2.4) are independent,
r1

has a

we can
(Of

t

distribution

readily

course,

with n-p--l degreees of freedom. Thus

assess the significance of any single

r

arid

r will not

studentized

residual.

be independent.) Perhaps even more

useful for our purposes is the fact that

r1/(s(1)v'1_h1)

(2.2.5)

.

-17—

and
2
(fl—P—l)S()

These results are proved

2

(n—p)s —

easily

by

using the matrix

Tberefore we think that a good

to

1

226

I—F;:

identities

in Appendix 1.

way to examine residuals is

look at the studentized residuals, both because they have equal

variances and because they are

easily

related to the t-distribution.

However this does not tell the whole story, since some of the most
influential data points can have relatively small studentized residuals
(and very small r1).

To illustrate with the simplest case, regression through

the origin, we

have

r.1

- (i)
where

1

x.1

(2.2.7)

xr/Z,1 ?

(2.2.8)

x.

J

(1) denotes an estimate obtained by removing the

(data point)

from

the computation. Thus the residuals are

row
related

to the

change in the least-square estimate caused by deleting one row. But each contains
different information since large values of J can be associated

with small fr.

arid vice

deletion as an important

versa. Therefore we are lead to consider row

diagnostic

tool, to be treated on at least an

equal footing with the analysis of residuals.
For

multivariate linear regression (2.2.8) becomes
-

(i)

(XTX)_l xTr./(l_h.)

(2.2.9)

— 18

—

S
where

the h. are the diagonal elements of H, the least-squares

proj ect ion

matrix defined

HY

Clearly

earlier. We will call this the "hat"

matrix

1
Y

since

(2.2.10)

the hat matrix plays a crucial role not only in the studentized

residuals but also in row deletion and other diagnostic tools. We now develop some
inportant resUlts (based on the discussion in Hoaglin and Welsch [13]) relating to
this

matrix.

2.3 The Hat Matrix
Geometrically

Y is the projection of Y onto the p-dimensional

subspace of n-space spanned by the columns of X. The element h.. of H
1]

has

a direct interpretation as the amount

of

leverage or influence exerted

by y. Thus a look at the hat matrix

X space, points at which the value of y has a large impact

in the
on

can reveal sensitive points

the fit.
The influence of the response value y on the fit is most directly

reflected in its leverage on the corresponding fitted value

y, and

this is precisely the infonnation contained in h1, the corresponding

diagonal

element of the hat matrix. When there are two or fewer explaiiatory

variables scatter plots will quickly reveal any x-outliers, arid it

is

not hard to verify that they have relatively large h values. When
p >

2,

scatter plots may not reveal "multivariate outliers," which are

separated in p-space from the hulk of the x—points but do not appear as
outliers in a plot of any single explanatory variable or pair of them

-

— 19

yet will be revealed by an examination of H . Looking at the diagonal

elements of H is not absolutely conclusive but provides a basic starting
point. Even if there were no hidden multivariate outliers, computing
and examining

looking

H (especially the h) is usually less trouble than

at all possible scatter plots.

a projection matrix, H is syiiunetric and idempotent (H2

As

H).

Thus we can write

'

2
E h..

h..
and it

is clear

understanding

that 0

h.ii +

h?.

(2.3.1)

h..
11 1. These limits are

and iterpreting

hj(Eh1),

useful

but they do riot yet tell us

when h1 is "large". It is easy to show, however, that
of a proj ection matrix are

either

elgenvalues is equal to the rank

rank (X)

0 or 1

of

and that the

the matrix.

p and hence trace H p, that

in

In

the

ninber

eigenvalues

of

non-zero

this case rank (H)

is,

n

E h.

(2.3.2)

1=1

The average size of a diagonal element, then, is p/n. If we were designing
an

experiment a desirable goal would be to have all the data points be about

equally

to us
and

S

influential or all h1 nearly

and we cannot

say that

leverage

h1 is

equal. Since the X data is given

design our experiment to

a leverage point

points can he

both

harmful

if h1

>

keep the h1

2pm.

and helpful.

equal,

we will follow [13]

We shall see later that

.

—20—

The quantity 2pm

worked well in practice and

has

there

is some

theoretical justification for its use. When the explanatory variables are
multivariate Gaussian it is possible to canpute the exact distribution of

certain

functions of the h. Let )( denote the nx(p_l) matrix obtained by

centering the explanatory variables.

Now

Y-Hy_Vy
and

thus the diagonal elements of the centered hat matrix are
h. —

Let

(2.3.3)

X(i)

.

th row

denote X with the i

removed and X(.) denote the centered

i.e. means based on
out. Finally note that

version of X

(1)

subtracted

(2.3.1-i)

,

—

x—x

n-l
—h-—

all but the 1th observation have been

—
(xj—x(i))

(2.3.5)

(x(.)—x1).

(2.3.6)

and
—

Using (Al.l ) and (2.3.5)

h.1 l+y
where y
Again

n-i2
(—)
(x.—x(.))(X(.)
—

-

--1

X())

using (Al.l ) and (2.3.6)
-

(fll)
n

cY.

1÷(n—l)
2
n

T
(x1—x(.))

—21-

where

-l

a

(x1-x(1)) (X(.)

X(.))

The distribution of (n-2)a is well known

—

T

(X..-X(.))

since

it is the Mahalanobis

distance between observation i and the mean of the remaining observations
[19,

p.

'480].

Thus
n (p-i)

F

(n-U(n-p)

p—l,n-p

.

(2.3.7)

Reversing the above algebraic manipulations we obtain
1

n

a

L-

and

hi-(1)

(n—i)cx + 1

Solving for a gives

n (h.-1/n

anl \. i-h.
and

from (2.3.7)

h.-l/n
1
n

1-h1

a EL
n-p

For nDderate p and moderate n the

a

cut-off point

F

p-i,n-p

95% point

(2.3.8)

for F is near 2. Therefore,

would be

h1

>

2(p—l)+ n E
n+p-2

which is approximated by 2pm.

(2.3.9)

.

—22—

From

h..

we have

as

equation

(2.3.1) we can see that

i. These two extreme cases can be

0 for all j

be

follows. If h 0, then y. must

by y or by any

other

whenever h 0 or 1,

A point

with

fixed at zero
x.

-

interpreted

it is not affected

0 when the model is a

line through the origin provides a simple example.
- the model always fits this
When
1, we have ;
h1

straight

y1

data

value exactly. This is equivalent to saying that, in some coordinate
system,

to

one parameter is determined completely

by y or, in effect, dedicated

one data point. The following theorems are proved in appendix 3.

If h1 1, there exists a nonsingular transformation, T
such that the least-squares estimates of a
have the following

Theorem:

T

properties: a1 y and

Theorem:

{a}2

do not depend on y1.

If X is nonsingular,

X . )
det(XT.
Ci) (i)

then

i

(1-h.) det(XTX)

(2.3.10)

.

Clearly when h 1 the new matrix X(1) formed by deleting a row is

and we cannot obtain the usual least-squares estimates. This is
leverage and does not often occur in

singular

extreme

practice.

.

—23—

discussion

To complete our

of the hat

matrix we

give a few simple

examples. For the sample mean all elements of H are

1/n

.

Here

and each h = p/n, the perfectly balanced case.
For a straight line through the origin

p = 1

h..

x.x.I

E

(2.3.11)

n

and clearly E h1 p 1.
i=l

Simple

linear regression is slightly more complicated but a few

steps of algebra give

(x.

+

=

- )(x. (2.3.12)

E (Xk 1

k
n
and

h.

2. We can see from (2.3.12) how x-values far fran

x

will

i=l

lead

to large values of h. It is this idea

that we attempt to capture by looking

in the

multivariate

case

at elements of the hat matrix.

2.' Row Deletion Diagnostics

We now

return to the basic formula

Since the variability

of

(XX) x

T

is measured by s((X X))

ofchangeis
DFBFrAS..

(2..l)

r1/(l-h1).

,

a more useful measure

ci) )
=

/ -1

s.vxTx)
(i)

jj

(2..2)

where

we have replaced s by

S() in order to make the denominator stochastically

independent of the numerator in the Gaussian case. To provide a

.

surrmary of the relative coefficient changes we suggest

jl DFBrAS .

NDFBTS1 \/
The term

has been incorporated to make NDFBAS more comparable across

data sets which may

value

(2..3)

have

was chosen because

different values of p and n. This normalizing
when X is an orthogonal matrix (but not necessarily

orthonormal)
x..

DFBETAS.. z

1-j

r.1

tl x2. /1-h.

and
h.

DFBFTAS.
ij

Since the average value of h1

1
1-h1

.

r.1

p/n, a rough average value for h1/(l-h1)

is p/(n-p). Clearly (2.L.3) could be modified to reflect the fact that
some coefficients may be more important than others to the model builder
(e.g., including only the main

estimates

of interest).

Another obvious row deletion diagnostic is the change in fit

DFFIT.

If we

x. (- (.))

=

1-h1

r1

•

(2

.

.)

scale this by dividing by s(1)Awe have

_

1h1

______
Il_hi

r.1

(2.4.5)

.

—25—

For across data set nonializatjon

h.)
/('2E)(
p 1-h1

1

A measure similar

to

this has

Clearly DFFITS arid

we will multiply by v'P/p to obtain

been

2

(2.4.6)

suggested by Cook [14].

in an orthogonal coordinate system.
When orthogonali-ty does not hold these two measures provide somewhat different
NDFBETAS agree

information. Since we tend to emphasize coefficients,our preference is for
NDFBETAS.

Deciding

when a difference like J

(-

-

J or

other diagnostic

statistic is large will depend, in part, on how this information is being

used. For example, large changes in coefficients that are not of particular
interest might not overly upset the model builder while a change in an
important coefficient may cause considerable concern even though the change

small

is

relative

to traditional estimation error.

We have used two

approaches

to measure the size of changes caused by

row deletion.

The first, called external comparison, generally uses measures
associated with the quantity whose changes are being studied. For example,

the standard error of a particular coefficient

(-

would be used with

The

second method, called internal comparison, treats each set of
diagnostic values (e.g.,
as a single data series
and

then finds, for example, the standard deviation of this series as

a measure of relative size. As we have noted, all of

we have discussed so far

of

are

the diagnostic measures

functions of r./v'l-h.
1
1arid

studentized residuals, it is natural to divide this

reasonable scaling before making plots,

etc.

in

view of our

by 5(i)

discussion

to achieve

a

—26—

.
Once 5(.) has been used, the temptation arises to try to perform formal
*
statistical tests because we 1iow the distribution of r.

In our opinion

this is not a very promising procedure because it puts too much emphasis
on residuals (although looking at studentized residuals is better than

using the raw residuals). We prefer to use external or internal
comparison

to make

decisions about which

data points

deserve further

specifically at the
studentized residuals as we did earlier. Using any Gaussian distributional

attention

theory

except, of course, when we

are looking

depends on the appropriateness of the Gaussian error distribution -

d tcJ:o

we will return to

atei

2. 5 ReessionStatiics

have

Most

users of statistics realize that

some

measure of variability

estiirtes

associated

with them.

often realized that regression statistics like t,
also

he

like
It

should

is less

and

F

should

thought of as having a variability associated with them.

One way to assess this variability is to examine the effects of row

deletion on these regression statistics. We have focused on three:

ATSTAT.

1

/I

-

s.e.() s.e.((.)).

AFSTAT

F(afl

0)

-

F(.)(ali

0)

•

—27—

Again W(:

should

ask when a difference is large enough

to merit

attention.

For ex-ten-ial comparison we would compare to the standard deviation of

t, F, or R2:
Statistic
t

F

Standard Deviation

n-p
n-p-2

1/2

(2(fl_p)2fl__\1/2
p(n_p_2)(n_p_L)J

R2

—

(p—i)

2

1/2

(p+n-2)2(p+n-1)

However, we tend to view internal comparison as more appropriate for
regression statistics.

Studying the changes in regression statistics is a good second order
diagnostic tool because if a row appears to he overly influential on
other grounds, an

if

examination of the regression statistics will show

the conclusions of hypothesis testing would be affected.

There is, of course, room for misuse of this procedure. Data points
could he removed solely on the basis of their ability (when removed) to

increase F2 or some other measure. While this danger exists we feel
that it is often offset by the ability to study changes in regression statistics

caused by row deletion. Again we want to emphasize that changes in
regression statistics should not be used as aprinary diagnostic tool.

—28—

S
2.6

Influence and Variance Decomposition

We now would like
way.

to consider perturbing

our assumptions

in a new

standard regression model (1.4) but with var(c)

Consider the

replaced by a2/w1 for just the i data point. In words, we are
perturhing the homoscedasticity assumption for this one data point.
In appendix 2 we show that

___
w.
1

TX)
-iT
x.r.

11
(l-(l-w.)h.)
CX

and it follows that

W.

T
1T
CX X) x.r

(2.6.2)

1

ii

T

w.1

Equation

X) x.r.

(X

____

w.0
1

(1—h.)

1

2

Ci)
1-h.

(2.6.3)

1

(2.6.2) tells us about infinitesimal changes in caused by small

changes in w1 about the value 1 and similarly for (2.6.3). From the mean value
theorem we know that

(2.6.4)

where 5 is between 0 and 1. Any one of (2.6.2), (2.6.3) or (2.6.4) can be used

for diagnostic purposes. We have chosen to emphasize

because of its

intuitive appeal and the fact that it is a compromise between (2.6.2) and (2.6.3).

—29—

Formula (2.6.2) can

influence

also

be considered as a function which represents the

of the 1th data point and can be linked to the

theory of robust

estimation [15] and the jackknife [16].

If we let
(2.6.5)

s2

and
1

(2.6.6)

w
.

then

•l

in appendix 2 we show that

.2__

r sL
I

w.1 L w1

r.2

—

Since

m 11

(XWX)

T1

(X X)

s

—

w.-1
1
1

2

Cx x)

T
T
x.x1(X X)

.

(2.6.7)

we would like to remove scale we define

r
DBVARS1

-

[xTx' 4x1(xTx)_h]jj

2

(2.6.8)

CX X)•

(n-p)s

JJ

as the scaled infinitesimal change in the variance of j. As a suniiiaxy measure
over

all of the coefficients we use
p

pjl

NDBVARS.
a •z
1

BVARS.. .

1]

(2.6.9)

where the n/p term is used to improve comparability across data sets.

—30--

If we used row deletion instead of derivatives, our basic measure
would

.

be

2 T' 2

-l

T x )..
. CX
s (X X).. — s (i)
(i) Ci)
jj

DFBVARS..

.

.

(2.6.10)

2 (XTX)T
JJ

with suuniary measure

ri p,

(

NDFBVARS.

1

The. measures
variables and

of

part

the

so far

discussed

jl
E

I

DFBVARS..

(2.6.11)

in this section include both the explanatory

response. If we wish to examine the X-matrix only, the second,

(2.6.8) provides a good way to do this. We notice that

r

T

-l

T

T -1

E (X X) x.x.(X x)
1

(x X)

j
and

.

13

define

-l

r
BETAVRD..

-i1

xx. (XTX)

L xTx)

11

J 33

(XTX).

13

suinaxy measure

with

NBErAVRD.

1

=

p
E

BETAVPD..

13

These measures provide a way to decompose the cross products matrix with respect

to the individual observations.

Again it is useful to look at the
holds

orthogonal X case. When

orthogonality

—31—

2
x..
11 —

BETAVRD..

1]

jj

arid

h.1

NBFTAVRD.

1

Since h has a strong intuitive appeal it may be a better suJrniry value even
when orthogonality does not hold. We have chosen not to multiply NBFTAVRD
by n/p (the average value for hi), so it is not useful across data sets.
If we examine the formula for DFBVARS we see that

this

quality could

be positive or negative. As we might expect, in some cases downweighting a
da.ta point can

weighting
the

inprove

our

estimate

of the variance of a coefficient. (Down—

corresponds to placing a minus sign in

front of DFBVARS.) One of

best ways to examine the tradeoffs of DFBETAS arid DFBVARS (or

BFTAVRD)

is to make a scatter plot. A high leverage point with small values of DFBETAS
may be a "good" observation because it is helping to reduce the variance of
certain coefficients. The setting aside of all high leverage points is
generally not an efficient procedure because it fails to take account of the
response data.

2.7 More Than One Row at a Tfrie
It

we

is natural to ask

if there might be groups of leverage points

are failing to diagnose because we are only looking at one

There

are easily

constructed examples where

this

at a time.

can happen.

One approach is to proceed sequentially - remove
point

row

that

the

"worst" leverage

(based perhaps on both NDFBLTAS and NBETAVRD), reexamine the diagnostic

measures arid remove the

next

"worst" observation, etc.

This does not fully

—32—

cope with

the

regression

problem of groups of leverage points

and just

as stepwise

can be troublesome, so can sequential row deletion.

A straightforward induction argument shows that
6..

ij

-

h. .(k

ij 1

,k ,.
2

. .k

t

)

det (I-H).

1, k

det

l'2'"t

(I-H)k k

1

where H is the hat matrix for all of the data, h. (k1,. . .

,kt)

denotes the

hat matrix for a regression with rows k1,.. .kt removed and the subscripts on I-H

denote a suiatrix formed by taking those rows and columns of I-H.
Even though all of these differences are based on H, multiple row deletion

will involve large amounts of computation. It is instructive to note that

1 h (k) --

(l-h1)(l-h,)

lhk
(1-h1)

(1—h1)

-

h

l -(l_(l)]
[1 —cor (rj,rk)]

The term cor (rI,rk) also appears when more rows are deleted and, in place of
looking at all possible subsets of rows, an examination of the correlation matrix
of the residuals for large correlations has provided useful clues to groups of
leverage points. This requires )iowing the off-diagonal values of H and -therefore
increases computational cost and perhaps storage requirements.

—33—

2.8 Interface with Robust and
It is natural to ask how

Ridge

the

Regression

above diagnostics could or should be

used with some of the newer estimation methods like robust arid

The

ridge regression.

first question is whether we should do diagnostics or robust

first.

or ridge

There is no clear answer, but some sort of iterative procedure is

probably called for.

However, it is possible to perform regression diagnostics after using
either a robust procedure or a ridge procedure. In the robust case we can
make use

of weights

(2.8.1)

p'

where P is the robust

loss

function, R are the robust estimates of and

SR is a robust estimate of the scale of the residuals, y1-x1

(A canpiete

discussion of weights is contained in [20].) We now imdify the data by forming
a diagonal matrix of weights, W, and using AiY,

Aix. This

revised data is

then the input to regression diagnostics. If the robust estimation procedure

has been

allowed to converge
T
(XWX)

A

will

to

be close to

XWY

and our procedures will accurately reflect what would happen

locally. Of course they do not reflect what would happen if a data point

were deleted and then robust estimation applied.
The ridge estimator [21] is given by

RD
There are many

(XTXtkI)

generalizations but

work. We assume that k has been

xTy

•

(2.8.2)

most will fit into the following frame-

chosen by sane means such as

those listed

—34—

in

[21]. Then we form

rxlI, Y

x

I

L
where

A

Iv•i

A

P<PJ

:j
LP

is a pxl vector of zeros (prior values tines v

So we now have "new" data XA and

A with

nxp rows. Clearly

(2.8.3)

We now perform regression diagnostics using XA and

row

with

index n+j >

n,

A

When we delete a

it is equivalent to saying we do not want

to "shrink"

zero (or its prior). In the Bayesian context
such a row is like setting the prior precision of to zero.

that parameter estimate
dropping

more general cases).

T
XA A •

(T
XA XA

-

in

toward

Plots of DFBETAS would then show the effects of such a process by looking

at those DFBFTAS values for index greater than n.

We can do sane diagnostics to decide if a ridge estimator is warTanted.

If we differentiate (2.8.2) with respect to k, then
3RD
3k

and

RD
3k

-1

T

(2.8.4)

(X X +kI)
T

-1

(2.8.5)

(X X)
o

Thus (2.8.5) provides infoniation about infinitesimal charges

If

xTx

were diagonal then

elgenvalues.

the
of

So

large

(2.8.5) has

about kO.

ccmponents j/X where A. are the

and/or A small would lead to a large value of

derivative. Since the ridge estimator depends heavily on the scaling

the

explanatory

using this

variables,

diagnostic measure.

so does (2.8. Li.) and

we recorimend scaling before

—35—

When diagnostics have been canpleted a few observations ny be suspect.

The rows can then be set aside and a new robust or ridge estimate caiiputed.

Diagnostics can -then be applied again. There are obvious limits of time and

money but we think that to passes through this process will often be r'thwhile.

—36—

2.9 An Example: An Inter-Country Life Cycle Savings Function

Arlie Sterling of NIT has made available to us. data he has

collected on fifty countries in order to undertake a cross-sectional

study of the life cycle saving hypothesis. The savings ratio
(aggregate personal saving divided by disposal incane) is explained
by per

capita disposable income, the percentage rate of change in per

capita disposable income and

that

two population

variables: per cent

less

15 years old and per cent over 75 years old. The data are averaged

over the decade 1960-1970 to remove the business cycle or other short-term
fluctuations.

According to the life cycle hypothesis, savings rates should be
negatively affected if non-members of the labor force constitute a large

part of the population. Income is not expected to be important since
age distribution and the rate of income growth constitute the core of

life cycle savings behavior. The regression equation and variable
definitions are then:

SR1

cOEF.1 +

+

COEP.2*POP151

COEF.5INGRO1

+

COEF.3*P0P751

+ COEF.4* INC.

(2.9.1)

.

—37—

the average aggregate personal savings rate in
country i from 1960-1970

SR1

POP15.

P0P751

=

the average % of the population under 15 years
of age from 1960-1970

=

the average % of the population over 75

of age

years

from 1960-1970

the average level of real per capita disposable
income in country I from 1960-1970 measured in

INC1

U.S. dollars

the average % growth rate

INCROi

of INC.

1960—1970.

from

A full list of countries, together with their numerical designation,
appears in Exhibit 1, and the data are in Exhibit 2. It is evident that
a wide geographic area and span of economic developnent

also

plausible to suppose that the quality

highly

variable. With these obvious

are

shown in Exhibit 3.

low

for

of

are included. It is

the underlying

data

is

caveats, the LS estimates of (2.9.1)

To coliment briefly, the R2 is not uncharacteristically

cross-sections, the population variables have correct negative signs -

COEF 3 has

a

small t statistic but COEF 2 does not - income

is

statistically

reflected in COEF 5 is significant
5 per cent level arid has a positive influence on the savings rate

insignificant, while income growth
the

as it should. Broadly speaking, these results are
life

at

consistent with the

cycle hypothesis.

The remainder of this section will be a guided tour through some
of the diagnostics

using

discussed

previously. The computations were performed

SENSSYS (acronym for sensitivity system), a ThOLL experimental

for regression diagnostics. Orthogonal
least-squares

decompositions are used

subsystem

in the

regression computations and this makes it possible to

get all

the diagnostic measures in addition to the usual LS results in less than
twice the computer time for the LS results alone.
of

—38—

David Jones and Steve Peters of the NBER Computer Research Center

Both have actively participated in analytical and

programmed SENSSYS.
empirical

have

aspects of the research.

Only a

selection of plots and diagnostics

will be shown for two

reasons.

One is that to provide the full battery of plots would be excessively tedious;

obtainable. The other

however, the missing plots and tables are readily

reason

is that we found these diagnostics to be among the ircre instructive

from examination

2.9.1

of this and

Residuals

The first plot, Exhibit

a

several other problems.

'+,

is

a normal probability plot.

Departure from

fitted line (which represents a particular Gaussian distribution with mean

equal to the intercept and standard deviation equal to the slope) is not substantial in the main body of the data for these studentized residuals, but

Zambia (46) is an extreme residual which departs fran the line. Different
information, an index plot of the r1, appears in Exhibit 5 which reveals not
only Zambia, but possibly Chile (7) as well to be an outlier; each exceeds
2.5 tines the standard error.

2.9.2

Leverage

Exhibit 6

and Diagonal

Hat

Matrix Entries

plots the h which, as diagnonals

of leverage points.

Most

of the h are

of

the hat matrix, are indicative

small, but

two

stand out

sharply:

Libya

(49) and the United States (44). Two others, Japan (23) arid Ireland (21) exceed
the 2pm

.20 criterion (which happens to be equal to the 95% significance level

based on the F distribution), but just barely. Deciding whether or not leverage
is potentially detrinental

analysis, although it

depends on what

happens elsewhere in the diagnostic

should be recalled that it is values near unity

the most severe problems, which has

not

happened

here.

that pose

—39—

2.9.3 Coefficient

An overview

based on

is

(2

of

Li..

3)

Perturbation

the effects

of individual row deletion

(see Exhibit

7)

NDFBETAS, the square root of the scaled sum of the squared

differences between the full data set and row deleted coefficients. The measure

used is scaled approximately as the t distribution so that values greater than 2

are a potential source of concern. T countries that also showed up as possible
high leverage candidates, Libya (49) and Japan (23), also seem to have a heavy
influence

(21), a marginal high leverage candidate,
influencing coefficient behavior. Individual plots

on the coefficients while Ireland

is also a marginal
of DFBEI'AS

candidate

for

(2 . L1. 2) follow next, fran which the following table has been constncted

8-11.

based on an examination of Exhibits

Noticeably Large Effects on

Population <15

Population >75

Japan

Ireland

(23)

Japan
The countries that

not

stand

from Row

Income

(21)

Incne

Growth

Libya (49)

(23)

out in

Deletion

Japan (23)

the

individual coefficients are perhaps,

surprisingly, the two that appeared in the overall measure. Ireland, in

addition, appears once. Except on the incane variable, the comparatively large
values are just about one LS standard error for each particular coefficient.

2.9.4 Variation in Coefficient Standard E'rors
Exhibit 12 is a surruliary measure of coefficient standard error variations

as

a consequence of row deletions, designated as NDFBVARS in (2.6.9). Since

these standard errors involve both error variance and elements from (XTX) 1,

0—

large values indicate simultaneous or individual extremes in residuals or

multivariate outliers in the X matrix. These quite numerous candidates
include:

Index
7

Country
Chile

21

Ireland

23

Japan

37

Southern Rhodesia

United

States

Zambia

Libya

L9

Of

these

candidate

while Chile

Japan

had

noticeable

coefficient changes,

and Zambia possess large residuals. Thus this particular

diagnostic may

errors

previously, while the only new

is Southern Rhodesia. Libya had both high leverage and large

coefficient changes, Ireland and

Plots

appeared

seven countries, six

have

some use as a comprehensive measure.

for percent changes in the individual coefficient standard

are shown in Exhibits 13-16. Large individual changes (here taken to

be in excess of 25%) appear for the United States with a 7% change for
the income variable, while the deletion of Libya increases the standard error

for the same variable by nearly 85%.

2.9.5 Change in Fit
The standardized change in fit, DFFITS

(2..6), with a row deleted,

while sjmi1ar in algebraic structure to coefficient change, conveys somewhat

different infonition of general interest with specific applications in a tine
series context. DFFITS can be viewed in some theoretical cases as having a

t distribution so that extremes of concern show up for values in excess of 2.
In Exhibit 17 three countries that surfaced previously reappear:

Japan (23), Zambia (6) and Libya (149). When coefficient changes

S

—41—

alone are considered as shown in Exhibit 7, Zambia did not appear, while

Ireland

(21)

different

information is contained in each.

A Provisional Sunmary

2.9.6

It

did. Thus somewhat

is now desinable to bring together the information that has

assembled

thus far,

to

see what

it

all adds up to. One useful

been

sunnai-y plot

is shown in Exhibit 18, which plots the sunuiary measure of NDFBETAS
against the corresponding hat matrix diagonal,
h.
The first point which emerges is that Japan (23) and Libya (49) have
both high leverage and a significant influence on the estimated parameters.
This is reason enough to view them as serious problems. (After
the analysis
had reached this point, we
were informed by Arlie Sterling that a data error
had been discovered for Japan.
data

is more similar

thus "proven

their

to

When corrected, he

the majority of countries.

worth" in bad

Ireland is an in-between

data detection

tells us that
These

the revised

diagnostics have

in a modest way. Second,

with moderately large leverage and a somewhat
disproportionate impact on the coefficient estimates.

Third, the United

case,

States

has

high leverage

combined with only meager

differential effect on the estijmted coefficients. Thus leverage in this
instance can be viewed as neutral or beneficial. It is important to note
that not all leverage points cause large changes in .
Exhibit

19

plots the suninary

the studen-tized residuals arid

of coefficient change, NDFBETAS
against

drives home the point that large
residuals do not necessarily coincide with large changes in coefficients; all of
visually

the large changes in coefficients are associated with standardized residuals
less than 2. Thus residual analysis alone is not a sufficient diagnostic tool.

Another summary plot, that of change in coefficient standard error,
NDFBVARS against leverage as measured by h in Exhibit 20 indicates the

close anticipated association between leverage and estimated parameter
variability. This is clearly shown by the diagonal line composed of (21) Ireland,
(23) Japan, (1-i4) United States and (9)
a

Libya. But residuals also can

have

large arid separate influence, as evidenced by the low leverage, high

standard error changes for (7)

H

Chile and (46) Zambia.

A final summary plot, Exhibit 21 of NDFBFI'AS against NDFBVARS, is revealing

in that all of the points noted outside the cutoff points
spotted

(3,2)

have been

in the previous diagnostics as worth another look for one reason or

another. Thus about 15% of the observations have been flagged, not an
excessive fraction for many data sets.

2.9.7 One Further Step
Since Libya (L9)

is

clearly an extreme

and probably

deleterious influence

on the original regression, a reasonable next step is to eliminate it to find
out whether its presence has

the h when Libya (9) has

masked other problems or not. Exhibit 22 plots

been

excluded in the data set. There is only one

noticeable difference since Ireland (21), Japan (23) and the United States (t4)

remain

high leverage points. Southeri Rhodesia (37) now appears as a

marginally significant leverage point, whereas it had previously been just
below the cutoff. The only really new fact is that Jamaica ('+7) now appears
as a prominent leverage point.

Jamaica

has furthermore now become a source of parameter influence which

is perhaps most effectively observed in the recalculation of scaled parameter
changes, NDFBETAS, in Exhibit 23 which reveals Jamaica as the single largest
source

of overall coefficient variation.

_L1.3_

This illustrates the proposition that perverse extreme points can mask the

impact of

most of

contain

The

still other perverse points. Yet the original analysis did
the pertinent

correlation matrix of

information

about exceptional data behavior.

the residuals discussed in Section 2.7 provided

clue, since the squared correlation between (47) and (49) was .173,
the hightest value. It is nevertheless a prudent step to reanalyze the
a

with

data

suspect points removed, to ascertain whether one or more extreme or

suspect data points have obscured or dominated others.
2.10

Final Comments

The question naturally

arises as

to whether the approach we have taken

in detection of outliers is more effective than simply examining each
individual column of the data to

look for detached observations. We believe

the answer is yes. Detached outliers
X matrix

for

did appear in

Libya (49) and Jamaica (47), but

not

column 5 (INGRO) of the

elsewhere. Libya, of

course, was "the villain of the piece" in the prior analysis. But leverage
points for minerous other countries were revealed by row deletion diagnostics,

while Jamaica, as matters turned out, was not a particularly troublesome data

point. In addition we discussed how various leverage points affected our
output -

coefficients,

fit, or both. So we conclude at this early stage of

investigation, that these new procedures have merit in uncovering discrepant
data that is not possible with a high degree of confidence by just looking at
our

the raw data.

References

[1]

Huber, P.J., "Robust Regression: Asyrnptotics, Conjectures and Monte Carlo,"
Annals of Statistics, 1 (1973), pp. 799—821.

A.,

"Multicoflinearity: Diagnosing its Presence and Assessing
the Potential Damage it Causes Least-Squares Estimation," Working Paper 154,
National Bureau of Economic Research, Computer Research Center for Economics
and Management Science, October 1976, Cambridge, Mass.

[2] Beisley, David

[3]

Silvey,

Imprecise Estimation," Journal
Statistical Society, Series B, Vol. 31, 1969, pp. 539—552.

S.D., "Multicollinearity and

Royal

of the

[4] Theil, H., Linear Aggregation of Economic Relations, North Holland, Amsterdam,
1954.

[5] Chow, Gregory C., "Tests of Equality Between Sets of Coefficients in Two Linear
Regressions," Econometrica, Vol. 28, 1960, pp. 591—605.
[6] Fisher, Franklin M., "Tests of Equality Between Sets of Coefficients in Two
Linear Regressions: An Expository Note," Econometrica, Vol. 38, 1970,
pp. 361—366.

[7] Goldfeld,

Stephen M. and

Richard

E. Quandt, "The Estination of Structural

Shifts by Switching Regression," Annals of Economic and Social Measurement,

2, No. 4, 1973, pp. 475—485.
[8] Quandt, Richard E., "A New Approach to Estimating Switching Regressions,"
Journal of the American Statistical Association, Vol. 67, 1972, pp. 306-310.
Vol.

[9]

R.L., J. Durbin and J.M. Evans, "Techniques for Testing the Constancy
Regression Relationships," Journal of the Royal Statistical Society,
Series B, Vol. 37, No. 2, 1975, pp. 149—163.

Brown,

of

[10] Theil, H., Principles of Econometrics, John Wiley and Sons, New York, 1971,
pp. 193—195.

[11] Anscombe, F.J. and Tukey, J.W., "The Examination and Analysis of Residuals,"
Technometrics, 5 (1963), pp. 141—160.

[12] Allen, David M., "The Relationship Between Variable Selection and Data
Augmentation and a Method for Prediction," Technanetrics, 16 (1974),

125—127.

[13]

Hoaglin,

D.C. and

Welsch,

Matrix in Regression and Anova,"
Statistics, Harvard University, December 1976.

R.E., "The Hat

Memorandnii N5- 3'41, Department of

[14]

pp.

Cook, R.D., "Detection of Influential Observations in Linear Regression,"
Technometrics, 19 (1977), pp. 15—18.

[15] Mallows, C. L., "On Some Topics in Robustness," Paper delivered at the Eastern
Regional INS meeting, University of Rochester, 1973.

Ai.i
Appendix 1. BASIC DIFFERENCE FORMULAS

The fundamental difference formulas are known as the ShermanMorrison-Woodbury Theorem

[19, p. 29].

T

x . )
(i) Ci)

T -1 + (XTx4x.(xTx
(X X)

-1

(X .

T -1
(X X)

From

-1

T

(X(1)X(1))

-

(XT)X(.
(i i) )xTx.(XTX
i (j) (i))
T
1—x1(X(1)X(1))

-l
(Al.2)

x

this comes

i)
A

and

=

(A1.l)

1—h1

A

1

T
T
(Xx)
xr1
(A1.3)

1—h.
1

since

t

-

(n-p-i) S1)

t

2

8(j)

we get

-

.
(n- p 1) S(2)

E

tin.

(r +

1-h.

t=l

1

(n—p)s2+

1
____

r.

-

1

(i-h1)2
n

2r

2

2

h .r.

Z

r2
n
_______ Z h2
_______
(1_h1)2 t1 ti

2

-

r.

_________

(1-h1)2
z

2
(n-p) S —

i
1—h1

by using

the

fact

that H annihilates the vector

of residuals

Ai.2

Finally we

obtain
2 T -l —
(n—p) s (X X)
(n—p—i)

2

—l

T

S(1) (X()X())

(ix)

i_

(X1)X(1))1 - (n-p)s2

l4x(xTx)'

•H

.

A2.l
Appendix 2. DIFFERENTIATION FORMULAS

Let
1
1

w=

1

1

and

(xwx)i xTwy.
From

(Al.l)

(A2.2)

we obtain
-1

T —1

(xTWx)

(X X)

-1

T
+ —(l—w1)(X X) x1x1(XX)
l—(l.-w1)h1

and

(A2.3)

then
3

T —1
(X WX)

_(XTX)

1xx

-1
(xTx)
(A2.L)

(1-(1-w1)h.)2
Some algebraic manipulation using (A2. 2) and (A2.
3) gives
A

-

T

TXP•
(l_w)

11llw.)h.
13.

where

A

and r1 are

(A2.5)

the least-squares estimates obtained when w11. Thus

____ -- (XTX)
___

X1 r.1
(1—(1-w.)h.)2

(A2.6)

A2.2

or equivalently (again

•

using A2.3)

= (XTWX)4 (y-x1

(A2.7)

It is also useful to look at the squared residual error

tl w
n

SSRW

2

(A2.8)

Using (A2.7) we have

SSR

n
-2

A

w (y_x

X.
A

+

-2

T
x1(y1x1 w1)

2

(y-x1
1

(y.-x1
1

+

For the data v'W Y

—l

fl

A

tl
)2

T

) vçx(X WX)

-l

)c

(A2.9)

(y1—x1

and v X

-l
iW X(XTWX)

and

xT

HR=O.
This

implies that

the

sum

aSSRW.

W.

because

of (A2.3).

in (A2.9)

is zero

1 1.)
1
A

(y.-x.

2

so

that
2

r

(A2.lO)

A2.3

Putting

(A2.Li) and

(A2.lO)

[SSR

together gives

(XTWx)_1]

—l
1
____________

(l_(l_w)h1)

When

T

i —
(X WX)
SSR..1
2

—1

xTx xTX1(XTX)

(l-(l—w1)h)

2

(A2.ll)

1 this is equivalent to

T -l
r (X X) —

2

(n—p)s

T 1T T -l

(X X) x1x(X X)

(A2.l2)

A3.1

Appendix 3. THEOREMS ON THE HAT MATRIX

In

this appendix we for!ially show

that

when h11 (we can take 1=1

without loss of generality), there exists a nonsingular transfonrtion T,
such

that

(T)1 y and

a2,... ,a

do not depend on y1. This implies

that, in the transformed coordinate system, the parameter a1 has been dedicated
to observation 1.

When h1l we have for the coordinate vector

,

0)'

He1

since h1

0,

Let P be any pXp nonsingular matrix whose first column

is (XTX)XTe1. Then

a

XP=I

A

where a is lx(p-1) arid 0 is (p—l)xl. Now let

QIL
with

is

I denoting the

-a

(p-1)x(p-1) identity matrix. The transfonition we seek

given by T PQ, which is nonsingular because both P and Q have inverses.

Clearly

CT

10
OA

A3.2

and the least-squares estimate of the parameter a T1 will have the first
residual, y1-a1, equal to zero since a2,. .. ,& cannot affect this residual.
This also

&2. . . ,&

implies that

will not depend on y1.

To prove the second theorem in Section 2.3

(1—h1) det (xTx)

det(X(.)TX(.))

we need first to show that

det
where u and

v

i_vTu

(I_uvT)

are column vectors. Let

Qu

Q

be an orthononnal matrix

such that
(A3.l)

!lulle1

where e1 is the first standard

det(I_uvT)

basis

vector. Then

det Q[I_uvT] QT
i -

det [I-I lul Ie1vTQT

I lull

which is just i_vTu because of (A3 .1). Now

det

x . Tx
(i)

(i)
.

= det

ii

[(I-xTx. (XTX)_l)

xTx]

and letting u = x1 and V x(X X) completes the proof since x(X
(We are indebted to David Gay for simplifying our original proof.)

X) x±h.

S

A4.l

Appendix 4. DiIBITS FOR SECTION 2.9

Exhibit No.

Title

1

Assignments of Row Indices to Countries

2

Data

3

Ordinary Least Squares Regression Results

4

Normal Probability Plot of Studentized Residuals

5

Studentized Residuals

6

Diagonal Elements of the Hat Matrix.

7

NBFBFI'AS: Square Roots of the Sum of Squares of the
Scaled Differences of LS Full Data and Row Removed
Coefficients (DFBETAS)

8-

11

DFBFTAS (for individual coefficients)
Summary of Relative Changes in

12

Coefficient

Standard

Errors: NDFBVARS

13 -

16

Individual Relative Change in Coefficient Standard
Errors: DFBVARS

in

17

Scaled Change

18

Scatter Plot of NDFBEI'AS versus Diagonal Elements of

Fit

the Hat Matrix
19

Scatter Plot of NDFBL'TAS versus Studentized Residuals

20

Scatter Plot of NDFBVARS versus Diagonal Elements of

the Hat Matrix
21

Scatter Plot of NDFBETAS versus NDFBVARS

22

Diagonals of Hat Matrix with Observation 49 Removed

23

NDFBETAS with Observation 49 Removed

EXHIBIT 1

POSITION

LABEL

1

10

AUSTRALIA
AUSTRIA
BELGIUM
BOLIVIA
BRAZIL
CANADA
CHILE
CHINA(TAIWAN)
COLOMBIA
COSTA RICA

ii

t'ENMARK

12
13
14

ECUADOR
FINLAND

15

GERMANY F.R.
GREECE

2
3

4

5

6
7

S
9

16
17

FRANCE

18

(3IJATEMALA
HONDURAS

19
20

ICELAND

2:L

IRELAND

22

23

24
25

26
27
28
29
30
31

32
33
34

INDIA

IrAL.y

JAPAN
KOREA
LUXEMBOURG
MALTA
NORWAY

NETHERLANDS
NEW ZEALAND
N:ECARAGUA
PANAMA

FARAGUAY

PERU
PHILLIPINES

:35

36
37
38
39
40
41

SOUTH AFRICA
SOUTH RHODESIA

42
43
44
45

TUNISIA

46
47
48
49
So

SPAIN
SWEDEN

SWITZERLAND
TURKEY

UNITED KINGDOM
UNITED STATES
VENEZUELA
ZAMBIA

JAMAICA
URUGUAY

LIBYA
MALAYSIA

EXHIBIT 2
V
IUSTRALIA
IUSTRIA
)ELGIUM

WLIVIA
)RAZIL

ANADA
:HILE
HINA (TAIWAN)

OLOMBIA
:OSTA RICA
JENMARK
CUADOR
INLAND

RANCE
3ERMANY F.R,

REECE

UATEMALA
IONDURAS
:CELAND
:NDIA
:RELAND
:TALY
JAPAN
:OREA

.UXEMBOURG
JALTA
IOR WAY

IETHERLANDS
JEW ZEALAND
JICARAOUA
'ANAMA
'ARAGUAY
'ERU

'HILLIPINES
'ORTUGAL
SOUTH AFRICA
IOUTH RHODESIA
PAIN

WEDEN
tWITZERLAND

URKEY
•UNISIA
JNITED KINGDOM
INITED STATES
'ENEZUELA
:AMBIA
IAMAICA
IRUGUAY
.IBYA

ALAYSIA

COL 2

11.43
12.07
13.17
S •

75

12.88
8 •

79

0.6
11.9
4.98
10.78
16.85
3,59
11 • 24

12.64
12.55
10.67
3.01
7.7
1 •

27

9.

11.34
14.28
21.1
3,98
10.35
15.48
10.25
14.65
10.67
7,3
4,44
2.02
12.7
12.78
12.49
11.14
13.3
11.77
6.86
14.13
5.13
2.81
7.81
7.56
9.22
18.56
7,72
9.24
8.89
4.71

29.35
23,32
23 • 8

41.89
42.19
31,72
39,74
44,75
46.64
47.64
24.42
46.31
27.84
25.06
23.31
25.62
46.05
47.32
34.03
41.31
31.16
24,52
27.01
41.74
21.8
32.54
25.95
24.71
32.61
45.04
43.56
41.18
44.19
46.26
28,96
31,94
31.92
27,74
21.44
23.49
43.42
46.12
23.27
29.81
46.4
45.25
41.12
28.13
43.69
47.2

COL 3

2.87
4.41
4,43
1.67
0.83
2.85
1.34
0.67
1.06
1.14
3,93
1.19
2.37
4,7
3,35
3.1
0.87
0.58
3.08
0.96
4.19
3,48
1.91
0.91
3,73
2.47
3,67
3.25
3.17
1.21
1.2
1.05
1.28
1.12
2.85
2.28
1.52
2.87
4,54
3,73
1.08
1.21
4.46
3,43

0.9
0.56
1.73
2.72
2,07
0.66

COL 4•
2329.68
1507.99
2108.47
189.13
728.47
2982.88
662.86
289,52
276,65
471.24
2496.53
287,77
1681.25
2213.82
2457.12
870.85
289.71
232.44
1900.1
88,94
1139,95
1390.

1257.2
207.

2449.3
601.05
2231,03
1740.7
1487.52
325.54
568.56
220.56
400.06
152.01
579.51
651.11
250.96
768.79
3299,49
2630.96
389.66
249.87
1813.93
4001.89
813.39
138.33
380.47
766.54
123.

242.6

S

EXHIBIT
POSITION

LABEL

I

AUSTRALIA
AUSTRIA
BELGIUM
BOLIVIA
BRAZIL
CANADA
CHILE
CHINACTAIWAN)
COLOMBIA
COSTA RICA
DENMARK
ECUADOR
FINLAND
FRANCE
GERMANY F.R.
GREECE
GUATEMALA
HONDURAS
ICELAND
INDIA
IRELAND
ITALY
JAPAN
KOREA
LUXEMBOURG
MALTA
NORWAY
NETHERLANDS
NEW ZEALAND
NICARAGUA
PANAMA
PARAGUAY
PERU
PHILLIPINES
PORTUGAL
SOUTH AFRICA
SOUTH RHODESIA
SPAIN
SWEDEN
SWITZERLAND
TURKEY
TUNISIA
UNITED KINGDOM
UNITED STATES
VENEZUELA
ZAMBIA
JAMAICA
URUGUAY
LIBYA
MALAYSIA

2
3
4
5
6
7

S
9
10
11

12
13
14

15
16
17
18
19
20
21
2?
23

S

24
25
26
27
28
29
30
31

32
33
34
35
36
37
38
39
40
41

42
43
44
45
46
47

48

S

50

1

U

EXHIBIT 2 CONTINUED

COL 5
AUSTRALIA
AUSTRIA
BELGIUM
BOLIVIA
BRAZIL
CANADA
CHILE
CHINACTAIWAN)
COLOMBIA
COSTA RICA
DENMARK
ECUADOR
FINLAND
FRANCE
GERMANY F.R.
GREECE
GUATEMALA
HONDURAS
ICELAND
INDIA
IRELAND
ITALY
JAPAN
KOREA
LUXEMBOURG
MALTA
NORWAY
NflHERLANDS
NEW ZEALAND
NICARAGUA
PANAMA
PARAGUAY
PERU
PHILLIPINES
PORTUGAL
SOUTH AFRICA
SOUTH RHODESIA
SPAIN
SWEDEN
SWITZERLAND
TURKEY
TUNISIA
UNITED KINGDOM
UNITED STATES
VENEZUELA
ZAMBIA
JAMAICA
URUGUAY
LIBYA
MALAYSIA

2.87
3.93
3.82
0.22
4.56
2.43
2.67
6.51
3.08
2,8
3.99
2.19
4.32
4.52
3.44
6.28
1.48
3.19
1.12
1.54
2.99
3.54
8.21
5.81
1.57
8.12
3.62
7.66
1.76
2.48
3.61
1.03
0.67
2.

7.48
2.19
2.

4.35
3.01
2.7
2.96
1.13
2.01
2.45
0.53
5.14
10.23
1.88
16.71
5.08

i

J.

4

.

CJEF .5

COEF

COEF. 2
COEF. 3

C'JEF. 1

.7

INGRO

P0P75
INC

POP15

flTEREFT

0.409694

—0.461193
—1.6915
—0. 00033?

28.5661

EST COEF

.

1.8836
0.000931
0.196197

7.35452

ERR

0.144642

910

TRACE OF XTXINU=3.82582

—8.361829
2. 88818

—1.561

3.88415

—3.18851

1—STAT

OWO)=1 .9234

=34.8683

RSQ=. 338456

CONDITION OF SCALED

SER=3. 80267

t'IOUAR=S

NE

F.. 4. 45 )=5 .

'—.i1

NOB=50

EXHIBIT 3

S

EXHIBIT 2 CONTINUED

COL5
AUSTRALIA
AUSTRIA
BELGIUM
BOLIVIA
BRAZIL
CANADA
CHILE
CHINACTAIWAN)
COLOMBIA
COSTA RICA
DENMARK
ECUADOR
FINLAND
FRANCE
GERMANY F.R.
GREECE
GUATEMALA
HONDURAS
ICELAND
INDIA
IRELAND
ITALY
JAPAN
KOREA
LUXEMBOURG
MALTA
NORWAY
NETHERLANDS
NEW ZEALAND
NICARAGUA
PANAMA
PARAGUAY
PERU
PHILLIPINES
PORTUGAL
SOUTH AFRICA
SOUTH RHODESIA
SPAIN
SWEDEN
SWITZERLAND
TURKEY
TUNISIA
UNITED KINGDOM
UNITED STATES
VENEZUELA
ZAMBIA
JAMAICA
URUGUAY
LIBYA
MALAYSIA

2.87
3.93
3.82
0.22
4.56
2.43
2.67
6.51
3.08
2.8
3.99
2,19
4.32
4.52
3.44
6.28
1.48
3.19
1.12
1.54
2.99
3,54
8,21
5.81
1.57
8.12
3.62
7,66
1.76
2.48
3,61
1.03
0.67
2.

7,48
2.19
2.

4.35
3.01
2.7
2.96
1.13
2.01
2.45
0.53
5.14
10.23
1.88
16.71

5.08

.
(—I

L
U

E

3.

1.

—1

-3.

jS;

.
DBIT i•

Y1.O8O6 -O.O18218

.

2.5

INLJERSE_HORMAL..

NORMAL PROBABILITY PLOT OF RSTIJDENT

ROBUST EQUATION

p-b

I-b

fl

'-4

z0

N
I-b

-r

0
-4
0
-TI
w

-4

C

0
in
—I

0)

ci'

DATA NAMES

TIME BOIJNDS:

—0.25

—a.a

e. 15

.

I TO 50
C3(P75

11

INOE

21

31

41

PLOT OF SELECTED COLUMNS OF DFBUARS

EXHIBIT 14

.

51

S

.

10O

2.00

300

4.00

1

11

7

21

.

INDEX PLOT OF

EiIBIT

1DFBET

41

S

51

1

TO

11

EXHIBIT 8

.

50

21

41

INDE> PLOT OF ;ELECTEO COLUMNS OF OFBETAS

DtTA NAIIES: C2P0P15)

TIME BOUNDS

-070 1

—3.20

030

.

51

.

.

DATA NAMES

TIME BOUNDS:

—0.70

e_ 10

0.50

5

C3 (P0P75

1 TO

11

9

21

.

31

41

INDEX PLOT OF SELECTED COLUMNS OF DFBETAS

E1IBIT

S

51

1

I

EHIBIT 10

.

TO 50

INDEX PLOT OF SELECTED COLIJMN3 OF OFEETAS

DATA flAMES: C4(niC)

TI ME BOUNDS

—0.26

—0.12

0.16

.
.

a.

.

.

BOIJNDS;

11

50

(INGRO)

1 TO

DATA NAMES: CS

TIME

_1.1t31

-€1.60

-

3.42

21

.

31

41

INDE< PLOT OF SELECTED COLUMNS OF OFOETAS

EXHIBIT 1J

.

51

5.0

11

21

31

INDE< PLOT OF HOF'JARS

EXHIBIT 12

S

41

51

.

.

—

I
TO

11

50

DATA HAMES C2 (p0p15)

TIME BOUNDS

0.80

0.20

13

21

.

41

INDEX PLOT OF SELECTED COLUMNS OF UJFUARS

HIBIT

S

51

DATA NAMES

TIME BOIJNDS:

—0.25

—a.a

e. 15

.

I TO 50
C3(P75

11

INOE

21

31

41

PLOT OF SELECTED COLUMNS OF DFBUARS

EXHIBIT 14

.

51

S

.

1

11

(C)

1 TO

OITA NAMES: C4

TIME BOUNDS

—0.42

—2.26

—'...4

-1s

50

INOE

21

.

31

41

PLOT OF SELEC:TEO COLUMHS OF OFBL)RS

DUiIBIT 15

.

51

11

50

DATA NAMES: C5(GRO)

TIME BOuNDS: I TO

—1.00

—0.60

—.'-

a

16

21

31

41

INOE< PLOT OF SELECTED COLUMNS OF OFBUARS

EUBIT

.

51

S

U,

.

(J)

I-

.

LL
C—,

H

0

x -jLI
w

0
•-4

".4

S

B

F

:.

0.080

8 . 08

1.00

2.ø13

4.oe

•

0.100

• aa.

•

•la •

•

a

a

a.
a

18

8.280

23

44

0.400

SCATTER PLOT OF HFBETAS US H

EiIBIT

.

0.500

H.

0.688

.

.

I

I-

0.0
—3.IiØ

4.00

—2.Oe

I

-1.00

• •

•
.
•

I
I

I

•

•

I

0.00

S

I

44

•
a

I I.•
1.00

I

23

SCATTER PLOT OF HOFETAS tJ5 RSTtJOEF4T

IBIT 19

3.00
RSTUDEHT

2,00

:•:•

0.0Ø

10.0

•

.7

9

0.100

.•

9

:

:... a

46

S

9

0.200

:37

44

0.400

SCATTER PLOT OF NOFBUARS US H

EXHIBIT 20

0.500
H

0.600

.

N

H

B

F

0

0.

•

e

S

.•

.

.. ..
•. .

0'0.0

i.o

2.

3.00

4J30

2.0

.

21

4.0

21

S

6.0

44

.46

8.0

10.0

SCATTER PLOT OF NOFBETAS US HDFBURS

EiIBIT

NDFBUARS.

12.0

.

14.8

be

3.e2O

c

34O

11

21

31

INDEX PLOT OF H

DffBIT 22

.

41

51

.

S

a—.

4.0

1

11

21

.

31

INOE> PLOT OF t1OFBET$

EXHIBIT 23

41

.

51

