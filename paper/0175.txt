NBER WORKING PAPER SERIES

SOME CONVERGENCE PROPERTIES OF BROYDEN'S METHOD

David M. Gay

Working Paper No. 175

COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
National Bureau of Economic Research, Inc.
575 Technology Square
Cambridge, Massachusetts 02139

July 1977

Preliminary

NBER working papers are distributed informally and in limited
numbers.
This report has not undergone the review accorded official
NBER publications; in particular, it has not yet been submitted for approval by the Board of Directors.

'NBER Computer Research Center. Research supported in part
by National Science Foundation Grant MCS76-0032' to the
National Bureau of Economic Research, Inc.

Abstract

In 1965 Broyden introduced a family of algorithms called
(rank-one) quasi—New-ton methods for iteratively solving sys-

tems of nonlinear equations. We show that when any member
of this family is applied to an nxn nonsingular system of
linear equations and direct-prediction steps are taken
every second iteration, then the solution is found in at

most 2n steps. Specializing to the particular family member known as Broydents (good) method, we use this result to
show that Broyden's method enjoys local 2n-step Q-quadratic
convergence on nonlinear problems.

Contents

1.
2.

Introduction .

1

Finite Termination on Linear Systems

3

3. Local 2n-Step Q-Quadratic Convergence
of Broyden's Method

9

4. Concluding Remarks

14

5. References

16

—1—

1. Introduction
In 1965 Broyden [1965] introduced a family of algorithms
called quasi-Newton methods for solving systems of nonlinear
equations, i.e. ,

for

finding x* e ]Rn such that f(x*)

0,

where f; JR" ÷ ]R" is differentiable. Broyden proposed a modified

form of Newton's method in which an approximation H to the

inverse of the true Jacobian matrix f(x) is used and updated
after each step. This leads to an iteration of the form
x.

1+1

x. -

promote

1

X.H.f(x.),
11 1

where the steplength A.
is chosen to
1

convergence. In what follows we shall usually restrict our

attention to direct prediction methods, i.e., A.

1 as in

Newton's method. By analogy with the DFP method [Davidon,
1959; Fletcher a

Powell,

1963] for unconstrained minimization,

and by considering what is desirable when f is linear, Broyden proposed updating H. in such a way that the quasi-Newton

equation H1÷1 [fx±÷1 -

f(x±)]

x1÷1 -

x holds. Since

new information is picked up in only one direction each step,

Broyden suggested obtaining H+1 from H. by means of a
rank 1 update, i.e., by adding a matrix of rank 1 to H1.
This leads to the following iterative procedure.

Choose nonsingular H0 e flXfl and x0 c 3R.

Fork 0,1,2, ...
(1.la)
(l.lb)

(1.lc)

5k

xk+l
31k

let

_Hkf(xk);
Xk +

f(xk+l) —

f(xk);

—2—

0 then

If

(l.ld)

such that VYk

(1.le)

else choose Vk c ]RrI

(l.lf)

and VH'Sk 0

(l.lg)

and let Hk+l H + (sk

-

HkYk)v

Because of (l.lf) and the Sherman-Morrison [l9Z9] formula,

Hk+l is nonsingular whenever Hk is, so induction shows that
Hk is nonsingular for all k, whence Sk

0

only if f(xk)

0.

Broyden's [1965] method (sometimes called his first or

good method) results from choosing vk
-- and is defined for

T

Vk

YkHk5k

0 and

0 only so long as SHkYk

second or bad method results from choosing

0.

Syk

Hksk/(skHkyk) in (l.le)

when

0 and is defined so long as

0

Broyden has shown that his (first) method converges
locally at least linearly on nonlinear problems [1971] and at

least R-superlinearly on linear problems [1970]. Later,
Broyden, Dennis, and Morg [1973] showed that both Broyden's
good and his bad method converge locally at least Q-super-

linearly. More and Trangenstein [1976] subsequently proved
that "locally" could be replaced by "globally" when a modified
form of Broyden's method is applied to linear systems of equa-

tions. In Section 2 of this paper we show that when any form
of (1.1), including Broyden's good and bad methods (so long as
they are defined), is applied to a system of linear equations
f(x)

Ax - b

in which A c ]RF1<1 is nonsingular, then the

iteration converges in at most 2n steps (i.e., x5

x

=

A

b

—3—

for all j > 2n).

We show further that this result also holds

when some nonunit steps are allowed (Sk

XkHkf(xk), with

0, 1). Specializing to Broydens good method, we show

Ak

in Section 3 that this method enjoys local 2n-step Q-quadratic

convergence on nonlinear problems. Section

presents some

concluding remarks.
2.

Finite Termination on Linear Systems
In this section we show that Algorithm (1.1) converges

in at most 2n steps when applied to an f representing a
nonsingular system of linear equations: f(x)

A nxn b £

Ax -

b,

where

and A is nonsingular. This follows as

lRnl,

an easy corollary to the following lemma, which holds even if

A is singular. The notation [ci used below denotes the
greatest integer less than or equal to a c ]R, while for non-

zero u, v
some real A

Xv for

v means that u

]RrI, the notation u
0.

Lemma 2.1

If A c

and Algorithm (1.1) is applied to f(x) E Ax

with the result that

pendent, then for 1 <
(2.1)

<

L(k+i)/2J,

(AHk_2j÷l)'fk_2j+l 0 <

Proof: Since k-l Ask_i
to hold for j

Note that 2m <

are linearly inde-

f(xk) and

j

i

<

j,

are linearly independent.

_AHklfkl, this is easily seen

1. Assume it true for j

k-i,

(l.la,b,c,d) that y.

-

whence k-2m-i > 0.

0 =>

m <

L(k+l)/2J.

Also note from

y11 0, ° k—l

0

>

b

—4—

0. Now

'k-2m-l
s

k-2m

-H k-2my k-2m

-H
f
k-2m k-2m
-H

AHk2(I - CI

AHic2m+i

-

k-2m+l
0 <

i

k-2m

< m,

-

+Hk-2mAMk-2m fk-2m

(I—AHk-2m)fk-2m

so

AHk2]fk2V2).

k2mk2m

Moreover,

Since

are linearly independent by the induction hypothesis

(2.1), we see by the two preceeding equations and induction on
that there exist

(ARk 2m+l

for 0 <
—

(I_AHk_2m)[(AHk_2m)' +

2m+l

—

m,

(dependent on k and m) such that

y

.(AHk21fk2

whence (I - AH
k-2m )(AH k-2m )'f k-2m

are linearly independent. But

(I -

,

<
0 —

—

2yk2l 0 by

AHk

(l.le,g), so k-2m-l _AHk2mlfk2ml and (AHk2ffl)'f2m
o <

I

<

m, are linearly independent. As before, we see that

there exist

(dependent on k and m) such that
(I -

(AHk2)'fk2

AHk

[(AHk2 l +
+

I

for 0 <

<

m,

i,i(AHk_2m_l)jfk_2m_l

whence we readily see that (AHk2ml)'fk2ml

o <

I

j

m+l, and the lemma follows by induction. I

<

m+1,

are linearly independent. Thus (2.1) holds for

Theorem 2.2: If f(x)

Ax - b

and A c

is nonsingular,

then Algorithm (1.1) converges in at most 2n steps (i.e.,

2n 0).

—5—

Proof: As noted above, Hk is nonsingular for all k; since
A is nonsingular, we thus see that if f22 is nonzero,
then the same is true of s

-H

2n—2

f
2n—2 2n—2

and

'2n-2 AS22. If f1 0, then necessarily 2-2
0 and Lemma 2.1 implies that f21 2n-2

SO 2n2

(since otherwise ]R would contain n+l linearly independent
vectors). Since s22 H21Y22 H21A522 by (l.le,g),
we have y22 As22 AH21y22 and hence
f

2n—l

AH 2n—l f2ri—l

,

so

f f2n-1 +A2n-lf -AH2n—l 2n—l 0.
f

2n—1

2n

Theorem 2.2 leaves several questions unanswered, such as
whether a full 2n steps may actually be required. Computer
runs suggest for small values of n that Broydents good and

bad methods may both require a full 2n steps. As we shall
now see, it is possible for arbitrary n and nonsingular A

to choose H, f, and the

Vk in (l.le) so that Algorithm

(1.1) requires a full 2n steps. This is the content of
Theorem 2.L., proof of which requires the following lemma.

0, vy1 0, and

Lemma 2.3: In Algorithm (1.1), if
Rank(I -

AHk)

Proof: I -

n-l, then Rank(I - AHk+l)

AHk+l

I -

AHk(I

-

[I

-

AHk]fkv)

(I -

AHk)(I + AHkfkv)

(I -

AHk)(I

-

n-l.

—6—

It suffices to show that (I -

Now Rank(I -

linearly independent of
(I -

0, so (I -

AHk)Yk

AHk)W
T

•

(I -

n-i and

AFIk)

0 whenever w is linearly

independent of kl• But vk(I w

0 whenever u is

AHk+i)u

T

k"k

0, so for

ykv)U, we have vw 0, while vYkl 0 by

assumption. Thus w is linearly independent of

whence (I -

(I -

AHk+l)u

0. •

AHk)W

Theorem

2.4.:

If I - AH0

<i

n-i,

are linearly independent, if vfk

o

<

vHk'sk

is nonsingular, if (AH)'f,

0 for k > 0,

0 and

and if VYkl 0 for k > 1,

then Algorithm (1.1) requires a full 2n steps to converge.
Proof: As in the proof of Lemma 2.1, we find
(I -

(2.2a)

k+l

(2.2b)

AHk+lfk+l

(2.2c)

_(vfk)(I -

(2.3a)

1

1

(2.3b)

i

<

(AH21)1f21 0 <
(I -

that

Rank(I —

n-l,

0,

I -

+

E

AH0

(AHk)1fk
is non-

are linearly independent,

that

Moreover, since I - AH1
j

AHk)[(AHk)'

In particular, since vf0

singular, and (AH0)1f0, 0 <

we see for j

and

_(vfk)(I - AHk)(AHk)fk,

(AHk÷l)1fk÷l

1.

for

AHk)fk,

AH2.1)

n—l.

n-j,
AH)(I

are linearly independent.
-

y0v),

we see for

—7—

Suppose (2.3) holds for 5

k <

n.

Since 2kl _AH2klf2kl,

we see from (2.2) and (2.3a) that 2k-i and (AH2k)'f2k,
o

<i

<

n-k-i,

are linearly independent. But

AH2)21

0, while Rank(I - H2k)
n-i by (2.3b)
and Lemma 2.3, so 2kl spans the null space of I - AH2k
(I -

and (I - AH2k)(AH2k)'f2k,

n-k-i,

0 <

must be linearly

independent (since otherwise 2k1 were a linear combination
of (AH2k)'f2k, 0 <
(2.3a) holds for j

i

<

n-k-i).

From (2.2) it follows that

k+l, while (2.3b) holds for j

by Lemma 2.3. Thus (2.3) holds by induction for 1 <

k+l

j

<

n.

In particular, f1 0, whence Algorithm (1.1) runs a full
2n steps before converging. I

Another question that Theorem 2.2 leaves unanswered is
what happens when a step-length parameter is introduced, i.e.,

when step (l.la) is replaced by Sk
0, 1,

Ak

(2.)

AkHkfk For

(2.2) becomes
(I -

(AHk+l)'fk+l

+

AHk) AHk)' +

E

(AHk)]fk +

i,ok

i.e., multiples of k are added to the right-hand sides of
(2.2). Thus the proof of Lemma 2.1 is unaffected if

Ak21 1, 1 <
Xk2m

1.

m <

L(k+i)/2J;

it seems essential only that

More generally, we see from (2.Ll.) that if
0 <

m,

are linearly independent, then the

—8—

set {(AHk)1fk I 0 < i < m+1} must contain at least m+l
linearly independent vectors, whence (AHk)1fk 0 < i <

must be linearly independent (since if (AHk)fk were dependent

on (AHk)1fk 0 < I < j, for some j < m, then (AHk)fk
could be expressed as linear combinations of these same vectors

> j). Hence the proof of Lemma 2.1 may be modified

for all

to obtain

Theorem 2.5: If Algorithm (1.1) is applied to a linear function
f(x)

by

Ax - b

with A c ]Rnl>(11 nonsingular and (l.la) replaced

XkHkfk (Xk
o —< i —< n, such that ko
5k

0), and if there are integers k.,

-l and X k.

1

k.>k.
then fk
— —
i— i-i+2 for 1<i<ri,
n

1 with

0.

Theorem 2.'-i. is readily generalized to allow X2k

1,

o < k < n. Whether a further generalization along the lines
of Theorem 2.5 is possible remains an open question.

—9—

Local 2n-Step Q-Quadratic Convergence of Broyden's Method

3

We now restrict our attention to the direct-prediction

version of Eroyden's (good) method. This amounts toAlgo—
rithm (1.1) with Vk

HkTsk/(skTHkyk). With this Vk,

it

is

well known (and easily seen from the Sherman-Morrison [19L49]

formula)
—l

that if
—l

Hk÷l

Hk

+

k

is nonsingular and Sk Hkyk
—1

T

0, then

T

Hk sk)sk /(sk sk). We shall find it some-

what more convenient to restate the direct—prediction Broyden's

method in terms of B Hk. Thus for k

0, 1, 2,

we are dealing with the following iteration (in which

(3.la) s kk
(3.lh)

Xk

+

Sk

(3.lc) k k+l —
(3.ld) If skBkyk
(3.le)

else Bk+l

In what follows,

vector norm

=

T

lxii

+ (

0 then k±l =

(x x)

- Bksk)sk/(sksk)

Bk.

denotes the Euclidean
or the corresponding induced

matrix norm. We may now state the main result of this section:

Theorem 3.1: Suppose f: n -]R is differentiable with

f(x)

-

f(y)

1

ff'(y +

T(x-y))(x-y)dT,

that f(x*)

0 with

0

A

E

f'(x*)

nonsingular, and that the Jacobian matrix f' j

—10—

Lipschitz continuous at x*, i.e., for some constant A

and all x sufficiently close to x,

f' (x <

f' (x) -

(3.2)

-

A

x.

Then there exist y, , >

(3.3) x_4 <
then

and B- A <

the. iterates produced by Algorithm (3.1) satisfy
-

(3.)

for

such that if

0

x

all Q >

-

<

0.

Proof: From (3.2) and the proofs of Theorems 3.2 and 14.3 of

[Broyden, Dennis, & Mor, 1973], there exist ,c >

< l/(LIA'II)
(3. 5)

Xk

Bk - A

(3.6)

with

such that if (3.3) holds, then

-

Xk+l

0

<

-

x

2,

<

(3.7)

(3.8)
and sBy 0 only if

x*. Fix 9 and let h

-

x*jI:

we must show that there is a y independent of 9 such that

IIx+2 -

x

<

yh2.

Consider the sequences ' x2,
B2,

..

. , x2

2 of matrices generated from

of vectors and

x and

—11—

A(x -

B by (3.1) with f(x) replaced by f(x)

B

i.e., k
=

k+1

xk), Sk
-

k

Bkfk, Xk+l

Xk

+

Sk,

(3.9)

B±1

for Sk

O

Ask and

-

Bk +

Bkskk/ksk)

with Bk+l = Bk

Sk

Similarly to (3.6)

0.

and (3.8), we have

and

k A <
<

(3.11)

W' s:w by ii.iuction that there ex.Ltt ',
(independent

of ) such that
-

(3.12a)

Ilx+ - xii

(3.12b)

for

0<j<2n.
Since B

with

-

,

establish

B

0

and x =

(3.12) holds for j =
0,
Suppose it holds for j
To
k.

0.
2 0
, =

0

(3.12b) for

IIS+k

(3.13)

-

Sk

+k -

x

0

k+1, we first note that

j

Bkf÷k Bf
IIBII IIBkII IIB+k - BkM IIf+kII
+

Now

nd

y1.h

BI! II+5II

k [fx+k -

Ii1II t'+k
f(xj+k)1

+

-

Lx÷k

-

xk)l

+

—12—

and (3.2) and (3.5) imply
A

- f(x+k)M

Lf* + T[XL+k_X*]) -

ft(x*)1(x_x*)dT

1
—

<

while for

xk)II

(3.13), (3.8),

(3.11),

by (3.12b), so

13,kh

and (3.12a) imply

skIl
+

where ,k lA'iI2y1 ,k
along with (3.12b) for j
with

xITdT

x*112 <

hAil ilX+k - Xkil

-

(3.1k)

—

tIAIIy2k,

'3,k

IJf(x+k) -

x*II /lIX2.+k

All(X

+

213k), which,
k+1

k, gives (3.12b) for j

+

12,k+1

12,k

It remains to show that (3.lL) and (3.12a) for j

k+1. If 5+k

imply (3.12a) for j

0, then

A

either +k+1 0 or
(3.1LI.) shows

0 or Sk

k

0, whence the reasoning above
<
—

if

(

+

3,k+l together with (3.6)

and (3.10), this shows for 15k+1

+

23k+1 that

YSk+1h2. On the other hand, if

!IB+k+l - Bk+lii '1i+k+11'
A

both

5+k

0 and Sk

0

in view of (3.ld), (3.9),

(3.15)

+k -

(as we henceforth assume), then

and (3.7), we need only show that
T

T

B+ks+k)sj+k/(s÷ks+k)
-

A A AT ATA
Bksk)sk/(sksk)

-

II+kII

16,kh,

—13—

k+l with

for then (3.12a) holds for j

1

Let Ak

max{y5 k+1' 11,k +

11,k+1

so that

A

Aks,+k. Since

.+k

ff'(xL+k+
T5+k)dT,
0

Ash, we have the follow-

bound on LHS(3.15), the left-hand side of (3.15):
LHS(3.15)

(3.16)

T

T

'

"T

(Ak_Bj+k)s+ks+k/(sj+ks+k) -

<

ATr

1

(ABk)sksk/(sksk)JI

-

[(A-A)

II

II

fV&+kl
T

T

(B+k_Bk)]s+ks+k/(sj+k +k )lI hf
T
- AT .TA
(A_Bk)[sj+ks+k/(sj÷ks+k) 5ksk/(sksk)

+

II +

II

1

Now IlAk — All

<

fhlf'(xj+k
0

+

+ Ts+k —
0

<

—

(3.7),

17

<

by (3.2), (3.5), and the

y7h

hAll +

X5/2.

Because of (3.5),

T
T
and the fact that Is÷ksj+k/(s+ks+k)l

find IhAk - All

<

Xh

'1f+k11

k,

From (3.12a)

<

we thus find that the first term in the right-

hand side (RHS) of (3.16) is bounded by (Ày7 +
A

(3.10)
where

1, we thus

and

-

for j

x*hJdT

x'Il

by (3.2) and (3.5), while jfIl

definition of h, where

f(x*)IldT

TSZ+k) —

and (3.6), lA —

llAIl

+

Bk!1

26).

ylk)h.

By

A

tli+kH

Moreover,

hA —

Bk1! IlBj+ks÷kll .

18IlS.Il

—1t—

S+kS+k -

AAT
SkSk

4+kS+k

SkSk

T

A

-

-

_______

S+k

I! S+kIJ

IIS+kIl

Sk

+

liSkil
A

+

_______

—

IIS.+kI!
<

2

+k" S+kII

[2/Ils÷I](s÷k

-

Sk)

6k

Sk

IIskIJ

IkkII

)

Sk/ Sk
+

[(IlI

-

- Sk/+k.

<

Because of (3.lq.), we therefore conclude that the second

term in RHS(3.16) is bounded by Y8Ykh2, so (3.15) holds
with ' 6

,.

Xy.7

+
.L.,_

+ Ly0y

. Thus

(3.12a) holds for

and by induction we see that (3.12) holds for

X2n x
We

j

by Theorem 2.2, so (3J4) holds with '

j

k+l,

2n. But

11,2n

I

could use the same techniques to prove a similar

result for Broyden's bad method, i.e., Vk
(l.2e). At the time of this writing, it remains an open
question whether a similar result holds for Broyden's method
with projected updates [Gay F Schnabel, 1977].
4.

Concluding

Remarks

Theorem 2.2 came as quite a surprise to a number of us

who had confidently shared the belief that Broyden's method

did not enjoy finite termination on linear problems. Among
other things, this theorem should serve to still the criticism
that Broyden [1970, p. 377] had in mind when he wrote, "Thus

—15—

Broyden's algorithm will not solve linear systems in a finite
number of steps and this has been held to be a disadvantage
of the method."

Theorem 3.1 is one interesting consequence of Theorem 2.2:
Broyden's method with unit step lengths enjoys local 2n-step
Q-quadratic convergence and hence has an R-order of (local)
convergence of at least 21/(2n) (see §9.2 of [Ortega S Rhein-

boldt, 1970]). This result nicely complements that of
[Broyden, Dennis, S More, 1973], which establishes the local

Q—superlinear convergence of Broyden's method. The Q-superlinear convergence assures that eventually more progress is
made in the current iteration than in the previous one, while
the 2n-step Q-quadratic convergence assures a definite amount
of progress at intervals of no more than 2n iterations.
Theorem 2.14 suggests that Broyden's good (or bad) method

often converges no faster than 2n-step Q—quadratically and
hence has an R-order of exactly 21/(2n)

If so, then we may

extend the comparison of asymptotic efficiencies in §6 of

[Brent, 1973] to include Broyden's method. According to
Brent's definition, Broyden's method would have efficiency
E(B)

(1og22)/(2n), the lowest of the methods compared. Of

course, this says little for practical applications, where
the bulk of execution time is consumed in finding the region
of fast local convergence and where the simple measure of
work (i.e., the number of equivalent function evaluations)

that Brent used may not suffice. Moreover, if (as we suspect)

—16—

a result similar to Theorem 3.1 holds for Broyden's method
with projected updates [Gay

Schnabel, 1977], then this

version of Broyden's method often enjoys (n+1)-step Q-quadra-

tic convergence, which gives it efficiency E(P) =

(log22)/(n+l),

the same as for the finite-difference Newton's method.
Acknowledgment
The author thanks John E. Dennis, Jr. both for helpful
comments on a draft of this paper and for suggesting that
Theorem 2.2 would probably imply the local 2n-step Q-quadratic
convergence of the direct-prediction Broyden's method on
nonlinear problems.

5.

References

Brent, R.P. (1973), "Some Efficient Algorithms for Solving
Systems of Nonlinear Equations", SIAM J. Numer.

Anal. 10, pp. 327-3.
Broyden, C.G. (1965), "A Class of Methods for Solving Nonlinear Simultaneous Equations", Math. Comput. 19,
pp. 577—593.
Broyden, C.G. (1970), "The Convergence of Single-Rank QuasiNewton Methods", Math. Comput. 2'i., pp. 365-382.
Broyden, C.G. (1971), "The Convergence of an Algorithm for
Solving Sparse Nonlinear Systems", Math. Comput. 25,
pp. 285—29Lf.

Broyden, C.C.; Dennis, J.E.; a More, J.J. (1973), "On the
Local and Superlinear Convergence of Quasi-Newton
Methods", J. Inst. Math. Appi. 12, pp. 223-25.

Davidon, W.C. (1959), "Variable Metric Method for Minimization", Argonne National Labs. report ANL-5990 Rev.
Fletcher, R.; a Powell, M.J.D. (1963), "A Rapidly Convergent
Descent Method for Minimization", Comput. J. 6,
pp. 163—168.

—17—

Gay, D.M.; F Schnabel, R.B. (1977), "Solving Systems of
Nonlinear Equations by Broyden's Method with Pro-.
jected Updates", NBER Working Paper No. 169.

Mor, J.J.; & Trangenstein, J.A. (1976), "On the Global
Convergence of Broyden's Method", Math. Comput. 30,
pp. 523_5140.

Ortega, J.M.; & Rheinboldt, W.C. (1970), Iterative Solution
of Nonlinear Equations in Several Variables,
Academic Press, New York.
Sherman, J.; & Morrison, W.J. (1949), "Adjustment of an
Inverse Matrix Corresponding to Changes in the
Elements of a Given Column or a Given Row of the
Original Matrix", Ann. !4ath. Statist. 20, p. 621.

