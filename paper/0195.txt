FINDING LEVERAGE GROUPS*

tvid E. Colerrn
Working Paper 195

National Bureau of Economic Research, Inc.
575 Technology Square
Cambridge, Massachusetts 02139

August 1977

This

work was supported by the National Science

Foundation under Grant MCS76-11989
Computer Research Center.

to the NBER

Abstract

A brief discussion of recent met1ds using the Hat Matrix for

identifying leverage points, and clustering techniques for finding
groups of data points is presented. The problem of identifying leverage
groups is addressed, and a heuristic algorithm for identifying both leverage
points and leverage groups is proposed. Semi-portable FORTRAN code

implementing the algorithm, a sample terminal session, and a discussion of
the terminal session are included.

Acknowledgements

author, especially, ,xuld

The
helpful

discussions,

like

to thank

Peters for many

a useful derivation, and for running the software on a

506 by 13 matrix. Virginia KLema, Roy Welsch,

provided

Steve

Ed Kuh, and Sally Coleman also

useful insight. Sandra 1briarty gave extensive technical assistance.

Key Words and Phrases: leverage, outliers, least squares, cluster
analysis, data

analysis,

hat matrix, mathematical

CR Categories 5.l'4 and 5.5.

software.

Table of Contents
Page

Introduction .
A problem

The Data Point Algorithm .
AnExample

Appendixi

10

Appendix 2

13

Appendix 3

11.1

Appendix4

19

Appendix5

23

References

24

Figures

Figurel.
Figure2

11

Fie 3

11

Introduction
Of prinry concerTl in regression (least squares), y X +

that the

X utrix be non-singular and

well-conditioned.

E, is

A secondary

concern, sometimes neglected, is the distribution of data (sample)

points

(rows

of X) over the space spanned by the coluims of non-singular

Although it is desirable, and frequently

X

data

issues then arise, the presence of leverage points, and

Two

presence of clusters (groups) of points.
Conceptually, a

points
is

the

is norrrlly distributed (in each column), -this often is not the

case.

the

assumed to be true that

and their

leverage point is far away

centroid; it

is

an

(in some sense) from other

outlier in X. If

p (for X, n by p)

larger than, say, 3 it is hard to spot leverage points by eye or scatter

plot because the hyper-parallelopiped representing the observation

space has 2 vertices. Furtherrrore, leverage is a relative property involving
n(n-l)/2 interpoint relationships. What is needed is a metric under which
each data point can be

assigued a number

indicating its leverage.

Hoaglin and Welsch [5] present the use of the so called "Hat Matrix",
H, to examine

diagonal

the

elements,

distribution of data points. In particular, they

use the

of H as indicators of leverage, as is motivated

by the derivation of H: ietting xT stand for the transpose of X, (XTX)l
stand

to ,

for

the me-trix inverse of xTx,

and

y

stand for the computed approximation

stand for the fit realized at the least squares solut ion

we have

xTx

xTy,

(XTX)_1XTy, X y = X(XTX)_lXTy.

—2—

If we

set H X(XTX)_1XT we have y = Hy ; H "puts the hat" on

y.

leverage of the th row of X, X, is seen in the influence of y on
the fit y., through h.. Since H is a symntric, idempotent matrix (a
proj ec-t ion matrix), the h lie between 0 and 1. In their recent paper,
The

Welsch and Kuh [8] develop the use of the h1 and related regression

statistics. They define a cutoff level of 2pm (for n > 2p) above which
an h1 is considered significant and row i is called a leverage point."
Andrews and Pregibon [1] have developed another technique in which points

with large h s are considered leverage points, and minors of X TX
are computed in order to identify groups of leverage points (leverage groups).
The problem of identifying clusters, or groups, has been approached

•

in many ways. As in the leverage point problem, nonhierarchical cluster
analysis

**

is multidimensional in nature, and seeks to reduce O(n2 )

relationships to n relationships, where each point is assigned
to a cluster on the basis of some specified criterion, often involving Euclidean
interpoint

distance. Kendall and Stuart [] give a heuristic procedure using ranking
which is moderately successful in partitioning data into groups. Gnanadesii<an

[3],

in

his chapter, "Multidimensional Classification and

Oliver [6] in his software documentation

Clustering," arid

on Cluster Analysis routines

describe a number of different clustering criteria and clustering procedures,
but the complexity of the problem constrains the algorithm to be molded by

its context. Since we are interested only in leverage groups, we will want
to

*

use criteria peculiar to assessing leverage.

See

Appendices 1 and

2,

**

See Gnanadesikan [3].

—3—

A Problem

[8],the h

As discussed by Welsch and Kuh

effectively reveal individual

points, but may not reveal those leverage points that geometrically
a group (are in close geometric proximity to one another). Proximity

leverage

form

to other data reduces the individual leverage, hence the h1, of any given
point.

A simple example mekes this clear. Consider X which

consists of a cloud of 20 points centered at the origin, uniformly randomly
distributed within a 5-space hypercube of side length 4, plus a point

at (10, 10, 10, 10, 10). The latter point has h21 of about . 951,
close to the maximum value of 1. When a 22nd point is added nearby, at

(10.1, 10.1, 10.1, 10.1, 10.1) we find that h21 and h22 are about

.483 and

.492. A 23" point at (10.2, 10.2, 10.2, 10.2, 10.2) yields h21, h22
and

to

h23 of .321, 328, and .334. These h1 contrast

to others corresponding

points within the cloud, which are as high as .340, .425, .469, and 482.

Sequential row

deletion

what constitutes

determine

a group,

high leverage, while the h1
sequential

is unreliable because it is hard to

and a group could collectively have

of its members might be rroderate. The

procedure proposed by Andrews and Pregibon El] can also

encounter difficulties for the same reasons. Welsch and Kuh [8]

mention

the possibility of identifying groups through the correlation matrix of

the residuals, but as they note, this requires the computation of the
n(n -1)12 elements,

which requires either considerably

nore storage or an 0 (n2p2 ) -operations algorithm, If groups can be identified,

we might prefer to replace row deletion with the substitution of a group by
the mean

(or

some other summary measure) of its members, This

crucial or expensive data is not lost,

and

way,

the h convey nore information.

Welsch and Kul-i [8] discuss other possible remedies.

—4—

S

1.

xg
Ib)

ia)

0

la) Meas irlig the para lel
distance of point j from

point i

ib) Finding oitdistancers.

0

S

0
l)

ic)

Finding leverage groups

headed by outdistancers.

—5—

The a]x)ve comprises the ntivation for a heuristic algorithm which

can be used to help identify leverage points and leverage groups. The
"Data Point Algorithm" (DPA) is O(n2p) operations, and requires little

extra storage beyond that of the X matrix, and thus is comparable

in cost to obtaining the h's and less expensive than obtaining the
or RP.<'s proposed by Andrews and Pregibon [1]:
Data Point Algorithm

1. Given

X, n by p with all constant columns deleted.

2. Center

the
3.

the

coluirn

data; X ÷ X

means of

-

X,

where the rows of X are identically

(The origin is now the centroid).

X.

Normalize each column by dividing by its
(The main

diagonal

4. Compute and

store

of the observation space hypercube

of

t1e £2

5. Compute for each point

that is,

nor times 2(pL'2)

the

each point

is

now of length

(row).

"normal" distance to all

other

points,

distance parallel to its normal vector, (see Figure

further

out

Sum the

(scaled) inverses of these distances for each point,

in the normal direction

1).

la).

Talj those points

(those with negative parallel distances).

to

obtain a

measure of local density.
6.. Single out those points with outdistance (further out) tallies of 0,
particularly those that
arid

to

Figure
*

have large £2 norms (relative to the others,

the maximum, 0.5). We call these points "outdistaHcers"

(see

ib).

Giv vector x
Given vector x

(x ,

1

(x1,

x2,

...

)T

x2, . . . ,x)',

the £ nori of x,

the £2 norm

ix! L

ixj!

21/2

of x, I lxi 12

x)
T

(xx)

1/2

—6—

7.

Each outdistancer is a leverage point, or the point furthest out
in a leverage group. A relatively low "density" value means a

point is isolated, a high value indicates the proximity (in the
normal direction) of other points.
8.

Get a sorted listing (possibly via Tukey [7], and

"Stem-and-Leaf" display)

of all points and

Hoaglin and Wasserman's

their normal distance to each

outdistancer. Establish a cutoff level for normal distances, below
which points form a leverage group "headed" by the outdistancer (see Figure lc).
A listing of a semi-portable interactive driver, DPA FORTRAN, and
the initialization routine, MATRIX

FORTRAN, which implement the DPA

algorithm can be found in Appendix 3.

By centering and normalizing the data, norms and distances can be

compared. The further out a given point is from the origin (the centroid)

and the fewer points are further out - the nre leverage it exerts. The
point furthest out in any normal direction exerts the nost leverage in

that direction. Any such point may be isolated, part of a tight group,
or anywhere on the continuum in-between. Again, we emphasize that the

group-inclusion function imposes a discrete, binary set of relationships
on a complex, continuous configuration, so there always is some arbitrariness
and simplification.

For our

purposes, we

by measuring distances only in

are

not

used), but

the normal directions (perpendicular distances

we increase complexity because normal distances are non-

symmetric, d1Rd2-1÷ d2 Rd1, unlike

are

"headed"

discussion

uld seem to reduce complexity

Euclidean

distances. Thus leverage groups

by outdistancing leverage points. An exanpie makes the above

clearer.

—7—

An Exair1e

We return to the example

in

discussed

three

aixve, X coirised of twenty points

around (10, 10, 10, 10, 10).
Appendix 4 contains the terminal session with DPA FORTRAN, to which the
reader should refer. *
a cloud about the origin and

DPA

points

FORTRAN carries out steps 1) -5) of the Data Point Algorithm.

Examining the OTJTDIS coluim, we see that

are

is

outdistancers.

8, 10, 17, 18, and 23
Point 23 especially catches our eye because its norm
points

listed as .5, the highest possible value. We now proceed to sequentially

examine

the

5 points singled out by step 6), using the Stem-and-Leaf

display (SLD) [7]. The SW for point 8 is done in units of lO_2, first of all
indicating

that

all but the three points isolated at the bottom of the display

are

relatively close

the

SW does show a well defined break in distances, at about .04. DPA

to point 8 (.01 is small relative to .5).

Nonetheless,

identifies points 17 and 19 to be part of the indicated group. We
adopt a convenient notation for leverage groups: (norm, cutoff value,

cutoff separation, outdistancer: other points in group), so we list
the

first leverage group identified as (.134, .04, .02, 8:

17, 19). The

norm indicates the extent of leverage, (low in this case). The cutoff
distance

indicates the approximate minimum normal-distance radius used

to define (contain) the group, (small, in this case). The cutoff separation
indicates the extent to which the group is isolated from the other points

(also small, in this case). Lastly, the header (outdistancer) of the
group, and the group members are

listed.

Execution was on an IBM VM37 0/15 8

computer, FORTRAN H(OPT(2))

compiler.

—8—

Ontinuing with the example, DPA finds (.112, .01, .02, 10:17, 18) —

which meansthat

to weak

leverage groups

overlap at

which has no wi-defined cutoff value, and (.114, . 01,

-, 17:-)-.

point 17, (.147, -,
.

03, 18:10). DPA

clearly identifies the leverage group near (10, 10, 10, 10, 10) in this contrived
example: (.500, .02, .38, 23:21, 22).
Turning to some "real" data, the example considered by Welsch and Kuh

taken

[8]

from an econometric study of life-cycle savings rates) serves as a good

case for

comparison of

The h1 identify points
of decreasing h1) and
indicates

the use of the h1, and

the

Data Point Algorithm.

49, 44, 23, and 21 to be leverage points

*

(in order

37, 6, 147, 14, and 39 to be "contenders". DPA FORTRAN

that of 49, 44, 23,

and 21, only 49 is an outdistancer; 44 is

outdistanced by 39, 23 by 28, and 21 by 2, 3, 14, 25, 314, 40 and 43. No
clear leverage groups are indicated; 18, 37, 39, and 49 are all outdistancers,
but

SLD's reveal no significant breaks in the sorted normal distances, The

design of DPA FORTRAN allows the user to identify "secondary" leverage groups those headed by a point outdistanced by only a few other points. We call

such points "k-outdistancers" where k is the number of outdistancing points.
DPA FORTRAN lists as l—outdistancers points 114, 23, 25, 43,

414, and

50.

By defining a new generalized data structure for leverage groups headed by k-out-

distancers: (norm, cutoff value, cutoff separation, k-outdistancer :

(outdistancing

points), other points in group) we can conveniently display the fact that
point 25 has a norm of . 311, is a l-outdistancer (outdistanced by point 39) and with
cutoff value of .05 and cutoff separation of .03 it heads a group containing
points 2,
(.311,

*

3, 11, 114, 15, 40, and

43

.05, .03, 25:(39), 2, 3, 11, 114,

See Appendix 5.

15, 40, 43).

—9—

We also

(.320,

have

3, 11, 1'4, 25,
The other 1-outdistancers are uninteresting.
In

.05, .03, 43:(39), 2,

conclusion, DPA FORTRAN shows points

of decreasing

norm) to

'.1.0).

39,

be outdistancers, each

'49, 18, and

with

a roughly

37 (in order

uniformly distributed

set of neigh]x)rs in the direction towards the origin (centroid). Lcosely speaking,
points 25 and '43 head up a leverage group outdistanced only by point 39, and

containing points 2, 3, 11, lIt, and '40. This set of data does not appear to
contain any remarkable features in the way of leverage points or groups.

- :iO

—

Appendix 1
X and

Aunted X

An

issue

in the leverage point

(group) problem is

whether to search

for leverage points in X, or in X augmented by the right-hand side;
y: Xy. The appeal of using Xy is that it contains all input data,
*
and a leverage measure, such as h (the diagonal of the
hat matrix for X y) can be computed for each point X y1 . The crucial
disadvantage of using X y is that such a measure as h can blur what are

two distinct cases: leverage points in X, and outliers in y. A leverage
point in X, X, is a point that (because of its position relative to
other points in X) has considerable influence on the fit, regardless of the

An outlier in X y is a point, X Iy5, with a y5 significantly
deviant from the fit at X obtained by fitting with all but point j.

value

Some indication of the distinction between these two cases in evident

in

the relation: h h. + r?/SSR,

Residuals. The h

where SSR is the Sum of the Squared

Xy space. The h1 measure

measure leverage in

upon X and y, but

leverage in X space. The r/SSR depend
rroderate h. they can

provide

an indication of outliers in y

Two examples contrast the use of the h, and

First,

(1,

consider the data, in (x,y) pairs:

(1, 2), (1.5, 3), and

outlier in xl y

(2.'49,

for

the h and r/SSR.

.5), (2, 1), (3, 1.5), (.5, 1),

3.5) (see Figure 2). Point 7 is clearly an

though not a leverage point

in X.

We find h

.609,

higher

than any other h1 by .031, so h reveals the isolation of point 7 in xl y
space. This contrasts to h7 .1419, less than h3 = .14214,and rISSR .190,
less than
and

r/SSR

.300, revealing that

second in the list of outliers in

y

point 7

is second in leverage in X,

(though h7 is large enough to cause

The author is indebted to Steve Peters for deriving this inporant relationship.

—U—

'I

.7
3

I
I

0

3

I

Figure 2

'I

I'

.9
4
1'

8'

I?. I
Figure 3

x

- 12

—

us to perhaps consider rISSR mare significant).
As a second example, consider the data: (1, (i/2) + e1) for I 1, 2,... ,7

and

is a random variable of unifonn distribution in the interval

(0,

.1);

plus the points (4, 25) and (15, 7.5) (see Figure 3). Points 8 and 9 are

both outliers in X!Y, but point 8 is an outlier in y, not X, and point 9

is a leverage point in X, not an outlier in y. We find h
=

.

999989 and

.817, followed by h .268, so the h distinguish points 8 and 9

from the other points, but not from each other. However, h8

.816, r/SSR

h9

.878, and r/SSR

.122,

.001. Clearly,the h1 and r?/SSR

distinguish the leverage point in X from the outlier in y.

The

above serves as niotivation to search for leverage points (or

nore generally, leverage groups) strictly in
scaled residuals to identify outliers in y.
being

the

X matrix, using the

If hat matrix diagonals are

used to identify leverage points, this approach has the added advantage

that the h1, unlike the h, are directly computable from the QR decomposition
of X - which can be used to solve xTx = xT.

See Welsch nd
residual, r.

Kuh [8] for

the possibly more useful statistic , the studentized
the estimated error variance
r./(s(. (1-h. 1)1/2), where s, .

for the "nt i"'fit1'

. is

- 13

-

Appendix 2
H is most reliably computed

via the

QR decomposition of X [2],

which uses Householder transfoniations (fonming orrLthogonal Q)

X

to reduce

to upper-triangular R. QR decomposition by Householder transformations, with

coluiru-i pivoting, is more stable than Grain-Schmidt orthogonalization, and yeilds a more

nearly orthogonal Q than MDdified Gram-Schimidt in the event of rank degeneracy.
To compute H, we have H x(xTx)_lxT, X QR. Therefore,
H QR(RTQTQR)_l RTQT QQT (Q

is m by n here). The QR decomposition

routine used need not store Q explicitly, storing instead the u's which
define the Householder transformations, 1_T (the u's can be stored in a
lower triangular matrix). Each h1 is computed by applying the Householder

transformations to a vector representing the 1th column of 'n' then setting

h1 to the dot product of the vector (the first p elements) with itself.
The h. are more cheaply computed (at the price of extra storage) by forming
Q

explicitly.

— 1L —

Appendix 3
DPA

FORTRAN

INTEGER NM,MN,N,P,I,J,K,OUT,IN,IPLUS1,IERR,IV1(300),OUTDIS(510)
INTEGER 1V2(300) ,IV(300)
DOUBLE PRECISION X(510,15),NORMS(510),DENSE(510),TEMP,DFP
DOUBLE PRECISION MAX,NRM1 NRM2,DIFF,T1 ,T2,DIST,EPSRV1 (510)
DOUBLE PRECISION DFLOATE'SORTDABS
LOGICAL SORTOR

DPA0001O
DPA0002O
11PA00030

DPA0004O
DPA000SO
LIPA0006O
DF A 00070

DATA NM/510/,MN/15/

DPA0008O
B PA 00090

C:::::GET DATA MATRIX AND PARAMETER VALUES,

DPAOO100
DPAOO1 10

CALL MATRIX(NM,MNi'N,P,X,EPSrSORTORpOUT,IN)
11FF = 2.0110 * DSORT(DFLOAT(P))

DPAOO12O
DPAOO13O
B PA 00140

c:u::cENTER THE DATA.

DFAOO1SO
0 PA 00160

DO 20 I=1,P
TEMP = 0.01,0
DO 10 J=1,N
TEMP = TEMP + X(J,I)
10
CONTINUE
TEMP = TEMP / DFLOAT(N)
MAX = 0.0110
DO 15 J=1,N
X(J,I) = X(J,I) — TEMP
IF (BABS(X(JI)) .GT. MAX) MAX =
15

CC)NTINUE

DPAOO17O
DPAOO18O
BPAOO19O
DPAOO200
OPAOO21O
t'PA00220
LIPAOO23O
11PA00240
11PA00250

DABS(X(J,I))

DPAOO26O
DPAOO27O
OP A0 0280

c:::UN0RMALIzE THE DATA SUCH THAT THE OBSERVATION SPACE IS SCALED INTO 0PA00290
C:::.UA HYPERCUBE OF MAIN DIAGONAL LENGTH 1.
E1PAOO300
['PA 00310

DO 20 J=1,N
X(J,I) =
20 CONTINUE

(X(J,I)

/ MAX) / DFP

['0 30 I=1,N

DENSE(I) = 0.0110
OUTDIS(I) = 0
30 CONTINUE
C

c:::::coMpuTE ROW L2 NORMS.
C

DO 0 I=1,N

TEMP = 0.000
['0 40 J=i,P
TEMP = TEMP + X(IrJ)*X(I,J)
40
CONTINUE
NORMS(I) = DSORT(TEMP)
50 CONTINUE

DPAOO32O
BPAOO33O
DFAOO34O
[1PA00350

DPAOO36O
DPAOO37O
BPAOO38O
DPAOO39O
DPAOO400
DPAOO41O
[1PA00420

DPAOO43O
DPAOO44O
LIPAOO45O

DPAOO46O
DPAOO47O
0PA00480
11PA00490

c:uCOMFUTE DISTANCES SQUARED.
['0 105 I=1,N

IF (I .EQ. N) GOTO 105
IPLUS1 = I + 1
NRM1 = NORMS(I)
DO 100 J=IPLUS1,N
[lIST = 0.0110

DPAOO500
DPAOO51O
DPAOO52O
['PAOOS3O

0PA00540
DPAOOS5O
DPA00560
DPAOO57O

DO 70 K1,P

01FF = X(I,K) — X(J,K)
01ST = 01ST + DIFF*DIFF
CONTINUE

70

.

U::COMPUTE NORMAL (PARALLEL) DISTANCES,

C

75

NRM2
NORMS(J)
Ti = (01ST + NRM1*NRM1 — NRM2*NRM2 / (2.000*NRM1)
12 = (0191 + NRM2*NRM2 — NRM1*NRM1) / (2.ODO*NRM2)
DENSE(I) = DENSECI) + 1.000 / (EPS + DABS(T1))
DENSE(J)
DENSE(J) + 1.000 / (EPS + DABS(T2))

C
c:::::TALLY OUTDISTANCING POINTS.
C

IF (Ti •LE. 0.000) OUTDIS(I) = OUTDIS(I) + 1
IF (12 .LE. o,or'o) OUTDISJ) = OUTDISJ + 1
100
CONTINUE
103 CONTINUE
WRITE(OUT,1001)
DO 110 I=1,N
WRITE(OUT,1002) I,NORMS(I),EIENSE(I),QUTDIS(I)
110 CONTINUE
C

C:::::cHEC1c INDIVIDUAL POINTS OF INTEREST.
C

120 WRITE(OUT,1003)
C

c:n::GET POINT INDEX.
C

READ(IN,1004) K
IF (K*(2*N + 1 — 2*1<)) 130v200,150

130 WRITE(OUT,1006) N
GO TO 120
C

C:::: :C0MPuTE DISTANCES.
C

150 NRM1
NORMS(K)
DENSE(K) = 0.000
RV1(K) = 0,000
riO 170 I=1,N
OUTDIS(I) =
IF (I .EO. K) GO TO 170
0151 = 0.000
DO 160 J=1,P
01FF = X(K,J) — X(I,J)
0181
01ST + DIFF*DIFF
160
CONTINUE
NRM2 = NORMS(I)
Ti = ([lIST + NRM1*NRM1 — NRM2*NRM2) / (2.000*NRM1)
DENSE(I) =
RV1(I) = Ti
170 CONTINUE
IF (.NOT. SORTOR) 0010 173

I

ri

C:::::soRl- ANtI PRINT NORMAL DISTANCES TO POINT K.

c
CALL ISORT1(N,OUTDIS,DENSE)

.

WRITE(OUT,lOio)
00 172 I=1,N
J = OUTt'IS(I)

WRITE(OUT,loii) IJDENSEJ)
172 CONTINUE
GO TO 120

DPAOO62O

0PA00630
DPAOO64O
DPAOO6SO
0PA00660
DPAOO67O
DPAOO6SO
0PA00690
DPAOO700
DPAOO71O
DPA00720
0PA00730
DPAOO74O
DPAOO75O
t'PA00760

DPAOO77O
DPAOO78O
t'FA00790

DPAOO800
DPAOOB1O
DPAOO82O
DPAOOB3O
DPAOO84O
DPAOO85O
DPAOOB6O
DPAOOG7O
0PA00880
DPAQOG9O
DPAOO900
DPAOO91O
DPAOO92O
DPAOO93O
DPAOO94O
DPAOO95O
DPAOO96O
0PA00970
DPAOO98O
t'PA00990

DPAO1000
DPAO1O1O
t'PAO1O2O

DFA01030
EIPAO1O4O

DPAO1OSO
DPAO1O6O
DPAO1O7O
DPAO1OBO
t'PAO1O9O
t'PAOilOO

DPAO111O
DPAO1 120

DPAO113O
DPAO114O
DPAO115O
DPAO116O
DPAO117O
IIPAO118O

DPAO119O
DPAO1200
t'PAO121O
11PA01220

C

cu:::r'o STEM

DPAOO5BO
DPAOO59O
DPAOO600
DPAOO61O

LEAF DISPLAY OF NORMAL DISTANCES TO POINT K.

DPAQ123O

DPAO124O
0PA01250
DPAO126O
DPAO127O

175 WRITE(OUT,1008) K
CALL SLDSPY(RV1,IV1,1V2,1V3,OUTrpIS,Bo,N,300,IERR,OUT)
CALL IERRIO(IERR,OUT,16,16H STEM & LEAF
)

.

DPAO128O

::::ESTABLISH CUTOFF DISTANCE,

0PA01290

C

EIPAO1300

WRITE(OUT,1012)
READ (IN,1013) 01ST
WRITE(OUT,1009) K
DO 180 I=1,N
IF (I •EQ. K) GO TO 180
IF (DABS(DENSE(I)) •LE. 01ST) LdRITE(OLJT,1004) I
IF (DENSE(I) •LE. 0.000) WRITE(OUT,1005) I
180 CONTINUE
GO TO 120

DPAO131O
DPA01320
t'PA01330

C
200 STOP
C

1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
012
13

FORMAT(/40H
I
NORMS
FORMAT(14,2t112,3,218)

DENSITY

OUTE'IS

FORMAT(/35H POINT CHECKING (TYPE 0 TO STOP:
FORMAT(14)
FORMAT(I8)
FORMAT(/25H INDEX MUST BE FROM 1 TO
FORMAT(112,3012,3)
FORMAT(/18H STEM & LEAF
FOR
,14)
FORMAT(/15H NEB OUT
FOR
p14)
FORMAT(/20H
I PT
01ST
I)
FORMAT(2I4,012,3)
FORMAT(/20H INPUT CUTOFF VALUE
)
FORMAT(F1O.2)

C

END

I4)

)

I)

0PA01340
0PA01350
DPAO136O
DPAO137O
0PA01380
0PA01390
0PA01400
DPAO141O
DPAO142O
0PA01430
0PA01440
DPAO145O

DPAO146O
DPAO147O
DPAO148O
0PA01490
DPAO1500
DPAO151O
DPA01520
0PA01530
DPAO154O
DPAO1SSO
DPAO156O
DPAO1S7O

SUBROUTINE MATRIX(NM,MN,N,P,X,EPS,SORTOR,OUT,IN)
INTEGER NMMN,N,P,OUT,IN
DOUBLE PRECISION X(NM,MN) pEPS
LOGICAL SORTOR
C

c:::::PARAMETER tIECRIPTION:
C

C
C
C
C
C
C
C
C
C
C
C
C
C

C
C
C

ON INPUT
NM IS THE DECLARED ROW DIMENSION OF X.

MN IS THE DECLARED COLUMN DIMENSION OF X.
ON ouipui:

N IS THE NUMBER OF ROWS IN X.
P IS THE NUMBER OF COLUMNS IN X.

X IS THE DATA MATRIX (WITH NO CONSTANT COLUMNS).
EPS IS A SMALL SCALING CONSTANT USED IN COMPUTING
THE DENSITY VALUES FOR EACH POINT.

C

SORTOR IS A LOGICAL FLAG WHICH CONTROLS THE
POINT—CHECKING PROCEDURE:
IF SORTOR IS •TRUE. SORTED DISTANCES ARE DISPLAYED,
IF SORTOR IS •FALSE. STEM & LEAF AND A USER—SPECIFIED
CUTOFF POINT IS USED.
C
C
C
C

OUT IS THE UNIT OUTPUT DEVICE.
IN IS THE UNIT INPUT DEVICE.
EPS = 1.OD—6
SORTOR = •FALSE,
OUT = 6
IN = 5

c:::::usER SHOULD SUPPLY THE DESIRED MATRIX CALL HERE.
C

CALL OETMAT(NM,MN,N,P,X)
RETURN
END

MAT0001O
MAT0002O
MAT0003O
MAT0004O
MAT0005O
MAT0006O
MAT0007O
MAT0008O
MAT0009O
MATOO100
MATOO11O
MATOO12O
MATOO13O
MATOO14O
MATOO15O
MATOO16O
MATOO17O
MATOO18O
MATOO19O
MATOO200
MATOO21O
MATOO22O
MATOO23O
MATOO24O
MATOO25O
MATOO26O
MATOO27O
MATOO28O
MAT00290
MATOO300
MATOO31O
MATOO32O
MATOO33O
MATOO34O
MATOO35O
MATOO36O
MATOO37O
MATOO3BO
MATOO39O
MATOO400
MATOO41O
MATOO42O
MATOO43O
MATOO44O
MATOO45O

- 18

—

Appendix 3 (cont.)

Other FORTRAN Routines
Used by DPA FORTRAN

ISORU

sorts N

real values in increasing

order through an

SLDSFY

is

part of a FORTRAN package implementing

Tukey's
It was

and
IERRIO

integer index vector.

Stem-and-Leaf Display [7].

written by D. Hoaglin and

appears

in

ROSEPACK

version 0.1,

is also in ROSEPACK version 0.11.
return

S. Wasserman
developed at NBERJCRC.

It prints an integer error

code along with a message. It

WRITE statement

and

FORMAT

statement.

can be replaced

by a

— 19 —

Appendix

start
ECUTION BEGINS.,

NORMS

I
1

2
3

4
5
6

DENSITY
0. 20711+04

14

o .93711-01

0.11411+04

3

0.11411+00

0 •

560D+03

96611—01
• 69711—01
• 90311—01

0.
0.
0.
0.
0.
0.

66911+03
36611+04
40511+04
15411+05
27311+03
30211+04

o •

0
0

7
a
9

0 •

24811-01

0.13411+00
0.86411—01
0.11211+00
0.92411—01

10
11

12
13

0.700D+03
0.11311+04

0 • 79711—01

0 •

0. 11711+00
0.9 1211—01

14
15
16

0.10211+00
0 •

17
18
19
20
21

OUTDIS

45411—01

o •

67211—01

0.14711+00
0. 11411+00

0.10011+00

29311+04

1

2
5
1

5
0
4
0
2
8

0,41811+03

1

0.
0.
0.
0.
0.
0.

2

24611+04
83811+03
56311+04
27111+03
41211+03
90411+03

2
7
0

0
1
1

74211—0 1
0. 48911+00
0. 49411+00

0.12011+04
0. 30311+03
0. 39211+03

2

0.50011+00

0.30311+03

0

0 •

1

POINT CHECKING (TYPE 0 TO s1op:
:::.

8

STEM & LEAF

FOR

8

STEM-AND—LEAF DISPLAY, N =

(
1
1

2

.

3
3
5
8

9
3
11

8
8
6

3

UNIT =

0.100011—02

23

)

0 10
11
2 17
3 15

41

5 147

61178
7 13

8 1348
9 I 169
10 I

11 126
12 1359
HI I

0.4893

0.4934

0.497A

IERR =

0 STEM & LEAF

INPUT CUTOFF VALUE
>.04
OUT

FOR

8

POINT CHECKING (TYPE 0 TO STOP)
10

::

STEM & LEAF

FOR 10

STEM—AND--LEAF

DISPLAY, N =

UNIT =

0,1000D--02

3

0 1057

3
6

2 1019

)

11

3 189

8
11

4

I 456

5 1666

3

11

13

HI I

3

IERR =

23

0.5431

0.5480

0 STEM & LEAF

INPUT CUTOFF VALUE
:::.

• 01

NEB OUT

FOR

10

17

18

POINT CHECKING (TYPE 0 TO STOPS
:::.

17

STEM & LEAF

FOR 17

STEM—AND—LEAF DISPLAY, N =

( UNIT =

0.10000—01 )

23

0.5530

0 10

1

T13
F1455

2
5

S

S I 6667777
0. I 8889

11

1 lOll

7
4

TI

F15

4
3
IERR =

HI I

0.6097

0.6150

0,6203

0 STEM & LEAF

INPUT CUTOFF VALUE
>0

NEB OUT

FOR

17

POINT CHECKING (TYPE 0 TO STOP):
18

:>

STEM & LEAF

FOR 18

STEM—AND—LEAF DISPLAY, N =

( UNIT =

11
21

3 17
4 1066
511236

10

6 148

2
11

7

I 0457

8 17

7
6
4
4

9 104
11 lB
10 I

3

HI I

0.4677

0 STEM & LEAF

INPUT CUTOFF VALUE
. .01

EB OUT

FOR

18

10

POINT CHECKING (TYPE 0 TO STOP:
:>

23

)

0 109

2

2
2
3
6

IERR =

0.l000r'—o2

23

0.4718

0.4759

23

STEM—AND--LEAF DISPLAY N =

( UNIT =

0.1000D—01

4.19
51
T133
F144

4
4
6
8
7
8
3

S I 6777777
5. I 88999

6 100

T13

1

IERR =

0.0

LO I

3

0 STEM & LEAF

INPUT CUTOFF VALUE
NEB OUT

FOR

23

21

22

POINT CHECKING (TYPE 0 TO STOP):

R; 1=0.20/1.16 16:42:39

0,0056

)

0.0112

— 23

—

Appendix 5

The Sterling 1)ata (X Matrix)

10.J TI
3
1
S

6
1

10
11

12
13
14

15

16
17

18
19
20
21

22
23
24

UN

LABEL
AUSTRALIA
AUSTRIA
BELGIUM
BOLIVIA
BRAZIL
CANADA
CHILE
CHINA(TAIWAN)

46,31
27.04
25.06
23.31
25.62

INDIA
IRELAND
ITALY
JAPAN
KOREA

2/

NORWAY

$1

34
35

41
43

MALTA
NETHERLANDS
NEW ZEALAND

NICARAGUA
PANAMA
PARAGUAY
PERU

PHILL1P1NES
PORTUGAL
SOUTH AFRICA

24.52

27.01
41.74

21.0
32.54
25.95
24.71

32,61
45.01
43.56
41.10
44.19
46.26

20.96
31.94

31,92

SPAIN
SWEDEN
SWITZERLAND

27.74
21.44

TURKEY
TUNISIA
UNITED KINGDOM
VENEZUELA
ZAMBIA
JAMAICA
URIJuLJAY
LIBY(,

.0

46.05
47.32
34.03
41.31
31,16

SOUTH RHOLIEgIA

UNITEr' STATES
4/.,

39.74
24,42

LUXEMBOURG

30

1.67

ECUADOR

ICELAND

MALAYSIA

15i07,99

4,43

DENMARK

GUATEMALA
HONI'URAS

2329.68

4.41

41,09
42.19
31.72
44.75
46.64
47.64

FINLAND
FRANCE
GERMANY F.R.
GREECE

2.87

23.11

COLOMBIA
COSTA RICA

25
26
28

29.35
23,32

23.49
43.42
46.12

23.27
29.01
46,4

45.2
41.12
20.13
43.69
47.2

0.83
2.85
1.34

0.67
1.06
1.14
3.93
1.19
2.37

4.7
3.35
3.1
0.07

0.50
3,08

0.96
4.19
3.40
1.91
0.91

3.73
2.47
3.67
3.25

3.17
1.21

1.2
1.05
1,28

1.12

2.85
2.28
1.52
2.87
4,54

3,73
1.08
1.21

4.46
3.43
0.9
0.56
1,73
2.72

2.07
0,66

2108.47
728,47
2982.88
662.86
289.52
276.65
471.24
2496.53
207,77
1681.25
2213.82
2457.12
870.85
289.71

232,44
1900.1
88.94
1139.95
1390.
1257.20
207.63

2449.39

601.05
2231.03
1740.7

1487.52
325.54
560.56
220.56
400.06
152.01
579.51
P651.11

250.96
760.79
3299.49
2630.96
309,66
249.07
1813.93
4001.09
813.39
138.33
3110,47

2.07
3.93
3,02

0.22
4.56

2.43
2.67
6.51

3.00
2.8
3.99
2.19
4.32
4.52
3.44
6.28
1.49

3.19
1.12
1.54

2.99

3'4
8.21
1.57
8•12

3.62
7.66
1.76

2.48
3.61
1,03
0.67
2.
7.48
2.19
2.

3,01

2.7

2.96
1.13
2.01

2,45
0.53
5.14
10.23

766.51

1.09

123.

16,71

242.6

5.011

— 2L —

References

1. Andrews, D. F. and Pregilx)w, D., "Finding the Outliers that Matter,"
Technical Report No • 1, University of Toronto, Graduate Department
of Statistics, March 1977.
2.

Businger,

P. and Golub, C., 'Linear Least Squares Solutions by Householder

Thansformations, in Wilkinson;' T. and Reinsch, C., (Eds.), Handbook for
Automatic Corriputat ion, V.2: Linear Algebra, Springer-Verlag, p. 111-118,
1971, Prepub. in Math. 7, p. 269—76. 1965.

3. Gnanadesikan, R., "Methods for Statistical Data Analysis of
Multivariate Observations," John Wiley and Sons,

New York, 177,

p. 82—120, 258—28'4.

4. Kendall, M. C. and Stuart, A., "The Advanced Theory of Statistics,"
Vol. 3, Ed. 2, Hafner Publ. Co., New York, 1968, p. 338-9.

5. Hoaglin, D. C. and Welsch, R. E., "The Hat Matrix in Regression and

ANOVA," WP 901-77, Alfred P. Sloan School of Management, MIT, Jan. 1977.

6.

Oliver, D., "Troll Experimental Programs: CLUSTER ANALYSIS,"
Computer Research Center for Economics and Management Science,

National Bureau of
7.

Economic Research, Inc., August 1975.

Thkey, J. W., "Exploratory Data Analysis," Addison-Wesley, 1977.

8. Welsch, R. E. and Kuh, E., "Linear Regression Diaguostics,"
MIT and NBER, Working Paper No. 173, March 1977.

