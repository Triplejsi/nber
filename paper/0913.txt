NBER WORKING PAPER SERIES

PANEL GAlA

Gary

chamberlain

working Paper NO. 913

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge MA 02138

June 1982

The research reported here is part of the llBR's research program
in Labor Studies.
Any opinions expressed are those ot the author
and not those ot the National Bureau ot Economic Research.

FIBER Working Paper 11913
June 1982

Panel

Data

ABSTRACT

We consider

linear

neity and show that it

invariant

predictor definitions of noncausality or strict exoge—

is restrictive

to assert that there exists a time—

latent variable c such that x is strictly exogenous conditional on c.

A restriction ot this sort is necessary to justity standard techniques tor
controlling tor unobserved individual eftects.

There is a parallel analysis tor

multivariate probit models, but now the distributional assumption for the individual eftects is restrictive.

This restriction can be avoided by using a con-

ditional likelihood analysis in a logit model.

Some ot these ideas are

illustrated by estimating union wage eftects tor a sample ot Young Men in the
National Longitudinal

Survey.

The results indicate that the lags and leads

could have been generated just by an unobserved individual eftect, which gives
some

support tor analysis ot covariance—type estimates.

cate a substantial omitted variable bias.

These estimates indi-

We also present estimates ot a model

ot temale labor torce participation, focusing on the relationship between par-

ticipation and fertility. Unlike the wage example, there is evidence against
conditional strict exogeneity;

approaches

if

we ignore this evidence, the probit

give conflicting results.

Gary

Chamberlain

Department ot Economics
University of Wisconsin
Madison,
(608)

Wisconsin
262—7789

53706

and logit

PANEL DATA
TABLE OF CONTENTS

1.

INTRODUCTION AND S1Th2(ARY

2.

SPECIFICATION

AND

IDENTIFICATION:

1

LINEAR

MODELS

2.1 A Production Function Excnpie
2.2 Fi-zed

2.3

Effects

cmd

12
12

Incidental Parczneters

Rcmdan Effects cmd Specification Analysis

2.4 A Consumer Demand Fxriple

15
19

2.4.a Certainty
2.4. b Uncertainty

2.4.c

2.5 strict

Labor

SuppZy

Eogentity Conditional on a Latent Variable

2.6 Lagged De&dent Variables
2.7 Reeithsal

23
26

Covairteea: Heteroskedasticity cud Serial
31

Correlation
2.7. a ifeteroekedasticity

2.7.b
3.

SeriaZ Correlation

SPECIFICATION

AND

IDENTIFICATION:

NONLINEAR MODELS

32

3.1

A Rcrzdom Effects Probit Model

32

3.2

A Fixed Effects Logit Modet: Conditional Likelihood

37

3.3 Serial

Correlation cznd Lagged Dependent tTartables

3.4 Duration Models

43
51

4.

INFERENCE

55

4.1 The Estimation of Linear Predictors

56

4.2 Inrposing•Restrictions: The Minimum Distance Estimator

58

4.3 Simultaneous Equations:
Three—Stage

44 Asymptotic

Least

Squares

Estimator

63

71

4.5 Z&ltivariate Probit Madel8

74

EMPIRICAL

78

APPLICATIONS

5.1 Linear Models: Union Wage Effects

6.

Two—coid

Efficiency: A Comparison with the Quasi-

Maximum Likelihood

5.

A Ggneralization of

78

5.2ATonline'Mode1s: Labor Force Participation

a5

CONCLUSION

90

APPENDIX

102

FOOTNOTES

104

REFERENCES

109

INTRODUCTION AND SUt(ARY

1.

The paper has tour parts:
specification
applications.

the specification ot linear models; the

nonlinear models; statistical

ot

and empirical

interence;

The choice of topics is highly selective. We shall focus

on a tew problems and try to develop solutions in some detail.

The discussion of linear models begins with the following specification:

+ ci •

(1.1)

(1.2)

E(uj)xji# .

.

.

. , c)

= 0

(1=1,

N; t-1,

I)

For example, in a panel of farms observed over several years, suppose that
.th

is a measure of the output of the 1— farm in the t— season,

is a

measured input that varies over time, c. is an unmeasured, fixed input
reflecting soil quality and other characteristics ot the tarm's location,
reflects unmeasured inputs that vary over time such as rainfall.

and

Suppose that data is available on (x11, .

for each of a large number of units, but
section regression of

on

.

.

. X, i'

y)

.

is not observed. A cross—

will give a biased estimate of

if c is

correlated with x, as we would expect it to be in the production function
example.

Furthermore, with a single cross section, there may be no internal

evidence ot this bias.

If T > 1, we can solve this problem given the

assumption is (1.2). The change in y satisfies

-

- x11)

and the leaSt squares regression of

-

—

x11)
on

—

Xj1

provides a

2

consistent

)

estimator of B(as HT +

if the change in x has sufficient

A generalization of this estimator when T > 2 can be obtained

variation.

from a least squares regression with individual specific intercepts.

The restriction in (1.2) is necessary for this result. For example,
consider

the

tollowing

=

autoregressive

+

+ uit

E(ujtIYj,t...j Cj) =

It

specification:

0.
—

is clear that a regression of

, -l

—

', 2T

u.

—

provide a consistent estimator of 6, since

will

—

on

not

is correlated with

Hence it is not sufficient to asstune that

c.)

Much

in

of our discussion wi U be directed at testing the stronger restriction

(1.2).

Consider the (inimtn mean—square error) linear predictor of
conditional on

(1.3)
Given

f

.

.

.

.

E*(cjIxil

XIT)

1I +

+ .

.

.+

the assumptions that variances are finite and that the distribution

(x1, •.

additional
predictor.

x, c) dcas not depend upon i, there are no

restrictions in (1.3); it is simply notation for the linear
Now consider the linear predictor ot

E*(yjtlxil,

.

.

.

.

+

Xj]• + .

given x1, .

.

.+

.

.

.

as the (t,s) element.

Form the TXT matrix Ti with it

Then the restriction

in (1.2) implies that rt has a distinctive structure:

U

=

+

where I is the I'<T identity matrix, £ is a TX1 vector of ones, and

An (X1,

.,

A,).

A test for this structure could usefully accompany

estimators of S based on change regressions or on regressions with
individual

specific

this tormulation suggests an

Moreover,

intercepts.

which is developed in the inference section.

alternative estimator for ,

This test is an exogeneity

test

and it is useful to relate it to
The novel teature is that we

Granger (1969) and Sims (1972) causality.
are testing tor noncausality conditional on
that ta).

is

a latent variable.

Suppose

the first period of the individual's (economic) life. Within

the linear predictor context, a Granger definition of "y does not cause
x conditional on a latent variable &' is

(1.9)

c)

Yu'

...,

=

x,

(t1,Z,...).

c1)

A Sims definition is

.

E*(yjtIxj x12, $

, c) E*(yj(xji, •

•

,

c) (t1,2,...).

In tact, these two definitions imply identical restrictions on the
covariance
directly

matrix of (x1

.

. •, X, y1

.

.•

The Sims form fits

into the IT matrix framework and implies the following restrictions:
B +

4

where B is a bwer triangular matrix and I is a T><1 vector.

We show how these nonlinear restrictions can be transformed into

linear restrictions on a standard simultaneous equations model. We show
also how a

A.' term can arise in an autoregressive model from the pro-

jection of an initial condition onto the x's.

In Sectin 3 we use a mui.tivariate probit model to illustrate the new

issues that arise in models that are nonlinear in the variables. Consider
the

tollowing

specification:

= S

+

+ ult

1 if it 10,
0

where,

otherwise

condit Lanai On

(i1, .

. .

. N; t1, .

C.1, ' Xv C1, the

.

. .

distribution ot

.

.

.

.u

is multivariate normal (J(O, ) with mean 9 and covariance matrix =
We

observe

.

.

.

.

but we do not observe

y11,

for a large number of individuals,

For example, in the reduced form of a labor

force participation model, y can indicate whether or not the
individual wc'rked during period t, x1 can be a measure ot the presence
of young children, and

can capture unmeasured characteristics of the

individual that are stable at least over the sample period.

In the

certainty model of Heckman and MaCurdy (1980), c, is generated by the
single lite-time budget constraint.

If we treat the

as parameters to be estimated, then there is a

severe incidental parameter problem.

The consistency of the maximum

likelihood estimator requires that T +

inference with N +

, but

we want to do asymptotio

for fixed T, which reflects the sample sizes in the

panel data sets we are most interested in.

So we consider a random

effects estimator, which is based on the following specification for the
distribution ot c conditional on x:

(1.4) c =n +

+ . .

where the distribution of

.+

+ vi,

conditional on

is N(O, 02).

This is similar to our specification in (1.3) for the linear model, but
there is an important difference;
whereas

predictor,

embodies

(1.4)

(1.3) was just notation for the linear
substantive

We are

restrictions.

assuming that the regression function of c on the x's is linear and that
Given these asst.nuptions,

the residual variation is homaskedastic and normal.
our analysis runs parallel to the linear case.

There is a matrix It of

multivariate probit coefficients which has the following structure:

II

is

where d±ag{cC1, . . . .

+

=

with

T }[S I + - '1,

diag{,

a diagonal matrix of normalization factors

We can impose these restrictions to obtain an
which is consistent as N

estimator of

for fixed T.

We can also

t

test whether It in tact has this structure.

A quite different treatment of the incidental parameter problem is
a

possible with a logit functional form for

zT

t=1y.it

1]x1,

ci) .

The sum

provides a sufficient statistic for ci. Hence we can use the

distribution ot

.

conditional

on

.

.

.

.

to obtain a conditional likelihood function that does not depend upon a..
Maximizing it with respect to

N -

'

as

bold

for

provides an estimator that is consistent as

fixed T, and the other standard properties

for maximtn likelihood

The power of the procedure is that it places no restrictions

well.

on the conditional distribution ot c given x.

It is perhaps the closest

A shortcoming is that

analog to the change regression in the linear model.

the residual covariance matrix is constrained to be equicorrelated. Just
as in the probit model, a key assumption is

.

(1.5)

and

.

.

=

. X1, c)

1Ix Cj)t

we discuss how it can be tested.

It is natural to ask whether (1.5) is testable without imposing

the

various functional form restrictions that underlie our tests in the probit
and logit cases.

some definitions.

First,

period of the individual's (economic)

on

...,

x.

does not cause x" is that

1

is the initial

life; an extension of Sims' condition

for x to be strictly exogenous is that
conditional

Suppose that t =

is independent of x1, x2, II!

An extension of Granger's condition for "y
conditional on

is independent ot yl

Unlike the linear predictor case, now strict exogeneity is
weaker than

noncausality.

x41,. x2, ...

conditional on x1, •.., xcznd onY1

is strictly exogenous and in addition
conditional on
static.

be independent of

Noncausality requires that
.

.

.

is independent of

If

.

x

i..,

then we shall say that the relationship of x to y is

Then our question is whether it is restrictive to assert that there
exists a latent variable c such that the relationship of x to y is static
conditional on c.

We know that this is restrictive in the linear predictor

case, since the weaker condition that x be strictly exogenous conditional
on c is restrictive.

there are no restrictions when we

Untortunately,

replace zero partial correlation by conditional independence,

it follows

that conditional strict exogeneity is restrictive only when combined with
tunctional

specific

torms — a

truly nonparaxaetric test cannot exist.

Section 4 presents our framework for inference. Let 4 3
(1,

.

.

.

. x1, y1,

.

.

y)

is independent and

and assume that

identically distributed (1.1.4.) for i

Let

1,2

be the vector

formed from the squares and cross—products of the elements in
framework is based on a simple observation:
predictor coefficients is a function of

Our

the matrix II of linear
if

is i.i.d. then so is

w.; hence our problem is to make inferences about a function of a population mean under random sampling.

This is straighttorward and provides

an asymptotic distribution theory for least squares that does not require
a linear regression function or homosked.asticity.
Stack the columns of II' into a vector 11 and let It a

r

Then

hQ.i),

where

the limiting distribution for least squares is normal

with covariance matrix
Bit'

We impose restrictions on II by using a minimum distance estimator.
The restrictions can be expressed es 1i

g(e), where e is free to vary

8

within some set T.

Given the sample ntean w =

E!1

w./N, we choose 9

to minimize the distance between w and g(6) , using the following distance
tunction:

miii [

OcT

- g(Q)]t

[-

r'(w.)
:1-

where V(w.,) is a consistent estimator of V(w1)

This is a generalized

least squares estimator for a multivariate regression model with nonlinear
restrictions on the parameters; the only explanatory variable is a constant
term.

The limiting distribution of

r

' —1
V

LW

is normal with covarlance matrix

wrj

An asymptotic distribution theory is also available when we use some
A_i

matrix other than V (w1) in the distance function.
that V (wi) is the optimal choice.

This theory shows

However, by using suboptimal norms,

we can place a number of commonly used estimators within this framework.
The results on efficient estimation have some surprising consequences.
The simplest example is a .uni-variate linear predictor: E*(3rtx1,
+
ITQ

+ 1t2x2. Consider imposing the restriction that

= 0;

x2)

=

we do

not want to maintain any other restrictions, such as linear regression,
homoskedasticity,

F

or normality. How shall we estimate it1? Let

be the estimator obtained from the least sares regression

of y on x1, x2. We want to find a vector of the form e, 0)
A
A

as close as

'I.

possible to (iT1, ff2) using V

('IT) in the distance function. Since we

are not using the conventional estimator of V(), the answer to this

minization problem is not, in general, to set

b

the estimator

obtained from the least squares regression of y on x1. We can do better
by using
b

and

is zero if iT2 = 0,

+ TTr2; the asymptotic mean of

and if

are correlated, then we can choose t to reduce the asymptotic

variance below that ot b

PC1
This point has a direct counterpart in the estimation of simultaneous
equations.

The restrictions on the reduced form can be imposed using a

minimin distance estimator.

This is more efticient than conventional

estimators since it is using the optimal norm.

In addition, there are

generalizations of two— and three—stage least squares that achieve this
efticiency gain at lower computational cost.

A related application is to the estimation of restricted ovariance
matrices.

Here the assumption to be relaxed is multivariate ni ina1ity.

We show that the conventional maximum likelihood estimator, whch assumes
normality,

a minimum distance estimator.

is asymptotically equivalent to

But that m±ninu

distance estimator is not. in general, using the optimal

norm. Hence there is a feasible minimt.un distance estimator that is as

least es good as the maximt likelihood estimator; it is strictly better
in general for nonnorinal distributions.

The minimum distance approach has an application to the multivariate

probit model of Section 3. We begin by estimating T separate probit
specifications in which all leads and lags of z are included in the
specification for each

=

+

+ . .

.+

'U

where F is the standard normal distribution function.

Each of the T

probit specifications is estimated using a maximum likelihood program
for univariate probit analysis.

There is some sacrifice ot efticiency

here, but it may be outweighed by the advantage of avoiding numerical
integration.

Given the estimator for 11, we derive its asymptotic

covariance matrix and then impose and test restrictions by using the
minimum

distance

estimator.

Section 5 presents two empirical applications, which implement the,

specifications discussed in Sections 2 and 3 using the inference procedures
from Section 4.

The linear example is based on the panel of Young Men

in the National Longitudinal Survey (Parties); y is the logarithm of the
individual's hourly wage and

includes variables to indicate whether or

not the individual's wage is set by collective bargaining; whether or not

he lives in an SHSA; and whether or not he lives in the South. We present
unrestricted least squares regressions ot

on

.

.

.

.

xT,

and we

examine the form of the It matrix. There are significant leads and lags,

but there is evidence in favor of a static relationship conditional on a
latent

variable; the leads and lags could be interpreted as just due to c,

with

.

..,. xT,

a) =

@

+ c.

The estimates of B that control for

c are smaller in absolute value than the cross—section estimates. The
union coefficient declines by 402, with somewhat larger declines for the
SNSA and region coefficients.

The second application presents estimates of a model of labor force
participation.

It is based on a sample of married women in the Michigan

Panel Study ot Income Dynamics.

We focus on the relationship between

11

participation and the presence ot young children.
fl

The

unrestricted

matrix for the prcibit specification has significant leads and lags;

but, unlike the wage example, there is evidence here that the leads and
lags are not generated just by a latent variable.
restriction,

then the resulting estimator of

It we do impose this

indicates that the cross—

section estimates overstate the negative effect of young children on the
woman's

participation

probability.

The estimates for the logit functional form present some interesting
contrasts to the probit results.

The cross—section estimates, as usual,

are in close agreement with the probit estimates.

But when we use the

conditional maximum likelihood estimator to control for c, the effect of
an additional young child on participation becomes substantially more

negative than in the cross—section estimates; so the estimated sign of
the bias is Opposite to that of the probit results.
method is having a first order ettect on the results.
ot possible explanations.

Here the estimation
There are a variety

It may be that the unrestricted distribution

for c in the logit form is the key.

Or, since there is evidence against

the restriction that

XjT) c) flftk' c)
perhaps we are finding that imposing this restriction simply leads to
different biases in the probit and logit estimates.

12

SPECIFICATION AND

2.

LINEAR MODELS

IDENTIFICATION:

2.1. A Production Function £xcnple
We

shall begin with a production function example, due to Mundlak

Suppose that a farmer is producing a product with a Cobb-Douglas

(1961)

technology:

(Q<$<1;
where

11, ....Nt4

th
is the logarithm of output on the i—

farm

in season t,

is

the logarithm ot a variable input (labor), c represents an input that is
fixed over time (soil quality), and u1 represents a stochastic input
(rainfall), which is not under the farmer's control. We shall assume that
the farmer knows the product price (P) and the input price (W), which do
not depend on his decisions, and that ha knows ci.
sion,

however, is made before knowing

where

=

S

+bt

[E(a'tIJ)]

and we shall assume that Xii: is

Then the tactor demand equation is

chosen to maximize expected profits.

(2.1)

The factor input deci-

+Zet (P/W)

+ c}/(1—),

is the infortiation set available to the farmer when he chooses

and we have suppressed the i

subscript.

is independent of Jt, so that the farmer cannot

Assume first that

do better than using the unconditional mean.

X,

c) =

+

C.

In that case we have

13

So it c is observed, only one period ot data is needed: the least squares
regression ot

c provides a consistent estimator of S as

on

N +

cz,

Now suppose that c is not observed by the econometrician, although it
is known to the farmer. Consider the least squares regression of
using just a single cross—section ot the data.

The

population

on

counterpart

is

E*(y11x1) a

+

where E* is the minimum mean—square error linear predictor (the wide—sense
tunction)

regression

11

= Cov(y1,

x1)/V(x.Q, it

We see from (2.1) that c and

E(y1)

-

it E(xi2.

are correlated; hence 't

not converge to B as N +

squares estimator of S does

and the least

, Furthermore,

with

a single cross section, there may be no internal evidence of this omitted—
variable bias.

Now the panel can help to solve this problem.

a least squares regression

to include farm specific indicator variables:

of

ott

(i")., .

.

. , N;

zeros except for a one in the

t-l
position.

c. as a set of parameters to be estimated.

Mundlak's solution was

is a bP<L vector ot

T), where

So this solution treats the
It is a 11tixed ettects"

tion, which we shall contrast with 11random ettects."

solu-

The distinction is

that under a fixed effects approach, we condition on the

so that their

distribution plays no role. A random effects approach invokes a distribution tor c.

In a Bayesian framework, S and the Cj would be treated sym-

metrically, with a prior distribution tor both.

Since I am only going to

J.

14

use asymptotic results on inference, however, a !lgentleu

distribution

That this need not be true for the c. is one of

will be dominated.

for

prior

1

the interesting aspects ot our problem.

We shall do asymptotic inference as N tends to infinity for fixed T.

Since the number of parameters (c.) is increasing with sample size, there
is a potential "incidental parameters" problem in the fixed effects approach.
This does not, however, pose a deep problem in our example.

The least

squares regression with the indicator variables is algebraically equivalent
to the least squares regression of

t1

fl,

=

where

f;=1 y/T, x

to a least squares regression of

-

the

Ix.

.,

—

— x1)

—

on

(id

If

=

—

on xi2 —

x11.

N;

T = 2, this reduces
Since

$(x2 —

least squares regrt ision will provide a consistent stimator of 8 if

there is sufficient variation in

2.2 Fixed ffeats and

—

x11.

3

Incidental Parameters

The incidental parameters can create real difficulties. Suppose that
is
periods

independently and identically distributed (i.i.cl.) across farms and

with V(u.)

likelihood
N +

a

Then under

a normality assi.ption,

estimator of a2 converges (almost surely) to

with T fixed.

tollowing

as

The failure to correct for degrees of freedom leads

to a serious inconsistency when T is small.
the

the maximum

autoregression:

For another example,

consider

15

v

'ii

=av.10+c.1 +u.1
=

yi2
Assume that tj

ii

+ ci + u2.
and u are i.i.d. conditional on y.
il

and ci, and that they

Consider the likelihood tunction

follow a normal distribution (N(O, a2)).

corresponding to the distribution of (17 y12) conditional on
The log-likelihood function is quadratic ir ,

cl

and C1.

c (given

and

the maximum likelihood estimator of S is obtained from the least squares
—

regression of

y11

N)

is correlated

Since

and

with

=

—

3niz

it

(i-i

—

on.

— y10) +

•

is clear that

a

(2.9)

— Y10)

'il Y0)

and the maximum likelihood estimator of
tribution of yio conditional on

is not con°istent. It the dis-

does not depend on S or

then the

likelihood function based on the distribution of (y±, yil, y12) conditional on Cj gives the same inconsistent maximum likelihood estimator of

8. If the distribution of (yio, yj1, 2)

is

obtained from the least squares regression of

converges, as N +

2.3. Rtdom

, to

stationary, then the estimator
—

on

a

(e—1)I2.5

Effects wtd Specification Analysis

We have seen that the success of the fixed effects estimator in the

production function example must be vieweu with some caution. The mci—

16

dental parameter problem will be even more serious when we consider nonlinear
So we shall consider next a random effects treatment of the pro-

models.

duction tunction example; this will also provide a convenient framework
for specification analysis.6
Assume that there is some joint distribution for (xil

XiT c.),

which does not depend upon i, and consider the regression function that does

not condition S c:

E(YjFx±i .

•

+

, xt)

•

.

The regression function for c1 given x.

will

generally be

predictor:

(2.2)

P + X1xi1 + eli +

xiT)

where X
(2.2)

.

But we can specify a minimum mean—square error

some nonlinear tunction.
linear

.

.. Xfl)d

+

= 111(x1) Cov(x ci). No restrictions are being imposed here —

is simply giving our notation for the linear predictor.
Now we have

E*(yjjx) a 4' +

+

Combining these linear predictors for the T

mulcivariate
(2.3)

8

linear predictor:

E*(zlxj) a
II

periods gives the following

+ It

Cov(, x!) v1(x1)

I + LA',

17

I

••,

where y =

is the lxi identity matrix, and Z is a T'<l

vector ot ones.

The ri matrix is a useful tool for analyzing this model. Consider first

estimation

the

it I =

of 6:

/

irk)_

we have

2

+

A]_ X2

x1

t•\

Hence

=ir

-n =71
11

21

—iT
12

22

So given a consistent estimator for TI, we can obtain a consistent estimator
tor 5.

The estimation of lE

is

almost a standard problem in multivariate

regression; but, due to the nonlinearity in

we are estimating only

a wide—sense regression tunction, and some care is needed.

It turns out that

is a way of looking at the problem which allows a straightforward

there

treatment, under very weak assumptions. We shall develop this in the section
on

interence.

We see in (2.3) that there are restrictions on the rt matrix. The off—

diagonal elements within the same co1 of if are all equal. The Th elements
of It are functions of the t+1 parameters '
obvious

specification test.

specification that TI =

I.

.

.

.

. Xpe

This suggests an

Or, backing up a bit, we could begin with the
Then passing to (2.3) would be a test for

whether there is a time—invariant omitted variable that is correlated with
the x's. The test of rt

$

I

+ £ A'

against an unrestricted Tt would be an

oimibus test of a variety of misspecifications, some of which will be con—
sidered next.

18

Suppose that there is serial correlation in u, with u Pu1 +

where

is independent of

and we have suppressed the i subscripts.

Now we have

u

Pu.

E(e t1J) =

So the factor

w
E(e t)

demand equation becomes

= (Zn

Suppose

t1

+ £n [E(et)1 +

Lvi

(PIW)

+

+

that there is no variation in prices across the farms, so that the
term is captured in period specific intercepts, which we shall suppress.

Then we have
+ (1_P')(Xix, + . .

where
So

+

=

matrix would indicate a distributed lead, even

after

If instead there is a first order moving average,

U

the if

tar c.

.+

then

a
E(e tlJ)

Pw

a

and a bit at algebra gives

w
E(e t)

controlling

W+

19

+

-

E(YIxi,

.

Once again there is a distributed lead, but now

.+

+

is not identified from

matrix.

the it

2.4.

.4 CordurneP

2.4.a.

Certainty

Demand Excarrple

shall follow Ghez and Becker (1975),

We
and

.

Heckzan and MaCurdy (1980),

MaCurdy (1981) in presenting a life—cycle model under certainty,

suppose

that the consumer is maximizing

t
=

t-1
subject to

¶

z 1—(t—1)

where P

1

—

1

0 (tnt

I),

is the rate of time preference, Y—l is the (nominal) interest

is consumption

rate,

c B,

in

period t,

is the price of the consumption good

in period t, and B is the present value in the initial period ot litetime
income.

In this certainty model, the consumer

single lifetime

constraint.

budget

It

the optimal consumption is positive in every period, then

U(C) =

(YP)

A convenient functional foro is u(c) *

(2.4)

faces a

+ p(t—1) + c + Ut

Atc6/o

(A>O 6 < 1);

then we have

20

where yt =

Z

Ct,

(1_)_1 Z At,

=

= Zn Pt, C =

(—1)

8 = (—1) -1, and

1

= (1_o)_1 Let (yp).

Note that

c is

determined by the marginal utility of initial wealth: u(C1)/P1 = 3V/DB.
We shall assume that At is not observed by the econometrician, and that
it is independent ot the F's.

Then the model is similar to the production

function example if there is price variation across consumers as well as
There will generally be correlation between c and (x1

over time.

XT).

As before we have the prediction that IT = $ I + Z X', which is testable.

A consistent estimator of S can be obtained with only two periods of data
since

(2.25)

We

= $

y

(x -

xci)

+

+ Ut —

shall see next how these results are affected when we allow for some

uncertainty.

2.4.b.

Uncertainty

We shall present a highly simplified model in order to obtain

some explicit results in the uncertainty case. The consumer is maximizing

E( Z pt_lu(c)l
tel
subject to

+ S1

B

+ .Y

C 0, St >

0 (tol

t)

21

The only source of uncertainty is the future prices. The consumer is
allowed to borrow against his tuture income, which has a present value
of B in the initial period.

The consumption plan must have

a function

only ot intormation available at date t.
It is convenient to set t
(t=l,2, .

If

•.).

u(C)

C1 a d13/P1,

(2.5)

=

and to assume that

Atc6/6

5a

is i.i.d.

then we have the following optimal plan:

10

(1—d1) B,

a JyS I1', S

(].—d)

(t2,3,

where

=

+ 1 —1

+

(PK A/Aj) [1/(1-)

f

It

[1 +

Ka

follows that

-

where

y,

x, u are

= (—l)(x

—

xi) ÷

defined as in (2.) and

a

+

•

see that, in this particular example, the

We

ZYL (PK)+ZflY.
appropriate

interpretation

of the change regression is very sensitive to the amount of information
available to the consumer.
(In

— en Ci) an <Liz

mator of

In the uncertainty case, a regression ot

— en P1) does not provide a co:.sistent esti-

in fact, the estimator converges to -1, with the implied

estimator of 6 converging to 0.

22

2.a.c.

Labor Supply
We

shall consider a certainty model in which the consumer is maxi—

mi z ing

=

(2.6)

t

t=1
subject

tt

(C ,L )

to

(PC + WL)

E

t=1

c B +

1-(t-1)
t-1

w

C>O, O<L<L (t=1, ....t),
is the wage rate, B is the present value

vhere Lt is leisure,

initial period of non.i.abor income, and L is the time endowment.

in

the

We shall

assume that the inequality constraints on L are not binding; the participation decision will be discussed in tbe section on nonlinear models.
If

is additively separable,

1J(C L) = tJ(C) +

and if U(L) PsL6fd then we have
+ p(t—1) + C +

(2.7)

where

a

£YLL, x a
Let

Let

$

C

=

and

Lit
Ln (YP).

(1-6)

determined by the marginal utility of initial wealth:

We shall assume that

Once again c is

Vf9B.

is not observed by the econometrician. There

will generally be a correlation between c and

.

.

.

.

since L1 depends

25

hypothesis in (2.8) implies that if T >

4,

there are (T—3)(T—2)/2 over—

restrictions.

identitying

Consider next a Granger definition of "y does not cause x conditional
on C":

(2.10)

.

.

.

.

(t=1, .

Define

.

.

.

.

. x,c)
I-fl.

.

the tollowing linear predictors:

ttt, + til, +...+ ttt,
E*(vt+i Jx1,

... x, Y1'

Then (2.10) is equivalent to p

.

.

.

0.

c)

.

0

(t=1,

t+].

.

.

.

t+1'

. I-fl.

We can rewrite the system, imposing

Cs

0, as tollows:

x

+

(2.11)

=

ts

ts — '
¼

av

t+1

—

t—1

—

( t+1 t t

/ )v ,

,s

t

+tx

+v

n tt +

E(x t+1
(SC t—l; t—2

In the equation for

and 2(t—1)

(3 5.

t < I

ao
1—1)

there are t unknown parameters,

It'

orthogonality conditions. Hence there are t—2 restrictions
1)

It follows that the Granger condition for "y does not cause x conditional on a" implies (T—3)(T—2)/Z restrictions, which is the same number

of restrictions implied by the Sims condition. In fact, it is a consequence
of Sims' (1972) theorem, as extended by Rosoya (1977), that the two sets of

26

restrictions

are

equivalent; this is not immediately Obvious from a direct

and tt.IQL

comparison of

In tens of tie

=

matrix, conditional strict exogeneity implies that

+ x

: :22

•c;i•

)

xa(:),

''

T2

ST1

...

B11

These nonlinear restrictions can be imposed and tested using the minimum

distance estimator to be developed in the inference section. Alternatively,
we can use the transformations

in

(2.9) or in (2.11). These

nations give us "siniultanecus equations"

transfor—

systems with linear restrictions;

can be estimated using three—stage least squares. A generalization of

(2.9)

three—stage least squares, which does not require homoskedasticity assumptions,
It is asymptotically equivalent to

is developed in the interence section.

restrictions

imposing the nonlinear

directly on II, using the minimum distance

estimator.

2.6.

Lagged Dependent Variables
For

as

a specific example, write the labor supply model in (2.7)

tollows:

(2.12)

+

+

•

this

reduces to (2.7)

a0
if 62 :

+

(tnt,

and

= 1. If we aSStt

that

V

3 V +

23

If At is independent of the W's, then we have

upon wages in all periods.
the prediction that 11 =

I +JX • If,

however, wages

are partly

deter-

mined by the quantity of previous work experience, then there will be lags
and leads in addition to those generated by c, and It will not have this
simple

structu.

It would be usetul at this point to extend the uncertainty model to
incorporate uncertainty about tuture wages.

Unfortunately,

a

comparably

simple explicit solution is not available.

But we may conjecture that the

correct interpretation of a regression of (Zn

Lt

—

Lit

on (Zn

W

—

Lit

is also sensitive to the amount of Information available to the consumer.

2.5.

Strict

E'ogeneity

Conditionat on a Latent Variable

We shall relate the specification analysis of It to the

causality definitions of Granger (1969) and Sims

(1972) .

Consider

a sample

in which t1 is the first period of the individual's (economic) life.
Sims definition of

12

A

is strictly exogenous" is

..) s.E*(y lxi

x)

In this case 11 is lower triangular: he elements above the main diagonal
are all zero.

This fails to hold in the nEdels we have 'baen considering,

due to the omitted variable c.

But, in some cases, we do have the following

property:

(2.8)

E*(yjx1,x21. ..,c)

It

. •.>

c) (t1,2,.. -)

was stressed by Granger (1969) that the assessment of noncausality

depends crucially on what other variables are being conditioned on. The

24

is that we are asking whether there exists some

novel teature at (2.8)
latent variable

such that x is strictly exogenous conditional on c.

(c)

The question is not vacuous since c is restricted to be time invariant.
Let us examine what restrictions are implied by (2.8) .

t1

l +

E*(uIxj,...,
Then

+ tT XT

x, c) =

+

i C +

(t1,

0

..,

t.

0 for s >

(2.8) is equivalent to

a scale normalization for a such. thai_ c,=.. 1.
system

0 Cs

with
a

(2.9)

Consider the

.

.

.

# 0, we can choose

If

Then we can rewrite the

.+

+

$ +

+

a

E(x u) '

by E(xu) 0.

T).

t) as follows:

>

+

S

St1

the

13

following linear predictors:
=

Define

0

T; t—2, .

(s—i

.

"instrumental variable"

. T)

orthogonality conditions implied

equation, we have T+1 unknown coefficients:

In the

, 8,.

.

and

cients are not identified.

T

orthogonality conditions.

In the

So these coetti—

equation, however, we have just

enough orthogonality conditions; and in the

equation (j < T—Z),

we

have j—l more than we need since there are T—j+1 unknown coefficients:

2' '

,i'

E(xsuT.)

a

0 (s—i.

we can identify 8,

1T—j'

8T—j

T) .
and

and T orthogonality conditions:

It follows that, subject to a rank condition,
for 2 C s C t 5 T 1. In addition the
—

27

where w is uncorrelated with the x's and

is i.i.d. and uncorrelated with

the x's and w, then we have the autoregressive, variance-components model of

Bales tra and Merlove (1966) 14 In keeping with our general approach, we shall

avoid placing restrictions on the serial correlation structure of v, our
inference procedures will be based ou the strict exogeneity condition that

x) = 0.
We can fit

substitution

this

model into

the II matrix

framework by using recursive

to obtain the reduced form:

yt 1Xj+

+

E*(u1x11. ..xQ

Ytc+ Ut,
0,

where

tt

+

C

62X+ 633Tt

uv+

53

1' 1t =

V1

+.

.

+

vi

(1 <s<t4. t—i.
(We are assuming
not

that (2.12) holds for t >

T).

1, but data on (x3, y) are

available.)

Hence this model satisfies the cond..tional strict exogeneity restrictions,

ft

where

- B+

B is lower triangular. The I A term is generated by the projection

of the initial condition (d2x +

63y)

on xl

15

XT.

Estimation can proceed by using the minimt distance procedure to impose

the nonlinear restrictions on fl Alternatively, we can complete the system

28

in

(2.12) with
+

=

•

+

+

this is just notation tar the identity

=

Then

X)+ [y1

II, X)]•

—

we can apply the generalized three—stage least squares estimator to be

It achieves the same limiting distri-

developed in the inference section.

bution at lower computational coat, since the restrictions in this form
are linear and can be imposed without requiring iterative optimization
techniques.

Now consider a £cond order autoregression:

61't 52tl +
XT)

Recursive

substitution

etix].

+

+ ti

(t1

0

T).

gives

+ .. +

+

x,) a o

tli

1t29

+

(tel

where

C1

52X + 53Y0

+ ¼'—i'

C2 —

and there are nonlinear restrictions on the parameters. .

the tollowing torm:

The if matrix

has

29

+

+

where B is lower triangular,

=

'''

and E*(cjx) =

Xx

(j=1,2)

This specification suggests a natural extension ot the conditional

strict exogeneity idea, with the conditioning set indexed by the number of
latent

variables.

We shall say that "x is strictly exogenous conditional

on c1, c2" if

E*(yt1... x1 x,

c1, ca)— E*(ytjxt, x_1, ...,

c1, c2).

We can also introduce a Granger version of this condition and generalize the
analysis in Section 2.5.

Sr'iat Correlation or Partial Adjustment?

Griliches'

(1967)

considered the problem ot distinguishing between

the following two models: a partial adjustment model,

(2.13)

a

+

16

+ Vt

and a model with no structural lagged dependett.variab].e but with a
residual following a first order M.aricov process:

(2.14)

ext + Ut,

Ut Pu_1 + a, e L.i.d.;
in both cases x is strictly exogenous:

E*(vtlxi, .

.xT)E(ulxll

xQsO

Ct—i

T).

30

In

the

serial correlation case, we have

=

as

— P8x.i + Py1 +

Griliches observed, the least squares regression will have a distinctive

pattern ——

the coefficient on lagged x equals (as N + cc) minus the product

at the coetticients on current x and lagged y.

I want to point out that this prediction does not rest on the serial
correlation structure of ii.
that u is

uncorralated with
E*(ytIx,

It is a direct implication at the assumption'

x.

,c_1, '—i $xt + E*(utIxt,
a
5

+

sxt

+ 'Pt

ut_i
+

—

Here p

x1, i:i

is simply notation for the linear predictor. In general u

not a first order process (E*(uIui,

uz) #

E*(utIut_i)), but this does

not attect our argument.

Within the II matrix framework, the distinction between the two models
is that (2.14) implies a diagonal It matrix, with no distributed lag, whereas
the partial adjustment specification in (213) implies that 11 ' B

+ y A',

with a distributed lag in the lower triangular B matrix and a rank one set
of lags and leads in I A'
We

can generalize the serial correlation model to allow for an individual

specific effect that may be correlated with x:

31

+ C + Ut,

yt =

.

E*(utlxi

xT)

=0.

Now both the serial correlation and the partial adjustment models have a

rank one st of lags and leads in ri, but we can distinguish between them
because oniy the partial adjustment model has a distributed lag in the B
matrix.

So the absence of structural lagged dependent variables is signalled

by the following special case of conditional strict exogeneity:

•., X,1, c) —

a),

E*(ytlxt,

In this case the relationship of x to y is "static" conditional on C. We
shall pursLe this distinction in nonlinear models in Section 3.3.

2.7.

2.7. a.

Resi

aL Covaricmaes:

Heteroskedasticity and Serial Corretation

ffeteroakedastioity

If E(cIx1) # E*(aIx) then there will be heteroskedasticity,
a

since the residual will contain
heteroskedasticity

random

is

+

s

a

coefticients:

+

E(w)
+

Another source of

a 0,

+ (w1xj +

If w is independent of x, then 11

0 I + £.

is relevant for the estimation of .

We

A',

and our previous discussion,

shall handle the heteroskedasticity

32

problem in the inference section by allowing,

to

—
U

be an arbitrary function of

2.7.b.

Serial

It

Correlation

may be ot interest to impose restrictions on the residual

covariances, such

as a variance—components structure together with an

Consider the homoskedastic case in

average scheme.

autoregressive—moving

which

35j)Zj —

—

the restrictions can be expressed as

does

not depend upon x.

Then

jk

jk9)' where the

g's are

parameter

vector.

known

functions and S is an unrestricted

We shall discuss a minimum distance procedure for

imposing such restrictions in the interence section.
3.
3.1.

SPECIFICATION

AND

IDENTIFICATION:

NONLINEAR MODELS

A Rwzd,m Effects Probit Model

Our treatment of individual effects carries over with some important
qualifications to nonlinear models.
participation

We shall illustrate with a labor force

If the upper bound on leisure is binding in (2.6)

example.

then

>

where

m is the Lagrange multiplier corresponding to the lifetime budget

constraint

(the marginal utility of initial wealth) .

individual i

works

in period t,

0 otherwise.

Let
Let

y. a

1

if

33

•L;i W =

c1x.

+

=

£i

+ e1.

ez.

where

contains measured variables that predirt wages and tastes tar

leisure.

We shall simplify the notation by supposing that

a single variable.

consists of

= 1 if

Then

—

(t-1)

+

(1-6) en

Lvi (yp) + en

t +

—

> 0,

which we shall write as

+ (t1) +

S

(31)

+ u

Now we need a distributional assumption for the .z's. We shall assume
that (u,

Ut) is independent of c and the x's, with a mu.ltivariate

normal distribution (N(O, E)).

So we have a probit model (suppressing the

i subscripts and period-specific intercepts):

P(y =

+

11x1,

where F( ) is the standard normal distribution function and

is the

th

t— diagonal element of Z.
Next we shall specity a distribution tar c conditional on x •

(x, ,

.

+XTXT+V
where v is independent of the x's and has a normal distribution (N(O, a2)).

34

There is a very important difference in this step compared with the linear
In the linear case it was not restrictive to decompose c into its

case.

linear projection on x and an orthogonal residual. Now, however, we are
assuming that the regression function E(cIx) is actually linear, that V is

independent of x, and that v has a normal distribution.

These are restric-

tive assumptions and there may be a payoft to relaxing them.
Given these assumptions, the distribution for

conditional on

but marginal on c also has a probit form:

•

P(Y a 11x1,
=

(at

.

.

.

. x)

A1x +

F[cLt(6x+

.

.

.+

+

Combining these T specifications gives the followthg matrix 0

coefficients: S
(3.2)

This

fl:diag{ct1, . .

. .ctT)I$IT+tXI.

differs from the linear case only in the diagonal matrix of normalization

factors

OLt.

There are

now nonlinear restrictions on It, but the identi-

fication analysis is still straightforward.
c1t

a11 t1

a

$ a tt
if + A1 +
for

A,

0.

can use a

scale

(t2

+

Then,

as

ratios

have

-

+

and a3A. Only

We

in

the

linear case, we can solve

of coefficients are identified, and so we

normalization such as

1.

P

35

for

As

inference, a computationally simple approach is to estimate I

cross—sectional probit specifications by maximum likelihood, where

are included in each ot the I specifications.
and we can use

This gives

..,

x1,

(t=1

T)

a Taylor expansion to derive the covarlance matrix of the
Then restrictions can be

asymptotic normal distribution tor

using a minimum distance estimator, just as in the linear case.

imposed on 11

We shall conclude our discussion of this model by considering the
interpretation ot the coefticients.

We began with the probit specification

that

P(y

So

on

XT, c) -

i]xj

Ft

(3x + c)1.

one might argue that the correct measure of the effect of

,

whereas we have obtained

estimate.

available.

which is then an under-

But there is something curious about this argument, since the

!!omitted variable!! v is independent ot
decompose

+ a)½S,

is based

in (3.1 )

into

+

.

.

.

.

Suppose that we

and that measurements on

become

Then this argument implies that the correct measure of the

effect of x is based on [V(uz) 4

. As

the data collection becomes

increasingly

successtul. there is less and lass variance left in the

residual

and [V(uz)

becomes arbitrarily large.

The resolution of this puzzle is that the effect of x depends upon
the value of a, and the effect evaluated at the average value for c is not
equal to the average ot the eftects, averaging over the distribution tor c.
Consider the effect on the probability that

to x"; using the average value for c gives

1 of increasing x

from x'

36

F[a (ext' + E(c)) I

The

with this measure is that it may be relevant for Only a small

problem

traction

+ £(c)) 1

(3x'

—

I think that a more appropriate

ot the population.

is

measure

the 'ean effect for a randomly drain individual:

f[P(yr

1Ix : X'1, a) —

1x a ••, c)]p(dc),

P(y

u(dc) gives the population probability measure for c.
We shall see how to recover this measure within our framework.

where

z X1x1 + .

.

let p (dz) and A(dv)

.+

—ijx.,

— Liz1,

c) a

a iIx,

c

the population probability

give

measures for the independent random variables z

Let

and

••

v.

Then

c)

z, v);

JP(y • 1 x, z, v).U(dz)U(dv)

z, v)P(dv[xz)u(dz)

t1x, z)J(dz),

a

1J(dvIxi z) is the conditional probability measure, which equals the
and Z. (it is important
unconditional measure since V is independent of

where

to note that the last integral does not, in general, equal
For

if

end z
p(y

are

correlated, as

a lixe)

fP(y a

they

1.Ix,

z)u(dz).)

are in our case, then
z)1.L(dzIx)

a

1

I1).

37

We have shown that

= lx = :Ct,

! [P(y

(3.3)

c) -

P(y

= I {P(y 1x f, z)

= x', c)]p(dc)

=

—

1Fx

=

x',

z)1p(dz).

The integration with respect to the marginal distribution for z can be
done using the empirical distribution fi.thction, which gives the following
consistent

(as N +cc) estimator of (3j•q,

2

(3.4)

{F[c(6x"+ X1x1
+

—

+

.

. .+

+. ..+

3.2 A Fixed Effects Logit Model: Conditional Likelihood
'A weakness in the probit model was the specification

tor

c conditional on x.

of a distribution

A convenient form was chosen, but it was only an

approximation, perhaps a poor one.

We shall discuss a technique that does

not require us to specify a particular distribution for C

conditional

on

it will, however, have its own weaknesses.

x;

Consider the tollowing specification:

(3.5)
where

+

(3.6)

-

+

a),

G(z) a e5ci +

are independent conditional on

y1

Suppose

Xr a)

1x1

c.

that TaZ and compute the probability that

1 conditional, on

2 1:
P(y2 =

1x1

x2, C,

+

.1 1) =

—

38

whicii does not depend upon c.
conditional

tunction

log-likelihood

L =

Given a random sample of individuals, the

Zn

is

-

+

(l-w) Ztt C(—$(x.2—x1)

I),

where

1 if (y11, y12) =
=

1

1)

y1x, i2 (1, 0) ,

(

B

(0,

{iy.1 +

This conditional likelihood function does not depend upon the
incidental

parameters.

It is in the form of a binary logit likelihood

function in which the two outcomes are (0.1) and (1,0) with e 1anatory
variables
linear

model.

This

—

is the analog of differencing in the wo period

The conditional tnaxizntun likelihood (ML) estimata of B

can be obtained simply from a ML binary loglt program.

This

conditional

likelihood approach was used by Rasch (1960, 1961) in his model for
intelligence

tests.20

The conditional ML estimator of
conditional likelihood function

is consistent provided that the

satisfies

regularity conditions,

which

impose mild restrictions on the c. These restrictions, which are
satisfied if the i are a random sample from some distribution, are discussed in Andersen (1970).

Furthermore, the inverse of the information'

matrix based on the conditional likelihood function provides a covariance

matrix

for the asymptotic (N + cQ) normal

distribution of the conditional

ML estimator of B.

These results should be contrasted with the jnconsistenc'? of che
standard fixed effects ft estinator, in which the likelihood tunction is

39

based on the distribution of

conditional on X1

following

limits

= 1 (i11 . . .

= 0,

For example, suppose that T = 2,

one if the C1

exist with probability

xT, c.

. N).

The

are a random sample

tram some distribution:

=

urn

urn

=

E

where
a

a

E[y1(1 e

E[(1—y.1) y21ci a

+ ci).

Andersen (1973, p 66) shows that the
probability one to 2 as N

•

A

estimator of

converges with

simple extension at his argument shows

that if G is replaced by any distribution function (C) corresponding to a
symmetric,

continuous, nonzero probability density, then the ML estimator of

converges with probability one to

2

The logit case is special in that

a a8 for any distribution for c.

general the limit depends on this distribution; but if all of the

once again we obtain convergence to 26 as N
For general T, conditioning on Zy (is].
conditional

log-likelihood

tunction:

In

• 0, then

OO•

N) gives the following

40

L =

i1
B. =

L

is

T
exp(jS E

tit [arp(3 E

{d =

t=1

(d1.

in the conditional logit

I

dT)Jdt

0

I,

Ni

deB,

or 1 and Edt

t=i

I

1'
t=1

form considered by McFadden (1974), with

the

set (B1) varying across the observations. Hence it can be
maximized by standard programs. There are T+]. distinct alternative sets
alternative

corresponding

to

0,1,

..,,

T.

Groups for

which

:

0 or T

contribute zero to L, however, and so only I—i alternative sets are relevant.
The alternative set for the group with

= s

has (

) elements,

corresponding to the distinct sequences of T tr als with s successes. For
example,

with :=3 and sal there are three alter ati.ves with the following

conditional p:obabjljtjes:
P(1,O,01x1, Cj

P(O1OIx

E

it

c., Z

•

Cj Et
D =

1)

ex1E(x1—x.3)

i+

exp[B(x1

1)

exp(B(x2 —

1)

lID,

exp [(x2—x13) ] +, 1.

41

A weakness in this apprOach is that it relies on the assumption that
the Yt are independent conditional on x, c, with an identical torm tor the
conditional probability each period:

P(y =

1x,

+ c)

c)

In the probit framework, these assumptions translate into

£

that V + u generates an equicorrelated matrix: a2

so

+ a21. We have

seen that it is straightforward to allow E to be unrestricted in the probit
framework; that is not true here.

An additional weakness is that we are limited in the sorts ot probability
statements that can be made.

We obtain a clean estimate ot the eftect ot

on the log odds:

Lit

I P(yta1xtxh1, c) /P(y slix

nx',

L"° kc—x" c)/ P(ytfbkttxl, C)j1 s" .1-

I

c) 1

x

the special feature of the logistic functional form is that this functiOn
of the probabilities does not depend upon c; so the problem of integrating
over the marginal distribution of c (instead of the conditional distribution
of a given x) does not arise.
probabilities

that one might

But this is not the only function of the
In the probit section we
want to know.

considered

P(Yt
which

lx = C, a)

depends upon c

for

—

probit

P(Y
or

11x a x',

logit,

and

c),

we averaged over

distribution tor c:
(37)

•r[P(y

s

a x",

c) —

1x

x', c)]ii(dc).

the

marginal

42

This requires us to specify a marginal distribution for c, which is what

We cannot estimate (3.7) if all

the conditioning argument trys to avoid.

we have is the conditional 1 estimate of 3.
is independent of

Our specification in (3.5) asserts that

x1, x1, .

.

.

conditional

.

on

This can be relaxed

c.

but the conditional likelihood argument certainly requires more

somewhat,

than

JIx, c) G($x + c);

P(y
to

see this, try to derive (3.6) with x2
:

the following specification (with

(3.8)

P(y =

lix, c)

y1, We can, however, implement

(x1

+ •. +

G(8 +

+

are independent conditional on x, c.

where

This

corresponds

to our specification of "x is strictly exogenous conditional on c" in

1 in the term Yc --

Section 2.5, except that

it

is

not straightforward

to allow a time—varying coefficient on C in the conditional likelihood
The

approach.

a

(3.9)

extension

1x,

C,

of (3.6) is
+

•

1)

+

+ 8t2x2

+ . .

.+

T)

where
we

a

—

(jnO,1)

So

it

can obtain consistent estimates of

x

has sutticient variation,
and

(s2

t).

43

Only these parameters are identified, since we can transtorm the model
+ 8. x1 + c without violating any restrictions.

replacing c by c

The restrictions in (3.5) or in (3.8) can be tested against the
alternative:

tollowing

We

1x, c) =

Ny,

(3.10)

tO +

can identify only lt —
•

'F)

+ .

.

.+

+ c).

and so we can normalize

The maximized values ot the conditional log likelihoods can

be used to form

statistics.

21

There are (T—2)(T—1)/2 restrictions in

passing from (3.10) to (3.8), and (3.5) imposes an additional
(T—1)(T-1-4)/2 —

1

restrictions.

3.3. Serial Correlation

And Lugged Dependent Va'iahles

Consider the tollowing two models:

1

t

(3.lla) y

a

1

>

(0 otherwise

(1 if y*.u >0

(.

)

t

..,

( 0 otherwise;

pu1 +

44

in both cases et is i.i.d. N(O, 2) Hecknan (1978) observed that we
22

In the first model,

can distinguish between these two models.

P(y It1 't-2' • •• = P(y
where p(

)

1tyi) = F(1Y

=

1't1' —V ..)

If we observed

ot the process.

i/cr),

In the second

is the standard normal distribution function.

model, however,

irrelevant.

=

depends upon the entire history

then previous outcomes would be

In fact, we observe only whether

> 0; hence conditioning

> 0 affects the distribution of

in addition on whether

and

So the lagged y implies a Markov chain whereas the Markov assumption for
the probit residual does not imply a Markov chain for the binary sequence
that it generates.

There is an analogy with the following linear models:

a

(3.12a)

+
a

(3.lZb)

u, Ut

a +

where at is i.i.d. N(O, a2). We know that if Ut a

+

then no

distinction would be possible, without introducing more structure, since

both models imply a linear Markov process. With the moVing
average

residual,

however, the serial correlation model implies that the

entire past history is relevant for predicting y. So the distinction
between the two models rests on the order of the dependence on previous
realizations of
We can still distinguish between the two models even when (u1, .

.

.

has a general wultivarlate normal distribution (N(P, Z)). Given nor—

.

UT)

45

malizations such as V(u) = 1

has T(T+1)/2 free parameters.
the 2

•1

T), the serial correlation model

(e—1

Hence if T > 3, there are restrictions on

— 1 parameters of the multinomial distribution for (y,

In particular, the most general multivariate probit model cannot generate

a Markov chain.

So we can add a lagged dependent variable and

identity Y.

This result relies heavily on the restrictive nature of the multi—
variate

probit functional

form. A more robust distinction between the two

models

is

pursue

this after first presenting a generalization ot strict exogeneity and

possible when there is variation over time in x.. We shall

noncausality

tor nonlinear models.

Let t=1 be the first period f the individual's (economic) life. An

definition

extension of Granger' s
independent ot

.

.

.

.

.

f

"y does not cause x"

condituna1

strict exogeueity condition is thst

conditional on

.

. ., x.

on

.

.

. x.

.

is

that

is

An extension of Sims'

is independent of

x2,

In contrast to the linear predictor case,

these two definitions are no longer equivalent.

23

For consider the tollowing

counterexample: let y1, y2 be independent Bernoulli random variables with
fly

z —1)

1) a

dent of x3 and

1/2 (t1,2). Let x3

71y2. Then

is indepen-

is independent if x3. Let all of the other random

variables be degenerate (equal to zero,

say) .

Then

x is strictly exogenous

but x3 is clearly not independent of yy y2 conditional on xl, x2. The
counterexample works for the following reason:
uncorrelated

it a random variable is

with each of two other random variables, then it is

Un—

correlated with every linear combination of them; but If it is independent
of each of the other random variables, it need not be independent of every
tunction ot them.

46

Consider the following modification of Sims' condition:

independent of x1, x2, ,,. conditional

..) .

(t=1,2, .
condition,
causality.

Chamberlain

on

•..,

'1' •'

shows that, subject to a regularity

(1982)

this is equivalent to our extended definition of Granger non—
The regularity condition is trivially satisfied whenever

has a degenerate distribution prior to some point.
our case since y,

.

.

.

It is straightforward
into

x,

have

to

So it is satisfied in

distributions.

degenerate

introduce a time—invariant latent variable

these definitions. We shall say that "rj

does

not caLae z conditional

an a latent variablec" if either
is independent of

...,

C

.

(t=l,2,. .

, y, conditional on

.1,

or
is

independent OfC÷1 z2,

4•,
they

x

•

conditional on

a

1'

are equivalent. We shall say that "z is strictly exogenous conditional

an a Latent variable &'
is

if

x23. .

independent of

..,

c

. conditional on

(t=l,2,

let us return to the problem of distinguishing between serial

Now

correlation and structural lagged dependent variables. Assume throughout
the discussion that

and

are not independent.

We shall say that the

relationship of x to y is static if

is strictly exogenous and
conditional on x.
x

is

independent

of

.

. .,

47

i

Then

propose the following distinctions:

There

is residual seria7 correlation i

f

conditzonal on

If

the relationship

.

f

is

not independent

..

to y is static, then there are no
structural lagged dependent variables.

Suppose that y and
y2

are binary arid consider the probability that

= 1
conditional on (x_ x2)

and

Since

generally

are

of

(0, 0) and

assumed to be dependent, the distribution of

difterenit in the two cases.

If

is not independent of

Note;that

this

(1, 0)
is

has a structural effect on

1 should differ in the two cases,

then the conditional probability of
so that

conditional on (xl, x2)

conditional on

condition is one-sided:

am

only offering a condition

there to be no structural effect of

on y. There can be distributed lag relationships in which we would not want to say that
has a
for

structural effect on y. Consider the production unictioni example with
serial correlation in rainfall; assume for the moment that there
variation

In

c.

is

no

If the serial correlation in rainfall is not incorporated

in the farmer's iniormationi set, then our definitions assert that there is

residual serial correlation but no structural lagged dependent variables,
since

the

does

ue

ship

of x to y is not static since x is not strictly exogenous.

relationship o x to y is static.
previous

Now suppose that the farmer

rainfall to predict future rainfall. Then the relation-

my not want to say that the relationship between

and

tural, since the technology does not depend upon Y1.

But we

is struc-

48

How are these distinctions affected by latent variables? It should
be clear that a time—invariant latent variable can produce residual
correlation. A major theme of the paper has been that such a

serial

latent variable can also produce a failure of strict exogeneity. So
consider conditional versions

f these properties:

There is residual seriaZ. correlation conditional on a
is not independent of
latent variable c
...,

if

conditional

y1.,

on

C;

Th

relationship of : to y is atatvc conditional on a latent
is strCctly exogenous conditional on a and
va'iahte a if
conditional on
C;
..,
independent

if i.

of x,

If the relationship of a to y is
latent variable C, the there are

static

conditional on a

no structural lagged

dependent txzriabiee.

surprising feature of te linear predictor definition of strict

A

axogeneity

is that it is restrictive to assert that there exists some.

time—invariant latent variable c such that x is strictly exogenous
This is no longer true when we use conditional

conditional on c.

independence to define strict exogeneity. For a counterexample,

is a binary variable and consider the conditional strict exogeneity

that

"Does there exist a time—invariant random variable C

question,

is independent ot

answer is

xl conditional on

variables

.

.

.

c?"

The

T

. x,)

th

and set cj if the j— outcome occurs (31, ..., 2

is independent of
For

.

such that

yes since we can order the 2.. possible outcomes of the binary

sequence (x1, . . .

Now

suppose

...,

conditional on c!

a rtondagenerate counterexample, let y and x be binary random
with

T
).

49

P(y

x

CL4,

2

1

>

tik

Tik = 1,
j,k=1

=

where

1,

a
-

= 0.

CL2

Let 1' =

(Ill,

4

4

E

ye,y
>0, Z
m
ui-tn

m1.

we can set
tl2 T21 t22). Then

'y =1,

where e is a vector of zeros except for a one in the m- component.

. . , 4).

{e,

m1, •

The components f y(6,X) give the probabilities P(y —

x

y and x are independent with fly = 1) a 6, P(x a 1) a A.

Set aX

Hence I is in the interior of the convex hull of

Now

the vector

consider

6(1—A)

(1—6)X
(1—6) (1—X)

A
in

=
CLk)

when

-Ui

with 0<6 <1,0< A<1. mThen 4y will
be in the interior
—
so that
convex hull of {e, ml,...,4} if we choose ç
Am

m)

of

the

is

sufficiently

Ta

close to a • Hence
Ui

4

4

E

y*e*,y*>O, Z

rn1

Let the coñponents of e*.be (tT1,

variable with P(c a in) — i (mel, •

fly

X

CL,

t2, t1, tfl). Let
. .

m) -

, 4), and set

tik

c be a random

50

Now y is independent of x conditional on c, and the conditional distributions are nondegenerate.

X, 1'

it (xi,

has a general multinomial distribution,

then a straightforward extension of this argument shows that there exists
a random variable c such that

is independent ot (x1

XT)

conditional on c, and the conditional distributions are nondegenerate.
Consider a linear one—

A similar point applies to tactor analysis.
tactor model.

The specification is that there exists a latent variable c

such that the partial correlations between
This is restrictive if T ) 3.

.

.

.

.

are zero given c.

But we now know that it is not restrictive

to assert that there exists a latent variable c such that

are

independent conditional on c.

It follows that we cannot test for conditional strict exogeneity

without imposing functional form restrictions; nor can we test for a
conditionally static relationship without restricting the functional
torms.

This point is intimately related to the fundamental difficulties
created by incidental parameters in nonlinear models.

The labor force

participation example is assumed to be static conditional on c. We
shall present some tests of this in Section 5, but we shall be jointly

testing that proposition and the functional forms — a
test cannot exist.

truly nonparametric

We stressed in the probit model that the specification

for the distribution of c conditional on x is restrictive; we avoided
such a restrictive specification in the logit model but only by imposing
a restrictive functional form on the distribution of y conditional on x, c.

51

3 . 4.

Evraticn

Models

In many problems the basic data is the amount of tine spent in a
state.

For example, a complete description ot an individual's labor

torce participation history is the duration ot the first spell ot parti-

cipation and the date it began, the duration ot the tollowing spell ot nonparticipation, and so on.

This complete history will generate a binary

sequence when it is cut up into fixed length periods, but these periods
may have little to do with the underlying process. 24

In particular, the measurement ot serial correlation depends upon the
period ot observation.

As the period becomes shorter, the probability that

a person who worked last period will work this period approaches one. So
finding significant serial correlation may say very little about the underlying process.

Or consider a spell that begins near the end of—a period; then

it is likely to overlap into the next period, so that previous employment
raises the probability ot current employment.

Consider the underlying process ot time spent in one state tollowed by
time spent in the other state.

It the individual's history does not help

to predict his future given his current state, then this is a Markov process.
Whereas serial independence in continuous time has the absurd Implication that
mean duration ot a spell is zero, the Marlcov property does provide a fruitful
starting point.

It has two Implications:

the individual's history

prior to the current spell should not attect the distribution ot the
length of the current spell; and the amount of time spent in the
current state should not attect the distribution ot remaining time
in that state.

So the first requirement of the Markov property is that durations
ot the spells be independent ot each other.

Assuming stationarity, this

52

implies

an alternating renewal process.

The second requirement is that the

distribution ot duration be exponential, so that we have an alternating Poisson

A test
what

shall

We

process.

refer to departures from this model as duration dependence.

of this Markov

property

sampling scheme is being used.

using binary sequences will depend upon
The simplest case is point sampling,

where each period we determine the individual's state at a particular point
in time, such as July 1 ot each year.
alternating

Poisson

Then it an individual is tollowing an

process, her history prior to that point is irrelevant

in predicting her state at the next interview.

So the binary sequence

generated by point sampling should be a Marlwv chain.

It is possible to test this in a fixed ettects model that allows each

individual to have her own two exponential rate parameters (Ca, c12) in
the alternating Poisson process.

The idea is related to the conditional

likelihood approach in the fixed effects logit model. Let $

be the

ijk

number of times that individual I is observed making a transition from state

j

to state k Ci, k =

1,2).

Then the initial state and these four transition

counts are sufficient statistics for the Markov chain.

Sequences with the

same initial state and the same transition counts should be equally likely.
This is

the Markov form of de Finettl's

(1975) partial exchangeability.25

So we can test whether the Markov property holds conditional on C11, C12
by testing whether there is significant variation in the sample frequencies
of sequences with the same transition counts.

this analysis is relevant if, for example, each year the survey question

is "Did you have a job on July 1?"
Dynamics,

In the Michigan Panel Study of Income

however, the most commonly used question 'for generating parti-

cipation sequences is "Did your wife do any work for money last year?"
interval

sap1ing.

leads to

a

more complex

This

analysis, since even If the

individual is follOwing an alternating Poisson process, the binary sequence

53

generated by this sampling scheme is not a Markov chain,

suppose that

= 1, so that we know that the individual worked at some point during the

the

is

What

previous period.

relevant, however, is the individualTs state at

will affect the probability that the spell

end of the period, and

of work occurred early in period t—l instead of late in the period.
Nevertheless,

it is possible to test whether the underlying process is

The reason is that if

Poisson.

alternating

: 0, we know that the

individual never worked during period t—l, and so we know the state at the
end of that period; hence Yt_2t 7t—3'

lila1,
NY, =
z

C2,

.

. are irrelevant.

So we have

7t-1' t—2'

ljc1, C2

= 1c1,

=

= 7t—d

t—d—1

0)

C2, d),

d is the number of consecutive preceding periods that the individual

where

was jfl state 1.
Let S01 be the number of times in the sequence that 1 is preceded by

0; let S011 be the number of times that 1 is preceded by 0, 1; etc. Then

sufficient statistics are
consecutive

ones at the beginning (n1) and at the end

For an example with T =

S0111

=.

.

•.., as well as the number of

. • 0;

5;

let Ti1

0, fl5 a

0, S01

of a sequence.

1, S011

= 1,

then we have

P(O, 1, 1, 0, ok)

= NY1

OIc)P(1O,

c)P(1lO,1,c)P(OIO,1,1,c)P(OIO,c);

p(Q, 0, 1, 1, Oe)

fly1

= O1c)P(OO,
E'°' c)P(11O,1,c)P(OlO,1,1, c),

26

54

where c =

(c1,

c2).

Thus these two sequences are equally likely conditional

n c, and letting p be the probability measure for c gives

p(O,1,1,O,O) =

=

I

P(O,1,1,O,OIc)u(dc)

I P(O,Q,1,1,OIc)p(dc) = P(O,O,1,1,O)

So the alternating Poisson process implies restrictions on the multinonial
distribution tor the binary sequence.

These tests are indirect. The duration dependence question is clearly
easier to answer using surveys that measure durations of spells.

Such

duration data raises a number of new econometric problems, but we shall not
pursue them here.

27

would simply like to make one connection with the

methods that we have been discussing.
Let us simplify to a one state process; for example,

can be t e
th

duration of the time interval between the starting date of the i—
job and his

individual's

job.

Suppose that we observe T > 1

jobs for each of the N individuals, a not innocuous assumption.
the restriction that

>

exp(x +

0

Impose,

by using the following specification:

C1 +

E*(ujIxj)
where

=

Ct11,

x).

Then

E*(Z4 Yk5j) $xj +
and our Section 2 analysis applies.

The strict exogenaity assumption has

55

a surprising implication in this context.
individual's age at the beginning of the

job.

2
1i,t—1 -- age is not strictly exogenous.

4.

Consider a sample Ei

y1a (y11,

.

.

y).

distributed (i.1.d..)

=

We

=

:tt—1

INFERENCE

N,where

shall assume that

(x11

XIK)

is independent and identically

to some mulcivariate

fourth moments and E(x x) nonsingular.
error

it

Then x. - x.

8

(xj, y!), 1=1

according

is the

Suppose that

distribution

with finite

Consider the minimum mean—square

linear predictors,29

—
x )—
iw-i.
which we

x

can write as
E*&jIx1)

ii

E(jx) 1E(xx)]1

We want to estimate fl subject to restrictions and to test those restrictions.
For example, we may want to test whether a submatrix ot II has the torm

6 I +LA'.
We shall not assume that the regression function E(y1x1) is linear.

Far although E(yjx c1) may be linear (indeed, we hope that it is), there
is generally no reason to insist that

is linear.

So we shall

present a theory of inference for linear predictors. Furthermore, even if
the regression tunction is linear, there may be heterdskedasticity —— due
to random coefticients,

tor example.

to be an arbitrary function of

So we shall allow E[(-4tx) (y— IIx) '

Ix]

56

4.1

The Estimation of Linear ErecHctcrs
Let

be the vector formed from the distinct elements of r.r

that have nonzero variance.

30

(j yP is

Since

i.i.d., it follows

This simple observation is the key to our results. Since

that w. is i.i.d.

our problem is to make inferences about a function

II is a function of

at a population mean, under random sampling.
and let 'rr be the vector formed from the columns of II'

Let ii =

(a

Then ¶ is a function of 1.1: a

Let w 4

h(p).

then Tr h(w) is the least squares estimator:

N

N

—1

vec[( E
1=1

—

E

1=1

By the strong law of large numbers, ; converges almost surely to p° as N +

where p°

A.S.> u°)
h(p)

is the true value of p. Let ii ah(p°). Since

is continuous at p =

The central limit theorem

that

implies

-

Since

we have 'Ti a.s.> ¶0

hQj)

is

v°) —2-->N(O, V(w))

differentiable at

-

=

D
—>
N(O,

0
the

5—method gives

Q),

where

c2= ______
j

We

_______

31

have derived the limiting distribution of the least squares estimator.

This approach was used by Cramer

(1946)

to obtain limiting normal distributions

for sample correlation and regression coefficients

(p.

367); he presents an

57

explicit formula for the variance of the limiting distribution of a sample
correlation coefficient (

p.

. Kendall arid Stuart (1961, p. 293) and

359)

Goidherger (1974) present the formula for the variance of the limiting distribution of a simple regression coefficient.

Evaluating the partial derivaives in the formula for 2 is tedious.
That calculation can be simplified since Tr has a "ratio" form. In the Case
of simple regression with a zero intercept, we have ir =

N

N

( E yx. —

-

Since
working

4/N

a.s.>

and

N

Z x/N)11.
1

E

i=1

E(y.xj/E(x)

i-i

obtain the same limiting distribution by

we

with

i=1
The definition

[(y1

O)j/(/(Zfl

a

0

of ita gives

—

x1)x1]

a 0,

and so the central limit

theorem implies that
( -

N(O E[(y1

-

Ox)2xZ]/[E(x2)I2}.

This approach was used by White (1980) to obtain the limiting distribution
for univariate regression coefficients.

32

In the Appendix (Proposition 7)

tollow White's approach to obtain

(4.1)

where

=

—

—

®!x1ii!x1]'

E(xx[). A consistent estimator of Q

the corresponding sample moments:

is

readily available from

we

38

=

(4.2)

[(y.

4

ft xp(y — ft xi)' ®

= It

If

=

V(y.x.) is
=

that the regression function is linear, than

x.,

E(V(yx1)

® !?cv

uncorrelated

with

then

E(V(YIx)] ®

If the conditional variance Is homoskeda9tic,
not

ç1(xx) sH

x.x/N.

where

If

-

that V(y.Lx.)

a

E

does

then

depend on
-

4.2

so

x

—1.

Imposing Restrictions: The P#ininrtmi Distance Estimator
Since II. is a function of

restrictions on 11 imply restrictions

an E(w). Let the dimension of LI • E(w1) be q.

We shall specify the restric-

tions by the condition that U depends only on a pXl vector e of unknoWn

parameters: i

g(®),

where g is a known function and p < q.

The domain ot

e is T, a subset of p-dimensional Euclidean space (B?) that contains the true
value

subset of

So

the restrictions Imply that

=

is

confined to a certain

59

We can impose the restrictions by using a minimum distance estimator:
choose B to

N
miii E [w. —
@cT

where

g(S)]'

i=1

P and

AN[wj
— —

is positive definite.

equivalent to the ,following one:

mm

—

g(9)j'

OcT

This minimization problem is

choose 6 to

A,1[
-—
-

The properties of 6 are developed, for example, in Malinvaud (1970, Chap. 9)

Since g does not depend on any exogenous variables, the derivation ot these
properties can be simplified considerably, as in Chiarig (1956: and Ferguson
(1958).

For completeness, we shall state a set of regDlarity concitions and
the properties that they imply:

Assunrptzon

a.s.> g(O°); T is a compact subset of

2.

g is continuous on T, and t(ffi) =

SET implies that 0

where Y is positive definite.

a.s.>

2.

Assumption

of

g<!0) for

that contains

—

—-— EU1 A); T contains a neighborhood

in which g has continuous second partial derivatives; rank

p, where G

Choose to

min[ e eT

(G)

60

Fro?ost tion. I. If Assumption 1
?rovosition2.

____ a0

is satisfied, then

If assumptions 1 and 2 are satisfied, then vW(e

-

e° —a--> N(O, A),

where
A =

If

A ! (2' '_)—•

(G''YG)G'

is positive semi-definite;

A is positive definite, then A -

hence an optimal choice for ''

Is A1,

definite matrix, and if AN a.s.>

NEaN —

Now

,

If Assumptions' and 2 are satisfied, if

Proposition 3.

() , NN

—

is a qxq positive—

then

g(a) 1 _!_> x2(q—)

consider imposing additional restrictions, which are expressed by

the condition that 6 =
a subset of

f(cO,

where

is sXl (s <

that contains the true value Il

p) .
50

The.

domain of ci is

6° 2

is

confined

to a certain subset of R.
Asawnption 2'.

is a compact

continuous

from

mapping

subset

of

that contains

a?;

into T; 2(a) 6 for a E T1 implias

f is a
C0;

contains a neighborhood of CL0 in which £ has continuous second partial

derivatives; rank (F) = s, where y
Let h(ct)

g(f(CS)J. Choose &to

tin (a.a-h(cL)F
&[a -h(csfl.
."
—

—1

61

3'.

Proposition

If Assumptions 1,
a.s.

N[aN

-

= N(aN -

-

then

definite, and if A\.

d1 =

2, and 2' are satisfied, if A ispositive
-

d2

s)

,

where

()]' 'NN - h(C)j,
g(6)
®
-

I

Furthermore, d1 — d2 is independent of

in their limiting joint distribution.

Suppose that the restrictions involve only II. We specify the

fL,

restrictions by the condition that It =

of 6 is T1, a subset of

where 5 is sxl and the domain

that includes the true value .

Consider

the

following estimator of 60: choose Ô to
—

!Q' ff1

— f(dfl,

where Q is iven in (4.2), and we assie that Q in (4.1) is
positive

definite.

-

If Tl and f satisfy assumptions 1 and 2, then

a.s.> (50

r) —L N(O, trJi' U1)

and
A

N[1T —

f(5Y]'

—

D
—>

2
x (MK—s),

where F

We can also estimate

by applying the minimum distance procedure to

w instead of to 7. Suppose that the components of

<!I1
11

where

Mw1) conformably: ii' =

are arranged so that

contains the components of

(.ij P. Set 2t

2i' )

Partition

=

(5', p.

62

Assurne that V(w.) is positive definite. Now choose B to

—

a.s.

where

() I'

—

—1

>V (wi)

g(@)

U
=

and

Then e1 gives an estimator of C;

it

has the same

limiting distribution as the estimator 6 that we obtained by applying the
A

distance procedure to it.

minimt.

36

This framework leads to some surprising results on efticient estimation.
For a simple example, we shall use a univariate linear predictor model,

E*(YjIX1, X12)

iT0 + 1T1X1 + 2j2

Consider imposing the restriction '112

of iT is

0.

Then the conventional estimator

the slope coefticient in the least squares regression ot y

on x1. We shall show that this estimator is generally less efficient than
the minimum distance estimator if the regression function is nonlinear or

if there is hetaroskedasticity.
Let iT1, ir2 be the slope coefficients in the least squares multiple

regression of y on x, x2. The minimum distance estimator of it1 under
the restriction

n

0

can be obtained as 6 a

ir1

+ 2 where t is chosen

to minimize the (estimated) variance of the limiting distribution of 6;

thi

gives

63

3

=

A

1

-

017
2

022

is the estimated covarianca between . and

where

=b

distribution. Since ?

1

=b

—

yx

Qb

2 x2x1

,

we

in their Limiting

have

A

-(b
X2X1

YX:J.

+--—-)ir.
2

22
a2 then

If E(yjx.1, x12) is linear and if V(y.x.1, x12)
—

x2)/V(x11) and

Cav(x11,

b. But in general &

is more

The source ot the etticiency gain is that the limiting

efticient than b

yxi
of IT2 has a zero mean (if 112

distribution

and 5

#

0), and so we cart reduce

variance without introducing any bias if iT2 is correlated with b

.

Under

TX1
the assumptions ot linear regression and homoskedasticity, b

and ff2

are

PC1

uncorralated; but this need not be true in the more general framework that
we are using.

4.3

Simultaneous Equations: A Generalization of 1x—and Three—
Stag'e Least Squares
Given the discussion on imposing restrictions, it is not sur—

prising

that two—stage least squares is not, in general, an efficient

procedure tar

combining instrumental variables.

with a simple example.

I shall demonstrate this

Assume that (1 z, x1, x.2) is i.i.d. according

to some distribution with finite tourth moments, and that

'5

yi

where E(v.x,1)

=

+ v.

E(v1x.2)

= 0.

Assume also that E(zx.1)

# 0,

E(z.x12)

0.

64

Then there are two instrumental variable estimators that both converge

a.s.

to 6:

N

N

1=1

./ E z.x..
y.x.
it).L]. 113

=

(j=1, 2),

)where the j, k element of A is

E[(y.—iSz )2x x
(j,k
Xjk

1,2).

E(zjxj.)E(zjxjk)

The two—stage last squares estimator combines

c

zsed

+

(assume

that E[(x1

least squares regression of z on

on the

x2

t2)'(x1 x2)] is nonsingular):
N

N
'5TSLS

and 6., by forming

=

+

yizi/E z =

(1—&)

where
N

Since

&

A

a.s.>

suggests

distribution

A

N

+

VW(STSLS — 6) has the same limiting distribution as

+

This

N

Ezxfl/Uri E

&

(1—cO(62—óuJ.

finding the t

of v'R(t(51—d) +

that

minimizes the variance of the limiting

(1—i) (62—6) 1. The

answer leads to the minimum

65

distance estimator:

choose 6 to

[()c:)]
g =

[C:) (:)]

+ (1—'r) 2

where

(A1'

z

+

+ 2X12 + A22),

The estimator obtained by using a

is the j,k element of A-i.

aod

consistent estimator of A has the same limiting distribution.
a since t is a function of fourth moments and

In general t

Suppose, for example, that z. =

x.2.

Then a

a

0

but I

is not.

0 unless

2
C,

E[(y.1

—

)

E(x2)

)] = 0.

It we add another equation, then we can consider the conventional
three-stage least squares estimator.

Its limiting distribution is derived

in the Appendix (Proposition 7); however, viewed as a jjiu distance
estimator, it is using the wrong norm in general.

Considar the standard simultaneous equations model:

y nfl Cj + u,
+

Ezi

Bxi S
We are continuing to assume that

where FtI + Ba 0 and
y.

is

with

E(ux) a 9,

(xj, y)s i.i.d. according to a distribution
N) , and that E(xx) is rionsingular.
finite tourth moments u=i, .
Nx1, x

I

s

KX1, rI

66

There arc restrictions on F and B: m(F, B) = 0, where tn is a known function.
Assume that the implied restrictions on 11 can be specified by the condition
that

=

vec(fl')

=

f(6),

where the domain of 6 is T1, a subset of

includes the true value 5°(s .s MEC).

that

Assume that Tl and f satisfy assumptions

1 and 2; these properties could be derived from regularity conditions on m,
as in Maljnvaud (197O, proposition 2, p. 670).

Choose

A
6 to

Al

—

mm

Q

[ii —

-

—

6cT1
where Q is given

in

(4.2) and we assume that

Then we have Ai(—6°)

positive definite. Let F =

where A = (F 1—1—].
Q F) .

in (4.1) is

generalizes Malinvaud s minrnum distanc

This

estimator (p. 676); it reduces to his estimator if t4u

so that 2

with
Now

is

uncorrela

d

E(u7u) (13 [E(xx)]1 (u7 = y a flO)
suppose that the only restrictions on 1' and B are that certain

coefticients

are

=

zero, together with the normalization restrictions that
in the

the coefficient of

structural equation i.s

give a* explicit formula for A.

yin

Write the

one.

structural. equation as

Z• +17.in
are the variables in

and x. that appear in

the

equation with unknown coefficients. Let there be F!

equations

and ast.e

(5j, ...,

Then we can

-m -in

where the components of

=

r0, A),

that

5)be

the true value

structural

is nonsingular. Let

sXl, and let £) and (d) be parametric repre-

sentations of r and B that satisfy the zero restrictions and the normalization
rule.

We can choose a

true value 6°,

compact

such that

set T1cR! containing a
is nonsingular for

5cT1.

neighborhood
Then iT =

o the

67

where

vec [—1

)

that f(6) =

Assume

are

parameters

vW(5

- 5°)

(1973,

=

implies that

0

so that the structural

and f satisfy Assumptions 1 and 2, and

Then

—a—> NU1 A). .s'The formula far

—

(r1 ® 1K !zxM

is block diagonal:

3:/as' is

given in Rcthenberg

cV

=

® !;n'

diag{E(z1

x!), ...,

E(zm

E(x x!). So we have

-X

(!tyz

(4.3)

® r)fl t}1

0
00'.is uncorrelated with x4x4,
v4
n r0y. + BN0x1. If u4u.
—s
—

reduces

-I-

then

this

to
A

which

0.

p. 69):

where

where

B(6)]'.

identified.

=

and

()

=

C! [i1(vv) ®

cr1i

!X}'

I

is the conventional asymptotic covarianca matrix for three—stage

least squares (Zeilner and ThieJ. (1962)).

I shall present a generalization of three—stage least squares that
has the same limiting distribution as the generalized minimum distance
estimator.

Let $ = vec(B)

and

note that 7 = — (F1 c' 1)6

have

+
=

(r1 ® I)

((F ® I)ir

+

SI' Q_l[A +

(1 ®

8]' 9[(r ®

I) + B],

Than we

68

where

a

Let

(I

=

£ C1) r

.

0 ,O r
-

xx!)

x

(I '2'

be the tollowing block—diagonal matrix:

S
-,zx

N
a

S

.zx

diag

L

z.1x,. •

•

Z

.

ir

i=1

and let

-.

S

sx=#i1
-.

N

=

Let

-,

=

E(vv°

®

j:j'

W=—iN
S (vv
\ x
.Li _'
N
—

i=].

where

•

-

-

ft.

t;

"

I'

v.ry1*Bxandr
-.1

Now

replace 0 by

s

(I

® s_i) 'Y(i ® sb,

and note that

(I ®
Then

$]

s

'X7

—

V
6.
mZX -

we have the tollowing distance tunction:

(s

_xy

This

s)((F ® ii +

—

S'
—zx

"—1
5)'
1Y
—

—

(s

—xy

— S' 5).
-4%

corresponds to Basmaxin's (1965)
17

squares.

interpretation of three—stage least

69

with

Zin±rnizing

I'
—G3
.

gives

respect to

"—1
= (S
—zx'1'

S'
) —1
-.zx

(S

"—1

s

—zx-- —xy ).
1F

U-

The

limiting distribution of this estimator is derived in the Appendix

(Proposition 7) .

We

record it es

— 50)

Proposition 4.

N(O, A), where A is given in (4.3).

This generalized three—stage least squares estimator is asymptotically
efticient within the class ot minimum distance estimators.

Our derivation of the limiting distribution of

relies on linearity.

For a generalized nonlinear three—stage least squares estimator, see
Hansen

(1982)

Finally, we shall consider the generalization ot two—stage least

squares.38 Suppose that

'il z5'
-1-il +v II'
't

0, z is

where

and rank [E(xjzh)] =

s1.

We complete

the system by setting

+

yin a
where E(x.u.) :0 (in = 2

M).

So

=

x.

(in

2

M), and

diag fz(z1 !) hf—i® E(x1x) }.
Let 5' a

••

:) and apply the minimum distance procedure to

obtain ?; since we are ignoring any restrictions on Ttm (in = 2,

.,

70

A

is

a limited information minimum distance estimator.

a

have

We

N(O, A11), and evaluating the partitioned

gives

inverse

A11 =

(4.4)

x) [Eft41)2 pi''

E(xz1)}1

. äj°z.1.

=

where

{E(z

We can obtain the same limiting distribution by using the following

generalization of two—stage least squares: Let

"' Ni)' F

—

•

and

•

N
a!.

where

estimator);

z (Y1

i1

A

—

(for example,

61

=

2

iI

could be an instrumental variable

then

1G2 =

!i

This is the estimator of

!ti)1 '

F

z*

that we obtain by applying generalized three—

stage least squares to the completed system, with no restrictions on
2

N).

The limiting distribution of this estimator is derived

in the Appendix (Proposition 7):

71

ETotosition

-

5.

NCJ

A11), where A11 is given in

This generalized twa—stage least squares estimator i

(A .4).

asymptotically efficient in the class of limited infonatior, minimum
distance

estimators.

4.4 Asymptotic Efficiency: A Comparison uith
Likelihood Estimator

Assume that

E(r.) =

r,

V(r1)

a

th

is i.i.d. (i=1,2....) from a distribution with

E,

where Z is

a JXJ

positive—deflalta matrix;

subject to restrictions.

Let cy = vec (Z)

the condition that a

and express the restrictions by

where g is a function fr ui T Thto

domain T C R that contains the true value G°(q =

2

(r. —1.

and let ;

the fourth

Suppose that we wish to estimate functions of I

moments are finite.

S

Quasi—lt.vizrwn

j

<

with a

j;jj/), Let

-— -

= vec(S).

If the distribution of

is niultivariate

normal, then the log-likelihood

tunction is

L =f6 lilt

-

If

there are no restrictions on

@0

is

+

-

-

then the maximum 1ike1thod estimator of

a solution to the following problem: Choose e

______

to soive

[C1 (9) (23 z_1(e)](; e g)) a

We shall derive the properties of this estimator when the distribution of

72

r, is not necessarily normal; in that case we shall reter to the estimator

as a asi-maximum likelihood estimator (eQ) .
MaCurdy (1979) considered a version of this problem and showed that,
-

under suitable regularity conditions,
distribution;

has

a limiting normal

the covariance matrix, however, is not given by the standard

intormation matrix

We would like to compare this distribution

tormula.

with the distribution ot the minimum distance estimator.
This 'comparison can be readily made by using theorem 1 in Ferguson (1958).

In our notation, Ferguson considers the following problem:

Choose S to

solve

wG, 6)

[-

g(O)I

0.

He derives the limiting distribution of Vice - 0°) under regularity conditions
on the tunctions W and ,.

These

regula ity conditions are particularly simple

in our problem since W does not depend on ;. We can state them as tollows:

Assumption 3.

E C i1' is an open set containing

one-to-one mapping of E into

g is a continuous,

with a continuous inverse; g has coütlnuous

second partial derivatives in E; rank (3g(G)/'] a p for 8c; E(e) is non—

singular for ec

-0
- as.

In addition, we shall need s

that VSiG -

> g(B

and the central linIt theorem result

(2' ti), where A =

V((r1— I°

® Q.

Then Ferguson's theorem implies that the likelihood equations almost
surely have a unique solution within

2°) _L N(O,

A), where

for sufficiently large N, and

73

(G'4'G) QtU A F

and G = agç°) /e', 'V =(E° ® :°)

.

U will

be convenient to rewrite

this, imposing the symmetry restrictions on E. Let o* be the J(J+1)/2x1
vector formed by stacking the columns of the lower triangle of E. We can
x [J(a÷i)/2J matrix T such that a

define a

=T

a*.

The

elements in each

row of T are all zero except for a single element which is one; T has full

rank. Lets I s*, g(B) :

column

VIi[s* -

then

vector

——> N(O,

where A* is the covariance matrix of the
0

formed from the columns of the lower triangle of (r

A

Consider

¶iJ* A '*

(Gt* 'p*

SC

-

—

I

)

'Y*.

Proposition

6.

to

-- —
- - A.[sk
that contains a neighborhood of

Then the tollowing result is implied by Proposition 2.

If Assumption 3 is satisfied, then

limiting distribution as

same

'jI*

g*(9)J'

where T is a compact subset of
and

G*)(G'*

the following Mnimum distance estimator: Choose

miii. IS< —

—

6°)

has the

-

A* is nonsingular, an optimal minimum distance estimator has p a.s.>

where

is an arbitrary positive real number.

is normal, then
!*,

g*(G), G

we can set

Now

If

A*),

I

a

(1/2)

ç*1

!*; but in general

If the distribution of

j not proportional to

since A* depends on fourth moments and '1' is a tunction ot second moments.

74

is less efticient than the optimal minimum distance

So in general

estimator that uses

-

-

=

(4.5)

—

where s is the vector formed from the lower triangle of
More

) Cr. ) ,
—

generally, we can consider the class of consistent estimators
A

chiang

that are continuously differentiable functions of s:6 êG*)
(1956) shows that the minimum distance estimator based on

minimal

asymptotic covariance matrix within this class.

distance

estimator based on

has the

The minfmt

in (4.5) attains this lower bound.

4.5. Multivricte Probit Models
Suppose that
=

I

if TI' x

+

> 0,

= 0 otherwise (i1, ..., N;

where the distribution of uj1 =

variate normal, N(O, E).

(Uj1

•.

opt, .

.

. H),

.

Urn) conditional on

There may be restrictions on

=

is multi—

(9

but we want to allow E to be unrestricted, except for the scale normalization
that the diagonal elements of E are equal to one.

In that case, the maximum

likelihood estimator has the computational disadvantage of requiring
numerical integration over M—l dimensions.

Our strategy is to avoid numerical integration. We estimate

by

maximizing the marginal likelihood function that is based on the distribution of y

im

conditional on x.

-1

75

P(y.im =

11x)
-i

=

p(TP

xj,

-rn-i.

where F is the standard normal distribution function.

assumptions we have T a.s.Y. °, the true value.
then we can impose the restriction that 1

111[1r

[11 —

We

-

Then-under

standard

If 1ii(!-.Jr°) D N(o, £2),

f(6) by choosing 6 to minimize

Qfl.

only need to derive a formula

for

Our estimator of ir is solving the following equation:

3QçlI)

where
N

N

2{Z

QOT)

i1

ri

Ln.F(IIT x) ÷

(1—y)

Lit [LP(111 x) IL.

Hence the asymptotic distribution of iT can be obtained from the theory
of "M—estimators." Huber (1967) provides general results, which do not
impose differentiability restrictions on SOT). His results cover, for
example, regression estimators based on minimizing the residual sum of
absolute

deviations.

We shall not need this generality here and shall

sketch the derivation tor the simpler, difterentiable case.

This case

has been considered by Hansen (1982), NaCurdy (1981a), and White (1982).41
Let

be i.1.d. according to a distribution with support Z C

Let G be an open, convex subset of
Z X ®

into

its

and let 1P(z, 0), be a function from

component is 1C' 9). For each BEg, 4i is a

measurable function of z, and there is a

with

76

=

For

is

each zZ, )

0,

6°)]

E[1P(z, e°)p'cz

=

C

a twice continuously differentiable function of 6. In

addition,

3=

raf(z1,

e°)

EL

is nonsingular, and

3'(z, e)

(Ic,

far

£, in

1,

..,

p)

6S®, where E[b(z1)j C QO

that

Suppose

such that

have a (measurable) estimator

r

and

N
E

i=1

for

A

—
tP(z1.
!)

s9

sufficiently large N a.s. By Taylor's theorem,

A. £ 'k5i' ØO) +

Si—i

+ I
-

c

NU

i:v' 2NiJ1

where
1

N atPk(zj,
1=1

and

Ak is

ability of

!°)

pa—

1

N
ki' k
E
i—i

ppv
—

on the line segment joining eN and G° (kal,

—

.., F)

(The measur-

follows from lemma 3 of Jezrnrich (1969).) By the strong law

of large numbers,

converges a.s. to the

row of J, and

77

N ki' -NQ

1

ee

(k, t, rnl

.

p).

sufficiently

—e°'

Hence (g

-N-1

_91

=

for N

<

N

1

E h(z.) a.s.> E[h(z1)]

Nk

+ 0-

a.s. and

N

- 6°))

E qi(1.,

in

a.s
large a.s., where

> J. By

the

central limit

thea rem,

oD
—'-N(O,

N

S

)

A).

vW i=i —

Hence

&!N)
Applying

N(O,

this result to our inultivarlate probit estimator gives

D

where

J

N(O,

diag{J1, •
-

J1,}

is

a block—diagonal matrix with

Z[{(F')2/[F(1—F)]} xycj]

(F and its derivative F'

are

evaluated at—mox
-1.

)

and

A

Eta

x11xj

a e with
n 2 mu

where the m, n element of the MXM matrix H is Ii
-F
em = 1(1—F) F'

(n1,, ,.,M)

78

(F and pt are evaluated at °'x1). We obtain a 'oonsistent estimator (Q)

of

by

of IT0,

replacing expectations by samp1 means and

using

in place

Then we can apply, the minimum distance theory of Section 4.2 to

impose restrictions on T.

5.

EMPIRICAL

APPLICATIONS

5.1 Linear Models: Union Wage Effects
We shall present an empirical example that illustrates some of the

preceding results.42 The data come from the panel of Young Men in the National

Longitudinal Survey (Panes)

The sample consists ot 1454 young men

who were not enrolled in school in 1969, 1970, or 1971, and who had complete
data on the variables listed in Table 1.

Table 2.1 presents an unrestricted

least squares regression ot the logarithm ot wage in 1969 on the union, SMSA,
and region variables for all three years.
constant,

schooling,

experience,

The regression also includes a

experience squared, and race.

This

regression

is repeated using the 1970 wage and the 1971 wage.

In Section 2 we discussed the implications of a random intercept (c).
If the leads and lags are due just to c, then the submatrices of j corres-

ponding to the union, SMSA, or region coefficients should have the form
S

I

+ 4

A'.

Consider,

tor example, the 3x3 subinatrix of union coefficients ——

the off—diagonal elements in each co1un should be equal to each other. So
we compare .043 to .046, .042 to .041, and'—.009 to .010; not bad.
In Table 2.2 we add a complete set ot union interactions, so that, tor'

the union variables at least, we have a general regression function.
the submatrix of union coefficients

is 3x7 If it equals

Now

0) + £

A', then

79

Table

1

CT-L4R4CTERISTICS OF NATIONAL LONGITUDINAL SURVEY YOUNG

'4ZN NOT ENROLLED IN SCHOOL IN 1969, 1970, 1971:

Means and Standard Deviations

N=1454
Variable

Mean

5.64
5.74
5.82

LW1
LR2
LW3
Ui

u2
u3

.336
.362
.364

U1tJ2

.270

thUS
U2U3

.262
.303

tJ1U2U3
SMSA1

.243
.697
.627
.622
.409
.404
.410

SMSA2
SMSA3
RNS1
RNS2

RNs3

11.7

S
EXF69

5.11

39.8

Er692

Deviation
.423
.426
.437

2.64
3.71

46.6

.264

RACE

Notes

Standard

to Tabte 1:

LW1, LW2, LW] — logarithm ot hourly earnings (in rents) on the
current

or last job in 1969, 1970, 1971; Ui,

112, U3 ——

I

if wages

on rurrent or last job set by rollertive bargaining, 0 it not, in
1969, 1970, 1971; SMSAJ., SMSA2, SMSA3 — 1 it respondent in SMSA.
0 if not, in 1969, 1970, 1971; RNS1, ENSZ, RNS3 —— 1 if respondent
in South, 0 if not, in 1969, 1970, 1971; 5 —— years ot srhooling
completed; ExPS9 —— (age in 1969 — S — 6); RACE —— 1 it respondent
blark, 0 it not.

80

TABLE 2

UNRESTRICTED LEAST SQUARES REGRESSIONS

2.1
(and Standard Errors) of:

Coefticients

Dependent

Ui

Variable

112

.171

LW].

u3

.042 —.009

SMSSAJ.

135

(.025) (.026) (.025) (.028)

LW2

.150

.048

.010

.086

(.023) (.028) (.026) (.027)

LW3

.041

.046

.132

.083

(.023) (.030) (.030) (.031)

SNSA2 SNSA3

RNS1

RNS2

RNSJ

—.001

—.016

—.020

—.108

(.055)

.032
(.054)

.053

.020

(.065)

(.061)

.003
(.058)

(.056)

(.081) (.081) (.070)
—.039

.065

—.155

(.099) (.109) (.092)

.088

.074

.056

—.232

(.079) (.093) (.078)

Notes to Table 2.1:
LU. regressions ic1ude (1,
calculated using 2 in (4.2)

S, Ei69,

Coefticients

EX2692, RACE).

(and Standard Errors)

The standard errors are

at:

Dependent

Variable

UI.

LWZ

—.047

—.072

(.042)

(.041)

—.019

.014
(.045)

—.085

—.072

—.022

(.053)

(.052)

(.040)

LW3

u3

(.044)

.127

LW1

112

—.050
(.037)

(.040)

tJ1U2

U1u3

tJ2US

i28

.092

(.072)

(.075)

.156
(.070)

—.182
(.104)

.181

.118
(.092)

.227
(.066)

—.229

.264
(.081)

.246
(.079)

—.256
(.113)

(.074)
.110
(.079)

Notes toTable 2.2:
All rressions include (SNSLI, SMSA2, SNSAS, RNSL, RNS2,RNS3, 1, S, EXP69,

E69,

RACE).

U11J2133

The standard errors are calculated using 12 in (4.2).

(.116)

81

in the first three columns, the oft—diagonal elements within a column should

be equal; in the last tour columns, all elements within a column should be
equal.

I first imposed the restrictions on the SMSA and region coetticients,
using the minimum distance estimator.
(4.2), and

=

The

Q is estimated using the formula in

minimum distance statistic (Proposition 3) is

is not a surprising value from a x2(1O)

which

6.82,

It we impose the

distribution.

restrictions o the union coefficients as well, then the 21 coefficients in
Table 2.2 are replaced by 8: one

and seven X's.

the minimum distance statistic (Proposition 3')

is not a surprising value from a

This gives an increase in

of 19.36 —

distribution.

here ak.ainst the hypothesis that all the lags and leads

In the :aninology of Section 3.3,

6.82 i 12.54,

which

So there is no evidence

are generated by

a.

the (linear predictor) relationsht of x

to y arpears to be static conditional on c

Consider a transtormation ot the model in which the dependent variables
are Ltd, LW2—LW1, and r..W3—LW2.

Start with a intiLtivariate

regression

on

all of the lags and leads (and union interactions); then impose the
restriction

that U, SMSA, and RNS appear in the LW2—LW1 and LW3-LW2

equations only as contemporaneous changes (S(Y_Y_jjXitX2X3)
This is equivalent to the restriction that c generates all of the 1ag: and
leads, and we have seen that it is supported by the data.

I also considered

imposing all ot the restrictions with the single exception ot allowing
separate coetticients tor entering and leaving union coverage in the wage
change

equations.

The estimates (standard errors) are .097 (.019) and

82

—.119 (.022). The standard error on the sum of the coefficients is .024,

so again there is no evidence against the simple model with E(ytxiiX2X3C)

ixt

+ C. 43
Table

3.1 exhibits the estimates that result from imposing the

We also give

restrictions using the optimal minimum distance estimator.
the conventional generalized least squares estimates.

They are minimum

distance estimates in which the weighting matrix (As) is the inverse of
N
(5.1)

We

A.

® Nil
A_i

give the conventional standard errors based on (F' Q

—1

1

F)

and the

standard errors calculated according to Proposition 2. which do not require
an assumption ot homoskedastic linear regression.

These standard errors

are larger than the conventional ones, by about 30%.

The estimated gain

in efticiency from using the appropriate metric is not very large; the
standard errors calculated according to Proposition 2 are about 10% larger

when we use conventional GLS instead of the optimum minimum distance
estimator.

Table 3.1 also presents the estimated A's.

Consider,

tor example,

an individual who was covered by collective bargaining in 1969. The linear
predictor of a increases by .089 if he is also covered in 1970, and it
increases by an additional .036 if he is covered in all three years. The
predicted c for someone who is always covered is higher by .102 than for
someone who is never covered.

Table 3.2 presents estimates under the constraint that A
increment in the distance statistic is 89.08 — 19.36 =

0. The

69.72, which is a

83

TABLE 3
RESTRICTED

ES TIMA TES

3.1
Coefticients

(and Standard Errors)

U
.107

B:

(.016)

.121
(.013)
(.018)

B

-.GLS

u2

U].

A:
—

—.02

(.O3

SMSA

tNS

.056
(.020)

—.082

.050

—.085

SMSA2

(.045)

(.017)

(.040)

(.021)

(.052)

1J3

—.067
—.082
(.040) (.037)

SMSA1

ot:

U1u2

U1U3

1J2U3

.156

.152

.195

(.057)

(.062)

(.059)

RNS1

RNs2

BNS3

—.021

-.128
(.068)

—.008

.032

.100

(.025)

(.046)

(.046)

(.072)

(.077)

x2(23) = 19.36
3.2 Restrict A a

11

:
x2(36) z 69.06

(.085)

SMSA3

.086

Coefticients

U1U2U3

(and Standard Errors)

ot:

SMSA

.157

.120

(.012)

(.013)

—.150

(.016)

a4

Jotes

to Tahia 3:
tix

E*(z Lx) -

SMSA1,

=

111x1 + F2x2, Xj

SNSA2, SMSA3, RNS1,

= (Ui,

R}S2, R}53); x = (1,

= ' 9' SMSA Y 5 13)
restrictions are expressed

are

as ii =

+ Z
F

minimum distance estimates with

A'; fl

in

The first standard error for
the second standard error for

on
'.—l

V

-1.

'—1

—i F)—1

A..i
Q

computed from NPT — P

'

A_i .¼
[IT —

Q

is unrestricted.

in (4.2);

=

=

S, EXP69, E)692 FACE)

is unrestricted. The

6, where 6

minimum distance estimates with

the table)

U2, U, U1U2, 131133, U21J3, U1JJ2U3,

and A

and èGLS

are

is not shown in

(5.1) (A

is the conventional one based

is based on

(Proposition 2) .

) (Proposition

3)

The X

2

statistics

are

85

surprisingly large value to come from a

(13)

distribution.

If we constrain

only the union XTs to be zero, then the increment is 57.06 — 19.36 = 37.7,

which

is surprisingly large coning from a x

(7)

distribution.

So there is

strong evidence for heterogeneity bias.
The union coefficient declines from .157 to .107 when we relax the
A =

0

restriction.

The least squares estimates for the separate cross

sections, with no leads or lags, give union coefficients of .195, .189,
and .191

in

1969, 1970, and 1971.

So

the decline in the union coefficient,

when we allow for heterogeneity bias, is 32% or 44% depending on which

biased estimate (.16 or .19) one uses.
also decline in absolute value.
separate

The SMSA and region coefficients

The least squares estimates for the

cross sections give an average SMSA coefficient of .147 and an

average region coefficient of — .131.

So the decline in the SMSA coefficient

is either 53% or 52%, and the decline in absolute value of the region
coefficient is either 45% or 37%.

5.2.

Nonlinear

Models:

Labor Force Participation

We shall illustrate some of the results in Section 3.

The sample

consists of 924 married women in the Michigan Panel Study of Income Dynamics.
The

sample selection criteria and the means and standard deviations of the

variables

are in Table 4. Participation status is measured by the question

"Did _______ do

any work for money last yeas?" We shall model participation

in 1968, 1970, 1972, and 1974.

In terms of the model described in Section 3.1, the wage predictors are
schooling,

experience,

and experience squared, where experience is measured

86

as age minus schooling minus six; the tastes for nortmarkat time are
predicted by these variables and by children.

The specification tor children

is a conventional one that uses the number of children of age less than six

(YS) and the total number of children in the family unit (K).46 Variables
that aftect only the litetime budget constraint in this certainty model
are captured by c.

In particular, nonlabor income and the husband's wage

are assumed to affect the wife's participation only through the lifetime
budget

constraint.

The individual effect Cc) will also capture unobserved

permanent components in wages or in tastes for nonmarket time.

Table 5 presents maximum likelihood

(ML)

estimates of cross—section

probit specifications for each of the four years.

Table 6 presents un-

restricted ML estimates for all lags and leads in YI( and K.

in the latent variable model (3.1) have constant variance,

residuals

then a1

It the

.

.

.=

(3.8). and the submatrices of TI corresponding to

YK and K should have the form

I + 1. X'.

There may be some indication

of this pattern in Table 6, but it is much weaker than in the wage
regressions in Table 2.

we allow for unequal variances and provide formal tests by using the
minimum distance estimator developed in Section 4.5.

In Table 7.1 we

impose the restrictions that

Jtadiag{cs1,

.,cZ4}[&f7I4+LXfl(, Kt4+tXKi

The minimum distance statistic is

coming from a X(19) distribution.

53.8,

which is a very surprising value

So the latent variable C does not appear

to provide an adequate interpretation ot the unrestricted leads and lags.

a7

It may be that the distributed lag relationship between current participation and previous births is more general than the one implied by

scing over the previous six years (YK) and over the previous eighteen
years

work.

It may be truittul to explore this in more detail in tuture

(K) .

Perhaps strict exogeneity conditional on c will hold when we use a

more general specification tor lagged births.

But we must keep in mind

that this question is intrinsically tied to the functional form restrictions —
we

saw in Section 3.3 that there always exist specifications in which

is

conditional on c.

independent ot

It we do impose the restrictions in Table 7.1, then there is strong

evidence that A

0. Constraining X a 9

in Table 7.2 gives an increase

in the distance statistic of 78.4 — 53.8 s 24.6, which is surprisingly
large to come from a X2(8) distribution.

In Table 7.3 we constrain all of the residual variances to be equal
(at = 1)

.

An

alternative interpretation ot the time varying coefticients

is provided in Table 7.4, where

and

In principle, we could also glow the
identified from changes

model gives very

over

imprecise

time

vary freely over time and

1.

to vary freely, since they can be

in the coefficients

of

c.

In tact that

results and it is difficult to ensure numerical

accuracy.
We shall interpret the coefficients on '(K and K by following the procedure in (3.4).

Table 8 presents estimates of the expected change in the

participation probability when we assign an additional young child to a
randomly chosen family, so that YK and K increase by one.
measure for the models in Tables 7.1, 7.3, and 7.4.

We compute this

The average change in

88

the participation probability is —.096. We can get an indication of omitted
variable bias by comparing these estimates with the ones based on Table
1.2, where X is constrained to be zero.
participation

probability

is — .122,

Now the average change in the

so that the decline in absolute value

when we control for c is 21%. An alternative comparison can be based on the
cross—section estimates, with no leads or lags, in Table 5.

Now the average

change in the participation probability is —.144, giving an omitted variable bias
ot 33%.

Next we shall consider estimates from the logit framework of Section 3.2.
Table

9 presents

(standard) maximum likelihood estimates of cross—section

logit specifications for each of the four years. We can use the cross—section
probit results in Table 5 to cc.iistruct estimates of the expected change in
the log odds of participation
tamily.

en we add a young child to a randomly chosen

Doing this in each of the four years gives —.502, —.598, —.683, and

—.703. With the logit estimates, we simply add together the coefficients on
YK and K in Table 9; this gives —.507, —.612, —.691, and —.729. The average
over the four years is —.621 for probit and —.635 for logit. so at this
point there is little difterence between the two functional torms.

Now allow for the latent variable Cc). Table 10 presents the conditional
maximum likelihood estimates for the fixed effects logit model.

The striking

result here is that, unlike the probit case, allowing for c leads to an
increase in the absolute value ot the children coefticients.

and

If we constrain

to be constant over time (Table 10.1), the estimatedehange in the

log odds of participation when we add an additional young child is —.898.
If we allow

and

to vary freely over time (Table 10.2), the average of

89

the estimated changes is —.883.

So the absolute value ot the estimates

increases by about 40% when we control for c using the logit framework.
The estimation method is having a first order eftect on the results.

It is commonly found that probit and logit

specifications,

when properly

interpreted, give very similar results; our cross—section estimates are an
example ot this.

But our attempt to incorporate latent variables has turned

up marked differences between the probit and logit
are

specifications.

There

a number of possible explanations for this. The probit specification
to have a normal distribution conditional on x with a linear

restricts
regression

tunction and constant variance.

The conditional likelihood

approach in the logit model does not impose this possibly false restriction.

On the other hand, the probit model has a more general specification for the
residual
We

covarianca matrix.
have seen that the restrictions on the probit

IT matrix,

which under-

lie our estimate of , appear to be false. An analogous test in the logit
framework is based on (3J.O). We use conditional ML to estimate a model

that includes YKsDt KsD (s

1,... ,4; t •

2,3,4),

variable that is one in period t and zero otherwise.

where D is a dummy
It is not restrictive

:0 exclude YK5D1 and lC'D since they can be absorbed in c. We include

fl S.Dti EXP68.D and EX26821]Dt (t—2,3,4). Then comparing the maximized
:on.ditional likelihoods for this specification and the specification in
ilso

Table 10.2 gives a conditional likelihood ratio statistic ot 47.5, which
is a very surprising value to come from a x2(16) distribution.
restrictions

underlying our logit estimates

of

So the

also appear to be false.

90

It may be that the false restrictions simply imply different biases in

the probit and logit

specifications.
6.

CONCLUSION

Our discussion has focused on models that are static conditional on
a latent variable.

The panel aspect of the data has primarily been used

to control tor the latent variable.

Much work needs to be done on models

that incorporate uncertainty and Interesting dynamics.

Exploiting

the

martingale implications ot time-additive utility seems truittul here,

as'

in Hall (1978) and Hansen and Singleton (1981) . There is, however, a

potentially important distinction between time averages and cross—section
averages.

A time average of forecast arrors over T periods should converge

to zero as T +

C3 But

an average of forecast errors across N individuals

surely need not converge to zero as N +

there my be common components

in those errors, due to economy—wide innovations. The same point applies
when we consider covariances of forecast errors with variables that are
in the agents' intormation sets.

It those conditioning variables are

discrete, we can think of averaging over subsets of the forecast errors;
as T -

these averages should converge to zero, but not necessarily

as N As

for controlling for latent variables, I think that future work

will have to address the lack ot identitication that we have uncovered.
it is not restrictive to assert that (y1, .

.

.

.

and (x1, .

independent conditional on some latent variable c.

.

.

.

are

91

TABLE 4
CYA&CTh'MSTICS OF [4rCHIGAN PAJVEE STUDY OF INCOME'
DYNAMICS M42&IED VOMEN

Mewts crid Stzdard Deviations
N = 924

Variable

Mean

LYP1
LFPZ

.499
.530
.529
.566

lFP3

lFP4

1.200

.969
.764
.551
.363

11(2

YK3
YK4

1.069
.895

.685
1.69
1.64
1.61
1.52

2.38
2.30

K2
K3
K4

2.11
1.84

2.1

12.1
17.2
368.

S

Efl68

Ei682

Deviation

Standard

8.5
301.

Notes to Table 4:
LFP1, . . . . LF24

——

1

work for money

it answered "yes" to !!Did

last year, 0 otherwise, reterring to 1968, 1970, 1972, 1974; YK.1,

YK4

—

.

number ot children ot age less than six in 1968, 1970, 1972,

1974; Ki ,

...., K4

—

number

of children of age less than eighteen

living in the family unit in 1968, 1970, 1972, 1974; S —— years of

schooling completed; E68 —— (age in 1968 —

S—6)

.

The

sample selection

criteria required that the women be married to the same spouse from
1968 to 1976; not part of the low income subsample; between 20 and 50

92

years old in 1968; white; out of school from 1968 to 1076; not disabled.

We required complete data on the variables in the Table, and that there
be no inconsistency between reported earnings and the answer to the parti—
cipation

question.

93

TABLE 5

ML PROBIT CROSS-SECTION ESTIMATES

Coefticients

Dependent
Variable

YtU

YX2

(and Standard Errors)

YKJ

YK4

a

—

-.246
(.046)

LFP1

—.063

—

(.055)

K3

-

-

—.075

—

—

—.077

(.067)
—

(.081)

iIQTES TO TABLE 5:

separate

fl.1

estimates each year. All specifications include (1, S,

EXP68, EXP682).

—

—

(.032)

— .366

LF24

K4

(.031)

—.342

LF23

K2

(.031)

—.293

LF22

K].

ot:

—.069
(.034)

94

TABLE 6
I

UNRESTRICTED ML PROBIT ESTIM4TES

Coefticients

Dependent

Variable
ISP1

LFP2

LFP3

LFP4

YK1

YKZ

(and Standard Errors)

YK3

YK4

—.205

—.017

(.081)

(.119)

—.160
.420
(p141) (.144)

—.047

—.238

—.047

.093

ot:

K3

K4

—.142
(.100)

—.196
(.110>

.063
(.090)

.320
(.077)

—.278
(.102)

—.250
(.110)

.177
(.090)

.204

.030
(.090)

fl
.176
(.076)

(.079) (.117)

(.140) (.142)

—.254
.214
(.080) (.116)

—.190

—.210

—.045

(.139) (141)

(.077)

(.102)

(.112)

—.195

—.211

—.282
(.138)

20
(.)75)

.083
(.100)

—.181

(.139)

(.079)

.252
(.118)

—.209

(.110)

.058
(.090)

NOTES TO TABLE 6:

Separate xt

Er682).

estimates

each

year.

All specifications include

(1, S,

EXP68,

