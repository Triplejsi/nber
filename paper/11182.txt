NBER WORKING PAPER SERIES

COHORT TURNOVER AND PRODUCTIVITY:
THE JULY PHENOMENON IN TEACHING HOSPITALS
Robert S. Huckman
Jason R. Barro
Working Paper 11182
http://www.nber.org/papers/w11182
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
March 2005

We thank David Cutler, Arnold Epstein, LeRoi Hicks, Barbara McNeil, Paul Oyer, Douglas Staiger, and
seminar participants at Harvard Business School, the Stanford Strategy Conference, the 14th Annual Health
Economics Conference, and the National Bureau of Economic Research’s Summer Insitute for helpful
comments. We also thank Juliana Pakes for information regarding the Charlson Index. We acknowledge
financial support from the Division of Research at the Harvard Business School. The views expressed herein
are those of the author(s) and do not necessarily reflect the views of the National Bureau of Economic
Research.
© 2005 by Robert S. Huckman and Jason R. Barro. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is
given to the source.

Cohort Turnover and Productivity: The July Phenomenon in Teaching Hospitals
Robert S. Huckman and Jason R. Barro
NBER Working Paper No. 11182
March 2005
JEL No. I1, J0
ABSTRACT
We consider the impact of cohort turnover—the simultaneous exit of a large number of experienced
employees and a similarly sized entry of new workers—on productivity in the context of teaching
hospitals. In particular, we examine the impact of the annual July turnover of house staff (i.e.,
residents and fellows) in American teaching hospitals on levels of resource utilization (measured by
risk-adjusted length of hospital stay) and quality (measured by risk-adjusted mortality rates). Using
patient-level data from roughly 700 hospitals per year over the period from 1993 to 2001, we
compare monthly trends in length of stay and mortality for teaching hospitals to those for
non-teaching hospitals, which, by definition, do not experience systematic turnover in July. We find
that the annual house-staff turnover results in increased resource utilization (i.e., higher risk-adjusted
length of hospital stay) for both minor and major teaching hospitals and decreased quality (i.e.,
higher risk-adjusted mortality rates) for major teaching hospitals. Further, these effects with respect
to mortality are not monotonically increasing in a hospital’s reliance on residents for the provision
of care. In fact, the most-intensive teaching hospitals manage to avoid significant effects on
mortality following this turnover. We provide a preliminary examination of the roles of supervision
and worker ability in explaining why the most-intensive teaching hospitals appear able to reduce
turnover’s negative effect on performance.
Robert S. Huckman
T17 Morgan Hall
Harvard Business School
Boston, MA 02163
and NBER
rhuckman@hbs.edu

Jason Barro
280 Beacon St . #5
Boston, MA 02116
and NBER
jason.barro@post.harvard.edu

Nearly all managers must deal with the consequences of employee turnover
within their organizations. Despite the importance of this issue, several authors have
observed that academic attention has been disproportionately focused on the causes
rather than consequences of turnover (Staw, 1980; Mobley, 1982; Glebbeek and Bax,
2004). One possible explanation for the dearth of studies on the effects of turnover is the
difficulty in answering this question empirically. Turnover is an endogenous
phenomenon that may occur for a wide variety of reasons that are not observed by the
researcher. For example, more productive workers may be more likely to remain with a
given company longer than less productive ones (Jovanovic, 1979). Under such
circumstances, it is difficult to make causal inferences concerning turnover'
s effect on
productivity and performance using firm-level data.
A second issue concerning the effect of turnover on firm performance is that
turnover, itself, appears in multiple forms. Many firms face a continuous stream of
individual turnover in which employees leave and are replaced by new workers at various
points throughout the year. In such settings, there is no one particular time during the
year when managers are required to train and orient a large portion of their workforces.
In contrast, other firms bring on new employees in large numbers at discrete
points in the year. For example, law and consulting firms tend to start most of their new
employees in late summer or early fall. These new employees must all be trained and
integrated into the firm at one time. In the law and consulting examples, the potential
negative effects of the large inflow of new workers may be buffered by the fact that firms
do not face the simultaneous exist of large portions of their experience workers. Rather,
these departures occur in a roughly continuous manner throughout the year.

1

An extreme, though not uncommon, form of this discrete scenario is what we
term cohort turnover—the simultaneous exit of a large number of experienced employees
and a similarly sized entry of new workers—and serves as the focus of this study.
Examples of cohort turnover can be found in changeovers that occur between military
units in combat, political administrations,1 and residents and fellows in teaching
hospitals. The use of replacement workers during large-scale labor strikes represents yet
another case of cohort turnover. Given the number of individuals transitioning either into
or out of employment at a specific point in time, cohort turnover raises concerns about
adverse effects on productivity due to factors such as operational disruption (Krueger and
Mas, 2004) or the loss of the tacit knowledge (Polanyi, 1966) held by departing workers.
In this paper, we consider cohort turnover among house staffs (i.e., residents and
fellows) in teaching hospitals. Residency represents a new physician'
s first assignment
following medical school and typically lasts from three-to-five years depending on the
doctor'
s area of specialization. In certain specialties, residency will be followed by a oneor two-year fellowship, during which the doctor receives further training in a subspecialty. At the beginning of every July, the most senior residents move on to
permanent medical positions or fellowships at other hospitals, and recent medical school
graduates arrive as first-year residents, also known as interns. This turnover leads to a
significant lack of continuity and a discrete reduction in the average experience of the
labor force at teaching hospitals every summer. In addition, this changeover may disrupt
established teams of doctors and other caregivers within hospitals. Either of these effects

1

For example, Boylan (2004) examines turnover in the United States Attorney’s Office and finds
that 40% of turnover between 1969 and 1999 occurred during a President’s first year in office.

2

may have potentially troubling consequences for the two determinants of hospital
productivity—resource utilization and clinical quality.
This “July phenomenon” is often mentioned in the lore of medical professionals.
Many physicians have, perhaps jokingly, counseled patients not to get sick in July. As of
yet in the medical literature, however, any identified July phenomenon has been limited
to declines in hospital efficiency (i.e. higher costs or lengths of hospital stay) without any
significant impact on clinical outcomes, such as mortality. As we discuss below, several
of these studies are limited either by relatively small sample sizes or issues concerning
their empirical strategy for identifying effects.
We examine the impact of the July turnover on hospital productivity using data on
all patient admissions from a large, multi-state sample of American hospitals over a fiveyear period. By comparing trends in teaching hospitals to those for non-teaching (i.e.,
control) hospitals over the course of the year, we find significant negative effects of the
residency turnover on both hospital efficiency (as measured by average length of stay
(LOS)) and clinical quality (as measured by risk-adjusted mortality rates). Over some
range, these effects appear to be increasing in the degree to which a hospital relies on
residents (as measured by the number of residents per hospital bed). Nevertheless, those
hospitals with the highest levels of residents per bed (i.e., high teaching intensity) appear
to be less affected—in terms of mortality—by the July phenomenon than facilities with
medium teaching intensities. We find initial evidence suggesting that this non-linearity
in the July phenomenon may be due to higher levels of supervision for new residents at
the most intensive teaching hospitals.

3

EMPLOYEE TURNOVER AND PRODUCTIVITY
As noted by several authors, (Staw, 1980; Mobley, 1982; Glebbeek and Bax,
2004), there is significantly more literature examining the causes, rather than
consequences, of turnover. Further, the literature that does address the consequences of
turnover is split over the direction of these effects.
In his balanced review of the potential consequences of turnover, Staw (1980)
notes several theoretical, negative effects of turnover on organizations. These include
selection, recruitment, and training costs for replacement workers, operational disruption,
and demoralization of remaining workers. In a sample of nearly 1,000 firms, Huselid
(1995) found a negative relationship between turnover and productivity (measured by
sales per employee) and corporate financial performance (as measured by Tobin'
s q).
One explanation for the hypothesized negative effect of turnover on productivity
and performance is that job exits interfere with learning by individuals or teams. Several
studies find that worker productivity improves with experience2 (Levhari and Sheshinski,
1973; Newell and Rosenbloom, 1981; Maranto and Rodgers, 1984, Hellerstein and
Neumark, 1995). While experience with a given firm likely leads to higher productivity,
the converse may also be true—more productive workers may be less likely to leave a
firm (Jovanovic, 1979). Further, Price (1977) notes that younger workers exhibit higher
rates of turnover than older workers. To the extent that instances of turnover are not
randomly distributed across workers with different levels of underlying productivity, the
turnover of individual employees may be an endogenous event.

2

Staw (1980) suggests that the opposite may be true. In many settings, the relationship between
tenure and performance may assume an inverted-U shape, with performance initially increasing and later
decreasing with tenure.

4

To address this issue, several studies have used levels of union presence as a
proxy for the workforce stability that firms experience in the absence of turnover. Brown
and Medoff (1978) suggest that this stability accounts for some portion of the positive
relationship they observe between unionization and productivity. Similarly, Clark (1980)
finds a positive relationship between unionization and productivity, though he suggests
that additional evidence is required to establish the degree to which this relationship is
explained by lower turnover. Freeman and Medoff (1984) provide a summary of the
factors—including, but not limited to, lower turnover—that may explain this relationship.
While these negative effects have received significant attention, several studies
discuss turnover as a contingent, or even positive, phenomenon.3 For example, Jovanovic
(1979) presents turnover as a key element in the process of improving matches between
employers and employees over time. Staw (1980) describes performance as a function of
both skill and effort. As a result, in many settings—particularly high-stress professions
that may lead to employee burnout—the relationship between tenure and performance
may assume an inverted-U shape. Employee skill will initially increase faster than effort
decreases; as burnout begins, however, effort will decline faster than skill improves.
Assuming that performance follows an inverted-U shape over time, turnover may thus
improve average performance. Beyond the replacement of less productive with more
productive workers, turnover of poor performing employees may also serve to improve
the morale and motivation of workers who remain with the firm (Staw, 1980).
The replacement of low performing workers and the increased motivation of
remaining employees both suggest that turnover could have particularly strong positive
3

Dalton and Todor (1979) and Staw (1980) provide reviews of both the positive and negative
effects of turnover on performance.
5

effects in settings requiring substantial levels of innovation and adaptation (Dalton and
Todor, 1979; Staw, 1980; Mobley, 1982). In support of this claim, experimental work
by Argote et al. (1995) finds that while turnover has a negative effect, on average, on
group performance, this effect is more pronounced for simple tasks than for complex
activities requiring innovation. Argote and Epple (1990), however, also find that
turnover does not appear to have a negative effect on firm productivity in settings where
work is relatively standardized, as the knowledge required to perform a task is codified
and can be easily transferred to new workers. Together, these studies suggest that tasks
requiring intermediate levels of innovation may be the most susceptible to performance
declines following turnover.
Beyond the nature of the task, several other factors may affect degree and
direction of turnover'
s impact on performance. These include the degree of hierarchy
within an organization (Carley, 1992), whether turnover itself is voluntary or involuntary
(Price, 1977), whether turnover occurs in a predictable manner (Staw, 1980), and the
absolute level of turnover in the organization. With respect to this final factor, some have
suggested that the relationship between turnover and performance exhibits an inverted-U
shape with a medium level of turnover being preferred to both low and high levels
(Abelson and Baysinger, 1984).
Beyond its mixed findings, much of the prior work on turnover either explicitly or
implicitly considers only individual turnover; significantly less attention has been
devoted to cohort turnover, the phenomenon we consider in this paper. One analog to
cohort turnover whose performance implications have been considered in the literature is

6

the turnover of management teams (e.g., Tushman and Rosenkopf, 1996).4 It is not clear,
however, how one should expect findings on management turnover to generalize to the
performance of line workers. While not a direct study of cohort turnover among line
workers, Krueger and Mas (2004) find evidence that a period of significant labor
unrest—including a strike and the large-scale use of replacement workers—in a Firestone
tire plant was associated with reduced product quality. The effects identified in their
work speak to the impact of labor unrest on performance, but do not separate the effects
of worker discontent from the impact of the cohort turnover that occurs when striking
employees are replaced.
One reason for the lack of attention to cohort turnover may be the fact that it
occurs less frequently than individual turnover. Nevertheless, it takes place in several
important settings including military deployments, changes of political administrations,
labor strikes, and, of course, annual house staff turnover in teaching hospitals. A second
reason for the lack of attention placed on cohort turnover may be the belief that the
answers to questions concerning its effects are obvious—given the sheer magnitude of
the change it brings, cohort turnover must have a detrimental impact on performance.
Despite the magnitude of cohort turnover, however, it often occurs in a predictable
fashion and the affected organizations should, theoretically, have time to prepare for its
occurrence. For example, attending physicians in teaching hospitals—being aware of the
turnover that occurs each July—may focus intently on supervising new residents at that
time of the year. As a result, the impact of worker turnover in settings where the

4

Much of the literature on the turnover of management teams focuses on the determinants of such
activity (e.g., Fee and Hadlock, 2004; Hayes, Oyer, and Schaefer, 2005), thereby treating turnover as the
dependent, rather than independent, variable.

7

supervisory staff (in this case, attending physicians) does not change over is not a priori
obvious.5 The presence of a stable cadre of potential supervisors in teaching hospitals
allows us to examine the impact of supervision as a potential moderator of turnover’s
effect on performance. We return to this issue at the end of our empirical analysis.
Perhaps the most attractive empirical characteristic of cohort turnover is that it
typically occurs for exogenous reasons that are independent of the performance of
individual workers. Such changeovers are mandated as a matter of policy, as is the case
with teaching hospitals where the annual turnover occurs regardless of the underlying
productivity of the physicians and hospitals involved. In contrast, instances of individual
turnover may occur due to particular characteristics of the departing or entering worker—
such as motivation or ability—that remain unobserved by the researcher and may bias
statistical estimates of turnover’s impact.

COHORT TURNOVER IN TEACHING HOSPITALS
It is widely agreed that teaching hospitals have two primary objectives—the
provision of high quality medical care and the training of new doctors. These related but
distinct objectives overlap within medical residency programs. Medical school graduates
in the United States apply for residencies at any of the roughly 800 teaching hospitals in
the country. Depending on a physician’s specialty, residencies typically last for three-tofive years, during which time residents represent an important piece of a hospital’s
system for delivering care.

5

We also note that the non-physician clinical staff (e.g., nurses, technicians) in teaching hospitals
does not turnover each July. These individuals may provide informal supervision to new residents.

8

Patient care in teaching hospitals is provided by teams of medical professionals
that include attending physicians, fellows, residents, and medical students. Much of the
care for patients is provided by a resident, who supervises medical students and is
supervised by the chief (i.e., most senior) resident in that field and an attending physician.
The daily activities of residents include admitting, treating, and discharging patients. In
some departments, fellows (i.e., physicians who have completed their residencies) my
provide an intermediate level of supervision between the residents and attending
physicians.
Residency programs in the United States are structured like schools. Each class
of residents enters together at the beginning of the academic year, and the senior
members of the program all graduate together. For residency programs, the year begins
and ends on July 1st. The annual transition, however, does not occur all on one day.
Typically, hospitals will complete the transition over a two-to-three week period, lasting
from the middle of June through the first week of July.
One might imagine that hospitals would transition the new interns into their
positions slowly. Anecdotal evidence, however, suggests that each cohort of residents
typically moves up one level and covers the entire set of responsibilities of the group it is
replacing. As a result, on day one new interns may have the same responsibilities that the
now-second-year residents had at the end of June (i.e., after they had a full year of
experience).
The changeover creates the potential for turmoil in teaching hospitals as each
cohort of doctors becomes comfortable with new roles and responsibilities. With respect
to this changeover, Claridge et al. (2001) note, “During this time of year, there is clearly

9

a feeling of apprehension among providers of health care, as well as among many
patients.” Gawande (2002) echoes these concerns:
In medicine we have long faced a conflict between the imperative to give
patients the best possible care and the need to provide novices with
experience. Residencies attempt to mitigate potential harm through
supervision and graduated responsibility…But there is still no getting
around those first few unsteady times a young physician tries to put in a
central line, remove a breast cancer, or sew together two segments of
colon. No matter how many protections we put in place, on average these
cases go less well with the novice than with someone experienced.
These anecdotal observations suggest the need for systematic analysis of the implications
of this annual turnover for medical productivity.
Most of the medical literature on staffing and performance in teaching hospitals
deals with issues concerning limitations on resident work hours6 or differences in
outcomes on weekends and weekdays7—two periods when the average level of on-dutyphysician experience is expected to differ substantially. There exists a limited set of
previous studies in the medical literature dealing with the July phenomenon.
Barry and Rosenthal (2003) test for a July phenomenon in a sample of 28
hospitals in northeast Ohio. Similar to our approach, they compare teaching and nonteaching hospitals in terms of LOS and risk-adjusted mortality, though they focus solely
on patients in intensive care units (ICUs). They do not find evidence of the July
phenomenon in this population, but do note that their findings may not generalize to nonICU patients. Specifically, they suggest that, given the severity of patients in this setting,
ICU residents may receive higher levels of supervision than their non-ICU counterparts.

6

Examples include Thorpe (1990), Laine et al. (1993), Leach (2000), Gaba and Howard (2002),
Steinbrook (2002), and Weinstein (2002).
7

Examples are Hendry (1981), Bell and Redelmeier (2001), and Dobkin (2002).

10

Some studies find a link between the July turnover and hospital inefficiencies.
Rich et al. (1993) examine several teaching hospitals in the Minneapolis area and find
that doctors spend less money on diagnostic tests and pharmaceuticals as their experience
increases throughout the academic year. These results, however, applied only to medical
(i.e., non-surgical) patients. Rich et al. (1993) use a difference-in-differences approach,
much as we do in this paper, to control for seasonal patterns. They utilize patient
outcomes in non-teaching hospitals as a baseline from which to estimate the impact of the
July turnover for teaching hospitals. They are able to identify some changes in efficiency
but are unable to find any evidence of mortality differences. Compared to our study, both
Rich et al. (1993) and Barry and Rosenthal (2004) rely on data from a small number of
hospitals, which may explain why they do not find significant effects on outcomes.
While they do not directly test for the presence of a July phenomenon, Griffith,
Wilson et al. (1997) examine patterns in test ordering among physicians in the neonatal
intensive care unit at a single hospital. They find that first-year interns are more likely to
incur higher charges than their more experienced colleagues. In addition to its small
sample size, this study is limited by the fact that it does not consider effects on medical
outcomes. As such, it is difficult to determine whether these higher charges reflect
beneficial attention to detail or unnecessary utilization due to inexperience.
A third study claims to reject the existence of a July phenomenon on any
dimension for the trauma unit at one particular hospital (Claridge et al., 2001). This
paper compares patient outcomes in April and May with those in July and August and
does not identify any significant differences between the two periods. Given their study
design, however, Claridge et al. (2001) are unable to control for seasonal variations in

11

patient outcomes that could affect outcomes at all hospitals regardless of teaching status.
For example, as we will illustrate later, patients admitted to hospitals in the winter have
higher mortality rates than those admitted in the summer. Without some baseline to
adjust for exogenous changes in patient outcomes, a comparison of outcomes for one
hospital at two times of the year may be confounded by these omitted variables.

DATA
The primary source of data for this analysis is the Healthcare Cost and Utilization
Project (HCUP) National Inpatient Sample (NIS) for each year from 1993 to 2001.8 NIS
contains discharge-level data for all inpatient cases at a sample of roughly 20% of the
community hospitals9 in the United States. Depending on the year, NIS includes
information for hospitals from between 17 and 33 states.
For each patient, NIS provides information on patient age and gender, expected
primary payer (i.e., Medicare, Medicaid, private including HMO, self pay, no charge, and
other), length of stay (LOS), total charges, and in-hospital mortality. In addition, NIS
includes detailed data on a patient’s principal and secondary diagnoses, principal and
secondary procedures, and diagnosis-related group (DRG).

8

The NIS database is administered by the Agency for Healthcare Research and Quality (AHRQ),
previously known as the Agency for Health Care Policy and Research (AHCPR).
9

The NIS definition of “community hospital” is the same as that used by the American Hospital
Association (AHA): “…‘all nonfederal, short-term, general, and other specialty hospitals, excluding
hospital units of institutions.’ Included among community hospitals are specialty hospitals such as
obstertrics-gynecology, ear-nose-throat, short-term rehabilitation, orthopedic, and pediatric. Excluded are
long-term hospitals, psychiatric hospitals, and alcoholism/chemical dependency treatment facilities
(Healthcare Cost and Utilization Project, 1999).”

12

We link the NIS data with information from the AHA Annual Survey of
Hospitals, which includes data on the operating and financial characteristics for more
than 6,000 hospitals each year. In addition to several other items, the AHA database
provides information on the number of hospital beds and full-time residents (including
interns) at each facility in a given year. Using this information, we are able to construct
our measure of teaching intensity—full-time residents per hospital bed.
Our final sample of facilities is limited to those that appear in both the NIS and
AHA databases. The appendix presents the number of hospitals that appear in our
sample and in the NIS by year. For each year and state, the table provides the number of
hospitals appearing in the NIS and in our matched NIS-AHA sample. Most of the
discrepancies between the matched sample and the NIS are due to the fact that certain
states opted not to provide the hospital identifiers required to match NIS and AHA data in
certain years. In other rare cases, a hospital may appear in the NIS but not the matched
sample because that facility did not appear in the AHA data for a given year.

EMPIRICAL METHODOLOGY
Hospital Categories
The source of identification in our empirical analysis is the varying degree to
which certain types of hospitals rely on residents. Initially, we divide hospitals into three
categories—non-teaching hospitals, minor teaching hospitals, and major teaching
hospitals. Non-teaching hospitals are those that are not listed as teaching hospitals in the
NIS. These facilities have few, if any, residents. As such, we would not expect them to
be affected by the July changeover. Those hospitals that are listed as teaching hospitals

13

in the NIS data are subdivided into two categories. Minor teaching hospitals are those
teaching hospitals that have resident intensities (i.e., full-time residents per inpatient
hospital bed) that are less than 0.25, while major teaching hospitals are those facilities
with teaching intensities equal to or greater than 0.25. This threshold for resident
intensity is used by the Medicare Payment Advisory Commission (MedPAC) to
distinguish minor and major teaching facilities (Medicare Payment Advisory
Commission, 2002).
Figure 1 illustrates that, in the aggregate, both LOS and mortality vary quite
substantially throughout the calendar year. Productivity appears to decline in the winter
months, as evidenced by increases in both LOS and mortality during that period. This
pattern has been noted by epidemiologists (e.g., Gemmell et al., 2000) and has been
attributed to a range of factors including the impact of seasonal disease (e.g., influenza
and respiratory illness) and weather. Key to the empirical strategy in our paper is the use
of non-teaching hospitals as a control for these seasonal changes in outcomes, which
should affect all hospitals regarding of teaching status. We can thus calculate “deseasoned” trends in LOS and mortality for teaching hospitals to determine the extent of
potential effects around the July turnover.
Table 1 presents descriptive statistics for each of the three hospital categories as
well as for the entire sample. The first row illustrates the differences in average teaching
intensity across the three groups. This average measure increases from 0.01 for nonteaching facilities to 0.10 and 0.53 for minor and major teaching hospitals, respectively.
In terms of both measures of facility size—hospital beds and admissions per year—the
hospitals get progressively larger as the level of teaching intensity increases. Teaching

14

intensity is also correlated with the demographics of a hospital’s patient base. In
particular, non-teaching hospitals attract older patients than either type of teaching
hospital. The average age for patients at non-teaching facilities is 49.0 versus 45.0 and
40.9 for minor and major teaching hospitals, respectively. In addition to having younger
patients, the major teaching hospitals in our sample also have a higher percentage of
Medicaid patients than the other groups. Moving from non-teaching to minor teaching to
major teaching, this percentage increases from 15% to 17% to 26%. This relationship is
consistent with the fact that many teaching hospitals are located in densely populated
cities.
The bottom portion of Table 1 presents information on the mortality rate and
average length of stay (LOS) for each type of hospital. The values are not adjusted for
differences in the severity of the case mix at each type of facility. While we perform
more sophisticated risk-adjustment in our later analysis, here we simply present each rate
for the entire population, as well as separately for patients younger than age 65 and those
65 and older. Average LOS increases with teaching intensity both for the entire
population and each of the age groups. This trend is consistent with the claim that major
teaching hospitals tend to attract the most complex cases among the three groups.
Overall mortality, however, is highest for the non-teaching facilities—2.7% versus 2.4%
for the minor and major teaching group. At first glance, this finding seems puzzling
given the fact that the LOS data suggests that major teaching hospitals were attracting the
most severe cases. Analysis of mortality by age category, however, reveals that, within
each group, the mortality rate either increases (age<65) or remains relatively flat (age
65+) with teaching intensity. These latter results suggest that the higher overall observed

15

mortality rate for non-teaching hospitals may be due to the higher average age of their
patients.

Basic Specification
Our multivariate analysis relies on a difference-in-differences framework that
follows the relative changes in risk-adjusted LOS and mortality for the three groups of
hospitals over the course of the year. The basic specification takes the following form:

Yh ,m ,t =

h

+

t

+

m

+

3m

⋅(

m

× MIN _ TCH h ,m ,t ) +

6
m =1

1

⋅ MIN _ TCH h ,m ,t +

2

⋅ MAJ _ TCH h ,m ,t +

6
m =1

4m

⋅(

m

× MAJ _ TCH h ,m ,t ) +

(1)
h ,m ,t

where Y represents the dependent variable of interest (i.e., risk-adjusted average LOS or
risk-adjusted mortality).
The first two terms on the right-hand side of (1) are vectors of fixed effects for
hospital and year, respectively. The third term, µm, represents a vector of fixed effects for
six multi-month periods during the year—January through March, April through May,
June, July through August, September through October, and November through
December. Given that the residency changeover begins in late June for many hospitals,
we isolate that month and then compare the change in the dependent variable from AprilMay to July-August for teaching hospitals to the similar change for non-teaching
hospitals to measure the impact of the July turnover.10

10

Due to the fact that the residency changeover begins in 3rd and 4th weeks of June at several
hospitals, mortality and LOS results for that month represent a mixture of outcomes from both before and
after the transition. We thus use the comparison of July-August to April-May to measure the July
16

MIN_TCH and MAJ_TCH are indicators for minor and major teaching hospitals,
respectively.11 The next two terms on the right-hand side of (1) are vectors of
interactions between the teaching hospital categories and the month effects. The
coefficients on the MIN_TCH (MAJ_TCH) interactions thus capture the extent to which
any seasonal pattern that is found for minor (major) teaching hospitals differs from that
for the non-teaching controls. Each of the observations in (1) is weighted by the total
number of cases for the hospital-month pair to account for the fact that all of the
dependent variables are averages. Finally, the standard errors are clustered by hospital to
address potential lack of independence in the error term, εh,m,t.

Risk-Adjustment of Dependent Variables
As suggested in Table 1, the average severity of patients likely differs across the
three types of hospitals. To the extent that the differences in patient severity for major
teaching, minor teaching, and non-teaching hospitals vary systematically over the course
of the year, risk adjustment is required to ensure proper identification of any July
phenomenon. For example, to the degree that relatively healthy individuals in the
population aged 65 and older move from cold climates in northeastern states—which tend
to have a high concentration of teaching hospitals—to warmer southern and western

phenomenon. This difference captures the change in the dependent variables from the two complete
months that precede the beginning of the changeover for any hospital to the two complete months that fall
after its conclusion for all hospitals.
Despite the inclusion of hospital fixed effects, the uniteracted coefficients on minor (β1)and
major (β2) teaching status are identified by those facilities that change between minor and major teaching
status across years. For example, a hospital may have a teaching intensity of 0.25 (making it a minor
teaching hospital) in one year and 0.30 (making it a major teaching hospital) in the next.
11

17

states during the winter months, the mortality risk for the hospitalized population in the
northeast will increase ceteris paribus during this period of the year.
The covariates in our risk-adjustment equation are patient age; age squared;
gender; an indicator for Medicaid as the primary payment source; indicators for a
patient’s state of residence; interactions of the state indicators with both the linear and
quadratic age terms; and the Charlson index—a measure of comorbidities that increase a
patient’s risk of mortality (Charlson et al., 1987). The Medicaid variable is included as a
proxy for the patient’s socioeconomic status.12 The interactions of the state-of-residence
and age terms are included to control for the fact that the average severity of patients,
conditional on age, may vary across geography.
Given that the in-hospital mortality variable is binary, we use logistic regression
to obtain estimated probability of death for each patient discharge. For LOS, we use a
simple linear regression to calculate predicted values. The risk-adjustment equations are
run separately for each calendar year. The observed and expected values for mortality
and LOS are then averaged by hospital and month. The risk-adjusted value of each
dependent variable is calculated as the ratio of the observed-to-expected rate for a given
hospital-year. For example, the risk-adjusted mortality rate (RAMRh,m,t) is:
RAMRh ,m ,t =

OMRh ,m ,t
EMRh ,m ,t

∗ OMRt

(2)

12

With linear and quadratic terms for patient age included in the regression, we do not include a
separate term for Medicare status. While it would be useful to include an indicator for HMO patients—
who may be healthier, on average, than patients in other payer categories—the HCUP data does not
distinguish HMO patients from those with other forms of private insurance (e.g., indemnity).
18

where OMRh,m,t and EMRh,m,t are the observed and expected mortality rates, respectively,
for hospital h in month m of year t. OMRt is the average mortality rate for the entire
sample in year t and is used simply to normalize the value of RAMRh,m,t.

RESULTS AND DISCUSSION
Base Results
Table 2 presents results from our estimation of (1), the basic regression using
three discrete categories of teaching status. The coefficients in this table represent the
change in the dependent variable for minor and major teaching hospitals relative to the
change for non-teaching hospitals over the same period. As noted earlier, we use the
period just prior to the resident turnover (April - May) as the baseline. A positive
coefficient thus indicates that, on average, the hospital group in question experiences a
larger increase in the outcome measure than does the non-teaching group over the same
period of time. For example, the value of 0.040 for the September-October coefficient
for the minor teaching group (Column 1) suggests that the change in LOS from AprilMay to September-October is 0.040 days greater for minor teaching than for nonteaching hospitals. Similarly, the November-December coefficient for the same group
suggests that change in LOS from April-May to November-December is 0.036 days
greater for minor than for non-teaching hospitals.
For our purposes, the coefficients of greatest interest are those in the period just
following the resident turnover (i.e., July-August). In terms of LOS (Column 1), the
July-August coefficient for minor teaching hospitals is 0.049 and is significant at the 1%
level. To provide a perspective on the magnitude of this effect, we note that the average

19

risk-adjusted LOS for minor teaching hospitals is 5.3 days. If we assume that LOS is
proportional to hospital costs, these results suggest that costs increase by roughly 0.9%
following the July turnover.
The estimated coefficients on LOS for minor teaching hospitals decline somewhat
in magnitude during the months from September to December and remain significantly
different from the April-May baseline. By January-March, LOS falls back to its value in
the April-May period. Nevertheless, the coefficients for September-October and
November-December are not significantly different than that for July-August, so we are
not able to reject the hypothesis that LOS for minor teaching hospitals increases in JulyAugust and remains at that higher level for the final four months of the calendar year.
The January-March coefficient, while not significantly different from April-May, is
significantly lower than the July-August value at the 1% level. This reduction in the
estimated coefficient over the course of the academic year suggests that house staffs may
benefit from experience-based improvement in performance over time.
Consistent with the view that the July phenomenon should increase with the
intensity of a hospital'
s teaching program, we find that, relative to minor teaching
hospitals, major teaching facilities show even stronger evidence of such a trend in LOS.
Specifically, these hospitals experience a positive and significant increase in LOS relative
to non-teaching hospitals following the July turnover, and the effect remains for
approximately six months. This increase appears to begin in June, and the estimated
coefficient for that month (0.057) is significant at the 5% level.13 The effect, however,
appears to strengthen in terms of both magnitude (0.111) and significance (1%) in the
13

As noted earlier, June represents a mixture of days before and after the turnover at many
hospitals. The coefficient on June may thus underestimate the immediate impact of the turnover.
20

July-August period. This effect represents a 1.9% increase relative to the average LOS
for major teaching hospitals (5.80 days). As with minor teaching hospitals, the effects
for September through December are significantly different from the April-May baseline
and decline in estimated magnitude over time. Again, however, these coefficients are not
statistically distinguishable from the July-August estimate. By January-March, LOS falls
to the point where it is insignificantly different from April-May, but significantly lower
than the July-August coefficient. These results provide additional support for the
contention that house staffs learn over the course of the academic year.
The fact that LOS, and, in turn, resource utilization, increase following the July
turnover does not provide conclusive evidence concerning its effects on medical
productivity. To address this issue, one also needs to consider the impact of this turnover
on medical quality. Column 2 presents results using risk-adjusted mortality—a proxy for
quality—as the dependent variable. While minor teaching hospitals do not experience
significant changes in mortality during the course of the academic year, major teaching
facilities do show evidence of a July phenomenon with respect to this outcome measure.
As with LOS, major teaching hospitals experience an increase in their risk-adjusted
mortality rate in June. This increase of 0.063 percentage points is significant at the 10%
level. For the July-August period, the magnitude of this effect nearly doubles (to 0.122
percentage points) and becomes significant at the 1% level. The magnitude of this July
phenomenon represents a 4.3% increase relative to the average mortality rate of 2.82%
for major teaching hospitals. Evidence of learning is again present in the coefficients for
the remainder of the academic year; the levels for September-October and NovemberDecember are both significantly different from April-May; the level for January-March—

21

though not distinguishable from April-May—is significantly lower than that for JulyAugust. While suggestive of learning, these results also imply that the negative effects of
the July turnover linger—albeit at slightly lower magnitudes—for the months from
August through December.

Testing for Patient Self-Selection and Increased Transfers
While consistent with declines in medical productivity following cohort turnover,
our results are also consistent with alternate explanations. One leading hypothesis is that
patients recognize July to be a time of turmoil for teaching hospitals and that those with
choice (i.e., elective patients) decide to avoid those facilities at that time of the year. Of
course, these elective patients are likely to be relatively healthier than those who lack
choice regarding their admission to the hospital. This self-selection of the patient base
could thus leave teaching hospitals with relatively sicker patient populations at precisely
the time we estimate their resource use to be increasing and their outcomes to be
declining. If such selection were occurring, we would be mistaken to attribute the effects
we observe to a decline in productivity.
We offer a test of the selection hypothesis in Column 3 of Table 2. If patients are
in fact self-selecting away from teaching hospitals in July and August, then teaching
hospitals should experience a decline in their number of admissions relative to nonteaching facilities during those months. We estimate a regression of the same form as
(1), but with the number of hospital admissions on the left-hand side. The results are not
consistent with a self-selection story. In particular, the coefficient in the July-August
time period for major teaching hospitals—the period most critical for the analysis of the

22

July phenomenon—is positive. This effect is actually in the opposite direction of that
which one would expect under the self-selection hypothesis.
The positive coefficient on admissions for major teaching hospitals in JulyAugust is related to a second potential explanation for our base results: that major
teaching hospitals are receiving a higher percentage of patients transferred from minor
teaching or non-teaching hospitals during the summer months due to the presence of
excess hospital capacity during warmer months. In Column 4, we thus repeat our
analysis using the percentage of cases transferred from another hospital as the dependent
variable. Again, we find no systematic change in transfer rates for either minor or major
teaching hospitals around the July turnover.

Robustness and Extensions
Results Using Medicaid Patients
To test the robustness of our base results, we take advantage of the empirical
regularity that Medicaid recipients are more likely to be treated by house staff physicians
than are patients with private health insurance. This phenomenon is typically attributed
to the fact that Medicaid recipients are less likely to have a regular source of physician
care than those with private insurance. James Tallon, president of the United Hospital
Fund, noted this fact in reaction to his organization'
s study of Medicaid in New York
City:
‘These findings indicate the medical residents play a major role in
providing primary care for low-income people who rely on Medicaid…
As these Medicaid beneficiaries are enrolled into managed care, we need
to evaluate whether we can accomplish the goals of managed care—
providing coordinated and continuous care—while relying so heavily on
this workforce of doctors in training.’ (United Hospital Fund, 1999)

23

To the extent that residents play a disproportionately large role in the treatment of
Medicaid patients, we would expect that the July phenomenon should be even more
pronounced for these individuals than for the rest of the population.
Table 3 presents results for LOS and mortality separately for the Medicaid and
non-Medicaid populations. In terms of LOS, both the Medicaid and non-Medicaid
populations show evidence of an increase for major teaching hospitals following the July
turnover. While the July-August coefficient is slightly bigger in absolute magnitude for
Medicaid (0.126) than for non-Medicaid (0.097) patients, the two coefficients are nearly
identical in terms of magnitude relative to the mean LOS for their respective populations.
Further, the coefficient for the Medicaid population is significant at 10% versus 1% for
the non-Medicaid group. Another difference between the Medicaid and non-Medicaid
populations with respect to LOS is that the latter does show evidence of a July
phenomenon for minor teaching hospitals that is not present for the former.
With respect to mortality, the July-August coefficient for major teaching hospitals
is substantially larger for the Medicaid (0.251) than for the non-Medicaid population
(0.093). Relative to the mean mortality rate for each population at major teaching
hospitals, the coefficient for the Medicaid group represents a 7.3% increase while that for
the non-Medicaid group translates into a 3.4% increase. While each coefficient is
significantly different from zero, we are not able to reject the null hypothesis that these
two coefficients are the same.14 Nonetheless, the fact that the coefficient for the

14

To test whether these coefficients are the same, we pooled the Medicaid and non-Medicaid
observations and interaction the time period indicators for each type of hospital with a Medicaid indicator.
We then examined the significance of the coefficient on these interaction terms. The coefficient on this
interaction for major teaching hospitals in July-August was significant at the 16% level, suggesting that we
cannot statistically distinguish the coefficients for the Medicaid and non-Medicaid populations.
24

Medicaid population is notably larger in both absolute and relative terms than that for the
non-Medicaid group is consistent with the story that the effects we are seeing are due to
the July turnover.

Results Using Narrower Categories of Teaching Intensity
Our base findings suggest that the negative impact of the July turnover on
productivity is increasing in teaching intensity (i.e. major teaching hospitals are more
affected than minor teaching hospitals). Intuitively, this makes sense, as teaching
intensity captures the degree to which a hospital relies on residents and, therefore, should
be correlated with the magnitude of the turmoil created by resident turnover.
Nevertheless, it is not theoretically clear that the relationship between teaching intensity
and the magnitude of the July phenomenon need be either linear or even monotonic. To
more directly test for potential non-linearity in this relationship, we estimate versions of
(1) in which we further divide the major teaching category into thirds on the basis of
teaching intensity.15
Table 4 presents the results of these regressions. We focus on the coefficients for
the three thirds of major teaching hospitals, as the results for the minor teaching group are
the same as in Table 2. For LOS, the July-August coefficient is positive and significant
for all three thirds. The magnitude of this coefficient increases from 0.086 for the lower
third to 0.125 for the middle third and slightly decreases to 0.120 for the upper third

15

Teaching intensities for the lower third of major teaching hospitals range from 0.25 to 0.34
residents per bed, the middle third ranges from 0.34 to 0.55, and the upper third includes all values greater
than 0.55.

25

(Column 1). None of these coefficients are statistically different from the other two at
conventional levels.
The results for risk-adjusted mortality (Column 2), however, differ somewhat
from those for LOS. Specifically, the July phenomenon appears to be focused in the
lower and middle thirds, but is not present in the upper third. The effects for the first two
groups (0.172 and 0.152, respectively) are both significantly different from zero and from
the coefficient for the minor teaching category, but are not significantly different from
each other. The effect for the upper third is substantially smaller in magnitude (0.054)
than that for the lower and middle thirds and is not significantly different from zero at
conventional levels. While the decline from the middle to upper thirds is only significant
at the 16% level, it is not clear if this reflects a true lack of significance or if it is simply
due to the relatively small sample size within each third of the major teaching category.
Nonetheless, these results suggest that the July phenomenon with respect to risk-adjusted
mortality does appear to diminish for the most-intensive teaching hospitals. Column 3
again suggests that the observed July phenomenon is not due to patient self-selection
away from major teaching hospitals around the July turnover.

Results Using Supervision Proxies
One can imagine two leading explanations for the non-linear July phenomenon (at
least with respect to mortality) observed in Table 4. First, it is possible that the mostintensive teaching programs (e.g., the upper third of major teaching) are more aware of or
sensitive to the possibility of performance declines around the July turnover. As a result,
these programs may provide higher levels of supervision to their house staffs in an effort

26

to avoid major disruptions. Alternatively, the most-intensive programs may be more
likely to be highly prestigious and attract residents and fellows who are capable of
learning more rapidly than those at lower-intensity programs. We refer to these two
explanations as the supervision and worker ability stories, respectively.
While we cannot definitively distinguish between these two explanations, we do
provide some suggestive evidence in Table 5. We supplement our existing data with
information from the American Medical Association’s FREIDA database, which provides
annual information on the number of residents and attending physicians for individual
residency programs within teaching hospitals. We were able to obtain this data for each
year from 1994 through 2000. For each hospital and year, we calculate the ratio of fulltime attending physicians to residents as a weighted average across all of a hospital’s
medical and surgical teaching programs. We use this ratio as a proxy for the level of
supervision provided to house staff at a given teaching hospital. This ratio is a not a
perfect proxy, as it reflects only potential (not actual) supervision levels and does not
account for unobserved factors. Nevertheless, it represents a reasonable approximation
of our variable of interest.
As a proxy for worker (i.e., house staff) ability, we use an indicator for whether
that facility was ranked as a top hospital in the United States by U.S. News & World
Report in at least one major specialty category in 2002. In that year, 205 acute care
hospitals (3.4% of the 6,045 considered) in the United States were included in this group
(Comarow, 2002). Though being included in this list is not a perfect measure of the
prestige of a hospital’s teaching programs, it does represent a reasonable proxy.

27

In the first two columns of Table 5, we divide major teaching hospitals into lowand high-supervision groups. We assign major teaching hospitals to high- and lowsupervision groups based on their attending-to-resident ratio in each year.16 Table 5
presents the results of these regressions using different thresholds to define major
teaching hospitals with high supervision levels. The first two rows show the July-August
coefficients of interest from regressions where high-supervision hospitals are defined as
the top 50% of major teaching hospitals with respect to the attending-to-resident ratio.
Thus, the “Major Teaching” coefficient suggests that major teaching hospitals with
supervision levels in the lower 50% had an increase in LOS of 0.108 days in July-August.
For high-supervision, major teaching hospitals, the estimated increase is smaller at 0.066
(=0.108-0.042) days. While the July-August increase for high-supervision hospitals
remains significantly different from zero at the 10% level, it is not significantly different
from the 0.108 increase for low-supervision hospitals.
The first two rows of Column 2 show similar results with respect to risk-adjusted
mortality. Low-supervision hospitals in the major teaching group show a July-August
increase of 0.156 (significant at the 10% level). The magnitude of this increase declines
to 0.090 (=0.156-0.066) for high-supervision hospitals and is significantly different from
zero at the 5% level. As seen by the coefficient on “Major Teaching x High
Supervision”, the July-August increase is not significantly different for the high- and
low-supervision groups.
The next four rows of Table 5 show analogous results for regressions where highsupervision hospitals are defined as those in the top 33% and top 20% of major teaching
16

A given hospital may thus be in the high-supervision group in one year and in the lowsupervision group in the following year.

28

hospitals, respectively, in terms of their supervision ratios. With respect to LOS, these
more stringent definitions of “high-supervision” hospitals do not have much of an effect
on the results reported above—low- and high-supervision hospitals in the major teaching
group experience similar increases in LOS in the July-August period.
The results for mortality are also consistent across the various definitions of high
supervision with one notable exception. The final two rows of Column 2 suggest that,
when high-supervision is defined as the top 20% of major teaching hospitals, highsupervision hospitals experience a significantly smaller increase in mortality during the
July-August period. This decline in the July phenomenon of 0.146 percentage points is
significant at the 10% level. Further, it reduces the overall July phenomenon for highsupervision hospitals to 0.004 percentage points (insignificantly different from zero at
conventional levels). This result suggests that major teaching hospitals with very highlevels of supervision appear to be able to avoid the adverse impact of the July changeover
on mortality performance.
Of course, it is possible that those hospitals with the highest levels of supervision
are also those with the highest levels of worker ability. In the next two columns of Table
5, we also split major teaching hospitals into two groups based on whether or not they
were ranked as a top hospital in at least one major specialty. The results of the LOS
regressions in Column 3 are similar to those in Column 1. While the ranked, major
teaching hospitals did have higher estimated increases in LOS than their unranked
counterparts, the coefficients for the two groups were not significantly different from
each other. The mortality results in Column 4 are also similar to those in Column 2. We
note that, when high supervision is defined as the top 20% of hospitals, the decline of

29

0.123 percentage points in the July-August coefficient is no longer significant at
conventional levels; it is, however, significant at the 15% level, and the overall effect for
high-supervision hospitals of 0.039 (=0.162-0.123) is insignificantly different from zero
(as in Column 2).
One might be concerned that the high supervision and ranking variables are so
highly correlated that it is not possible to identify independent effects. We address this
issue in two ways. First, we note that the weighted correlation between high supervision
(defined by the 20% threshold) and being ranked is only 0.31. Second, the final two
columns of Table 5 illustrate the effect of hospitals being ranked without controlling for
supervision levels. We find that ranked, major teaching hospitals experience a July
phenomenon that is 0.10 percentage points smaller than that for their unranked
counterparts. Nevertheless, this difference is not significant at conventional levels,
though it is significant at the 15% level. It is possible that, with more refined data, we
might find that higher levels of house-staff ability reduce the magnitude of the July
phenomenon. That said, our initial results suggest that supervision—particularly at very
high levels—may play a role in mitigating the disruptive effects of turnover for major
teaching hospitals, even after controlling for differences in worker ability.

CONCLUSION
This study examines the relationship between cohort turnover and productivity
using a unique setting in which turnover occurs exogenously—the July turnover of house
staffs in teaching hospitals. We find that both minor and major teaching hospitals
experience a significant increase in resource utilization—measured by average LOS—

30

immediately following the July turnover, and that the effect appears to last for several
months. We also find that teaching hospitals with medium teaching intensity experience
a significant increase in patient mortality over the same period. The confluence of
increased resource utilization and increased mortality (i.e., decreased quality) during the
July-August period implies that this cohort turnover reduces medical productivity.
Nevertheless, those hospitals with the highest teaching intensities (i.e., the greatest
reliance on residents for the provision of care), seem to avoid the disruption of the July
phenomenon with respect to changes in their average mortality rates. We provide
preliminary evidence suggesting that higher supervision levels play a role in mitigating
the impact of the July turnover in major teaching facilities.
The magnitude of the estimated effects is substantial and appears to last for
roughly six months. We find that average LOS—our proxy for resource utilization and
cost—for the average, major teaching hospital increases by roughly 2% following the
July turnover and remains between 1% and 2% higher throughout the final six months of
the calendar year. Similarly, the average, major teaching hospital experiences an increase
in risk-adjusted mortality of roughly 4% (not percentage points) in the July-August
period. This effect also remains at levels between 2% and 4% for the last six months of
the calendar year. For the average major teaching hospital, this translates into between
7.8 and 13.8 “accelerated” deaths (i.e., deaths that occur earlier than they would have
occurred in the absence of the July turnover) per year. Based on a total of roughly 200
major teaching hospitals in the United States, the July phenomenon is thus associated
with roughly 1,500 to 2,750 accelerated deaths per year in the United States.17
17

To the extent that this effect is sizeable, one might ask why it may go unnoticed by teaching
hospitals. A possible answer to this question is that the July phenomenon occurs at a time of the year when
31

Determining the social cost of this increase in mortality requires assumptions about the
expected longevity of these individuals in the absence of the July turnover. Such
assumptions are beyond the scope of this paper.
Beyond its findings with respect to the July phenomenon, this paper has broader
implications for the study of the effects of labor turnover on organizations. It provides
empirical support for the contention that cohort turnover has negative implications for
productivity on average, though these effects do not increase linearly with the intensity of
turnover. We find initial evidence suggesting that supervision can mitigate this negative
effect. This latter finding implies that, even if firms are not able to reduce the levels of
turnover they face, they may be able to manage its effects.
One question that is not addressed by our study is the degree to which managers
should be concerned about turnover-related declines in productivity. On one hand, these
declines likely reflect the costs associated with valuable on-the-job training. On the
other, they may be larger than necessary to obtain the desired training benefit for new
employees. In the case of teaching hospitals, we thus are not arguing that an optimal
residency system would result in no systematic change in productivity throughout the
year. Presumably, no system can guarantee that residents will be as productive at the
beginning of their tenure as they will be at its end. Ultimately, the important question to
answer is whether declines in productivity are higher than necessary to train new workers
efficiently. The question of optimal supervision levels in the face of significant on-thejob training is an interesting issue for further study in contexts both within and outside of
the hospital industry.
the overall trend in mortality is declining. As a result, an increase in mortality relative to non-teaching
hospitals may not appear as an increase in absolute mortality.

32

REFERENCES

Abelson, Michael and Barry Baysinger (1984). “Optimal and Dysfunctional Turnover:
Toward an Organizational Level Model,” Academy of Management Review, 9(2), 331341.
Argote, Linda, Chester Insko, Nancy Yovetich, and Anna Romero (1995). "Group
Learning Curves: The Effects of Turnover and Task Complexity on Group Performance,"
Journal of Applied Social Psychology, 25(6), 512-529.
Argote, Linda and Dennis Epple (1990). “Learning Curves in Manufacturing,” Science,
247, 920-924.
Barry, William and Gary Rosenthal (2004). “Is There a July Phenomenon? The Effect of
July Admission on Intensive Care Mortality and Length of Stay in Teaching Hospitals,”
Journal of General Internal Medicine, 18, 639-645.
Bell, Chaim and Donald Redelmeier (2001). “Mortality Among Patients Admitted to
Hospitals on Weekends as Compared With Weekdays,” New England Journal of
Medicine, 345(9), 663-668.
Brown, Charles and James Medoff (1978). “Trade Unions in the Production Process,”
Journal of Political Economy, 86(3), 355-378.
Boylan, Richard (2004). “Salaries, Turnover, and Performance in the Federal Criminal
Justice System,” Journal of Law and Economics, 47, 75-92.
Carley, Kathleen (1992). "Organizational Learning and Personnel Turnover,"
Organization Science, 3(1), 20-46.
Charlson, Mary, Peter Pompei, Kathy Ales, and C. Ronald MacKenzie (1987). “A New
Method of Classifying Prognostic Comorbidity in Longitudinal Studies: Development
and Validation,” Journal of Chronic Diseases, 40(5), 373-383.
Claridge, Jeffrey, Andrew Schulman, Robert Sawyer, Anousheh Ghezel-Ayagh, and
Jeffrey Young (2001). “‘The July Phenomenon’ and the Care of the Severely Injured
Patient: Fact or Fiction?” Surgery, 130(2), 346-353.
Clark, Kim (1980). “The Impact of Unionization on Productivity: A Case Study,”
Industrial and Labor Relations Review, 33, 451-469.
Comarow, Avery (2002). “America’s Best Hospital,” U.S. News & World Report, 133(3),
45.

33

Dalton, Dan and William Todor (1979). “Turnover Turned Over: An Expanded and
Positive Perspective,” Academy of Management Review, 4(2), 225-235.
Dobkin, Carlos (2002). “Hospital Staffing and Inpatient Mortality,” mimeograph.
Fee, C. Edward and Charles Hadlock (2004). “Management Turnover Across the
Corporate Hierarchy,” Journal of Accounting and Economics, 37, 3-38.
Freeman, Richard and James Medoff (1982). What Do Unions Do?, New York, NY:
Basic Books.
Gaba, David and Steven Howard (2002). “Fatigue Among Clinicians and the Safety of
Patients,” New England Journal of Medicine, 347(16), 1249-1255.
Gawande, Atul (2002). Complications: A Surgeon’s Notes on an Imperfect Science, New
York, NY: Metropolitan Books.
Gemmell, Islay, Philip McLoone, F.A. Boddy, Gordon Dickinson, and G.C.M. Watt
(2000). "Seasonal Variation in Mortality in Scotland," International Journal of
Epidemiology, 29, 274-279.
Glebbeek, Arie and Erik Bax (2004). "Is High Employee Turnover Really Harmful? An
Empirical Test Using Company Records," Academy of Management Journal, 47(2), 277286.
Griffith, Charles, John Wilson, Nirmala Desai, and Eugene Rich (1997). “Does Pediatric
Housestaff Experience Influence Tests Ordered for Infants in the Neonatal Intensive Care
Unit?” Critical Care Medicine, 25(4), 704-709.
Hayes, Rachel, Paul Oyer, and Scott Schaefer (2005). “Co-Worker Complementarity and
the Stability of Top Management Teams,” working paper.
Healthcare Cost and Utilization Project (1999). The HCUP Nationwide Inpatient Sample
(NIS), Release 6, 1997, Rockville, MD: Agency for Healthcare Research and Quality.
Hellerstein, Judith and David Neumark (1995). “Are Earnings Profiles Steeper than
Productivity Profiles? Evidence from Israeli Firm-Level Data,” Journal of Human
Resources, 30(1), 89-112.
Hendry, R. (1981). “The Weekend—A Dangerous Time to be Born?” British Journal of
Obstetrics and Gynaecology, 88(12), 1200-1203.
Huselid, Mark (1995). "The Impact of Human Resource Management Practices on
Turnover, Productivity, and Corporate Financial Performance," Academy of Management
Journal, 38(3), 635-672.

34

Jovanovic, Boyan (1979). “Job Matching and the Theory of Turnover,” Journal of
Political Economy, 87(5), 972-990.
Krueger, Alan and Alexandre Mas (2004). "Strikes, Scabs, and Tread Separations: Labor
Strife and the Production of Defective Bridgestone/Firestone Tires," Journal of Political
Economy, 112(2), 253-289.
Laine, Christine, Lee Goldman, Jane Soukup, and Joseph Hayes (1993). “The Impact of a
Regulation Restricting Medical House Staff Working Hours on the Quality of Patient
Care,” Journal of the American Medical Association, 269(3), 374-378
Leach, David (2000). “Residents’ Work Hours: The Achilles Heel of the Profession?”
Academic Medicine, 75(12), 1156-1157.
Levhari, David and Eytan Sheshinski (1973). “Experience and Productivity in the Israel
Diamond Industry,” Econometrica, 41(2): 239-253.
Maranto, Cheryl and Robert Rodgers (1984). “Does Work Experience Increase
Productivity? A Test of the On-the-Job Training Hypothesis,” Journal of Human
Resources, 19(3): 341-357.
Medicare Payment Advisory Commission (2002). Report to the Congress: Medicare
Payment Policy (Washington, DC: Medicare Payment Advisory Commission).
Medoff, James and Katherine Abraham (1985). “Experience, Performance, and
Earnings,” Quarterly Journal of Economics, 95(4): 703-736.
Mobley, William (1982). Employee Turnover: Causes, Consequences, and Control.
Reading, MA: Addison-Wesley.
Newell, Allen and Paul Rosenbloom (1981). “Mechanisms of Skill Acquisition and the
Power Law of Practice,” in John Anderson, ed., Cognitive Skills and Their Acquisition.
Hillsdale, NJ: Erlbaum, 1-55.
Polanyi, Michael (1966). The Tacit Dimension, New York, NY: Anchor Day Books.
Price, James (1977). The Study of Turnover, Ames, IA: Iowa State University Press.
Rich, Eugene, Steven Hillson, Bryan Dowd, and Nora Morris (1993). “Specialty
Differences in the ‘July Phenomenon’ for Twin Cities Teaching Hospitals,” Medical
Care, 31(1): 73-83.
Staw, Barry (1980). "The Consequences of Turnover," Journal of Occupational
Behaviour, 1(4): 253-273.

35

Steinbrook, Robert (2002). “The Debate Over Residents’ Work Hours,” New England
Journal of Medicine, 347(16): 1296-1302.
Thorpe, Kenneth (1990). “House Staff Supervision and Working Hours: Implications of a
Regulatory Change in New York State,” Journal of the American Medical Association,
263(23): 3177-3181.
Tushman, Michael and Lori Rosenkopf (1996). “Executive Succession, Strategic
Reorientation and Performance Growth: A Longitudinal Study in the U.S. Cement
Industry,” Management Science, 42(7), 939-953.
United Hospital Fund (1999). “Medical Residents Provide Large Share of Primary Care
at NYC Ambulatory Care Facilities,” press release,
http://www.uhfnyc.org/press_release3159/press_release_show.htm?doc_id=98095,
accessed December 13, 2004.
Weinstein, Debra (2002). “Duty Hours for Resident Physicians—Tough Choices for
Teaching Hospitals,” New England Journal of Medicine, 347(16): 1275-1278.

36

Figure 1: Average Length of Stay and Mortality Rate by Month, 1993-2001
5.5

3

5.4
2.5

5.2
2
5.1

5

1.5

4.9
1
4.8

4.7
0.5
4.6

4.5

0
Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

Month
Average LOS

Source: NIS, 1993-2001.

37

Mortality Rate

Sep

Oct

Nov

Dec

Average Mortality Rate (Percent)

Average Length of Stay (Days)

5.3

Table 1: Descriptive Statistics by Hospital Type, 1993-2001
Non-Teaching
Standard
Deviation
Mean

Minor Teaching
Standard
Deviation
Mean

Major Teaching
Standard
Deviation
Mean

Full Sample
Standard
Deviation
Mean

Residents Per Inpatient Bed

0.01

0.04

0.07

0.08

0.53

0.28

0.10

0.19

Inpatient Hospital Beds

135

115

337

206

493

260

187

177

4,824

5,016

13,923

8,562

21,222

10,049

7,181

7,684

Inpatient Admissions/Year
Patient Age

49.0

9.2

45.0

9.0

40.9

8.2

46.5

9.4

Medicaid Admissions/Total Admissions

15%

13%

17%

14%

26%

16%

17%

14%

Medicare Admissions/Total Admissions

39%

14%

33%

12%

26%

10%

35%

14%

4.8
3.7
6.5

2.2
1.9
3.0

5.2
4.4
6.9

1.4
1.3
1.9

5.9
5.4
7.5

1.4
1.3
2.2

5.1
4.2
6.7

1.9
1.7
2.7

2.6%
0.8%
5.3%

1.6%
1.0%
2.2%

2.4%
1.0%
5.2%

0.7%
0.4%
1.3%

2.4%
1.4%
5.4%

0.7%
0.5%
1.5%

2.5%
1.0%
5.3%

1.3%
0.8%
1.9%

Observed Average Length of Stay
Total
Age<65
Age 65+
Observed Mortality
Total
Age<65
Age 65+
Observations (hospital-years)

4,873

1,041

318

Percentage of Total Sample

78.2%

16.7%

5.1%

Note: Observations are at the hospital-year level and cover the five-year period from 1993 to 2001.
Source: NIS, 1993-2001.

38

6,232

Table 2: Base Regressions Using Minor and Major Teaching Categories
Change in Dependent Variable Relative to Non-Teaching Baseline (Reference
Period=April-May)

Minor Teaching
Jan-Mar
Apr-May
June
Jul-Aug
Sep-Oct
Nov-Dec
Major Teaching
Jan-Mar
Apr-May
June
Jul-Aug
Sep-Oct
Nov-Dec
Mean of Dependent Variable
Minor Teaching
Major Teaching
Observations
2
Adjusted R

Risk-Adjusted LOS

Risk-Adjusted
Mortality

-0.003 (0.012)

-0.012 (0.018)

0.010
0.049
0.040
0.036

(0.016)
(0.013) ***
(0.012) ***
(0.014) ***

0.006
0.014
0.003
-0.008

0.018 (0.021)
0.057
0.111
0.092
0.073

(0.028)
(0.024)
(0.021)
(0.020)

**
***
***
***

Total Admissions
1.5

(0.024)
(0.019)
(0.020)
(0.020)

-8.0
-0.5
-5.2
-25.9

-0.037 (0.027)

4.7

0.063
0.122
0.088
0.074

(0.034)
(0.039)
(0.043)
(0.037)

*
***
**
**

-5.6
30.6
2.9
-31.6

Transfer Rate
(Transfers/Total
Admissions*100)

(3.4)

-0.02 (0.04)

(2.8) ***
(3.2)
(3.1) *
(3.9) ***

-0.02
-0.03
-0.03
0.02

(8.2)
(8.2)
(8.8) ***
(8.9)
(9.1) ***

(0.04)
(0.04)
(0.05)
(0.06)

0.03 (0.06)
0.05
-0.09
-0.01
0.14

(0.06)
(0.08)
(0.08)
(0.07) **

5.28
5.80

2.60
2.82

1,259
1,855

4.32
5.25

74,521
0.738

74,521
0.465

74,521
0.983

74,521
0.822

*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.
Note: The level of observation is the hospital-month. All regressions include fixed effects for hospital, year, and multi-month period,
though these coefficients are not shown in the table for ease of presentation. Standard errors (in parentheses) are heteroskedasticity
robust and clustered by hospital. In regressions with risk-adjusted LOS and risk-adjusted mortality as the dependent variable,
observations are weighted by the total number of cases for the relevant hospital-month.

39

Table 3: Regressions on Medicaid and Non-Medicaid Populations
Change in Dependent Variable Relative to Non-Teaching Baseline (Reference
Period=April-May)
Medicaid
Risk-Adjusted LOS
Minor Teaching
Jan-Mar
Apr-May
June
Jul-Aug
Sep-Oct
Nov-Dec
Major Teaching
Jan-Mar
Apr-May
June
Jul-Aug
Sep-Oct
Nov-Dec
Mean of Dependent Variable
Minor Teaching
Major Teaching
Observations
2
Adjusted R

-0.059 (0.052)
0.091
0.059
0.074
0.100

(0.062)
(0.051)
(0.048)
(0.051) **

-0.014 (0.058)
0.087
0.126
0.135
0.119

(0.091)
(0.070) *
(0.076) *
(0.068) *

Non-Medicaid
Risk-Adjusted
Mortality
-0.182 (0.083) **
-0.011
-0.103
-0.134
-0.207

(0.115)
(0.093)
(0.086)
(0.092) **

Risk-Adjusted LOS
0.004 (0.012)
-0.003
0.045
0.035
0.026

0.027 (0.024)

0.194
0.251
0.142
0.082

0.048
0.097
0.078
0.063

(0.115) *
(0.110) **
(0.096)
(0.111)

0.003 (0.017)

(0.016)
(0.012) ***
(0.012) ***
(0.014) **

0.014 (0.084)

(0.024)
(0.019)
(0.017)
(0.022)

Risk-Adjusted
Mortality

0.009
0.026
0.018
0.010

(0.024)
(0.019)
(0.020)
(0.019)

-0.032 (0.028)
**
***
***
***

0.027
0.093
0.072
0.070

(0.036)
(0.038) **
(0.040) *
(0.039) *

6.12
6.71

3.25
3.42

5.14
5.59

2.54
2.73

71,486
0.407

71,491
0.148

74,500
0.789

74,500
0.430

*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.
Note: The level of observation is the hospital-month. All regressions include fixed effects for hospital, year, and multi-month period,
though these coefficients are not shown in the table for ease of presentation. Standard errors (in parentheses) are
heteroskedasticity robust and clustered by hospital. In regressions with risk-adjusted LOS and risk-adjusted mortality as the
dependent variable, observations are weighted by the total number of cases for the relevant hospital-month.

40

Table 4: Regressions Using Major-Teaching Subcategories
Change in Dependent Variable Relative to Non-Teaching
Baseline (Reference Period=April-May)
Risk-Adjusted
Risk-Adjusted LOS
Mortality
Admissions
Minor Teaching
Jan-Mar
Apr-May
June
Jul-Aug
Sep-Oct
Nov-Dec

-0.003

(0.012)

-0.012

(0.018)

1.537

0.010
0.049
0.040
0.036

(0.016)
(0.013) ***
(0.012) ***
(0.014) **

0.006
0.014
0.003
-0.009

(0.024)
(0.019)
(0.020)
(0.020)

-7.978
-0.454
-5.217
-25.919

0.005

(0.029)

-0.026

(0.050)

0.051
0.086
0.074
0.075

(0.034)
(0.028) ***
(0.029) **
(0.033) **

0.032
0.172
0.096
0.142

0.085

(0.039) **

-0.042

0.027
0.125
0.102
0.089

(0.055)
(0.058) **
(0.056) *
(0.047) *

0.106
0.152
0.086
0.028

(3.437)
(2.762) ***
(3.234)
(3.084) *
(3.905) ***

Major Teaching
Lower Third
Jan-Mar
Apr-May
June
Jul-Aug
Sep-Oct
Nov-Dec
Middle Third
Jan-Mar
Apr-May
June
Jul-Aug
Sep-Oct
Nov-Dec
Upper Third
Jan-Mar
Apr-May
June
Jul-Aug
Sep-Oct
Nov-Dec
Mean of Dependent Variable
Minor Teaching
Major Teaching
Observations
2
Adjusted R

-0.025

(0.024)

0.088
0.120
0.100
0.059

(0.039)
(0.031)
(0.024)
(0.024)

-0.043
**
***
***
**

0.055
0.054
0.084
0.053

25.548 (12.836) **

(0.050)
(0.087) **
(0.096)
(0.072) **

-15.866
18.759
-4.515
-14.258

(0.045)

-10.918 (13.541)

(0.053) **
(0.056) ***
(0.061)
(0.074)

-3.480 (9.388)
23.981 (12.723) *
-1.929 (11.254)
-27.852 (15.348) *

(0.036)
(0.057)
(0.044)
(0.050) *
(0.032)

(20.378)
(14.384)
(11.793)
(13.929)

-0.790 (14.165)
2.400
48.835
15.018
-52.478

(11.215)
(16.910) ***
(20.280)
(15.186) ***

5.28
5.80

2.60
2.82

1,259
1,855

74,521
0.738

74,521
0.465

74,521
0.984

*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.
Note: The level of observation is the hospital-month. All regressions include fixed effects for hospital,
year, and multi-month period, though these coefficients are not shown in the table for ease of
presentation. Standard errors (in parentheses) are heteroskedasticity robust and clustered by hospital.
In regressions with risk-adjusted LOS and risk-adjusted mortality as the dependent variable,
observations are weighted by the total number of cases for the relevant hospital-month.

41

Table 5: July-August Coefficients for Categories of Major Teaching Hospitals
Change in Dependent Variable Relative to Non-Teaching Baseline (Reference Period=April-May)
Risk-Adjusted LOS

Risk-Adjusted
Mortality

0.103 (0.031) ***
-0.060 (0.057)
0.058 (0.057)

0.162 (0.094) *
-0.038 (0.107)
-0.091 (0.074)

0.133 (0.064) **
-0.040 (0.083)

0.080 (0.038) **
-0.016 (0.042)
0.038 (0.043)

0.148 (0.068) **
-0.012 (0.081)
-0.102 (0.075)

0.150 (0.054) ***
-0.146 (0.082) *

0.077 (0.034) **
-0.005 (0.049)
0.034 (0.046)

0.162 (0.059) ***
-0.123 (0.084)
-0.070 (0.080)

Risk-Adjusted LOS

Risk-Adjusted
Mortality

High Supervision = Top 50%
Major Teaching
Major Teaching x High Supervision
Major Teaching x Ranked

0.108 (0.032) ***
-0.042 (0.049)

0.156 (0.094) *
-0.066 (0.105)

High Supervision = Top 33%
Major Teaching
Major Teaching x High Supervision
Major Teaching x Ranked

0.086 (0.036) **
-0.006 (0.044)

High Supervision = Top 20%
Major Teaching
Major Teaching x High Supervision
Major Teaching x Ranked
Observations

0.083 (0.030) ***
0.005 (0.050)

73,287

73,287

73,287

73,287

Risk-Adjusted
LOS
0.101 (0.028) ***

Risk-Adjusted
Mortality
0.145 (0.047) ***

0.045 (0.043)

-0.100 (0.064)

74,521

74,521

*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.
Note: The level of observation is the hospital-month. All regressions include fixed effects for hospital, year, and multi-month period, though these coefficients are not
shown in the table for ease of presentation. Regressions also include a full set of interactions with multi-month periods for each of the following variables: minor
teaching, major teaching, major teaching x high supervision, and (in Columns 3 and 4) major teaching x ranked. For ease of presentation, only the July-August
coefficients for the various types of major teaching hospitals appear in the table. Standard errors (in parentheses) are heteroskedasticity robust and clustered by
hospital. Observations are weighted by the total number of cases for the relevant hospital-month. The smaller sample size in the first four columns is due to the fact
that data on supervision levels are not available for 1993 and 2000. In these years, all major teaching hospitals are thus excluded from the sample.

42

Appendix: Number of Hospitals in the Sample and the NIS by State and Year, 1993-2001

State

1993

1994

Full
Sample NIS

Full
Sample NIS

1995

1996

1997

Sample

Full
NIS

Sample

Full
NIS

Sample

Arizona
California
Colorado
Connecticut
Florida
Georgia
Hawaii
Illinois
Iowa
Kansas
Kentucky
Maine
Maryland
Massachusetts
Michigan
Minnesota
Missouri
Nebraska
New Jersey
New York
North Carolina
Oregon
Pennsylvania
Rhode Island
South Carolina
Tennessee
Texas
Utah
Vermont
Virginia
Washington
West Virginia
Wisconsin

13
95
27
7
165

13
96
28
7
166

12
101
21
7
162

12
102
22
7
163

15
104
21
9
140

15
105
22
9
141

15
102
21
8
137

15
103
21
8
138

14
106
18
9

75
70

75
70
72

77
64

77
64
71

73
54

73
54
61

72
53

72
53
60

73
52

Total Hospitals
Total States

40
30

40
30

42
27

42
27

1998
Full
NIS
14
107
18
9
117
115
3
73
52
62

Sample
14
97
20
7

75
53

1999
Full
NIS
14
97
20
7
109
114
4
75
53
56

Sample
13
95
18
6

70
54

2000
Full
NIS
13
95
18
6
98
100
4
70
54
58
11
23
15

31
10
13
16

14
93
22
6
55
60
3
69
54
57
31
10
13
16

14
93
22
6

69
54

21
24
21
66
994
28

634
24

986
33

39
25

39
19

39
19

35
18

35
18

32
17

32
17

48

49

46

47

44

44

38

39

38

38

39

40

16
49
36
18
42

16
49
36
18
42

19
62

19
62

18
59

18
59

17
58

17
58

19
56

19
56

17
52

17
52

17
46

17
46

19
57

19
57

19
53

19
53

17
51

17
51

17
50

17
50

16
52

16
52

18
47

18
47

18
43

18
43

51

46
52

41
50

34
64

34
72

34
68

13

13

16

16

17

17

14

20
31
93
14

23

23

21

21

22

22

22

22

20

20

24

24

47
25

47
25

85

85

92

92

80

80

76

76

71

71

67

67

66

66

21
24
21
66

786
15

913
17

779
15

904
17

775
16

938
19

752
16

906
19

616
16

1012
22

594
16

984
22

622
18

984
24

674
21

43

12
94
18
7

Full
NIS

16
4
24
25
18
35

39
25

20
60

Sample

12
94
18
7
57
60
3
65
38
36
30
9
12
16
30
37
22
21
14
43
34
19
42
1
18
36
90
16
4
24
25
18
35

11
23
15

20
60

52

Sample

2001
Full
NIS

65
38
30
9
12
16
37
21
14
43
34
19
42
1

