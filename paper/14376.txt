NBER WORKING PAPER SERIES

EFFICIENCY BOUNDS FOR MISSING DATA MODELS WITH SEMIPARAMETRIC
RESTRICTIONS
Bryan S. Graham
Working Paper 14376
http://www.nber.org/papers/w14376

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
October 2008

I would like to thank Gary Chamberlain, Jinyong Hahn, Guido Imbens, Michael Jansson and Whitney
Newey for comments on earlier draft. Helpful discussions with Oliver Linton, Cristine Pinto, Jim Powell,
Geert Ridder as well as participants in the Berkeley Econometrics Reading Group and Seminars are
gratefully acknowledged. This revision has benefited from Tom Rothenberg's skepticism, discussions
with Michael Jansson, Justin McCrary, Jim Powell, the comments of a co-editor and three especially
meticulous/generous anonymous referees. All the usual disclaimers apply. This is a heavily revised
version of material which previously circulated under the titles "A note on semiparametric efficiency
in moment condition models with missing data", "GMM 'equivalence' for semiparametric missing
data models", and "Efficient estimation of missing data models using moment conditions and semiparametric
restrictions". The views expressed herein are those of the author(s) and do not necessarily reflect the
views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2008 by Bryan S. Graham. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.

Efficiency bounds for missing data models with semiparametric restrictions
Bryan S. Graham
NBER Working Paper No. 14376
October 2008, Revised October 2010
JEL No. C1,C14,C21
ABSTRACT
This paper shows that the semiparametric efficiency bound for a parameter identified by an unconditional
moment restriction with data missing at random (MAR) coincides with that of a particular augmented
moment condition problem. The augmented system consists of the inverse probability weighted (IPW)
original moment restriction and an additional conditional moment restriction which exhausts all other
implications of the MAR assumption. The paper also investigates the value of additional semiparametric
restrictions on the conditional expectation function (CEF) of the original moment function given alwaysobserved covariates. In the program evaluation context, for example, such restrictions are implied
by semiparametric models for the potential outcome CEFs given baseline covariates. The efficiency
bound associated with this model is shown to also coincide with that of a particular moment condition
problem. Some implications of these results for estimation are briefly discussed.
Bryan S. Graham
New York University
19 W 4th Street, 6FL
New York, NY 10012
and NBER
bsg1@nyu.edu

An online appendix is available at:
http://www.nber.org/data-appendix/w14376

1

Introduction

Let Z = (Y10 ; X 0 )0 be vector of modelling variables, fZi g1
i=1 be an independent and identically

distributed random sequence drawn from the unknown distribution F0 ,
meter vector and

aK

1 unknown para-

(Z; ) a known vector-valued function of the same dimension.2 The only prior

restriction on F0 is that for some

0

RK

2B

E [ (Z;

0 )]

= 0:

Chamberlain (1987) showed that the maximal asymptotic precision with which

(1)
0

can be estimated

under (1) (subject to identi…cation and regularity conditions) is given by If ( 0 ) =
0

= E [@ (Z;

0 ) =@

0]

and

0

= V ( (Z;

0

0
0

1
0

0,

with

)) :3

Now consider the case where a random sequence from F0 is unavailable. Instead only a selected
sequence of samples is available. Let D be a binary selection indicator. When D = 1 we observe Y1
and X, when D = 0 we observe only X.4 This paper considers estimation of

0

under restriction

(1) and the following additional assumptions.
Assumption 1.1 (Random Sampling) fZi ; Di g1
i=1 is an independent and identically distributed

random sequence from F0 .

Assumption 1.2 (Observed Data) For each unit we observe D; X and Y = DY1 :
Assumption 1.3 (Conditional Independence) Y1 ? DjX:
Assumption 1.4 (Overlap) Let p0 (x) = Pr(D = 1jX = x), then 0 <
x2X

p0 (x)

1 for all

Rdim(x) :

Restriction (1) and Assumptions 1.1 to 1.4 constitute a semiparametric model for the data.
Henceforth I refer to this model as the semiparametric missing data model or the missing at random
(MAR) setup. Robins, Rotnitzky and Zhao (1994, Proposition 2.3, p. 850) derived the e¢ cient
in‡uence function for this problem and proposed a locally e¢ cient augmented inverse probability
weighting (AIPW) estimator (cf., Scharfstein, Rotnitzky and Robins, 1999; Bang and Robins, 2005;
Tsiatis, 2006). Cheng (1994), Hahn (1998), Hirano, Imbens and Ridder (2003), Imbens, Newey and
Ridder (2005), and Chen, Hong and Tarozzi (2008) develop globally e¢ cient estimators.
The ‘MAR setup’has been applied to a number of important econometric and statistical problems, including program evaluation as surveyed by Imbens (2004), non-classical measurement error
2

Extending what follows to the overidenti…ed case is straightforward.
Throughout upper case letters denote random variables, lower case letters speci…c realizations of them, and
calligraphic letters their support. I use the notation E [ Aj c] = E [ Aj C = c], V ( Aj c) = V ar ( Aj C = c) and
C ( A; Bj c) = Cov ( A; Bj C = c) :
4
An earlier version of this paper considered the slightly more general set-up with (Z; ) = 1 (Y1 ; X; )
D) Y0 : Results for this extended model, which contains the
0 (Y0 ; X; ) with (X; Y ) observed where Y = DY1 + (1
standard causal inference model and the two-sample instrumental variables model as special cases (cf., Imbens, 2004;
Angrist and Krueger, 1992), follow directly and straightforwardly from those outlined below.
3

1

(e.g., Robins, Hsieh and Newey, 1995; Chen, Hong and Tamer, 2005), missing regressors (e.g.,
Robins, Rotnitzky and Zhao, 1994), attrition in panel data (e.g., Robins, Rotnitzky and Zhao,
1995; Robins and Rotnitzky, 1995; Wooldridge, 2002), and M-estimation under variable probability
sampling (e.g., Wooldridge, 1999, 2007). Chen, Hong and Tarozzi (2004), Wooldridge (2007) and
Egel, Graham and Pinto (2008) discuss several other applications.
The maximal asymptotic precision with which

0

can be estimated under the MAR setup has

been characterized by Robins, Rotnitzky and Zhao (1994) and is given by
Im ( 0 ) =
with

0

=E

0 (X) =p0 (X)

+ q (X;

0 ) q (X;

0
0)

0
0

1
0

0;

, where

(2)
0 (x)

= V( (Z;

0 )j x)

and q (x; ) =

E [ (Z; ) jx] :

The associated e¢ cient in‡uence function, also due to Robins, Rotnitzky and Zhao (1994), is

given by
(z;
for

= (p; q 0 ;

0)

=

1
0

d
(z;
p0 (x)

0)

q (x; 0 )
(d
p0 (x)

p0 (x))

(3)

0 )0 :

The calculation of (2) is now standard. Knowledge of (2) is useful because it quanti…es the
cost – in terms of asymptotic precision – of the missing data and because it can be used to verify
whether a speci…c estimator for

0

is e¢ cient. To simplify what follows I will explicitly assume that

Im ( 0 ) is well-de…ned (i.e., that all its component expectations exist and are …nite, and that all its
component matrices are nonsingular).

This paper shows that the semiparametric e¢ ciency bound for

0

under the MAR setup, co-

incides with the bound for a particular augmented moment condition problem. The augmented
system consists of the inverse probability of observation weighted (IPW) original moment restriction (1) and an additional conditional moment restriction which exhausts all other implications of
the MAR setup. This general equivalence result, while implicit in the form of the e¢ cient in‡uence
function (3), is apparently new. It provides fresh intuitions for several ‘paradoxes’ in the missing
data literature, including the well-known results that projection onto, or weighting by the inverse
of, a known propensity score results in ine¢ cient estimates (e.g., Hahn, 1998; Hirano, Imbens and
Ridder, 2003), that smoothness and exclusion priors on the propensity score do not increase the
precision with which

0

can be estimated (Robins, Hsieh and Newey, 1995; Robins and Rotnitzky,

1995; Hahn, 1998, 2004) and that weighting by a nonparametric estimate of the propensity score
results in an e¢ cient estimator (Hirano, Imbens and Ridder, 2003; cf., Hahn, 1998; Wooldridge,
2007; Prokhorov and Schmidt, 2009; Hitomo, Nishiyama and Okui, 2008).
This paper also analyzes the e¤ect of imposing additional semiparametric restrictions on the
conditional expectation function (CEF) q (x; ) = E [ (Z; ) jx]. If

target parameter is

0

(Z; ) = Y1

, as when the

= E [Y1 ] ; then such restrictions may arise from prior information on the

form of E [Y1 jx]. Such restrictions may arise in other settings as well. For example, if the goal

is to estimate a vector of linear predictor coe¢ cients in the presence of missing regressors, then
2

a semiparametric model for the CEFs of the missing regressors given always-observed variables
generates restrictions on the form of q (x; ) (cf., Robins, Rotnitzky and Zhao, 1994).5
Formally I consider the semiparametric model de…ned by restriction (1), Assumptions 1.1 to 1.4
and the additional assumption.
Assumption 1.5 (Functional Restriction) Partition X = (X10 ; X20 )0 , then
E [ (Z;
where q (x; ; h (x2 ) ; ) is a known K

0 ) jx]

= q (x;

1 function,

0 ; h0 (x2 ) ;

aJ

0)

1 …nite dimensional unknown parameter,

and h ( ) an unknown function mapping from a subset of X2

Rdim(X2 ) into H

RP .

To the best of my knowledge the variance bound for this problem, the MAR setup with ‘functional’ restrictions, has not been previously calculated. In an innovative paper, Wang, Linton
and Härdle (2004) consider a special case of this model where

(Z; ) = Y1

a partial linear structure, as in Engle et al (1986), on E [ Y1 j x] such that q (x;
x01 0

+ h0 (x2 )

0.

. They impose
0 ; h0 (x2 ) ;

0)

=

In making their variance bound calculation they assume that the conditional

distribution of Y1 given X is normal with a variance that does not depend on X. They do not
provide a bound for the general case but conjecture that it is “very complicated”(p. 338). The result given below extends their work to moment condition models, general forms for q (x; ; h (x2 ) ; )
and, importantly, does not require that

(Z; ) be conditionally normally distributed and/or ho-

moscedastic.
Augmenting the MAR setup with Assumption 1.5 generates a middle ground between the fully
parametric likelihood-based approaches to missing data described by Little and Rubin (2002) and
those which leave E [ (Z;

0 ) jx]

unrestricted (e.g., Cheng, 1994; Hahn, 1998; Hirano, Imbens and

Ridder, 2003). Likelihood-based approaches are very sensitive to misspeci…cation (cf., Imbens,
2004), while approaches which utilize only the basic MAR setup require high dimensional smoothing
which may deleteriously a¤ect small sample performance (cf., Wang, Linton and Härdle, 2004;
Ichimura and Linton, 2005). Assumption 1.5 is generally weaker than a parametric speci…cation
for the conditional distribution of

(Z;

0)

given X, but at the same time reduces the dimension

of the nonparametric smoothing problem. Below I show how to e¢ ciently exploit prior information
on the form of E [ (Z;

0 ) jx].

I also provide conditions under which consistent estimation of

0

is

possible even if the exploited information is incorrect.
Section 2 reports the …rst result of the paper: an equivalence between the MAR setup and
a particular method-of-moments problem. Equivalence, which is suggested by the form of the
e¢ cient in‡uence function derived by Robins, Rotnitzky and Zhao (1994), was previously noted for
special cases by Newey (1994a) and Hirano, Imbens and Ridder (2003). I discuss the connection
between their results and the general result provided below. I also highlight some implications of
the equivalence result for understanding various aspects of the MAR setup. Section 3 calculates
5

The formation of predictive models of this type is the foundation of the imputation approach to missing data
described in Little and Rubin (2002).

3

the variance bound for

0

when the MAR setup is augmented by Assumption 1.5. I discuss when

Assumption 1.5 is likely to be informative and also when consistent estimation is possible even if it
is erroneously maintained.
2

Equivalence result

Under the MAR setup the inverse probability weighted (IPW) moment condition
E

D
(Z;
p0 (X)

0)

= 0;

(4)

is valid (e.g., Hirano, Imbens and Ridder, 2003; Wooldridge, 2007). The conditional moment
restriction
E

D
p0 (X)

1 X = 0 8 X 2 X;

(5)

also holds and nonparametrically identi…es p0 (x) : While the terminology is inexact, in what follows
I call (4) the identifying moment and (5) the auxiliary moment.
Consider the case where p0 (x) is known such that (5) is truly an auxiliary moment. One e¢ cient
way to exploit the information (5) contains is to, following Newey (1994a) and Brown and Newey
(1998), reduce the sampling variation in (4) by subtracting from it the …tted value associated with
its regression onto the in…nite-dimensional vector of unconditional moment functions implied by
(5):6
s (Z;

0)

D
(Z;
p0 (X)
D
(Z;
=
p0 (X)
=

D
D
(Z; 0 )
p0 (X)
p0 (X)
q (X; 0 )
(D p0 (X)) :
p0 (X)

0)

1; X

E

0)

That this population residual is equal to the e¢ cient score function derived by Robins, Rotnitzky
and Zhao (1994) strongly suggests an equivalence between the GMM problem de…ned by restrictions
(4) and (5) and the MAR setup outlined above. One way to formally show this is to verify that the
e¢ ciency bounds for

0

in the two problems coincide.7 The bound for

0

under the MAR set-up is

given (2) above, while under the moment problem it is established by the following theorem.
Theorem 2.1 (GMM Equivalence) Suppose that (i) the distribution of Z has a known, …nite
support, (ii) there is some
1; : : : ; L and some 0 <

0

2B

RK and

0

= ( 1; : : : ;

0
L)

where

l

= p0 (xl ) 2 [ ; 1] for each l =

< 1 (with X = fx1 ; : : : ; xL g the known support of X) such that restrictions

6

The notation E [ Y j X; Z] denotes the (mean squared error minimizing) linear predictor of Y given X within a
subpopulation homogenous in Z:
E [ Y j X; Z] = X 0 (Z) ;

(Z) = E XX 0 Z

1

E [ XY j Z] :

Wooldridge (1999b, Section 4) collects some useful results on conditional linear predictors. See also Newey (1990) and
Brown and Newey (1998).
7
An alternative approach to showing equivalency would involve verifying Newey’s (2004) moment spanning condition for e¢ ciency.

4

(4) and (5) hold, (iii)

0

0
0

and Im ( 0 ) =

1
0

0

are nonsingular and (iv) other regularity conditions

hold (cf., Chamberlain (1992b), Section 2), then Im ( 0 ) is the Fisher information bound for

0:

Proof. See the supplemental materials.
The proof of Theorem 2.1 involves only some tedious algebra and a straightforward application
of Lemma 2 of Chamberlain (1987). Assuming that Z has known, …nite support makes the problem
fully parametric. The unknown parameters are the probabilities associated with each possible
realization of Z, the values of the propensity score at each of the L mass points of the distribution
of X,

0

= ( 1; : : : ;

0
L) ,

and the parameter of interest,

0:

The multinomial assumption is not apparent in the form of Im ( 0 ), which involves only condi-

tional expectations of certain functions of the data. This suggests that the bound holds in general
since any F0 which satis…es (4) and (5) can be arbitrarily well-approximated by a multinomial distribution also satisfying the restrictions. Chamberlain (1992a, Theorem 1) demonstrates that this
is indeed the case. Therefore Im ( 0 )

1

is the maximal asymptotic precision, in the sense of Hájek’s

(1972) local minimax approach to e¢ ciency, with which

0

can be estimated when the only prior

restrictions on F0 are (4) and (5). Since this variance bound coincides with (2) I conclude that (4)
and (5) exhaust all of the useful prior restrictions implied by the MAR setup.8
The connection between semiparametrically e¢ cient estimation of moment condition models
with missing data and augmented systems of moment restrictions has been noted previously for
the special case of data missing completely at random (MCAR). In that case Assumptions 1.1 to
1.4 hold with p0 (X) equal to a (perhaps known) constant. Newey (1994a) shows that an e¢ cient
estimate of

0

can be based on the pair of moment restrictions
E [D (Z;

0 )]

= 0;

C (D; q (X;

0 ))

= 0;

with q (X; ) as de…ned above. Hirano, Imbens and Ridder (2003) discuss a related example with
X binary and the data also MCAR. In their example e¢ cient estimation is possible with only a
…nite number of unconditional moment restrictions. Theorem 2.1 provides a formal generalization
of the Newey (1994a) and Hirano, Imbens and Ridder (2003) examples to the missing at random
(MAR) case.
The method-of-moments formulation of the MAR setup provides a useful framework for un8
A referee made the insightful observation that the moment condition model (4) and (5) and the MAR setup are
equivalent in the stronger sense that they impose identical restrictions on the observed data. This, of course, also
implies that they contain identical information on 0 . The complete data vector is given by (D; X; Y1 ), with only
(D; X; Y ) = (D; X; DY1 ) observed. Since Y1 is not observed whenever D = 0 we are free specify its conditional
D

distribution given X and D = 0 as desired. Choosing Y1 j X; D = 0
Y1 j X; D = 1 ensures conditional independence
(Assumption 1.3). Manipulating the identifying moment (4) we then have, writing (Z; 0 ) = (X; Y1 ; 0 ),
E

D
p0 (X)

(X; Y;

0)

= E p0 (X) E
= E [E [

D
p0 (X)

(X; DY1 ;

(X; DY1 ;

0 )j X; D

0)

X; D = 1

= 1]] = E [E [

(X; Y1 ;

0 )j X]] ;

which yields (1). Finally, the auxiliary restriction (5) ties down the conditional distribution of D given X and ensures
Assumption 1.4 is satis…ed. I thank Michael Jansson for several helpful discussions on this point.

5

derstanding several apparent paradoxes found in the missing data literature. As a simple example
consider Hahn’s (1998, pp. 324 - 325) result that projection onto a known propensity score may
be harmful for estimation of

= E [Y1 ]. Formally he shows that, for p0 (x) = Q0 constant in x
P
P
and known, the complete-case estimator, bcc = N Di Y1i = N Di ; while consistent, is ine¢ cient.
0

i=1

i=1

Observe that for the constant propensity score case bcc is the sample analog of the population solution to (4). It consequently makes no use of any information contained in the auxiliary moment
(5). However, that moment will be informative for

= E [ Y1 j x]
0 varies with x,
b
consistent with Hahn’s (1998) …nding that the e¢ ciency loss associated with cc is proportional to
V (q (X;

0 )).

0

if q (x;

0)

Similar reasoning explains why weighting by the (inverse of) the known propensity

score is generally ine¢ cient (cf., Robins, Rotnitzky and Zhao, 1994; Hirano, Imbens and Ridder,
2003; Wooldridge, 2007). The known weights estimator ignores the information contained in (5).
That smoothness and exclusion priors on the propensity score do not lower the variance bound
also has a GMM interpretation. Consider the case where the propensity score belongs to a parametric family p (X;

0) :

If

0

is known, then an e¢ cient GMM estimator based on (4) and (5) is

given by the solution to
N
1 X
s
N

N
1 X
b
b;
=
0; q
N

i=1

i=1

(

Di
p (Xi ;

0)

(Zi ; b)

qb(Xi ; b)
(Di
p (Xi ; 0 )

with qb(x; b) a consistent nonparametric estimate of E [ (Z;

0 )j x].

p (Xi ;

)

0 ))

= 0;

Now consider the e¤ect of

with the consistent estimate b. From Newey and McFadden (1994, Theorem 6.2),
this replacement does not change the …rst order asymptotic sampling distribution of b because

replacing

0

E [@s ( 0 ; q0 ;

0]

0 ) =@

= 0: Furthermore, if the known propensity score is replaced by a consistent
nonparametric estimate, pb(x), then the sampling distribution of b is also una¤ected (Newey 1994b,
Proposition 3, p. 1360). Since the M-estimate of

based on its e¢ cient score function has the

0

same asymptotic sampling distribution whether the propensity score is set equal to the truth or
instead to a noisy, but consistent, estimate, knowledge of its form cannot increase the precision with
which

0

may be estimated.

Another intuition for redundancy of knowledge of the propensity score can be found by inspecting the information bound for the multinomial problem. Under the conditions of Theorem
2.1 calculations provided in the supplemental materials imply that the GMM estimates of
0

(recall that

0

0

and

contains the values for the propensity score at each of the mass points of the

distribution of X) have an asymptotic sampling distribution of
p

N

"

b
b

#

"

0
0

#!

D

!N

"

0
0

# "
;

Im ( 0 )
0

1

0
Im ( 0 )

1

#!

;

with Im ( 0 ) as de…ned in (2) and Im ( 0 ) as de…ned in the supplement. As is well-known, under

block diagonality sampling error in b does not a¤ect, at least to …rst order, the asymptotic sampling
properties of b. While block diagonality is formally only a feature of the multinomial problem, the
6

result nonetheless provides another useful intuition for understanding why prior knowledge of the
propensity score is not valuable asymptotically.
Finally the combination of redundancy of knowledge of the propensity score, and the structure of
the equivalent GMM problem, suggests why the IPW estimator based on a nonparametric estimate
of the propensity score is semiparametrically e¢ cient (Hirano, Imbens and Ridder, 2003): when a
nonparametric estimate of the propensity score is used the sample analog of both (4) and (5) are
satis…ed. In contrast the IPW estimator based on a parametric estimate of the propensity score will
only satisfy a …nite number of the moment conditions implied by (5), hence while it will be more
e¢ cient than the estimator which weights by the true propensity score (e.g., Wooldridge, 2007), it
will be less e¢ cient than the one proposed by Hirano, Imbens and Ridder (2003).
3

Semiparametric functional restrictions

Consider the MAR setup augmented by Assumption 1.5. To the best of my knowledge, the maximal
asymptotic precision with which

0

can be estimated in this model has not been previously char-

acterized. In order to calculate the bound for this problem I …rst consider the conditional moment
problem de…ned by (4) and (5) and
E [ (Z;
with (Z;

0 ; h0 (X2 ) ;

0)

=

(Z;

0)

0 ; h0 (X2 ) ;

q (x;

0 ) jX]

0 ; h0 (x2 ) ;

to this problem to calculate a variance bound for

0.

0) :

= 0;

(6)

I apply Chamberlain’s (1992a) approach

I then show that this bound coincides with

the semiparametric e¢ ciency bound for the problem de…ned by restriction (1) and Assumptions
1.1 to 1.5 using the methods of Bickel, Klaassen, Ritov and Wellner (1993). The value of …rst
considering the conditional moment problem is that it provides a conjecture for the form of the
e¢ cient in‡uence function, therefore sidestepping the need to directly calculate what is evidently a
complicated projection.
To present these results I begin by letting q0 (X) = q (X;

0 ; h0 (X2 ) ;

0) ;

@q0 (X)
@h0

X2

@q0 (X)
@ 0

X2

(Z;

0)

=

q0 (X) ;

G0 (X) =
K J

h
0 (X2 )
P P
h
0 (X2 )
K J

@q0 (X)
@ 0

=E D

@q0 (X)
@h0

0

=E D

@q0 (X)
@h0

0

@q0 (X)
@h0

1
h
0 (X2 )

h
f
Im
( 0 ) = E DG0 (X)0
J J

7

0 (X)

1

0 (X)

1

h
0

(X2 ) ;

H0 (X2 ) = E
K P
i
1
G0 (X) ;
0 (X)

@q0 (X)
X2
@h0

(Z;

0)

and
0
K K

h
= E H0 (X2 )

1
h
H0 (X2 )0
0 (X2 )

The variance bound for

0

i

f
+ E [G0 (X)] Im
( 0)

1

E [G0 (X)]0 + E q0 (X) q0 (X)0 :

in the conditional moment problem de…ned by (4), (5) and (6) is

established by the following Theorem.
Theorem 3.1 (Efficiency with Functional Restrictions, Part 1) Suppose that (i) the
RK ;

0

where

< 1 (with X = fx1 ; : : : ; xL g the

l

= p0 (xl ) 2 [ ; 1] for each l = 1; : : : ; L and some 0 <

known support of X),

2D

0

RJ

and h0 (x2;m ) =

0

= ( 1; : : : ;

RP for each m = 1; : : : ; M (with

2L

0;m

2B

0
L)

distribution of Z has a known, …nite support, (ii) there is some

X2 = fx2;1 ; : : : ; x2;M g the known support of X2 ) such that restrictions (4), (5) and (6) hold, (iii)
f ( )=
and Im
0

1
0
0
0 0

0

are nonsingular and (iv) other regularity conditions hold (cf., Chamberlain

f ( ) is the Fisher information bound for
1992b, Section 2), then Im
0

0:

Proof. See the supplemental materials.
Note that if X1 = ? and X2 = X, such that E [ (Z;

0 )j x]

f ( )
is unrestricted, then Im
0

simpli…es to Im ( 0 ) above. Therefore, Theorem 2.1 may be viewed as a special case of Theorem

3.1. As with Theorem 2.1, the validity of the bound for the non-multinomial case follows from
Theorem 1 of Chamberlain (1992a).
The form of
f

(Z;

0

0;

suggests a candidate e¢ cient in‡uence function of
0)

=

1
0

DH0 (X2 )

1
h
0 (X2 )

f
+ DE [G0 (X)] Im
( 0)

where

= h; ; H;

h;

h

1

@q0 (X)
@h0

G0 (X)0

0 (X)

0

1

0 (X)

1

(Z;

0)

(7)

o
(Z; 0 ) + q (X; 0 ) :

; ; G ; with G = E [G (X)]. Note that each of the three components of

(7) are mutually uncorrelated. The next Theorem veri…es that (7) is the e¢ cient in‡uence function
under the MAR setup with Assumption 1.5 also imposed.
Theorem 3.2 (Efficiency with Functional Restrictions, Part 2) The semiparametric
e¢ ciency bound for

0

in the problem de…ned by restriction (1) and Assumptions 1.1 to 1.5 is equal

f ( ) with an e¢ cient in‡uence function of
to Im
0

f

(Z;

0;

0 ).

Proof. See the supplemental materials.
Theorem 3.1 implies that Assumption 6 can be exploited to more e¢ ciently estimate

0.

However

its use also carries risk, if false, yet nevertheless erroneously maintained by the data analyst, an
inconsistent estimate of

0

may result. This tension, between e¢ ciency and robustness, is formalized

by the next two Propositions which together provide guidance as to whether prior information of
the type given by Assumption 1.5 should be utilized in practice.

8

The …rst Proposition characterizes the magnitude of the e¢ ciency gain associated with correctly
exploiting Assumption 1.5. De…ne:
1 (Z;

0;

0) = D

K 1
2 (Z; 0 ;
J 1

IK
p0 (X)

= DG0 (X)0

0)

H0 (X2 )
0 (X)

1

1
h
0 (X2 )

(Z;

@q0 (X)
@h0

0

1

C

0 (X)

1

(Z;

0)

0) :

Proposition 3.1 Under (1) and Assumptions 1.1 to 1.5
Im ( 0 )

1

f
Im
( 0)

1

=

1

V ( 1)

0

0
1; 2

C

V ( 2)

0 0
1; 2

10

0:

0

(8)

Proof. See the supplemental materials.
Equation (8) has an intuitive interpretation. The …rst term in parentheses
0 (X)

V ( 1) = E

H0 (X2 )

p0 (X)

1
h
H0 (X2 )0
0 (X2 )

;

equals the asymptotic variance reduction that would be available by additionally imposing Assumption 6 if

0

were known.

The additional (asymptotic) sampling uncertainty induced by having to estimate

0

is captured

by the second term
C ( 1;

2) V ( 2)

1

C ( 1;

2)

f ( ) is the information bound for
where Im
0

0

f
= E [G0 (X)] Im
( 0)

1

E [G0 (X)]0 ;

in the semiparametric regression problem (cf., Cham-

berlain, 1992a):

D (Z;

0)

= Dq (X;

The more precisely determined

0 ; h0 (X2 ) ;
0,

0)

+ DV;

E [ V j X; D = 1] = E [ V j X] = 0:

the greater the e¢ ciency gain from imposing Assumption 1.5.

The size of E [G0 (X)] also governs the magnitude of the e¢ ciency gain. Conditional on X2 ,
@q0 (X)
@h0

h (X ) 1
2
0

subpopulation. That

h
0
is9

@q0 (X)
@h0

(X2 ) is a weighted linear predictor of

1
h
0 (X2 )

h
0

(X2 ) = E

0 (X)

and hence G0 (X) is equal to the di¤erence between
9

@q0 (X)
@ 0

given

@q0 (X)
@h0

in the D = 1

@q0 (X) @q0 (X)
; X2 ; D = 1 ;
@ 0
@h0
@q0 (X)
@ 0

and its predicted value based on a

The notation E!(X) [ Y j X; Z; D = 1] denotes the weighted conditional linear predictor
E!(X) [ Y j X; Z; D = 1] = XE DX! (X)

1

X0 Z

1

E DX! (X)

1

Y Z :

This is the population analog of the …tted value from a generalized least squares regression in a subpopulation
homogenous in Z and with D = 1:

9

weighted least squares regression in the D = 1 subpopulation. The average of these di¤erences,
E [G0 (X)], is taken across the entire population; it will be large in absolute value when the distribution of X1 conditional on X2 di¤ers in the D = 1 versus D = 0 subpopulations. This will
occur whenever X1 is highly predictive for missingness (conditional on X2 ). In such situations the
e¢ ciency costs of sampling uncertainty in b are greater (relative to the known 0 case) because
estimation of

0

requires greater levels of extrapolation.

An example clari…es the discussion given above. Assume that
q (X;

0 ; h0 (X2 ) ;

0)

= X10

0

(Z;

+ h0 (X2 )

0)

= Y1

0

with

0:

This is the model considered by Wang, Linton and Härdle (2004). In addition to being of importance
in its own right, it provides insight into the program evaluation problem (where the means of two
missing outcomes, as opposed to just one, need to be estimated). Wang, Linton and Härdle’s (2004)
prior restriction includes the condition that V ( Y1 j X) =

2
1

is constant in X: For clarity of exposition

I also assume homoscedasticity holds, but that this fact is not known by the econometrician. Let
e0 (X2 ) = E [ p (X)j X2 ] = Pr ( D = 1j X2 ) ; specializing the general results given above to this model
and evaluating (8) gives
Im ( 0 )

1

f
Im
( 0)

1

=
(E [E [ X1 j X2 ]

1
1
X2
p (X)
e0 (X2 )
E [ X1 j X2 ; D = 1]])0 (E [E [ X1 j X2 ]
E [e0 (X2 ) V ( X1 j X2 ; D = 1)]
2
1

E E

E [ X1 j X2 ; D = 1]])

0;

which shows that the e¢ ciency gain associated with correctly exploiting Assumption 1.5 re‡ects
three forces. First, substantial convexity in p (X)

1

, which will occur when overlap is limited,

increases the e¢ ciency gain.10 This gain re‡ects the semiparametric restriction allowing for extrapolation in the presence of conditional covariate imbalance. The next two e¤ects re‡ect the fact that
the …rst source of e¢ ciency gain is partially nulli…ed by having to estimate
given X2 in the D = 1 subpopulation then the information for
the precision with which

0

0

0.

If X1 varies strongly

is large which, in turn, increases

may be estimated. On the other hand if there are large (average) dif-

ferences in the conditional mean of X1 given X2 across the D = 1 and D = 0 subpopulations, then
estimating

0

requires greater extrapolation which –when

0

is unknown –decreases the precision

with which it may be estimated.
Proposition 3.1 provides insight into when correctly imposing Assumption 1.5 is likely to be informative. A related question concerns the consequences of misspecifying the form of q (X; ; h (X2 ) ; ).
Under such misspeci…cation the conditional moment restriction (6) will be invalid. Nevertheless the
e¢ cient score function may continue to have an expectation of zero at

=

0.

This suggest that

an M-estimator based on an estimate of the e¢ cient score function may be consistent even if As10

When some subpopulations have low propensity scores E [ 1=p (X)j X2 ]
(Jensen’s Inequality).

10

1=E [ p (X)j X2 ] will tend to be large

sumption 1.5 does not hold. The following proposition provides one set of conditions under which
such a robustness property holds.
Proposition 3.2 (Double Robustness) Let q (X) = q (X;
arbitrary,
(Z;
(Z;
0) =
h
i
@q (X)
h
E
X2 and 0 (X2 ) ;
@h0
f

1.1 to 1.4
(ii)
0 (x)

(Z; ;

=

0;

=

0 (x2 )

of q (X) :

0)

=

0)
h
0

q (X), and rede…ne

0 (X)

= V(

0)

(Z;

with
0 )j X) ;

and h (X2 )
H0 (X2 ) =

(X2 ), and G0 similarly. Under restriction (1) and Assumptions

is mean zero if either (i)

= h ;

; h (X2 ) ;

; H0 ;

h;
0

h
0

;

0 ; G0

=

0;

=

0

and Assumption 1.5 holds or

and (a) p0 (x) = e0 (x2 ) for all x 2 X ; (b)

for all x 2 X , and (c) at least one element of h (x2 ) enters linearly in each row

Proof. See the supplemental materials.
Note that there is a tension between the robustness property of Proposition 3.2 and the e¢ ciency
gain associated with Assumption 1.5. Mean-zeroness of
that those variables entering q (X; ; h (X2 ) ;

0)

f

(Z; ;

0)

under misspeci…cation requires

parametrically do not a¤ect either the probability

of missingness or the conditional variance of the moment function (1). Under such conditions an
estimator based on

f

(Z; ;

0)

will perform no better, at least asymptotically, than one based on

the e¢ cient score function derived by Robins, Rotnitzky and Zhao (1994). In particular we have:
Corollary 3.1 Under the conditions of part (ii) of Proposition 3.2
Im ( 0 )

1

f
Im
( 0)

1

= 0:

Proof. See the supplemental materials.
Collectively Propositions 3.1 and 3.2 suggest that estimation while maintaining Assumption 1.5
will be most valuable when the econometrician is highly con…dent in the imposed semiparametric
restriction.
References
Angrist, Joshua D. and Alan B. Krueger. (1992). “The e¤ect of age at school entry on educational attainment: an application of instrumental variables with moments from two samples,”
Journal of the American Statistical Association 87 (418): 328 - 336.
Bang, Heejung and James M. Robins. (2005). “Doubly robust estimation in missing data and
causal inference models,” Biometrics 61 (4): 962 - 972.
Bickel, Peter J., Chris A.J. Klaassen, Ya’acov Ritov and Jon A. Wellner. (1993). E¢ cient and
adaptive estimation for semiparametric models. New York: Springer-Verlag, Inc.
Brown, Bryan W. and Whitney K. Newey. (1998). “E¢ cient semiparametric estimation of
expectations,” Econometrica 66 (2): 453 - 464.

11

Chamberlain, Gary. (1987). “Asymptotic e¢ ciency in estimation with conditional moment
restrictions,” Journal of Econometrics 34 (1): 305 - 334.
Chamberlain, Gary. (1992a). “E¢ ciency bounds for semiparametric regression,”Econometrica
60 (3): 567 - 596.
Chamberlain, Gary. (1992b). “Comment: sequential moment restrictions in panel data,”Journal of Business and Economic Statistics 10 (1): 20 - 26.
Chen, Xiaohong, Han Hong, Elie T. Tamer. (2005). “Measurement error models with auxiliary
data,” Review of Economic Studies 72 (2): 343 - 366.
Chen, Xiaohong, Han Hong and Alessandro Tarozzi. (2004). “Semiparametric e¢ ciency in
GMM models of nonclassical measurement errors, missing data and treatment e¤ects, Mimeo.
Chen, Xiaohong, Han Hong and Alessandro Tarozzi. (2008). “Semiparametric e¢ ciency in
GMM models with auxiliary data,” Annals of Statistics 36 (2): 808 - 843.
Cheng, Philip E. (1994). “Nonparametric estimation of mean functionals with data missing
at random,” Journal of the American Statistical Association 89 (425): 81 - 87.
Egel, Daniel, Bryan S. Graham, and Cristine Pinto. (2008). “Inverse probability tilting and
missing data problems,” NBER Working Paper No. 13981.
Engle, Robert F., C. W. J. Granger, John Rice and Andrew Weiss. (1986). “Semiparametric
estimates of the relation between weather and electricity sales,” Journal of the American
Statistical Association 81 (394): 310 - 320.
Hahn, Jinyong. (1998). “On the role of the propensity score in e¢ cient semiparametric estimation of average treatment e¤ects,” Econometrica 66 (2): 315 - 331.
Hahn, Jinyong. (2004). “Functional restriction and e¢ ciency in causal inference,” Review of
Economics and Statistics 86 (1): 73 - 76.
Hájek, Jaroslav. (1972). “Local asymptotic minimax and admissibility in estimation,” Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability 1: 175 194 (L. M. Le Cam, J. Neyman & E. L. Scott, Eds.). Berkeley: University of California Press.
Hirano, Keisuke, Guido W. Imbens and Geert Ridder. (2003). “E¢ cient estimation of average
treatment e¤ects using the estimated propensity score,” Econometrica 71 (4): 1161 - 1189.
Hitomo, Kohtaro, Yoshihiko Nishiyama and Ryo Okui. (2008). “A puzzling phenomenon in
semiparametric estimation problems with in…nite-dimensional nuisance parameters,” Econometric Theory 24 (6): 1717 - 1728.

12

Ichimura, Hidehiko and Oliver Linton. (2005). “Asymptotic expansions for some semiparametric program evaluation estimators,” Identi…cation and Inference for Econometric Models:
Essays in Honor of Thomas Rothenberg: 149 -170 (D.W.K Andrews & J.H. Stock, Eds).
Cambridge: Cambridge University Press.
Imbens, Guido W. (2004). “Nonparametric estimation of average treatment e¤ects under
exogeneity: a review,” Review of Economics and Statistics 86 (1): 4 - 29.
Imbens, Guido W., Whitney K. Newey and Geert Ridder (2005). “Mean-square-error calculations for average treatment e¤ects,” IEPR Working Paper 05.34.
Little, Roderick J. A. and Donald B. Rubin. (2002). Statistical Analysis with Missing Data.
Hoboken, N.J.: John Wiley & Sons, Inc.
Newey, Whitney K. (1990). “Semiparametric e¢ ciency bounds,” Journal of Applied Econometrics 5 (2): 99 - 135.
Newey, Whitney K. (1994a). “Series estimation of regression functionals,”Econometric Theory
10 (1): 1 - 28.
Newey, Whitney K. (1994b). “The asymptotic variance of semiparametric estimators,”Econometrica 62 (6): 1349 - 1382.
Newey, Whitney K. (2004). “E¢ cient semiparametric estimation via moment restrictions,”
Econometrica 72 (6): 1877 - 1897.
Newey, Whitney K. and Daniel McFadden. (1994). “Large sample estimation and hypothesis
testing,” Handbook of Econometrics 4 : 2111 - 2245 (R.F. Engle & D.L. McFadden). Amsterdam: North Holland.
Prokhorov, Artem and Peter J, Schmidt. (2009). “GMM redundancy results for general missing
data problems,” Journal of Econometrics 151 (1): 47 - 55.
Robins, James M., Fushing Hsieh and Whitney Newey. (1995). “Semiparametric e¢ cient estimation of a conditional density function with missing or mismeasured covariates,” Journal
of the Royal Statistical Society B 57 (2): 409 - 424.
Robins, James M. and Andrea Rotnitzky. (1995). “Semiparametric e¢ ciency in multivariate
regression models,” Journal of the American Statistical Association 90 (429): 122 - 129.
Robins, James M. and Andrea Rotnitzky. (2001). “Comments,” Statistica Sinica 11 (4): 920
- 936.
Robins, James M., Andrea Rotnitzky and Lue Ping Zhao. (1994). “Estimation of regression
coe¢ cients when some regressors are not always observed,”Journal of the American Statistical
Association 89 (427): 846 - 866.
13

Robins, James M., Andrea Rotnitzky and Lue Ping Zhao. (1995). “Analysis of semiparametric
regression-models for repeated outcomes in the presence of missing data,” Journal of the
American Statistical Association 90 (429): 106 - 121.
Scharfstein, Daniel O., Andrea Rotnitzky and James M. Robins. (1999). “Rejoinder,”Journal
of the American Statistical Association 94 (448): 1135- 1146.
Tsiatis, Anastasios A. (2006). Semiparametric Theory and Missing Data. New York: Springer.
Wang, Qihua, Oliver Linton and Wolfgang Härdle. (2004). “Semiparametric regression analysis
with missing response at random,” Journal of the American Statistical Association 99 (466):
334 - 345.
Wooldridge, Je¤rey M. (1999a). “Asymptotic properties of weighted M-estimators for variable
probability samples,” Econometrica 67 (6): 1385 - 1406.
Wooldridge, Je¤rey M. (1999b). “Distribution-free estimation of some nonlinear panel data
models,” Journal of Econometrics 90 (1): 77 - 97.
Wooldridge, Je¤rey M. (2002). “Inverse probability weighted M-estimators for sample selection, attrition and strati…cation,” Portuguese Economic Journal 1 (2): 117 - 139.
Wooldridge, Je¤rey M. (2007). “Inverse probability weighted estimation for general missing
data problems,” Journal of Econometrics 141 (2): 1281 - 1301.

14

E¢ ciency bounds for missing data models
with semiparametric restrictions, supplemental material: proofs
This appendix contains proofs of the results contained in the main paper. All notation is as de…ned in the main
text unless explicitly noted otherwise. Equation numbering continues in sequence with that established in the main
text. To simplify notation let denote the true parameter value 0 unless explicitly stated otherwise (similarly the
‘0’subscript is removed from other objects, such as the propensity score, when doing so does not cause confusion).
A

Proof of Theorem 2.1

The proof closely follows that of Theorem 1 in Chamberlain (1992) and consists of three steps.
Step 1: Demonstration of equivalence with an unconditional GMM problem The …rst step is to show that
restrictions (4) and (5) are, in the multinomial case, equivalent to a …nite set of unconditional moment restrictions.
Under the multinomial assumption we have X 2 fx1 ; : : : ; xL g for some L: Let the L 1 vector B have a 1 in the lth
P
row if X = xl and zeros elsewhere and l = Pr (X = xl ) (observe that L
l=1 l = 1). Denote the value of the selection
probability at X = xl by l and de…ne = f 1 ; : : : ; L g0 ; this vector gives the values of p ( ) at each of the mass
points of X. Using this notation we can write p (X) = B 0 :
Under the multinomial assumption restrictions (4) and (5) are equivalent to the L + K 1 vector of unconditional
moment restrictions
#
"
1
B BD0
m1 (Z; )
= 0:
E [m (Z; ; )] = E
=E
D
m2 (Z; ; )
(Z; )
0
B

To verify that this is the case note that by iterated expectations
0

B
B
E [m1 (Z; )] = B
@
and hence E [m1 (Z; )] = 0 if and only if E

h

D
p(X)

1E

LE

h

D
p(X)

h

D
p(X)

i 1

1

X = x1

1

X = xL

..
.

C
C
C;
i A

i
1 X = 0 for all X 2 fx1 ; : : : ; xL g : We also have
D
p (X)

E [m2 (Z; ; )] = E

(Z; ) = 0;

so E [m (Z; ; )] = 0 if and only if (4) and (5) are satis…ed as claimed.
Step 2: Application of Lemma 2 of Chamberlain (1987) Chamberlain (1987, Lemma 2) shows that for Z a
multinomial random variable the variance bound for under the sole restriction that E [m (Z; ; )] = 0 is
M 0V

1

1

M
22

where

M 0V

1

1

is the lower-right K

M

K block of M 0 V

1

1

M

with

22

V

def

E m (Z; ; ) m (Z; ; )0 ;

M

def

E

@m (Z; ; ) @m (Z; ; )
;
:
@ 0
@ 0

The application of Chamberlain’s result requires that M has full column rank and that V is non-singular. The
calculations made in Step 3 below demonstrate that these conditions are implied by the assumption that has full
column rank, p (X) is bounded away from zero and non-singularity of :

15

The …nal step is to solve for an explicit expression for

Step 3: Calculation of the bound

M 0V

V

we have the lower right-hand block, letting
V22
K K

V11
0
V12

=

V12
V22

=E
=E
=

E

.

;

(Z; ) and q (X) = E [ j X], given by

=

= E m2 (Z; ; ) m2 (Z; ; )0
"

1

M
22

This requires some simple, albeit tedious, algebra. Partitioning V0

L+K L+K

1

(9)

#

0

X
p (X)

V ( jX)
1 p (X)
+
q (X) q (X)0 + q (X) q (X)0
p (X)
p (X)

XL

l=1

l

l

+

1

l

l

l

ql ql0 + ql ql0 ;

where ql = E [ (Z; ) jxl ] and l = V ( jxl ) :
The upper right-hand block is similarly derived as
V12
L K

= E m1 (Z; ) m2 (Z; ; )0
"

D
B0

=E B

1

=E B
=

1

1

1

(10)
0

D (Z; )
B0

#

p (X)
q (X)0
p (X)
1

1

q1

L

1

L
L

0

qL

:

Finally the upper left-hand block is given by
V11
L L

D
B0

=E B

= E BB 0
n
= diag

1

1

1

D
B0

1 B0

p (X)
p (X)

1

1

L

1

1

L
L

(11)

o

:

Now partition M
M

L+K L+K

M1
M2

=

0
M2

;

where, from similar calculations to those made above, we have
M1 =
L L

diag

n

L
L

1
1

o

;

M2 =

1

K L

q1

L

1

qL
L

;

M2
K K

= :

(12)

Applying standard results on partitioned inverses then yields
M

1

=

M1 1
M2 1 M2 M1

1

0
M2

1

!

:

Note that the existence of M1 1 and M2 1 follows from the assumptions that p (X) is bounded away from zero and
the assumption that has full column rank.
Redundancy of knowledge of the propensity score suggests that M 1 V M 10 will be block diagonal. A su¢ cient

16

condition for this is that (cf., Prokhorov and Schmidt, 2009)
0
V12
= M2 M1 1 V11 :

(13)

To verify that this condition holds use (11) and (12) to show that
M2 M1 1 V11 =

1

1

1
1

q1

L

1

L
L

qL

;

0
which equals V12
as required. Exploiting the resulting simpli…cations yields

M

1

VM

10

M1 1 V11 M1

=

1

0

0

M2

1

0
V12
V11 1 V12 M2

V22

10

!

;

and hence
M
By M2 M1

1

1

VM

10
22

= M2

1

0
V12
V11 1 V12 M2

V22

10

:

0
= (q1 ; : : : ; qL ) and (13) we have V12
V11 1 V12 equal to
0

V12 V11 1 V12 = M2 M1 1 V11 M1
=

L
X

1
l

l=1

=E

1

l
l

10

M20

ql ql0

p (X)
q (X) q (X)0 ;
p (X)

and hence, using (9),
V22

0
V12
V11 1 V12 = E

V ( jX)
+ q (X) q (X)0 = :
p (X)

Using this result and taking the partitioned determinant gives
det (V ) = det (V11 ) det V22

0
V12
V11 1 V12 = E

1

p (X)
det f g ;
p (X)

and hence V is non-singular under overlap (Assumption 1.4) and non-singularity of :
Since M2 = we have Im ( 0 ) = 0 1 as claimed. For completeness the upper left-hand portion of the full
variance covariance matrix is given by
M111 V11 M1110 = Im 1 (
where f (x) =
B

PL

l=1 l

0)

= diag

p (x1 ) (1 p (x1 ))
;
f (x1 )

;

p (xL ) (1 p (xL ))
f (xL )

1 (x = xl ) :

Proof of Theorem 3.1

The …rst two steps of the proof of Theorem 3.1 are analogous to those of Theorem 2.1 and therefore omitted. The
actual calculation of the bound, while conceptually straightforward, is considerably more tedious. Details of this step
are provided here.
Assume that the marginal distributions of X1 and X2 have I and M points of support with probabilities 1 ; : : : ; I
and &1 ; : : : ; &M . Let L = I M and im denote the joint probability Pr X1 = x1;i ; X2 = x2;m . Let = ( 1 ; : : : ; M )0
be the values of h ( ) at each of the mass points of X2 (for simplicity I assume that dim (h (x2 )) = P = 1 in the
calculations below, but the results generalize). Let C be a M 1 vector with a 1 in the mth row if X2 = x2;m and
zeros elsewhere. Finally it is convenient to use the shorthand = q (X) q (X)0 : In what follows I use both the single
and double subscript notation to denote a point on the support of X as is convenient. We can map between the two
notations by observing that xim = xl for l = (i 1) M + m.
For the multinomial case the conditional moment problem de…ned by (4), (5) and (6) is equivalent to the uncon-

17

ditional problem

with

0

=

0

;

2

0 0

; 0;

3
m1 (Z; )
E [m (Z; )] = E 4 m2 (Z; ; ; ; ) 5 = 0;
m3 (Z; ; )
and
D
B0

m1 (Z; ) = B
L 1

1 ;

m2 (Z; ; ; ; ) = (B

D
B0

IK )

LK 1

D
B0

m3 (Z; ; ) =
K 1

q(X; ; C 0 ; )

(Z; )

;

(Z; ) :

Partition V = E m (Z; ) m (Z; )0 as

V

L+KL+K L+KL+K

0

V11
= @ V21
V31

V22
V32

V34

1

A;

where, using calculations similar to those given in the proof of Theorem 2.1, we have
V11
L L
V31 =

K L

1
1

1

= diag
1

;:::;

1

q1 ; : : : ;

1

1

1

1

L

L

L

1
L

L

;

V12
L KL

L

qL ;

V32

=

1

K KL

= (0; : : : ; 0) ;
1

;:::;

L

L

1

V22
KL KL
;

= diag
XL

V33 =

l=1

K K

L

1

1

;:::;

1
l

l

+

L

L

L

1

l

l
l

ql ql0 + ql ql0 :

We can partition the Jacobian matrix

M

L+KL+K L+M +J+K

0

M1
=@ 0
M3

0
M2
0

0
M2
0

1

0
0
M3

A;

where

M1 =

1

diag

;:::;

1

L L

L

;

L

M3 =
where Hi = diag f

i1 rh qi1 ; : : : ; iM rh qiM g

The variance bound for
V

1

M2
KL M
1

H10 ; : : : ; HI0

=

q1

L

1

qL
L

;

0

;

M2
KL J

0

1r

B
@

=

M3 = :

q1

1

C
..
A
.
L r qL

for i = 1; : : : ; I with qim = q(xim ; ; h x2;m ; ).

is given by the lower right-hand K K block of M 0 V

1

1

: We begin by calculating

M

: Partition V
B11
0
B12

V =

B12
B22

;

with
B11

= diag

L+KL L+KL

Now partition V

1

V11

;

V22

B12

=

V31

L+KL K

V32

0

;

B22 = V33 :

as
V0

1

=

C11
0
C12

C12
C22

(14)

;

where the partitioned inverse formula gives
C11
L+KL L+KL

= diag

V11 1

V22 1

+ D0 E [ ]

1

D;

18

0
C12
K L+KL

=

E[ ]

1

D;

C22
K K

= E[ ]

1

;

0
with D = A0 ( L IK )0 = B12
B111 and A =
Expression (14) follows since

=
0
We also have C12
=

XL

l=1

l

l

XL

l=1

0
C22 B12
B111 =

+

1

l

l
l

= E[ ]

E[ ]

1

0
B
B
@

1

K matrix.

1

1

l=1

l

l

l

ql ql0 +

l
l

:

D and

0
C11 = B111 + B111 B12 C22 B12
B111 = diag
f
We now evaluate Im
( ) = M 0V

aL

XL

ql ql0 + ql ql0

1

0
l q l ql

0

qL

1

0
B12
B111 B12

C22 = B22
=

q1

V11 1

+ D0 E [ ]

V22 1

1

D:

M to

M10 V11 1 M1
0
0
0
M20
M20

h

h

0
V22 + (

L

IK ) E [ ]

1

V22 1 + (

L

IK ) E [ ]

1

1

M30

E[ ]

1

(

L

(

L

(

L
0

0

i

M20
M20

IK ) M2
i
IK )0 M2

IK ) M2

h

h

0
V22 + (

L

IK ) E [ ]

1

(

L

V22 1 + (

L

IK ) E [ ]

1

(

L

1

M30

E[ ]

1

(

i
IK )0 M2
i
IK )0 M2

IK )0 M2

L

0
M20 ( L IK ) E [ ]
M20 ( L IK ) E [ ]
M30 E [ ] 1 M3

1
1

M3
M3

1

C
C
C;
A

where I have made use of the equality M10 A = M30 :
f
Observe that, as in the standard semiparametric missing data model, Im
( ) satis…es Stein’s condition for redundancy of knowledge of the propensity score for : However the structure of the bound does indicate that knowledge
of the …nite dimensional parameters and nonparametric portions of the CEF of (Z; ) given X does increase the
precision with which can be estimated.
The variance bound for 0 is given by the lower right-hand K K block of the inverse of this matrix. Because
of block diagonality we only need to consider the lower right-hand block. Partition this block as
B11
0
B12

B12
B22

;

where B11 , B12 and B22 are rede…ned to equal
B11 =

M20 V22 1 M2
M20 V22 1 M2

M20 V22 1 M2
M20 V22 1 M2

B12 =

M20 (
M20 (

L
L

+
IK )
IK )

M20 (
M20 (
E[ ]

IK )
IK )

L
L
1

M3 ;

19

E[ ]

1

(

L

IK )0 M2

B33 = M30 E [ ]

1

M3 :

(

L

IK )0 M2

The information bound is therefore given by
0
B12
B111 B12

f
Im
( ) = B22

= M30 E [ ]

1

M30 E [

M3

M20 V22 1 M2
M20 V22 1 M2
+

M20 (
M20 (
(

E[ ]

IK )0 M2

L

= M30

L

E[ ] +

(

(

1

(

IK )0 M2

L

(

IK )0 M2

L

M20 V22 1 M2
M20 V22 1 M2

IK )
IK )

L

0]

M20 (
M20 (

1

IK )0 M2

L

IK )0 M2

L

M20 V22 1 M2
M20 V22 1 M2

1

(

L

E[ ]

1

M3

IK )0 M2

L
1

M20 V22 1 M2
M20 V22 1 M2

IK )
IK )

L

M20 (
M20 (

IK )
IK )

L
L

#

1

M3 ;
1

where I have used the identity A 1 A 1 U (B 1 + U 0 A 1 U ) 1 U 0 A 1 = A + U BU 0
:
Using the partitioned inverse formula and multiplying out the expression in [ ] above then gives
f
Im
( ) = M30

+ M2

[E [ ] + (
M2

M20 V22 1 M2
M2

M2

L

h
IK )0 M2

M20 V22 1 M2

M20 V22 1 M2
M20

1

M20 V22 1 M2
1

M20

M20 V22 1 M2
1

M20 V22 1 M2
1

V22 M2

M20

1

V22 M2

0

#

1

M20 V22 1 M2
(

L

#

IK )

1

M3 :

We can now use the explicit expressions for V0 and M0 give above to generate an interpretable bound. The
required calculations are tedious but straightforward (details are available from the author upon request), they give
f
an information bound of Im
( ) as de…ned in the main text of the paper.
C

Proof of Theorem 3.2

In calculating the e¢ ciency bound for the semiparametric missing data model de…ned by restriction (1) and Assumptions 1.1 to 1.5 above, I follow the general approach outlined by Bickel, Klaassen, Ritov and Wellner (1993) and,
especially, Newey (1990, Section 3). First, I characterize the nuisance tangent space. Second, I demonstrate pathwise
di¤erentiability of the parameter of interest, : The e¢ cient in‡uence function for this model equals the projection
of the pathwise derivative onto the tangent space. In the present example the direct calculation of this projection
appears to be particularly di¢ cult. However inspection of the variance bound associated with the conditional moment
problem de…ned by restrictions (4), (5) and (6) provides a conjecture for the form of the e¢ cient in‡uence function.
The third and …nal step of the proof therefore involves demonstrating that (i) this conjectured in‡uence function lies
in the model tangent space and (ii) that it is indeed the required projection (i.e., that it satis…es equation (9) in
Newey (1990, p. 106)). The result then follows from an application of Theorem 3.1 in Newey (1990).
Step 1: Characterization of the nuisance tangent space
for (Y; X; D), making use of Assumption 1.3, is given by
f (y; x; d) = f ( y1 j x)d p (x)d [1

Recalling that Y = DY1 , the joint density function
p(x)]1

d

f (x) :

Assumption 1.5 also requires that f ( y1 j x) satisfy the restriction
Z

(z;

0 ; h0 (x2 ) ; 0 ) f

20

( y1 j x) dy1 = 0;

where

(z; ) =

(x; y1 ; ) and
(z; ; h (x2 ) ; ) =

(x; y1 ; )

q (x; ; h (x2 ) ; ) :

Consider a regular parametric submodel with f (y; x; d; ) = f (y; x; d) at
given by
f (y; x; d; ) = f ( y1 j x; )d p (x; )d [1 p(x; )]1
and satis…es the restriction

Z

(z; ( ) ; h (x2 ; ) ;

0) f

0.

=
d

The submodel joint density is
(15)

f (x; )

(16)

( y1 j x; ) dy1 = 0:

The submodel score vector equals
s (y; x; d; ) = ds ( y1 j x; ) +

d p (x; )
r p (x; ) + t (x; ) ;
p (x; ) [1 p (x; )]

(17)

where
s (y; x; d; ) = r log f (y; x; d; ) ;

s ( y1 j x; ) = r log f ( y1 j x; ) ;

t (x; ) = r log f (x; ) :

By the usual mean zero property of (conditional) scores we have
(18)

E [ s ( Y1 j X)j X] = E [t (X)] = 0;
where suppression of in a function means that it is evaluated at its population value (e.g., t
Condition (16) imposes additional restrictions on s ( Y1 j X) beyond conditional mean
structure of these restrictions di¤erentiate (16) with respect to through the integral and
= 0:
@q0 (X) @ ( 0 )
@q (X) @h (X2 ; 0 )
+ 0 0
= E (Z; 0 ; h0 (X2 ) ; 0 ) s ( Y1 j X)0
@ 0
@ 0
@h
@ 0

(x) = t (x; 0 )).
zeroness. To see the
evaluate the result at
X :

The conditional covariance between (Z; 0 ; h0 (X2 ) ; 0 ) and s ( Y1 j X) has a particular structure induced by the
semiparametric restrictions on the form of E [ (Z; )j x] :
From (17), (18) and the above equality the tangent set is evidently
T = fds ( y1 j x) + a (x) [d

(19)

p (x)] + t (x)g ;

where a (x) is unrestricted and t (x) and s ( y1 j x) satisfy
E [t (X)] = 0
E [ s ( Y1 j X)j X] = 0
E

(Z;

0
0 ; h0 (X2 ) ; 0 ) s ( Y1 j X)

X =

@q0 (X)
@ 0

@q0 (X)
@h0

c+

k (X2 ) ;

with c a constant matrix and k (x2 ) an unrestricted matrix-valued function of x2 :
Step 2: Demonstration of pathwise di¤erentiability Under the parametric submodel
unconditional moment restriction
E [ (Z; ( ))] = 0:
Di¤erentiating under the integral and evaluating at
@ ( 0)
=
@ 0
To demonstrate pathwise di¤erentiability of

1
0

E

=

0

gives

(Z;

0)

@ log f (Y1 ; X;
@ 0

0)

we require F (Y; X; D) such that

@ ( 0)
= E F (Y; X; D)s (Y; X; D)0 :
@ 0

21

0

:

( ) is identi…ed by the

It is easy to verify that the function
D
(Z;
p0 (X)

1

F (Y; X; D) =

0

0 ; h0 (X2 ) ; 0 )

+ q (X;

0 ; h0 (X2 ) ; 0 ) ;

satis…es this condition (cf., Hahn 1998).
Step 3: Veri…cation that conjectured e¢ cient in‡uence function equals the required projection Inspection of the variance bounds associated with the conditional moment problem suggests the candidate e¢ cient in‡uence
given by (7) in the main text. I …rst verify that f (Z; 0 ; 0 ) lies in the model tangent space. The last term in (7)
plays the role of t (x). A zero plays the role of a (x) [d p (x)]. Finally the …rst two terms in (7) play the role of
ds ( y1 j x) : To see this note that in addition to being both conditionally mean zero we have
E

"

(

(Z;

f
+E [G0 (X)] Im
( 0)

@q0 (X)
@ 0

=

c+

1

G0 (X)0

@q0 (X)
@h0

0

@q0 (X)
@h0

h
1
0 (X2 )

0 ) H0 (X2 )

(X)

1

(Z;

1

0 (X)

o0

0)

(Z;

0)

#

X

k (X2 )

with
E [G0 (X)]0
n
h
1
H0 (X2 )0
0 (X2 )

f
c = Im
( 0)

k (X2 ) =

1

h
0

o
(X2 ) c :

The candidate e¢ cient in‡uence function therefore belongs to the model tangent space as required.
I next show that f (Z; 0 ; 0 ) is indeed the required projection by verifying that it satis…es
E

hn

f

F (Y; X; D)

(Z;

o i

0; 0)

0

= 0,

for all 2T

(cf., equation (9) in Newey (1990, p. 106)). We have

F (Y; X; D)

f

(Z;

0; 0)

1

=

0

D

(

1
p0 (X)

h
1
0 (X2 )

H0 (X2 )

@q0 (X)
@h0

0

f
E [G0 (X)] Im
( 0)

(X)
1

1

G0 (X)0

(X)

1

o

(Z;

0) :

By the conditional independence of Y1 and D given X (Assumption 1.3) and conditional mean zeroness of (Z; 0 ) it
f
(Z; 0 ; 0 ) is orthogonal to any functions of the form a (x) [d p (x)] and t (x) :
is easy to show that F (Y; X; D)
All that remains is to show orthogonality with ds ( y1 j x). We have
E

hn

F (Y; X; D)

f

(Z;

o

0; 0)

=E

"

0

Ds ( Y1 j X)0
(

1

IK

i

H0 (X2 )

h
1
0 (X2 )

f
E [G0 (X)] Im
( 0)

1

@q0 (X)
@h0

G0 (X)0

0

(X)
p (X)

1

(X)
p (X)
)

@q0 (X)
@ 0
where I have made use of the special structure of the conditional covariance E

22

(Z;

1

c+
0) s

@q0 (X)
@h0

k (X2 )

;

( Y1 j X)0 X : Multiplying

out terms yields
E

hn

=

f

F (Y; X; D)
1
0

o

(Z;

i
Ds ( Y1 j X)

0; 0)

@q0 (X)
c + H0 (X2 ) k (X2 )
@ 0

E

h
1
0 (X2 )

H0 (X2 )

h
0

(X2 ) c

f
E [G0 (X)] Im
( 0) 1

0 (X2 ) c

f
E [G0 (X)] Im
( 0) 1

f
+ E [G0 (X)] Im
( 0) 1

h
0

(X2 )0

h
0

0

f
+E [G0 (X)] Im
( 0) 1

h
0

=

1
0

fE [G0 (X)] c

H0 (X2 ) k (X2 )
h
1
0 (X2 )

h
0

(X2 ) c

(X2 ) k (X2 )
i
(X2 )0 k (X2 )

E [G0 (X)] cg = 0;

f
where the …rst equality follows from iterated expectations and the second from the de…nitions of G0 (X) and Im
( 0)
in the main text.
The result then follows from an application of Theorem 3.1 in Newey (1990).

D

Proof of Proposition 3.1

The di¤erence in the variance bounds is given by
Im (

0)

1

f
Im
(

0)

1

=

1
0

(

10

0)

0

0

;

with 0 and 0 as de…ned in the main text.
First observe that E [G0 (X)] has the covariance representation
E [G0 (X)] = E
with

1

and

2

h
1
0 (X2 )

h
0

0
1; 2

(X2 ) = C

;

as de…ned in the main text. This follows since

D
(Z;
p0 (X)

E

@q0 (X)
@h0

@q0 (X)
@ 0

0)

(Z;

0
0)

0 (X)

1

@q0 (X)
@ 0

@q0 (X)
@h0

h
1
0 (X2 )

=E

h
0

@q0 (X)
@ 0

(X2 )
@q0 (X)
@h0

1
h
0 (X2 )

h
0

(X2 ) ;

and also
"

E DH0 (X2 )

h
1
0 (X2 )

@q0 (X)
@h0

0

0 (X)

1

(Z;

0)

(Z;

0
0)

0 (X)

@q0 (X)
@ 0

1

@q0 (X)
@h0

h
1
0 (X2 )

h
0

(X2 )

Similar calculations yield the variance representations
V ( 1) = E

0 (X)

p0 (X)

H0 (X2 )

h
1
H0 (X2 )0
0 (X2 )

with the result directly following.

23

;

h
V ( 2 ) = E DG0 (X)0

0 (X)

1

i
G0 (X) ;

= 0:

E

Proof of Proposition 3.2

Part (i) follows from Theorem 3.2. For part (ii) condition (a) implies the equality.
"

E DH0 (X2 )
"

= H0 (X2 ) E

@q (X)
@h0

0

@q (X)
@h0

h
1
0 (X2 )

0

0 (X)

(Z;

@q (X)
@h0

1

0 (X)

1

X2

0)

#

X2

1

E

"

#
@q (X)
@h0

0

0 (X)

1

(Z;

0)

#

X2 :

Condition (b) implies that 0 (X) = 0 (X2 ) : Let L (X2 ) L (X2 )0 = 0 (X2 ) be the Cholesky decomposition of
0 (X2 ). This implies that the term to the right of the last equality equals

H0 (X2 ) E

"

1

L (X2 )

0

@q (X)
@h0

@q (X)
@h0

1

L (X2 )

X2

#

1

E

"

0

@q (X)
@h0

1

L (X2 )

L (X2 )

1

(Z;

Since all expectations in the above expression condition on X2 ; L (X2 ) may be treated as non-stochastic so that
1

L (X2 )

@q (X)
@h0

1

H0 (X2 ) = E L (X2 )

X2 :

Recall that a linear predictor passes through the mean of the outcome variable at the means of the predictor variables
(when a constant is included). Condition (c) implies that each row of @q (X) =@h0 includes such a constant and hence
that
L (X2 )

1

E[

(Z;

0 )j X2 ]

= L (X2 )
"
E

E
and therefore that
"

E DH0 (X2 )

"

H0 (X2 )

L (X2 )

1

@q (X)
@h0

L (X2 )

1

@q (X)
@h0

0

@q (X)
@h0

h
1
0 (X2 )

1

1

0 (X)

(Z;

0

0n

0)

L (X2 )

1

L (X2 )

1

#

X2 = E [

@q (X)
@h0

X2

#
o
(Z; 0 ) X2 ;

(Z;

0 )]

=

#

1

E [q (X)] :

This implies that the …rst part of f (Z; ; 0 ) has mean E [q (X)] :
Using conditions (a), (b), (c), and arguments analogous to those given immediately above we have
L (X2 )

1

G0 (X) = L (X2 )

1

L (X2 )

1

E
h
so that E L (X2 )

1

"

@q0 (X)
@ 0
@q0 (X)
@h0

L (X2 )

G0 (X) X2

i

1

E

"

L (X2 )

@q (X)
@h0

= L (X2 )

1

0

@q (X)
@h0

1

1

L (X2 )

@q0 (X)
@ 0

E [ G0 (X)j X2 ] = 0.

E [G0 (X)] = 0: This implies that the second part of
has mean E [q (X)]. The result follows as claimed.

f

(Z;

24

;

0

L (X2 )
#

1

@q (X)
@h0

X2

#

1

X2 ;

The law of iterated expectations then gives

0 ) is mean zero. The third part of

f

(Z;

;

0)

#

0 ) X2 :

F

Proof of Corollary 3.1

From the proof to Proposition 3.2 we have E [G0 (X)] = 0. So the result follows if
E

0 (X)
p0 (X)

h
1
H0 (X2 )0
0 (X2 )

H0 (X2 )

= 0:

Under conditions (a) and (b) of part (ii) of Proposition 3.2 we have
0 (X)
1
H0 (X2 ) h
H0 (X2 )0
0 (X2 )
p0 (X)
2
"
(X
)
1
0
2
= E4
H0 (X2 ) E
L (X2 )
e0 (X2 )
e0 (X2 )

E

=E
E

"

0 (X2 )
e0 (X2 )

L (X2 )

L (X2 )
E L (X2 )
e0 (X2 )
@q (X)
@h0

1

where L (X2 ) L (X2 )0 =
E L (X2 )
E

"

0 (X2 )

@q (X)
@h0

1

L (X2 )

1

0

1

@q (X)
@h0

1

L (X2 )

1

0

@q (X)
@h0

L (X2 )

1

@q (X)
@h0

X2

#

1

H0 (X2 )

X2

@q (X)
@h0

X2

#

1

E L (X2 )

1

@q (X)
@h0

X2

0

3

L (X2 )0 5 ;

as above. Observe that
X2

@q (X)
@h0

0

L (X2 )

1

@q (X)
@h0

X2

#

1

E L (X2 )

1

@q (X)
@h0

X2

is equal to the multivariate conditional linear predictor of the K
K identity matrix given L (X2 )
h
i
1 @q (X)
evaluated at E L (X2 )
X2 ; therefore this object equals IK and we have
@h0
E

0 (X)

p0 (X)

3

05

H0 (X2 )

h
1
H0 (X2 )0
0 (X2 )

=E

0 (X2 )

e0 (X2 )

L (X2 ) L (X2 )0
e0 (X2 )

0

;

1

@q (X)
@h0

= 0;

as required.

References
Bickel, Peter J., Chris A.J. Klaassen, Ya’acov Ritov and Jon A. Wellner. (1993). E¢ cient and
adaptive estimation for semiparametric models. New York: Springer-Verlag, Inc.
Chamberlain, Gary. (1987). “Asymptotic e¢ ciency in estimation with conditional moment
restrictions,” Journal of Econometrics 34 (1): 305 - 334.
Chamberlain, Gary. (1992). “Comment: sequential moment restrictions in panel data,”Journal of Business and Economic Statistics 10 (1): 20 - 26.
Hahn, Jinyong. (1998). “On the role of the propensity score in e¢ cient semiparametric estimation of average treatment e¤ects,” Econometrica 66 (2): 315 - 331.
Newey, Whitney K. (1990). “Semiparametric e¢ ciency bounds,” Journal of Applied Econometrics 5 (2): 99 - 135.

25

