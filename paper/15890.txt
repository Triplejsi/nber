NBER WORKING PAPER SERIES

THE TERM STRUCTURE OF INTEREST RATES IN A DSGE MODEL WITH RECURSIVE
PREFERENCES
Jules van Binsbergen
Jesús Fernández-Villaverde
Ralph S.J. Koijen
Juan F. Rubio-Ramírez
Working Paper 15890
http://www.nber.org/papers/w15890

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
April 2010

First version: June 2007. We thank George Constantinides, Xavier Gabaix, Lars Hansen, Hanno Lustig,
Monika Piazzesi, Stephanie Schmitt-Grohé, Martin Schneider, Martín Uribe, Stijn Van Nieuwerburgh,
and seminar participants at the University of Chicago, Yale, Stanford, the SED, the University of Pennsylvania,
and the SITE conference for comments. Beyond the usual disclaimer, we must note that any views
expressed herein are those of the authors and not necessarily those of the Federal Reserve Bank of
Atlanta or the Federal Reserve System. Finally, we also thank the NSF for financial support. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau
of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2010 by Jules van Binsbergen, Jesús Fernández-Villaverde, Ralph S.J. Koijen, and Juan F. Rubio-Ramírez.
All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit
permission provided that full credit, including © notice, is given to the source.

The Term Structure of Interest Rates in a DSGE Model with Recursive Preferences
Jules van Binsbergen, Jesús Fernández-Villaverde, Ralph S.J. Koijen, and Juan F. Rubio-Ramírez
NBER Working Paper No. 15890
April 2010
JEL No. E2,E3,G12
ABSTRACT
We solve a dynamic stochastic general equilibrium (DSGE) model in which the representative household
has Epstein and Zin recursive preferences. The parameters governing preferences and technology are
estimated by means of maximum likelihood using macroeconomic data and asset prices, with a particular
focus on the term structure of interest rates. We estimate a large risk aversion, an elasticity of intertemporal
substitution higher than one, and substantial adjustment costs. Furthermore, we identify the tensions
within the model by estimating it on subsets of these data. We conclude by pointing out potential extensions
that might improve the model's fit.

Jules van Binsbergen
Graduate School of Business
Stanford University
518 Memorial Way
Stanford, CA 94305-5015
jvb2@gsb.stanford.edu
Jesús Fernández-Villaverde
University of Pennsylvania
160 McNeil Building
3718 Locust Walk
Philadelphia, PA 19104
and NBER
jesusfv@econ.upenn.edu

Ralph S.J. Koijen
Chicago Booth
5807 South Woodlawn Avenue
60637 Chicago, Illinois
ralph.koijen@chicagobooth.edu
Juan F. Rubio-Ramírez
Duke University
P.O. Box 90097
Durham, NC 27708
juan.rubio-ramirez@duke.edu

1. Introduction
In this paper, we study whether an estimated dynamic stochastic general equilibrium (DSGE)
model in which the representative household has Epstein and Zin recursive preferences can
match both macroeconomic and yield curve data. After solving the model using perturbation
methods, we build the likelihood function with the particle …lter and estimate the structural
parameters, those describing preferences and technology, via maximum likelihood and macroeconomic and yield curve data. We also estimate the model on subsets of the data to illustrate
how the structural parameters are identi…ed.
The motivation for our exercise is that economists are paying increasing attention to
recursive utility functions (Kreps and Porteus, 1978, Epstein and Zin, 1989 and 1991, and
Weil, 1990).1 The key advantage of these preferences is that they allow separation between the
intertemporal elasticity of substitution (IES) and risk aversion. In the asset pricing literature,
researchers have argued that Epstein and Zin preferences account for many patterns in the
data, possibly in combination with other features such as long-run risk. Bansal and Yaron
(2004) is a prime representative of this line of work. From a policy perspective, recursive
preferences generate radically bigger welfare costs of the business cycle than those coming
from standard expected utility (Tallarini, 2000). Hence, they may change the trade-o¤s that
policy makers face, as shown by Levin, López-Salido, and Yun (2007). Finally, Epstein and
Zin preferences can be reinterpreted, under certain conditions, as a case of robust control
preferences (Hansen, Sargent, and Tallarini, 1999). The natural question is, thus, whether
we can successfully take an empirically plausible version of these models to the data.
Our paper makes three main contributions. The …rst contribution is to study the role of
Epstein and Zin preferences in a full-‡edged production economy with endogenous capital
and labor supply and their interaction with the yield curve. Having a production economy
is crucial for many questions in which economists are interested. For example, one of the
most attractive promises of integrating macroeconomics and …nance is to have, in the middle
run, richer models for policy advice. Fiscal or monetary policy will have implications for the
yield curve because they trigger endogenous responses on the accumulation of capital. These
e¤ects on the yield curve may be key for the propagation mechanism of policy. Similarly, we
want to learn how to interpret movements in the yield curve as a way to identify the e¤ects of
1

Among many others, Backus, Routledge, and Zin (2004 and 2007), Bansal, Dittman, and Kiku (2007),
Bansal, Gallant, and Tauchen (2008), Bansal, Kiku, and Yaron (2007), Bansal and Yaron (2004), Campanale,
Castro, and Clementi (2010), Campbell (1993 and 1996), Campbell and Viceira (2001), Chen, Favilukis
and Ludvigson (2007), Croce (2006), Dolmas (1996), Gomes and Michealides (2005), Hansen, Heaton, and
Li (2008), Kaltenbrunner and Lochstoer (2008), Lettau and Uhlig (2002), Piazzesi and Schneider (2006),
Rudebusch and Swanson (2008), Tallarini (2000), and Uhlig (2007). See also Hansen et al. (2008) for a
survey of the literature.

3

policy interventions on variables, such as investment, that are central to the business cycle.
This is particularly easy to see if we concentrate on the consumption process that drives
the stochastic discount factor under recursive preferences. Except in a few papers,2 researchers interested in asset pricing have studied economies in which consumption follows an
exogenous process. This is a potentially important shortcoming. First, production economies
place tight restrictions on the comovements of consumption with other endogenous variables
that exogenous consumption models are not forced to satisfy. Second, in DSGE models, the
consumption process itself is not independent of the parameters …xing the IES and risk aversion. In comparison, by …xing the consumption process in endowment economies, a change
in preferences implicitly translates to a change in the labor income process. This complicates the interpretation of preference parameters. Finally, considering production economies
with labor supply is quantitatively relevant. Uhlig (2007) has shown how, with Epstein and
Zin preferences, leisure signi…cantly a¤ects asset pricing through the risk-adjusted expectation operator. Thus, even when leisure enters separately in the period utility function, the
recursive formulation will make the stochastic discount factor dependent on leisure.
Unfortunately, working with Epstein and Zin preferences is harder than working with
expected utility. Instead of the simple optimality conditions of expected utility, recursive
preferences imply necessary conditions that include the value function itself.3 Therefore,
standard linearization techniques cannot be employed. Hence, the literature has resorted
to either simplifying the problem by working only with an exogenous ‡ow for consumption,
or using costly computational solution algorithms such as value function iteration (Croce,
2006) or projection methods (Campanale, Castro, and Clementi, 2010). The former solution
precludes all those exercises in which consumption reacts endogenously to the dynamics of
the model. The latter solution makes estimation, a basic goal of our paper, exceedingly
challenging because of the time spent in the solution of the model for each set of parameter
values.
We get around this obstacle by solving the equilibrium dynamics of our economy with
perturbation methods. Thus, we illustrate how this approach is a fast and reliable way
to compute models with Epstein and Zin preferences. Our choice is motivated by several
considerations. First, we will show how perturbation o¤ers insights into the structure of the
2

Among recent examples, Backus, Routledge, and Zin (2007), Campanale, Castro, and Clementi (2010),
Croce (2006), or Rudebusch and Swanson (2008).
3
Epstein and Zin (1989) avoid this problem by showing that if we have access to the total wealth portfolio, we can derive a …rst-order condition in terms of observables that can be estimated using a method of
moments estimator. However, in general we do not observe the total wealth portfolio because of the di¢ culties in measuring human capital, forcing the researcher to proxy the return on wealth. See, for instance,
Campbell (1996) and Lustig, Van Nieuwerburgh, and Verdehaln (2007).

4

solution of the model that enhance our understanding of the role of recursive preferences. In
particular, we will learn that the …rst-order approximation to the policy functions of our model
with Epstein and Zin preferences is equivalent to that of the model with standard utility and
the same IES. The risk aversion parameter does not show up in this …rst-order approximation.
Instead, risk aversion appears in the constant of the second-order approximation that captures
precautionary behavior. This constant moves the ergodic distribution of states, a¤ecting,
through this channel, allocations, prices, and welfare up to a …rst order. More concretely,
by changing the mean of capital in the ergodic distribution, the risk aversion parameter
in‡uences the average level and the slope of the yield curve. Risk aversion also enters into
the coe¢ cients of the third-order approximation that change the slope of the response of the
solution to variations in the states of the model.
Furthermore, in practice, perturbation methods are the only computationally feasible procedure to solve the policy-oriented, medium-scale DSGE models that have dozens of state
variables (Christiano, Eichenbaum, and Evans, 2005, and Smets and Wouters, 2007). Consequently, our solution approach has an important applicability. Moreover, since implementing
a third-order perturbation is feasible with o¤-the-shelf software such as Dynare, which requires minimum programming knowledge by the user, our …ndings may induce researchers to
explore recursive preferences in further detail.
In contemporaneous work, Rudebusch and Swanson (2008) also use perturbation methods
to solve a DSGE model with recursive preferences. Their model di¤ers from ours in that
they do not include endogenous capital, which in a production model imposes important
constraints on the properties of returns. In addition, they rely on an approximation to the
returns on bonds through a consol. Andreasen and Zabczyk (2010) show, however, that this
may introduce computational biases. In terms of methodology, we estimate the model via
maximum likelihood, whereas Rudebusch and Swanson calibrate the parameters.
The second contribution of our paper is to show how to estimate the model by maximum
likelihood. In studying the asset pricing implications of full-‡edged equilibrium models, it
is common practice to calibrate the parameters.4 While this approach illuminates the main
economic mechanism at work, it might overlook some restrictions implied by the model. This
is relevant, since various asset pricing models can explain the same set of moments, but the
economic mechanism generating the results, be it habits, long-run risks, or rare disasters,
is quite di¤erent and implies diverse equilibrium dynamics. Our likelihood-based inference
imposes all cross-equation restrictions implied by the equilibrium model and is, therefore,
4

Famous examples are Campbell and Cochrane (1999) and Bansal and Yaron (2004). A notable exception
is Chen, Favilukis, and Ludvigson (2007), who estimate an endowment economy in which the representative
agent has habit-type preferences.

5

much more powerful in testing asset pricing models.
The third contribution of our paper is to the fast-growing literature on term structure
models. These models are successful in …tting the term structure of interest rates, but
this is typically accomplished using latent variables.5 Even though some papers include
macroeconomic or monetary policy variables, such variables still enter in a reduced-form
way. Our approach imposes much additional structure on such models, but the restrictions
directly follow from the assumptions we make about preferences and technology. Such models
obviously underperform the statistical models,6 but they improve our understanding as to
which preferences and technology processes induce a realistic term structure of interest rates.
Furthermore, as we have argued before, macroeconomists require a structural model to design
and evaluate economic policies that might a¤ect the term structure of interest rates in an
environment with recursive preferences.
Summarizing, this paper is the …rst one to show how to use perturbation techniques
in combination with the particle …lter to overcome the di¢ culties in estimating production
models with recursive preferences. To do so, we rely on a prototype real business cycle
economy with Epstein and Zin preferences and long-run growth through a unit root in the
law of motion for technological progress.
As our …rst step, we perturb the value function formulation of the household problem
to obtain a third-order approximation to the solution of the model given some parameter
values in a trivial amount of time.7 Given our econometric goals, an additional advantage
of our solution technique is that we do not limit ourselves to the case with unitary IES, as
Tallarini (2000) and others are forced to do.8 There are three reasons why this ‡exibility
might be important. First, because restricting the IES to one seems an unreasonably tight
restriction that is hard to reconcile with previous …ndings. Second, a value of the IES equal
to one implies that the consumption-wealth ratio is constant over time. This implication of
the model is hard to verify because total wealth is not directly observable, since it includes
human wealth. However, di¤erent attempts at measurement, such as Lettau and Ludvigson
(2001) or Lustig, van Nieuwerburgh, and Verdelhan (2007), reject the hypothesis that the
ratio of consumption to wealth is constant. Third, the debate between Campbell (1996) and
5

See, among others, Dai and Singleton (2000 and 2002), Du¤ee (2002), Cochrane and Piazzesi (2005 and
2008), and Ang, Bekaert, and Wei (2008).
6
Campbell and Cochrane (2000) make a similar point in relation to consumption-based and reduced-form
asset pricing models.
7
In companion work, Caldara et al. (2010) document that this solution is highly accurate and compare it
with alternative computational approaches.
8
There is also another literature, based on Campbell (1993), that approximates the solution of the model
around a value of the IES equal to one. Since our perturbation is with respect to the volatility of the
productivity shock, we can deal with arbitrary values of the IES.

6

Bansal and Yaron (2004) about the usefulness of the Epstein and Zin approach pertains to the
right value of the IES. By directly estimating this parameter using all economic restrictions
implied by production economies, we contribute to this conversation.
The second step in our procedure is to use the particle …lter to evaluate the likelihood
function of the model (Fernández-Villaverde and Rubio-Ramírez, 2007).9 Evaluating the
likelihood function of a DSGE model is equivalent to keeping track of the conditional distribution of unobserved states of the model with respect to the data. Our perturbation solution
is inherently non-linear. These non-linearities make the conditional distribution of states
intractable and prevent the application of more conventional methods, such as the Kalman
…lter. The particle …lter is a sequential Monte Carlo method that replaces the conditional
distribution of states by an empirical distribution of states drawn by simulation.
We estimate the model with US data on consumption growth, output growth, …ve bond
yields, and in‡ation over the period 1953.Q1 to 2008.Q4. The point estimates imply a high
coe¢ cient of risk aversion, an IES well above one, and substantial adjustment costs of capital. However, we …nd that the model barely generates a bond risk premium and substantially
underestimates the volatility of bond yields. The model is able, however, to reproduce the
autocorrelation patterns in consumption growth, the 1-year bond yield, and in‡ation. To
better understand the model’s shortcomings and how the parameters are identi…ed, we reestimate the model based on subsets of our data. First, we omit in‡ation from our sample.
The estimates we then …nd imply a bond risk premium that is comparable to the one we
measure in the data, and the model reproduces the empirical bond yield volatility. However, this “success” is explained by the fact that, in this case, the volatility of in‡ation is
too high. Finally, we estimate our model based only on bond yields. The estimates and
implications are remarkably similar to the previous case in which we omit the observations
on in‡ation. This leads us to conclude that the parameters are mostly identi…ed from yield
and in‡ation data. This also illustrates the large amount of information regarding structural
parameters in …nance data and the importance of incorporating asset pricing observations
into the estimation of DSGE models.
The rest of the paper is structured as follows. In section 2, we present our model. In
section 3, we explain how we solve the model with perturbation and what we learn about the
structure of the solution. Section 4 describes the likelihood-based method based on the use
of the particle …lter. Section 5 reports the data and our empirical …ndings. Section 6 outlines
several extensions and section 7 concludes. Three appendices o¤er further details.
9

A recent application of the particle …lter in …nance includes Binsbergen and Koijen (2010), who use the
particle …lter to estimate the time series of expected returns and expected growth rates using a present-value
model.

7

2. A Production Economy with Recursive Preferences
In this section, we present a simple production economy that we will later take to the data and
use it to price nominal bonds at di¤erent maturities. The only deviation from the standard
stochastic neoclassical growth model is that we consider Epstein and Zin preferences, instead
of standard state-separable constant relative risk aversion (CRRA). In addition, we add a
process for in‡ation that captures well the dynamics of price increases in the data and that
will allow us to value nominal bonds.
2.1. Preferences
There is a representative household whose utility function over streams of consumption ct
and leisure 1

lt is:
Ut =

where

ct (1

1

lt )1

1
Et Ut+1

+

0 is the parameter that controls risk aversion,
1
1

1
The term Et Ut+1

we have that

1

1

1

;
0 is the IES, and

:

1
1

is often called the risk-adjusted expectation operator. When

= 1;

= 1 and the recursive preferences collapse to the standard CRRA case. The

Epstein and Zin framework implies that the household has preferences for the timing of the
resolution of uncertainty. In our notation, if
of uncertainty, and if

<

1

> 1 , the household prefers an early resolution

, a later resolution. The discount factor is

and one period

corresponds to one quarter.
2.2. Technology
There is a representative …rm with access to a technology described by a neoclassical production function yt = kt (zt lt )1 , where output yt is produced with capital, kt , labor, lt , and
technology zt . This technology evolves as a random walk in logs with drift :
log zt+1 =
where "zt
".

N (0; 1): The parameter

+ log zt +

" "zt+1 ,

(1)

scales the standard deviation of the productivity shock,

This parameter, also called the perturbation parameter, will facilitate the presentation of

8

our solution method later on. We pick this speci…cation over trend stationarity motivated by
Tallarini (2000), who shows that a unit root representation such as (1) facilitates matching
the observed market price of risk in a model close to ours. Similarly, Álvarez and Jermann
(2005) calculate that most of the unconditional variation in the pricing kernel comes from
the permanent component. Part of the reason, as emphasized by Rouwenhorst (1995), is that
period-by-period unit root shifts of the long-run growth path of the economy increase the
variance of future paths of the variables and, hence, the utility cost of risk.
2.3. Budget and Resource Constraints
The budget constraint of the household is:
ct + it +

bt
bt+1 1
= rt kt + wt lt + ;
pt Rt
pt

(2)

where pt is the price level of the …nal good at time t, it is investment in period t, kt is capital
in period t, bt is the number of one-period uncontingent bonds held in period t that pay one
nominal unit in period t + 1, Rt

1

is their unit price at time t, wt is the real wage at time t,

and rt is the real rental price of capital at time t; both measured in units of the …nal good. In
the interest of clarity, we include in the budget constraint only the one-period uncontingent
bond we just described. Using the pricing kernel, in section 2.6, we will write the set of
equations that determine the prices of nominal bonds at any maturity. In any case, their
price in equilibrium will be such that the representative agent will hold a zero amount of
them. The aggregate resource constraint is
(3)

yt = ct + it :
2.4. Dynamics of the Capital Stock
Capital depreciates at rate . Thus, the dynamics of the capital stock is given by:
kt+1 = (1
in which:
G

it
kt

=

it
kt

) kt + G

a1

it
kt

1

1

kt ;

1

+ a2 ;

denotes the adjustment cost of capital as in Jermann (1998). We normalize:
a1 =

e

1+
1
9

;

(4)

and
1

a2 = e

1+

;

such that adjustment costs do not a¤ect the steady state of the model.
2.5. In‡ation Dynamics
In our data, we will include nominal bond yields at di¤erent maturities as part of our observables. Hence, we need to take a stand on how in‡ation, log

t,

evolves over time. Since

we want to keep the model as stylized as possible, we take in‡ation as an exogenous process
that does not a¤ect allocations. Therefore, money is neutral in our economy.10 Also, the
representative household has rational expectations about the process.
Following Campbell and Viceira (2001), among others, in‡ation is assumed to follow the
process:
log
where ! t

t+1

= log + (log

t

log ) +

N (0; 1), ! t ? "zt ; and log

t

(

! ! t+1

log pt

+

0 " "zt+1 )

+ (

! !t

+

log pt 1 . The parameters

1 " "zt ) ,

0

and

1

cap-

ture the correlation of unexpected and expected in‡ation with innovations to the technology
process, "zt+1 and "zt respectively. As before,

is the perturbation parameter.

This speci…cation allows us to accomplish two objectives. First, it lets us consider a
correlation between innovations to in‡ation expectations and innovations to the stochastic
discount factor. This implies that bond prices do not move one to one with expected in‡ation
and that we have an in‡ation premium. Second, the MA components capture the negative
…rst-order autocorrelation and the small higher order autocorrelations of in‡ation growth
reported by Stock and Watson (2007). These authors prefer an IMA(1,1) process for in‡ation
instead of our ARMA speci…cation. Unfortunately, we cannot handle a unit root process for
in‡ation because the perturbation method to be used to solve the model requires that in‡ation
have a steady-state value. To minimize the e¤ects of our stationarity assumption, we will
calibrate

to be 0.955 (the highest possible value for

numerical instabilities) and
of

such that we do not su¤er from

to 1.009 to match the observed average in‡ation. Our choice

is close to the value estimated by Stock and Watson (2007) when an ARMA(1,1) model

related to the one described here is estimated over a sample period similar to ours.
10

This is not as restrictive an assumption as it might seem. Nominal rigidities, while important to capture
business cycle dynamics, are not very useful for matching asset pricing properties (see, for instance, De Paoli,
Scott, and Weeken, 2007, or Doh, 2009). This is particularly true once we account for, as we do, part of the
relation between price changes and technology shocks through our process for in‡ation.

10

2.6. Pricing Nominal Bonds
Given our process for in‡ation, we now move to price nominal bonds. In Appendix 8.1, we
show that the stochastic discount factor (SDF) for our economy is given by:
ct+1 (1

Mt+1 =

ct (1

!1

lt+1 )1
lt )1

ct
ct+1

!1

1
Vt+1
1
Et Vt+1

1

:

where the value function Vt is de…ned as:
Vt = max Ut ;
ct ;lt ;it

subject to (3) and (4). We switch notation to Vt because it is convenient to distinguish
between the utility function of the household, Ut , and the value function that solves the
household’s problem Vt . Note that since the welfare theorems hold in our model, this value
function is also equal to the solution of the social planner’s problem, a result we use in the
appendices in a couple of steps. Nothing of substance depends on working with the social
planner’s problem except that the notation is easier to handle.
Hence, the Euler equation for the one-period nominal bonds is:
Et Mt+1
which can be written as:
2
ct+1 (1
Et 4
ct (1

1

lt+1 )
lt )

1

!1

1
t+1

ct
ct+1

=

1
;
Rt

1
Vt+1
1
Et Vt+1

!1

1

3

1 5
1
=
:
Rt
t+1

But what is more important for us, we can also compute bond prices recursively using
the following formula:
Et Mt+1

1
t+1

1
Rt+1;t+m

=

1
Rt;t+m

;

(5)

1
with Rt;t+m
being the time-t price of an m-periods nominal bond. Note that we write Rt;t+1 =

Rt and Rt+1;t+1 = 1.
Disappointingly, we do not have any analytic expression for the equilibrium dynamics of
the model. In the next two sections, we will explain, …rst, how to use perturbation methods to
solve for these dynamics. Second, we will show how to exploit the output of the perturbation
to write a state-space representation of the model and how to exploit this representation to
evaluate the associated likelihood function.
11

3. Solving the Model Using Perturbation
As we will explain in detail momentarily, we solve our economy by perturbing the value
function of the household plus the equilibrium conditions of the model de…ned by optimality
and feasibility. The advantage of perturbation over other methods such as value function
iteration or projection is that it produces an answer in a su¢ ciently fast manner as to make
likelihood estimation feasible.11
We are not the …rst to explore the perturbation of value functions. Judd (1998) proposes
the idea but does not elaborate much on the topic. More recently, Schmitt-Grohé and Uribe
(2005) use a second-order approximation to the value function to rank di¤erent …scal and
monetary policies in terms of welfare.
Our solution approach is also linked with that of Benigno and Woodford (2006) and
Hansen and Sargent (1995). Benigno and Woodford (2006) present a new linear-quadratic
approximation to solve optimal policy problems that avoids some problems of the traditional
linear-quadratic approximation when the constraints of the problem are non-linear.12 Thanks
to this alternative approximation, the authors …nd the correct local welfare ranking of di¤erent
policies. Our method, as theirs, can deal with non-linear constraints and obtain the correct
local approximation. One advantage of our method is that it is easily generalizable to higherorder approximations without complication. Hansen and Sargent (1995) modify the linearquadratic regulator problem to include an adjustment for risk. In that way, they can handle
some versions of recursive utilities like the ones that motivate our investigation. Hansen and
Sargent’s method, however, imposes a tight functional form for future utility. Moreover, as
implemented in Tallarini (2000), it requires solving a …xed-point problem to recenter the
approximation to control for precautionary behavior. This step is time consuming and it
is not obvious that the required …xed-point exists or that the recentering converges. Our
method does not su¤er from those limitations.
In this paper, we …nd a third-order approximation to the value function and decision rules
that yields outstanding accuracy as measured by Euler equation errors. We stop at order
three because third-order terms allow for a time-varying risk premium, an important feature
of the data that we want to capture, while keeping computational costs at a reasonable
level.13 Since we need to solve the model hundreds of times for di¤erent parameter values in
our estimation, speed is of the utmost importance. Obtaining higher-order approximations
is conceptually straightforward but computationally cumbersome.
11

Also, as documented by Caldara et al. (2010) while exploring how to compute a model similar to ours,
the accuracy of a high-order perturbation is excellent even far away from the steady state of the model.
12
See also Levine, Pearlman, and Pierse (2007) for a similar treatment of the problem.
13
See also Binsbergen (2009)

12

In our exposition, we use a concise notation to illustrate the required steps. Otherwise,
the algebra becomes too involved to be developed explicitly in the paper in all its detail. In
our application, the symbolic algebra is undertaken by a computer employing Mathematica,
which automatically generates Fortran 95 code that we can evaluate numerically.
3.1. Basic Structure
Since our model is non-stationary, we need to make it stationary by rescaling the variables
by zt 1 . Hence, for any variable xt , we denote its normalized value by x~t = xt =zt 1 . Also,
remember that the stochastic processes are written in terms of a perturbation parameter .
When

= 1, we are dealing with the stochastic version of the model and when = 0 we are
dealing with the deterministic case with steady state e
kss and log zess = .
Thus, we can write the value function, V e
kt ; log zet ; , and the decision rules for conkt ; log zet ;
sumption, c e

kt ; log zet ;
l e

kt ; log zet ;
, investment, i e

kt ; log zet ;
, capital, k e

, and labor,

, as a function of the two rescaled states, e
kt and log zet and the perturbation

parameter, . Since money is neutral in this model, the above-described value function and
decision rules do not depend on in‡ation. This is extremely helpful because it allows us to
…rst solve for them without considering in‡ation and, only then, using the previous …ndings,
to solve for nominal bond prices that do depend on in‡ation.
Now, we …nd the approximations to the value function and the decision rules for consumption, labor, investment, and capital. Later, we will …nd the approximations to the
kt e
kss ; log zet log zess ; 1 as the vector of states in di¤erences
bond yields. De…ne st = e

with respect to the steady state, where sit is the i component of this vector at time t for
i = f1; 2; 3g. Under di¤erentiability conditions, the third-order Taylor approximation of the
value function, evaluated at

= 1, around the steady state is

1
1
V e
kt ; log zet ; 1 ' Vss + Vi;ss sit + Vij;ss sit sjt + Vijl;ss sit sjt slt ;
2
6

(6)

where each term V:::;ss is a scalar equal to a derivative of the value function evaluated at the
steady state:
Vss
Vi;ss
Vij;ss

V e
kss ; log zess ; 0 ;

Vi e
kss ; log zess ; 0

Vij e
kss ; log zess ; 0
13

for i = f1; 2; 3g ;

for i; j = f1; 2; 3g ;

and
Vijl e
kss ; log zess ; 0

Vijl;ss

for i; j; l = f1; 2; 3g ;

where we have followed the tensor notation:

3
X

Vi;ss sit =

Vi;ss si;t ;

i=1
3 X
3
X

Vij;ss sit sjt =

Vij;ss si;t sj;t ;

i=1 i=1

and
Vijl;ss sit sjt slt

3 X
3 X
3
X

=

Vijl;ss si;t sj;t sl;t ;

i=1 j=1 l=1

that eliminates the symbol

P3

i=1

when no confusion arises.

Expression (6) has interesting properties. For example, when we evaluate it at e
kss ; log zess ; 1

(the values of capital and productivity growth of the steady state and positive variance of
shocks), all terms will drop, except Vss , V3;ss , V33;ss , and V333;ss . But it turns out that all
the terms in odd powers of

(in this case, V3;ss and V333;ss ) are identically equal to zero.
Therefore, a third-order approximation of the value function evaluated in e
kss ; log zess ; 1 is:
1
kss ; log zess ; 1 ' Vss + V33;ss ;
V e
2

where 12 V33;ss is a measure of the welfare cost of the business cycle, that is, of how much utility
changes when the variance of the productivity shocks is

2

instead of zero.14 This quantity

14

This welfare cost can easily be transformed into consumption equivalent units by computing the decrease
in consumption ' that makes the household indi¤erent between consuming (1 ') css units per period with
certainty or ct units with risk. The steady-state value function is:
1

Vss =

(1

1
which implies:

Then:

e
css (1

1

lss )

+

1
2

'=1

zess
(1

1
2

)

)

!1

e
css (1

1

lss )

;

1

zess

V33;ss = (e
css (1

(1

)

1
zet
6
1
6
61 +
1
2 e
4
css (1 lss )

14

1

')) (1
31

7
7
V33;ss 7 :
5

1

lss )

:

is not necessarily negative, both because of the Jensen’s inequality (our productivity process
is in logs) and because concave utility functions may have convex indirect utility functions
(see Cho and Cooley, 2000, for an example in a real business cycle model). The term V33;ss
is also a key di¤erence of perturbation from the standard linear-quadratic approximation,
where constants are dropped because they are irrelevant for the optimization. This is yet
another advantage of perturbation: V33;ss measures the e¤ects of risk in the value function
and we want to keep it for welfare evaluation.
Following the same tensor notation as before, the decision rule for any control variable
var (consumption, labor, investment, and capital) can be approximated in a similar way as
1
1
var e
kt ; log zet ; 1 ' varss + vari;ss sit + varij;ss sit sjt + varijl;ss sit sjt slt ;
2
6

in which we de…ne

varss
vari;ss
varij;ss
and
varijl;ss

var e
kss ; log zess ; 0 ;

vari e
kss ; log zess ; 0

for i = f1; 2; 3g ;

kss ; log zess ; 0
varij e

for i; j = f1; 2; 3g ;

varijl e
kss ; log zess ; 0

for i; j; l = f1; 2; 3g :

The problem is that the derivatives V:::;ss and var:::;ss are not known. A perturbation
method …nds them by taking derivatives of a set of equations describing the equilibrium of
the model and applying an implicit function theorem to solve for these unknown derivatives.
But once we have reached this point, there are two paths we can follow to obtain a set of
equations to perturb. The …rst path, the one in this paper, is to write down the equilibrium
conditions of the model plus the de…nition of the value function. Then, we take successive
derivatives with respect to states in this augmented set of equilibrium conditions and solve
for the unknown coe¢ cients, which happen to be the derivatives of the value function and
decision rules that we need to get our higher-order approximations. This approach, which
we call equilibrium conditions perturbation (ECP), allows us to get, after n iterations, the
n-th-order approximation to the value function and to the decision rules.
A second path would be to take derivatives of the value function with respect to states
and controls and use those derivatives to …nd the unknown coe¢ cients. This approach, which
we call value function perturbation (VFP), delivers after (n + 1) steps, the (n + 1)-th-order

15

approximation to the value function and the n-th-order approximation to the decision rules.
This alternative may be more convenient when it is di¢ cult to eliminate levels or derivatives
of the value function from the equilibrium conditions or when the value function is smoother
than other equilibrium conditions.
3.1.1. Augmented Equilibrium Conditions
We derive now the set of augmented equilibrium conditions to implement the ECP approach.
The household’s problem is given by:
Vt =

max

ct ;lt ;kt+1 ;it

ct (1

1

lt )1

+

1
Et Vt+1

1

1

;

subject to the budget and resource constraint in (2) and (3), and the dynamics of the capital
stock in (4).15
We …nd the …rst-order conditions of the household with respect to consumption, labor,
capital, and investment:

2

qt = E t 4

t+1 rt+1

@Vt
= t;
@ct
@Vt
=
t wt ;
@lt
0

+ qt+1 @1

it+1
kt+1

@G

+

kt+1

@kt+1

13

A5 ;

and
it
kt

@G
t

where

t

= qt

kt

@it

;

is the Lagrangian multiplier associated with the resource constraint and qt is the

Lagrangian multiplier associated with the law of motion of capital. Using the last two …rstorder conditions and the fact that Mt+1 =

t+1 = t ,

the Euler equation for capital can be

written as:
it
kt

1

2

6
= Et 6
4

ct+1 (1 lt+1 )1
ct (1 lt )1

a2 rt+1 +

1

ct
ct+1
1

it+1
kt+1

1

15

1
Vt+1

1

1

1
Et Vt+1

+ a1 +

a2
1

it+1
kt+1

1

1

3

7
7:
5

We could reduce the number of controls by substituting out of the budget constraints. We choose not to
do so, since we will employ these …rst-order conditions in the solution method.

16

If

! 1, that is, if we do not have adjustment costs, this equation simpli…es to:
t+1

1 = Et

(rt+1 + 1

) ;

t

which is the standard equation without adjustment costs. Also, Tobin’s Q is de…ned as:
qt
t

1
=
a2

1

=
(e
which also equals 1 if

1

it
kt

1+ )

1

it
kt

1

;

! 1:

Using the …rst-order conditions with respect to consumption and leisure, we can relate
consumption, leisure, and the wage rate:
1

ct
1

lt

) kt zt1 lt :

= (1

Hence, the augmented set of equilibrium conditions to be used to solve for Vt ; it , kt+1 , ct ,
and lt is:

it
kt

1

Vt =
2

6
= Et 6
4

1

lt )1

ct (1

ct+1 (1 lt+1 )1
ct (1 lt )1

a2 rt+1 +
1

ct
ct+1

it+1
kt+1

lt

1

1
Et Vt+1

1
= (1

;
1

1
Vt+1

1

ct
1

1

1

1

1
Et Vt+1

+

+ a1 +

a2
1

) kt zt1 lt ;

it+1
kt+1

1

1

3

7
7;
5

ct + it = kt zt1 lt1 ;
and
kt+1 = (1

) kt + G

it
kt

kt ;

together with the law of motion for log zt .
After normalizing the set of equilibrium conditions as described in Appendix 8.3, we write
them in more compact notation:
F e
kt ; log zet ;
17

= 0;

where F is a 5-dimensional function (and where all the endogenous variables in the previous
equation are not represented explicitly because they are functions themselves of e
kt ; log zet and
) and 0 is the vectorial zero.

3.1.2. Approximating the Value Function and Decision Rules
The task now is to take successive derivatives of F and solve for the unknown coe¢ cients
of the Taylor expansions of the value function and decision rules that we presented before.
These unknown coe¢ cients appear in these derivatives because the augmented equilibrium
conditions are expressed in terms of the di¤erent variables and we need to di¤erentiate them
with respect to the states. Those are precisely the terms in the Taylor expansions.
The …rst step is to evaluate F at = 0. This gives us the steady-state values for Vess ; eiss ,
e
kss , e
css , and e
lss . This steady state is identical to the steady state of the stochastic neoclassical
growth model with a standard CRRA utility function.

Next, to …nd the …rst-order approximation to the value function and the decision rules,
kt ; log zet ;
and evaluate them
we take …rst derivatives of the function F with respect to e
kss ; log zess ; 0 that we just found to get
at the steady state e

kss ; log zess ; 0 = 0 for i 2 f1; 2; 3g :
Fi e

This step gives us 15 di¤erent derivatives (5 equilibrium conditions times the 3 variables of
F ). With this quadratic 15-equations system, we …nd the …rst-order derivatives of the value
function and of 4 decision rules (Vt ; it , kt+1 , ct , and lt ) evaluated at the steady state. We pick
the stable solution by checking the appropriate eigenvalue.
To …nd the second-order derivatives to the value function and decision rules, we derive Fi
with respect to e
kt ; log zet ;
and evaluate them at the steady state e
kss ; log zess ; 0 from the
…rst step to obtain:

Fij e
kss ; log zess ; 0 = 0 for i; j 2 f1; 2; 3g :

This gives us yet a new system of equations. Then, we plug in the terms that we already
know from the steady state and from the …rst-order approximation and we get that the only
unknowns left are the second-order terms of the value function and of the decision rules.
Quite conveniently, this system is linear and it can be solved quickly. Indeed, all the systems
resulting from derivatives higher than the …rst one will be linear in the unknown coe¢ cients.
Repeating these steps, we can get any arbitrary-order approximation to the value function
and decision rules. As mentioned above, in this paper we stop at order three.

18

3.1.3. Approximating Nominal Bonds Yields
To complete our computation, we also need to approximate the yield of nominal bonds. To
do so, we take advantage of our recursive bond price equation (5). First, de…ne:
sat = e
kt

e
kss ; log zet

log zess ; log

t

log ; ! t ; 1

which is the state vector in deviations with respect to the mean augmented with the di¤erence
of in‡ation with respect to its mean and the in‡ation innovation ! t (sa stands for states
augmented).
Then, in similar fashion to the value function and the decision rules, a third-order Taylor
approximation to the yields is:
kt ; log zet ; log
Rm e

1
1
' Rm;ss + Rm;i;ss sat + Rm;ij;ss sait sajt + Rm;ijl;ss sait sajt salt
2
6

t; !t; 1

for all m, in which we de…ne:
Rm;ss

kss ; log zess ; log ; 0; 0 ;
Rm;ss e

Rm;i e
kss ; log zess ; log ; 0; 0

Rm;i;ss

kss ; log zess ; log ; 0; 0
Rm;ij e

Rm;ij;ss
and:
Rm;ijl;ss

Rm;ijl e
kss ; log zess ; 0

for i = f1; 2; 3; 4; 5g ;

for i; j = f1; 2; 3; 4; 5g ;

for i; j; l = f1; 2; 3; 4; 5g :

Since in our data set we observe bond yields up to 20 quarters, we need to consider
Et Mt+1

1
t+1

1
Rt+1;t+m

=

1
Rt;t+m

;

for m 2 f1; : : : ; 20g. This set of 20 …rst-order conditions can also be written, in more compact

notation,

We can use Fe evaluated at

Fe e
kt ; log zet ; log

t; !t;

= 0:

= 0 and the steady-state value Vess ; eiss , e
kss , e
css , and e
lss found

above to …nd the steady-state values for Rt;t+j for m 2 f1; : : : ; 20g, log
two are, obviously, log

and 0.

t,

and ! t . These last

To …nd the …rst-order approximation to the nominal bond yields, we proceed as we did
for the perturbation of the value function and decision rules. We take …rst derivatives of
19

the function Fe with respect to e
kt ; log zet ; log

t; !t;

and evaluate them at the steady state

e
kss ; log zess ; log ; 0; 0 to get:

Fei e
kss ; log zess ; log ; 0; 0 = 0 for i 2 f1; 2; 3; 4; 5g :

After substituting for the steady-state values and the …rst-order derivatives of the value
function and decision rules that we found above, this step gives us a system of equations on
the …rst-order derivatives of the yields evaluated at the steady state.
To …nd the second-order approximation to the nominal bond yields, we take derivatives on Fei with respect to e
kt ; log zet ; log t ; ! t ;
and evaluate them at the steady state
e
kss ; log zess ; log ; 0; 0 to get

kss ; log zess ; log ; 0; 0 = 0 for i; j 2 f1; 2; 3; 4; 5g :
Feij e

We can use this linear system (together with the steady-state values, the …rst- and secondorder derivatives of the value function and decision rules, and the …rst-order derivatives of
the nominal bond yields that we found above) to …nd the second-order derivatives of the
yields evaluated at the steady state. Repeating these steps, we can get any arbitrary-order
approximation. We stop at order three because that would be enough to get the time-varying
risk premium that we are interested in.
3.2. Role of
Direct inspection of the derivatives that we presented before (since the expressions are inordinately long, we cannot include them in the paper) reveals that:
1. The constant terms Vss , varss , or Rm;ss do not depend on , the parameter that controls
risk aversion.
2. None of the terms in the …rst-order approximation, V:;ss , var:;ss , or Rm;:;ss (for all m)
depend on .
3. None of the terms in the second-order approximation, V::;ss , var::;ss , or Rm;::;ss depend
on ; except V33;ss , var33;ss , and Rm;33;ss (for all m). This last term is a constant that
captures precautionary behavior caused by the presence of productivity shocks.
4. In the third-order approximation only the terms of the form V33:;ss ; V3:3;ss , V:33;ss and
var33:;ss ; var3:3;ss , var:33;ss and Rm;33:;ss ; Rm;3:3;ss , Rm;:33;ss (for all m) that is, terms on
functions of

2

; depend on .
20

These observations tell us three important facts. First, a linear approximation to the
decision rules does not depend on the risk aversion parameter or on the variance level of the
productivity shock. In other words, it is certainty equivalent. Therefore, if we are interested
in recursive preferences, we need to go at least to a second-order approximation. Second,
given some …xed parameter values, the di¤erence between the second-order approximation to
the decision rules of a model with CRRA preferences and a model with recursive preferences
is just a constant. This constant generates a second, indirect e¤ect, because it changes
the ergodic distribution of the state variables and, hence, the points where we evaluate the
decision rules along the equilibrium path. In the third-order approximation, all of the terms
on functions of

2

depend on . Thus, we can use them to further identify the risk aversion

parameter, which is only weakly identi…ed in the second-order approximation as it shows
up only in one term and is not identi…ed at all in the …rst-order approximation. These
arguments also demonstrate how perturbation methods can provide analytic insights beyond
computational advantages and help in understanding the numerical results in Tallarini (2000),
who implements a recentering scheme that incorporates into the …rst-order approximation an
e¤ect similar to the second-order approximation constant.16

4. Estimation
Once we have our solution from the previous section, we use it to write a state-space representation of the dynamics of the states and observables that will allow us to evaluate the
likelihood function of the model. For this last step, and since our solution is inherently
non-linear (remember that the risk aversion parameter only a¤ects the second- and thirdorder coe¢ cients of the approximation), we will rely on the particle …lter as described in
Fernández-Villaverde and Rubio-Ramírez (2007).
4.1. State-Space Representation
As econometricians, we will observe per capita consumption growth, per capita output growth,
the 1-, 2-, 3-, 4-, and 5-year nominal bond yields, and in‡ation. Per capita consumption
growth and per capita output growth will provide macro information. The price of the
16

This characterization is also crucial because it is plausible to entertain the idea that the richer structure
of Epstein and Zin preferences is not identi…ed (as in the example built by Kocherlakota, 1990). Fortunately,
the second- and third-order terms allow us to learn from the observations. This is not a surprise, though,
as it con…rms previous, although somehow more limited, theoretical results. In a simpler environment, when
output growth follows a Markov process, Wang (1993) shows that the preference parameters of Epstein and
Zin preferences are generically recoverable from the price of equity or from the price of bonds. Furthermore,
equity and bond prices are generically unique and smooth with respect to parameters.

21

nominal bonds provides us with …nancial data. Later, we will …nd that including …nance
data is key for the success of our empirical strategy.
Since our DSGE model has only two sources of uncertainty, the productivity shock and
the in‡ation shock, we need to introduce measurement error to avoid stochastic singularity.
It is common to have measurement error in term structure models. The justi…cation comes
from the idea that we do not observe zero coupon bonds. Instead, we observe the market
prices of bonds with coupons and we need some procedure to back out the zero coupon
bonds. This procedure induces measurement error. Similarly, National Income and Product
Accounts (NIPA) can only provide researchers with an approximated estimate of output and
consumption. Therefore, we will assume that all the variables (except in‡ation) are observed
subject to a measurement error.17
It is easier to express the solution of our model in terms of deviations from steady state.
Thus, for any variable vart , we let vd
art = vart

varss . Also, we introduce a constant to keep

track of means. Then, the law of motion for the states is
1

0

1 0
b
e
ki;ss sit + 12 kij;ss sit sjt + 16 kijl;ss sit sjt slt
k t+1
B
C B
B log
B
zet+1 C
B \
C B " "zt+1
B \ C B [
B log t+1 C = B log t + ( ! ! t+1 + 0 " "zt+1 ) +
B
C B
B !
C B !
@ t+1
A @ t+1
1
1

! !t

+

[

et
1 " log z

C
C
C
C
C:
C
C
A

Since our observables are
Yt = ( log ct ;
we need to map

log yt ; Rt;t+4 ; Rt;t+8 ; Rt;t+12 ; Rt;t+16 ; Rt;t+20 ; log

log ct and

1

0

log yt into the model-scaled variables e
ct and e
ct

yet 1 . We start with consumption. We observe that
that ct = e
ct zt

t)

log ct = log ct

by our de…nition of re-scaled variables. Thus:
log ct = log ct
log e
ct + log xt
log e
ct

log e
ct

log e
ct
1

+

log ct
1

1

1

1

and yet and

and we have

=

+ log xt

+

log ct

;

1

=

z "zt 1 :

17
Our exogenous process for in‡ation already has a linear additive innovation ! t+1 , which will make an
additional measurement error di¢ cult to identify.

22

And since b
e
ct = e
ct
Equivalently,

e
css ; we can write

log ct = log b
e
ct + e
css

log b
e
ct

log yt = log b
yet + yess

log b
yet

1

1

\
+e
css + log
zet 1 + :
\
+ yess + log
zet 1 + :

Hence, in order to simplify our state-space representation, it is convenient to consider
b
\
e
ct 1 ; b
yet 1 ; log
zet 1 as additional (pseudo-)state variables. It is also the case that we need

to map log
log

t

t

into our states. Since the law of motion of in‡ation is
log

= (log

t 1

log ) + (

\
we need to also consider log
t 1; !t

1

! !t

+

0 " "zt )

+ (

! !t 1

+

1 " "zt 1 ) ;

as additional (pseudo-)state variables. We use the

notation St to refer to the vector of augmented state variables.
Once this is done, our state-space representation can be written as a transition equation

St+1

0 b
e
k t+1
B
\
B log
zet+1
B
B \
B log t+1
B
B ! t+1
B
B
B 1
=B
B b
ct
B e
B b
B yet
B
B log
[
zet
B
B [
@ log t
!t

1

0

C B
C B
C B
C B
C B
C B
C B
C B
C B
C B
C=B
C B
C B
C B
C B
C B
C B
C B
C B
A B
@

ki;ss sit

+

1
k si sj
2 ij;ss t t

+

1

1
k
si sj sl
6 ijl;ss t t t

" "zt+1

[t + (
log

! ! t+1

+

0 " "zt+1 )

+

! t+1
1
ci;ss sit + 21 cij;ss cit cjt + 16 cijl;ss cit cjt clt
yi;ss sit + 21 yij;ss sit sjt + 16 yijl;ss sit sjt slt
[
log
zet
[t
log
!t

23

! !t

+

[

et
1 " log z

C
C
C
C
C
C
C
C
C
C
C;
C
C
C
C
C
C
C
C
C
A

and a measurement equation
0

B
B
B
B
B
B
B
B
Yt = B
B
B
B
B
B
B
@
where

log b
e
ct

log e
css + ci;ss sit + 21 cij;ss cit cjt + 61 cijl;ss cit cjt clt

1

log b
yet

log yess + yi;ss sit + 21 yij;ss sit sjt + 16 yijl;ss sit sjt slt

\
+e
css + log
zet 1 +

\
ess + log
zet 1 +
1+y

i
R4;ss + Ri;4;ss
sat + 12 Rij;4;ss sait sajt + 61 Rijl;4;ss sait sajt salt

i
sat + 12 Rij;8;ss sait sajt + 61 Rijl;8;ss sait sajt salt
R8;ss + Ri;8;ss
i
R12;ss + Ri;12;ss
sat + 12 Rij;12;ss sait sajt + 16 Rijl;12;ss sait sajt salt
i
R16;ss + Ri;16;ss
sat + 12 Rij;16;ss sait sajt + 16 Rijl;16;ss sait sajt salt
i
R20;ss + Ri;20;ss
sat + 12 Rij;20;ss sait sajt + 16 Rijl;20;ss sait sajt salt

\
log + log
t 1+

1 1;t

2 2;t

[+

et
0 log z
3 3;t

error vector. We assume that

i;t

! !t 1

+

\

et 1
1 " log z

4 4;t
5 5;t
6 6;t
7 7;t 0
N (0; 1) for all i 2 f1; : : : ; 7g and

0
i;t

1 0
C B
C B
C B
C B
C B
C B
C B
C B
C+B
C B
C B
C B
C B
C B
C @
A

is the measurement
?

j;t

for i 6= j and

i; j 2 f1; : : : ; 7g. The eighth element, the one corresponding to in‡ation, is missing since we
assume no measurement error for in‡ation.

If we de…ne Wt+1 = ("zt+1 ; ! t+1 )0 and Vt =

1;t

2;t

3;t

4;t

5;t

6;t

7;t

0

; we

can write our transition and measurement equations more compactly as
St+1 = h (St ; Wt+1 ) ;

(7)

Yt = g (St ; Vt ) :

(8)

and

4.2. Likelihood
We stack the set of structural parameters in our model in the vector:
=( ; ; ; ; ; ; ; ;
The likelihood function L YT ;

values, where Yt =

fYs gts=1

0;

;

1;

";

!;

1

;

2

;

3

;

4

;

5

;

6

;

7

)0 :

is the probability of the observations given some parameter

for t 2 f1; : : : ; T g is the history of observations up to time t.

Unfortunately, this likelihood is di¢ cult to evaluate since we do not even have an analytic
expression for our state-space representation. We tackle this problem by using a sequential

24

1 1;t
2 2;t
3 3;t
4 4;t
5 5;t
6 6;t
7 7;t
! !t

1

C
C
C
C
C
C
C
C;
C
C
C
C
C
C
A

Monte Carlo.18 First, we factorize the likelihood into its conditional components:
T

L Y ;

=

T
Y
t=1

L Yt jYt 1 ;

;

where L (Y1 jY0 ; ) = L (Y1 ; ) : Then, we condition on the states and integrate with respect

to them to get
t 1

L Yt jY

;

=

Z Z Z

L Yt jW1t ; W2t 1 ; S0 ;

p W1t ; W2t 1 ; S0 jYt 1 ;

(9)

for t 2 f2; : : : ; T g where W1;t = "zt , W2;t = ! t , Wit =
and

L (Y1 ; ) =

dW1t dW2t 1 dS0 ,

Z Z Z

fWi;s gts=1

L Y1 jW11 ; S0 ;

for i = 1; 2 and t 2 f1; : : : ; T g ;

p W11 ; S0 ;

dW11 dS0 :

(10)

These expressions illustrate how the knowledge of p (W11 ; S0 ; ) and of the sequence
p W1t ; W2t 1 ; S0 jYt 1 ;

T
t=2

(11)

;

is crucial for our procedure. If we know W1t ; W2t 1 ; S0 , computing L Yt jW1t ; W2t 1 ; S0 ;

is relatively easy; it is a change of variables from W2;t and Vt to Yt : The same is true for

L (Y1 jW11 ; S0 ; ) if we know (W11 ; S0 ). However, given our model, we cannot characterize

neither p (W11 ; S0 ; ) nor the sequence (11) analytically. Even if we could, these two previous
computations still leave open the issue of how to solve for the integrals in (9) and (10).
A common solution to these problems is to substitute p (W11 ; S0 ; ) and
p W1t ; W2t 1 ; S0 jYt 1 ;

T
t=2

;

by an empirical distribution of draws from them. If we have such draws, we can approximate
the likelihood using
L Yt jYt 1 ;

'

N
1 X
L Yt jw1t;i ; w2t
N i=1

18

1;i

; si0 ;

;

This is not the only possible algorithm to do so, although it is a procedure that we have found useful
in previous work. Alternatives include DeJong et al. (2007), Kim, Shephard, and Chib (1998), Fiorentini,
Sentana, and Shephard (2004), and Fermanian and Salanié (2004).

25

where w1t;i ; w2t

1;i

; si0 is the draw i from p W1t ; W2t 1 ; S0 jYt 1 ;
L (Y1 ; ) '

and

N
1 X
L Y1 jw11;i ; si0 ;
N i=1

;

where w11;i ; si0 is the draw i from p (W11 ; S0 ; ).
Del Moral and Jacod (2002) and Künsch (2005) provide weak conditions under which the
right-hand side of the previous equation is a consistent estimator of L YT ;

and a central

limit theorem applies. A law of large numbers will ensure that the approximation error goes
to 0 as the number of draws, N , grows.
Drawing from p (W11 ; S0 ; ) is straightforward in our model. Given parameter values, we
solve the model and simulate from the ergodic distribution of states. Santos and PeraltaAlva (2005) show that this procedure delivers the empirical distribution of w11;i ; si0 that
we require. Drawing from

p W1t ; W2t 1 ; S0 jYt 1 ;

T
t=2

is more challenging. A popular

approach to do so is to apply the particle …lter (see Fernández-Villaverde and Rubio-Ramírez,
2007, for a more detailed explanation and references).
The basic idea of the …lter is to generate draws through sequential importance resampling (SIR), which extends importance sampling to a sequential environment. The following
proposition, formulated by Rubin (1998), formalizes the idea:
Proposition 1. Let w1t;i ; w2t
sequence

N

w
e1t;i ; w
e2t 1;i ; sei0 i=1

1;i

; si0

N
i=1

be a draw from p W1t ; W2t 1 ; S0 jYt 1 ;

N
w1t;i ; w2t 1;i ; si0 i=1

be a draw with replacement from

. Let the
where the

resampling probability is given by

Then w
e1t;i ; w
e2t

1;i

; sei0

N
i=1

L Yt jw1t;i ; w2t 1;i ; si0 ;
qti = PN
t;i
t 1;i i
; s0 ;
i=1 L Yt jw1 ; w2

is a draw from p W1t ; W2t 1 ; S0 jYt ;

:

.

Proposition 1, a direct application of Bayes’theorem, shows how we can take a draw from

p W1t ; W2t 1 ; S0 jYt 1 ;

to get a draw from p W1t ; W2t 1 ; S0 jYt ;

by building importance

weights depending on Yt . This result is crucial because it allows us to incorporate the information in Yt to change our current estimate of W1t ; W2t 1 ; S0 . Thanks to SIR, the Monte Carlo
method achieves su¢ cient accuracy in a reasonable amount of time. A naïve Monte Carlo,
n
oT
N
in comparison, would just draw simultaneously a whole sequence of
w1t;i ; w2t 1;i ; si0 i=1
t=1

without resampling. Unfortunately, this naïve scheme diverges because all the sequences become arbitrarily far away from the true sequence of states, which is a zero measure set. Then,
the sequence of simulated states that is closer to the true state in probability dominates all
26

the remaining ones in weight. Simple simulations show that the degeneracy appears even
after very few steps.
Given w
e1t;i ; w
e2t

1;i

for states to generate

N

from p W1t ; W2t 1 ; S0 jYt ; , we can
i=1
N
w1t+1;i ; w2t;i ; si0 i=1 from p W1t+1 ; W2t ; S0 jYt ;

; sei0

apply the law of motion
. This transition step

puts us back at the beginning of proposition 1, but with the di¤erence that we have moved
forward one period in our conditioning, from tjt

1 to t + 1jt.

4.3. Estimation Algorithms
Our paper emphasizes the likelihood-based estimation of DSGE models. In the interest of
space, we will show results for maximum likelihood and comment brie‡y on how to …nd results
for Bayesian estimation. Obtaining the maximum likelihood point estimate is complicated
because the shape of the likelihood function is rugged and multimodal. Moreover, the particle
…lter generates an approximation to the likelihood that is not di¤erentiable with respect to
the parameters, precluding the use of optimization algorithms based on derivatives. To circumvent these problems, our optimization routine is a procedure known as covariance matrix
adaptation evolutionary strategy, or CMA-ES (Hansen, Müller, and Koumoutsakos, 2003,
and Andreasen, 2007). The CMA-ES is one of the most powerful evolutionary algorithms for
real-valued optimization and has been applied successfully to many problems.
The CMA-ES approximates the inverse of the Hessian of the log-likelihood function by
simulation. In each step of the algorithm, we simulate m candidate parameter values from
the weighted mean and estimated variance-covariance matrix of the best candidate parameter
values of the previous step. By selecting the best parameter values in each step and by adapting the variance-covariance matrix to the contour of the likelihood function, we direct the
simulation toward the global maximum of our objective function. Thanks to the estimation
of the variance-covariance matrix from the simulation, we by-pass the need of computing any
derivative. Andreasen (2007) documents the robust performance of CMA-ES and compares
it favorably with more common approaches as simulated annealing.
To reduce the “chatter” of the problem, we keep the innovations in the particle …lter
(that is, the draws from the exogenous shock distributions and the resampling probabilities)
constant across di¤erent passes of the algorithm. As pointed out by McFadden (1989) and
Pakes and Pollard (1989), this is required to achieve stochastic equicontinuity.
The standard errors reported below come from the bootstrapping procedure described by
Efron and Tibshirani (1993, chapter 6). The estimated model is used to generate 100 arti…cial
samples of data. These arti…cial series are used to re-estimate the model 100 times and the
standard errors get computed as the standard deviations of the MLE taken across these 100
replications. This bootstrapping procedure accounts for the …nite-sample properties of the
27

MLE and avoids the numerical instabilities that often appear while inverting the matrix of
second derivatives of a likelihood function. These instabilities would be even more acute in
our case since we are obtaining a non-di¤erentiable approximation of the likelihood function.
With respect to Bayesian inference, the posterior of the model:
p

jYT / R

L YT ;
p( )
;
T
L (Y ; ) p ( ) d

is di¢ cult, if not impossible, to characterize because the likelihood itself is only approximated
by simulation. However, once we have an estimate of L YT ;

thanks to the particle …lter,

we can draw from the posterior and build its empirical counterpart by using a MetropolisHastings algorithm. As mentioned before, we omit details to keep the paper focused.

5. Data and Main Results
5.1. Data
We take as our sample the period 1953.Q1 to 2008.Q4. Our output and consumption data
come from the Bureau of Economic Analysis NIPA. We de…ne nominal consumption as the
sum of personal consumption expenditures on non-durable goods and services. We de…ne
nominal gross investment as the sum of personal consumption expenditures on durable goods,
private non-residential …xed investment, and private residential …xed investment. Per capita
nominal output and consumption are de…ned as the ratio between our nominal output and
consumption series and the civilian non-institutional population over 16. For in‡ation, we
use the gross domestic product de‡ator. The data on bond yields are from CRSP Fama-Bliss
discount bond …les, which have fully taxable, non-callable, non-‡ower bonds. Fama and Bliss
construct their data by interpolating observations from traded Treasuries. This procedure
introduces measurement error, possibly correlated across time and cross-sectionally (although
in our estimation, and just to reduce the number of parameters to maximize over, we do not
allow for these correlations).
5.2. Summary Statistics
Table 1 reports the summary statistics from our data. Key observations are as follows. First,
the volatility of output growth is higher than the volatility of consumption growth. Second,
the yield curve is, on average, upward sloping. This points to a positive nominal bond risk
premium. Third, the volatilities of bond yields are downward sloping for maturities of one
year and longer. These are well-known facts and we will study how the model scores along

28

Mean
St.dev.
25%
50%
75%

Cons. gr.

Output gr.

1Y

2Y

Yields
3Y

4Y

5Y

In‡.

Hours

2.06%
1.96%
0.98%
2.11%
3.25%

1.67%
3.74%
-0.22%
1.84%
3.82%

5.56%
2.91%
3.42%
5.36%
7.15%

5.76%
2.87%
3.63%
5.45%
7.31%

5.93%
2.80%
3.84%
5.59%
7.44%

6.06%
2.76%
3.99%
5.65%
7.57%

6.15%
2.72%
4.03%
5.71%
7.67%

3.43%
2.33%
1.79%
2.76%
4.46%

49.99%
1.12%
49.36%
49.99%
50.79%

Table 1: The table reports the summary statistics of consumption growth, output growth,
bond yields, in‡ation, and hours worked. All statistics are expressed in annual terms. The
sample period is 1953.Q1 to 2008.Q4.
these dimensions. Also, we do not include hours per capita in our observables because our
model is not capable of generating enough ‡uctuations in hours. In any case, we want to
put some restrictions on the behavior of the model-based hours. For this reason, we build
a series of hours worked per capita using the index of total number of hours worked in the
business sector and the civilian non-institutional population between 16 and 65. We normalize
hours worked to have mean 0.5 during the sample period (this normalization level is per se
irrelevant) and make

a function of the rest of the parameters such that, in steady state,

hours worked in our model are always 0.5 for any value of the rest of the parameters.

5.3. Estimation Results
In this section, we report the parameter estimates and assess the extent to which the model
can match the properties of the macro and yield data. To fully understand how the parameters
are identi…ed in our model, we estimate the model in three steps. First, we estimate the model
using all data. Second, we estimate the model excluding in‡ation. Third, we use only bond
yields. By studying which parameters change by changing information sets, we improve our
understanding of which moments pin down which parameters.
Before proceeding, we …x a subset of the parameters. We do this because estimating a
third-order approximation model, which as we argued before is important for identi…cation, is
extremely time consuming. Time constraints make it unfeasible, in practice, to estimate the
whole set of parameters. Thus, in addition to the calibrated in‡ation parameters described
above, we set

= 0:045;

= 0:3; and

= 0:0294. The value of

is chosen to match the

average growth rate of per capita output that we have in our sample. The values of and are
quite standard in the literature. Finally, we set the standard deviation of the measurement
error shocks such that the model explains 75 percent of the standard deviation observed in
the data.
29

5.3.1. Data set I: Consumption, output, bond yields, and in‡ation
We report our …rst …ndings in table 2. The table displays estimates of the parameters of
the model. In the …rst column, we list our estimated parameters. In the second and third
columns, we report the estimates and standard errors if we use consumption growth, output
growth, …ve bond yields, and in‡ation in estimation. The fourth and …fth column report the
results if we exclude in‡ation from the estimation. The last two columns contain the results
if only the …ve bond yields are used in estimation.

Data

0

1
"
!

Cons. gr., Output gr., Yields, In‡ation

Cons. gr., Output gr., Yields

Yields

MLE

Std.Error

MLE

Std. Error

MLE

Std.Error

0.994
79.34
1.731
0.032
-0.053
-0.522
-0.046
0.008
0.002

0.0001
12.234
0.2124
0.0061
0.0088
0.1018
0.0093
0.0009
0.0002

0.994
88.23
2.087
0.063
-0.012
-0.174
0.235
0.008
0.003

0.0001
10.157
0.2348
0.0071
0.0045
0.0598
0.124
0.0008
0.0003

0.994
96.75
1.775
0.026
-0.055
-0.175
0.102
0.008
0.003

0.0002
20.125
0.4614
0.0125
0.0124
0.0625
0.0897
0.0012
0.0005

Table 2: Point Estimates and Standard Errors
We discuss now the result for the whole data set, which we take as our benchmark case, and
explore the other columns in the subsections below. We start with the preference parameters.
We estimate the discount factor, , to be 0.994. This value, a relatively standard result in the
literature, allows us to match the nominal yield level (remember that we have both in‡ation
and long-run growth and that both factors a¤ect the nominal yield level). The coe¢ cient
that controls risk aversion, , is estimated to be around 79, which is rather high.19
We estimate the IES to be 1.73. An estimate higher than one resonates with the parameter values picked in the long-run risks literature (for instance, Bansal and Yaron, 2004).
Therefore, we …nd little support for the notion that the IES is around one, an assumption
that is commonly used for convenience, as Campbell (1993), Tallarini (2000), and others do.
This is not a surprise, because a value of the IES equal to one implies that the consumptionwealth ratio is constant over time. As we mentioned in the introduction, checking for this
19

We need to be careful assessing this number, since our model includes leisure. It happens that, given the
Cobb-Douglas speci…cation of our utility aggregator of consumption and leisure, relative risk aversion is equal
to . This does not need to be the case with other aggregators of consumption and leisure. See Swanson
(2009) for a careful investigation.

30

implication of the model is hard because wealth is not directly observable, since it includes
human wealth. However, di¤erent attempts at measurement, such as Lettau and Ludvigson
(2001) or Lustig, van Nieuwerburgh, and Verdelhan (2007), reject the hypothesis that the
ratio of consumption to wealth is constant.
The combination of the estimated values for the parameters controlling risk aversion and
IES suggests:
1
1

1

=

1
1

79:34
1
1:731

=

185:51

indicating very di¤erent attitudes toward intertemporal substitution and toward substitution
1

across states of nature. Moreover, since in our point estimate we have that

, our

representative household has a very strong preference for an early resolution of uncertainty.
Finally, the high value of the IES, higher than one, generates, according to the formula we
derived in section 3.1, a trivial welfare cost of aggregate ‡uctuactions.
The adjustment cost parameter, , is estimated to be 0:032, which indicates substantial
adjustment costs. This estimate comes about because our data favor a situation in which
capital cannot adjust easily to smooth consumption. When this is the case, the SDF ‡uctuates
more and it is easier to match both premium and volatility of the yield curve. The volatility
of the technology process,

",

is 0:00756. This number is similar to many estimates in the

literature and allows us to nicely match output and consumption volatility. Since the …rstorder approximation of our model behaves in the same way as the one from a simple real
business cycle model, and this one is also able to match output and consumption properties,
this …nding is not a surprise.
The parameter controlling the MA component of the in‡ation process, , is well into negative terms,

0:522, and close to the value reported by Stock and Watson (2007), allowing

us to capture the negative …rst-order autocorrelation and the small higher-order autocorrelations of in‡ation growth observed in the data.20 Since the nominal yield curve slopes up in
the data,

0

and

1

are estimated such that the correlation between innovations to in‡ation

expectations and innovation to the stochastic discount factor expectations implies that in‡ation is bad news for consumption growth; that is, such that (

0

+

1)

"

is negative (see,

for a similar reason, Piazzesi and Schneider, 2006). The problem is that observed in‡ation
volatility imposes a constraint on the maximum for the absolute value of (
!

0

+

1)

"

and

(estimated to be 0.00201) and, hence, while we can match in‡ation volatility the model is

barely able to generate an upward-sloping term structure. We will come back to this point
20

Stock and Watson (2007) split their sample into two groups: 1960:Q1 to 1983:Q4 and 1984:Q1 to 2004:Q4.
Their estimated values for are lower (in absolute value) for the …rst group and higher for the second. Our
sample period 1953.Q1 to 2008.Q4 includes their two groups and, as expected, our estimate is right in the
middle of their two estimates.

31

momentarily.
Table 3 displays means (panel A) and volatilities (panel B) of consumption growth, output
growth, …ve bond yields, and in‡ation. In each panel, the …rst row displays the sample
moments in the data. The second row corresponds to the estimates of the model for which
we use all available data. The third row uses the estimates based on consumption growth,
output growth, and …ve bond yields, but omit in‡ation data in estimation. The last row uses
the estimates that we obtain using only bond yields in estimation. As before, the sample
period for the data is 1953.Q1 to 2008.Q4.
Table 3 tells us that the model that uses all the data does a fair job at matching the mean of
consumption growth and the average level of the yields. However, it has a bit more problems
with output growth and with the average slope of the yields. The di¤erence between the
5-year and 1-year yields amounts to 59 basis points in the data, whereas our model produces
an average yield spread of only 17 basis points. Beeler and Campbell (2009) and Koijen et
al. (2010) show that it is also a challenge to generate realistic nominal bond risk premia
in standard long-run risks models. Furthermore, our estimated model does reasonably well
with in‡ation volatility, but underestimates the volatility of bond yields by about a factor
of two. Hence, our model has a di¢ cult time jointly reproducing the salient features of the
term structure of nominal interest rates and in‡ation.
Table 4 has a structure similar to table 3, but reports the autocorrelation of consumption
growth (panel A), the 1-year bond yield (panel B), and in‡ation (panel C) for lag lengths
varying from one quarter to ten quarters. The model is able to generate the autocorrelation
patterns remarkably well. This is an advantage of a likelihood-based method, which tries
to match the whole set of moments of the data, including the autocorrelations, instead of
focusing on a limited set of moments, as the GMM.
5.3.2. Data set II: Consumption, output, and bond yields
To gain further insight into why the model does not generate a substantial bond risk premium
and volatility of bond yields, we re-estimate our model using only parts of the data. We …rst
omit the observations on in‡ation. In column 4 of table 2, we see how omitting in‡ation
leads to an increase in the risk aversion coe¢ cient and changes the estimates of the in‡ation
parameters. This is because these parameters are no longer disciplined by observed in‡ation.
In particular,

0

and

1

are estimated such that the absolute value of (

0

+

1)

"

and

!

are twice as big. Table 3 shows that this leads to a dramatic improvement in terms of the
bond risk premium and volatilities of bond yields. The model now replicates the observed
bond risk premium and bond yield volatilities, at least for shorter maturities. This success
of the model is accomplished at a cost. We now overestimate the volatility of in‡ation; it
32

Panel A: Means

Observed Data
All data
All data, but no in‡ation
Yields

Cons. gr.

Output gr.

1Y

2Y

Yields
3Y

4Y

5Y

In‡ation

2.06%
2.12%
2.12%
2.12%

1.67%
2.11%
2.11%
2.10%

5.56%
5.92%
5.63%
5.54%

5.76%
5.98%
5.78%
5.74%

5.93%
6.05%
5.92%
5.90%

6.06%
6.09%
6.04%
5.99%

6.15%
6.09%
6.13%
6.11%

3.43%
3.67%
3.65%
3.68%

Panel B: Volatilities

Data
All data
All data, but no in‡ation
Yields

Cons. gr.

Output gr.

1Y

2Y

Yields
3Y

4Y

5Y

In‡ation

1.96%
2.40%
2.54%
2.32%

3.74%
2.91%
2.95%
2.85%

2.91%
1.79%
3.28%
3.31%

2.87%
1.64%
3.00%
3.03%

2.80%
1.50%
2.75%
2.78%

2.76%
1.38%
2.53%
2.56%

2.72%
1.28%
2.33%
2.46%

2.33%
2.32%
3.75%
3.79%

Table 3: Means (Panel A) and volatilities (Panel B) of consumption growth, output growth,
…ve bond yields, and in‡ation.
is 2.33% in the data, and the estimates imply a volatility of in‡ation of 3.75%. Omitting
in‡ation data is inconsequential for matching the autocorrelation patterns in consumption
growth, the 1-year bond yield, and in‡ation in table 4.
This exercise illustrates the importance of a joint estimation of in‡ation and structural
parameters. Without the constraint of having to jointly match in‡ation and the yield curve,
the model is su¢ ciently ‡exible to capture selected aspects of the data. This is an excellent
example of how simple calibration exercises, by focusing on a set of moments selected by the
researcher without tight discipline, are fraught with peril.
5.3.3. Data set III: Bond yields
As a last exercise, we estimate the model parameters using only information contained in bond
yields. Perhaps surprisingly, the model estimates and their implications for the term structure
are roughly una¤ected if we omit consumption growth and output growth in estimation. The
risk aversion parameter increases even further, to 96.75, and the adjustment cost falls to
0.026. The slope of the average nominal yield curve increases slightly We read this result as
indicating that yield data (slopes and volatilities) carry a large amount of information about
structural parameters of the economy, including the discount factor, risk aversion and the IES.
This emphasizes the potentiality of incorporating …nance data into the standard estimation
33

Panel A: Consumption growth
Lag length
Data
All data
All data, but no in‡ation
Yields

1Q
0.32
0.37
0.38
0.48

2Q
0.17
0.04
0.03
0.07

3Q
0.18
-0.01
-0.01
-0.01

4Q
0.09
-0.07
-0.07
-0.08

5Q
-0.03
-0.05
-0.06
-0.06

6Q
0.06
-0.07
-0.07
-0.07

7Q
0.01
-0.03
-0.03
-0.05

8Q
-0.151
-0.05
-0.05
-0.06

9Q
-0.04
-0.06
-0.06
-0.06

10Q
0.01
0.02
0.02
0.01

6Q
0.71
0.75
0.75
0.74

7Q
0.67
0.7
0.71
0.69

8Q
0.64
0.66
0.67
0.65

9Q
0.61
0.62
0.63
0.61

10Q
0.58
0.58
0.6
0.57

6Q
0.67
0.63
0.73
0.73

7Q
0.61
0.58
0.68
0.68

8Q
0.59
0.54
0.64
0.64

9Q
0.55
0.50
0.60
0.60

10Q
0.54
0.47
0.56
0.56

Panel B: 1-year bond yield
Lag length
Data
All data
All data, but no in‡ation
Yields

1Q
0.95
0.96
0.97
0.95

2Q
0.91
0.92
0.93
0.91

3Q
0.87
0.88
0.89
0.87

4Q
0.81
0.83
0.83
0.82

5Q
0.76
0.79
0.8
0.78

Panel C: In‡ation
Lag length
Data
All data
All data, but no in‡ation
Yields

1Q
0.88
0.80
0.94
0.94

2Q
0.83
0.77
0.90
0.90

3Q
0.8
0.74
0.86
0.86

4Q
0.77
0.69
0.81
0.81

5Q
0.71
0.66
0.77
0.77

Table 4: Autocorrelation of consumption growth (Panel A), the 1-year bond yield (Panel B),
and in‡ation (Panel C).

34

of DSGE models as a key additional source of information. Also, this result con…rms Hall’s
(1988) intuition that a cross-section of asset yields is highly informative about the values of
preference parameters.

6. Extensions
Despite some empirical shortcomings, our previous estimation has shown that a rich DSGE
model with production and recursive preferences can be successfully taken to the data. Thus,
we have opened the door to a large number of potential extensions. We discuss several that
can be solved using our estimation procedure and that we believe might improve the …t of
the model to the data. We leave them, though, for future work, since they will complicate
the current paper, already a lengthy piece with much new content to digest.
Predictable technology growth We assume technology growth is i.i.d., which might
be too restrictive. We can extend the model to feature a predictable component in technology
growth. Such a model is analyzed, for instance, in Croce (2006) and relates to the long-run
risk literature (Bansal and Yaron, 2004, Hansen, Heaton, and Li, 2008, and Kaltenbrunner
and Lochstoer, 2008).
Internal habit formation In our speci…cation of recursive preferences, the period
utility is of the CRRA type. We can enrich the model to allow for habit formation in the
period utility. Habit formation preferences have been successfully applied in asset pricing by
for instance Constantinides (1990) and Campbell and Cochrane (1999).
Taylor rule We assumed an exogenous process for in‡ation. A natural extension of our
model is to endogenize the price process using, for instance, a Taylor rule as in Ang, Dong,
and Piazzesi (2007). In this case, we not only embed the Taylor rule in an arbitrage-free
term structure model, but also impose all equilibrium restrictions implied by an otherwise
standard DSGE model.
Variable rare disasters Gabaix (2009) shows that variable rare disasters might be a
fruitful way to think about asset pricing in a production economy. Gabaix constructs a model
in which the real business cycle properties of the model are una¤ected relative to a standard
model without rare disaster, but the asset pricing properties are improved substantially. We
can enrich our model and estimate such models as well. This extension, however, would
depend on our ability to have a perturbation method that can properly capture the e¤ect of
large, yet rare shocks.
35

7. Conclusions
We have studied the term structure of interest rates in a DSGE model in which the representative agent household has Epstein and Zin preferences. We have estimated the model by
maximum likelihood using a solution method that perturbs the value function. Our estimation procedure, thus, imposes all economic restrictions implied by the equilibrium model.
Our paper has methodological and substantive contributions. Methodologically, we have
shown how such a rich model can be accurately computed and estimated, thanks to the
combination of perturbation methods and the particle …lter. This leads the way for a large set
of future applications. Our substantive …ndings are that the data indicate large levels of risk
aversion, high levels of the IES, and high adjustment costs. The cross-equation restrictions
imposed by the equilibrium of the model, in particular by the presence of endogenous physical
capital accumulation, limits the ability of the model to jointly account for the slope of the
nominal yield curve and the associated volatilities. However, we have pointed out a number
of potential avenues of improvement that may solve this problem. All those can be explored
for the …rst time in the context of a likelihood-estimated DSGE models that can move toward
the integration of macro and …nance observations with the tools that we have provided in
this paper.

36

8. Appendices
In the next four appendices, we o¤er some further technical details about several parts of the
paper. First, we show how to derive the SDF of the model. Second, we show that the value
function representing the social planner’s problem formulation of our model is homogeneous of
degree . With these two results, in the third appendix, we write a stationary representation of
the model. The last appendix explains how we maximize the resulting loglikelihood function.
8.1. Derivation of the SDF
First, to economize on notation, we rewrite the household’s preferences as
1

ct (1

Ut =
where

1

1

lt )

+

1
1

1
Et Ut+1

1

= 1= .

In the optimum, the household holds any asset with price pt and payo¤ xt+1 such that:
@
Ut (ct
@

pt ; ct+1 + xt+1 )

= 0:
=0

Thus:
@
Ut (ct
@ct

1
pt ; ct+1 + xt+1 ) pt = Ut Et Ut+1

1
1

1

t

E Ut+1

@Ut+1
xt+1 :
@ct+1

For the left-hand side, we have:
@
Ut (ct
@ct
ct (1

lt )1

1

+

1
Et Ut+1

= Ut ct (1

pt ; ct+1 + xt+1 ) =
1
1

1

ct (1
lt )1

1

lt )1

ct

1

(1

lt )1

ct 1 :

The optimality condition therefore implies (and switching from Ut to Vt to respect our
notational convention):
1

=

1
Et Vt+1

1

t

ct (1 lt )1
c t 1 pt
h
E Vt+1 Vt+1 ct+1 (1 lt+1 )1

37

1

1
ct+1
xt+1

i

and hence:
1
Et Vt+1

pt =
2

ct+1 (1

= Et 4

ct (1

1

t

h
Et Vt+1

1

lt+1 )1

ct+1 (1

1
ct+1
xt+1

1

ct (1 lt )1
ct 1
0
!1
1
lt+1 )1
ct+1
Vt+1
@
1
1
1
ct
lt )
Et Vt+1

1
1

1

i

3

A

xt+1 5 :

The SDF is therefore given by:
ct+1 (1

Mt+1 =

ct (1
ct+1 (1

=

ct (1

!1

1

lt+1 )
1

lt )

!1

1

lt+1 )
1

lt )

0

1
Et Vt+1

ct @

1
Et Vt+1

ct+1

0

1

1

11

Vt+1

ct @

ct+1

1

1

1
Vt+1
1

A

1

A

which is the formula that we use in the main body of the paper.
8.2. Homotheticity
Taking advantage of the fact that the welfare theorems hold in our model, its equilibrium
can be characterized by the solution of the social planner’s problem:
V (kt ; zt ; ) = max
ct ;lt

ct (1

lt )1

1

+ Et V 1

1

(kt+1 ; zt+1 ; )

1

(12)

subject to:
ct + kt+1 = kt (zt lt )1

+ (1

) kt + G

it
kt

kt :

As mentioned before, nothing of substance in the following argument depends on working
with the social planner’s problem, rather than with the competitive equilibrium. It just avoid
having to deal with heavier notation (as we would need to do, for example, in models with
nominal rigidities).
We now show that the value function is homothetic of degree , a result that we use in
the main text to rewrite our problem in a stationary form. We build on the argument by
Epstein and Zin (1989), Dolmas (1996), and specially Backus, Routledge, and Zin (2007).

38

First, note that for any a > 0, we can rework the resource constraint as:
ct kt+1
+
=
a
a

kt
a

zt
lt
a

1

+ (1

)

kt
+G
a

it
kt

kt
:
a

For our purposes, it su¢ ces to show that a function that is homogeneous of degree
satis…es (12). If this is the case, by uniqueness of the solution to the Bellman equation,
we know that V needs to be homogeneous of degree . Hence, consider a value function
homogeneous of degree

of the form:
Ve (kt+1 ; zt+1 ; ) = a Ve (kt+1 =a; zt+1 =a; ) ;

We plug this function into the Bellman equation:
Ve (kt ; zt ; ) = max

1

ct (1

ct =a;lt

= a max

ct =a;lt

h
+ Et a

1

lt )
ct
a

lt )

) e1

V

h
+ Et Ve 1

1

(1

(1

1

(kt+1 =a; zt+1 =a; )

i

(kt+1 =a; zt+1 =a; )

i

1

1

1

1

where the term after a is the value function Ve (kt+1 =a; zt+1 =a; ) subject to:
ct kt+1
+
=
a
a

kt
a

zt
lt
a

1

+ (1

)

kt
+G
a

it
kt

kt
:
a

This in turn implies:
V (kt ; zt ; ) = a V (kt =a; zt =a; ) :
In other words, if we divide capital and productivity by a, we can divide consumption by a
and still satisfy the Bellman equation. This shows that the value function is homogeneous of
degree .
The homotheticity result can be strengthened to show that
V (kt ; zt ; ) = zt V k~t ; 1;

= zt Ve k~t ;

:

(13)

This stronger result can be useful in some investigations where we want to characterize the
structure of the value function. This formulation would not make much di¤erence in our
perturbation approach because, in any case, we would need to take derivatives of (13) with
respect to zt to …nd the coe¢ cients of the decision rule associated with zt :

39

8.3. Stationary Recursive Form of the Model
Taking advantage of the result in appendices 8.1 and 8.2, we can easily make the model
stationary by de…ning vg
art =

vart
zt 1

for any nonstationary variable vart . Note that, given the

law of motion of productivity, we have that:
zet =

zt
= exp ( +
zt 1

z "zt ) :

We can go equation by equation. First, the value function:
ct (1
zt 1e

zt 1 Vt = max

lt )

+
1

lt )1

e
ct (1

Vt = max

1

1

1

zt
1

(1

)

1
Et Vt+1
1

1
Et Vt+1

+ zet

1

)
1

:

Second, the optimality condition between leisure and consumption:
zt 1e
ct
1
= zt 1 w
et )
1 lt

1

1

Third, the stochastic discount factor:
zt et+1
=
zt 1 et
zet

e

t+1

et

ct+1 (1
zt e

zt 1e
ct (1
1

= zet

1

!1

lt+1 )1
lt )1

lt+1 )1

e
ct+1 (1

lt )1

e
ct (1

zt 1e
ct
zte
ct+1
!1

Fourth, the Euler equation for capital:
zt 1eit
zt 1 e
kt

!1

eit
e
kt

2

= Et 4zet

!1

2

e

t+1

e

= Et 4zet

t

e

0

@a2 rt+1 +

t+1

et

0

@a2 rt+1 +

zteit+1
zt e
kt+1
eit+1
e
kt+1

!1 0

zt

(1

(1

e
ct
e
ct+1

@1

+ a1 +

et+1 1
=1
e
t+1
40

zt

+ a1 +

!1 0

t

=w
et

lt

@1

Fifth, the Euler equation for nominal bonds:
Rt Et zet

e
ct

)

1
Vt+1

)

!1

1
Et Vt+1
!1
1
Vt+1
1
Et Vt+1

a2
1
a2
1

1

)
1

zteit+1
zt e
kt+1

eit+1
e
kt+1

!1

!1

1

1

113

AA5 )

113
AA5

Sixth, output:
zt 1 yet = zt 1 e
kt (zt lt )1

zt1

) yet =

Seventh, the resource constraint:

zt1 1

e
kt (lt )1

) yet = e
kt (e
zt lt )1

zt 1e
ct + zt 1eit = zt 1 yet ) e
ct + eit = yet

Eight, law of motion for capital:

!1 1 1
e
zt 1 it
a2
A zt 1 e
kt )
) zt 1 e
kt + @a1 +
1
e
1
zt 1 kt
0
!1 1 1
e
it
a2
Ae
= (1
)e
kt + @a1 +
kt
1
e
1
kt
0

zt e
kt+1 = (1
zet e
kt+1

Ninth, and …nally, input prices:

zt 1 yet
)w
et
lt
zt 1 yet
rt =
) rt =
zt 1 e
kt

zt 1 w
et = (1
Collecting all terms:
Vt = max

eit
e
kt

!1

zet

et+1
e
t
2

1

= zet

= Et 4zet

et+1
e

t

0

)

1

1

lt )1

e
ct (1

1
1

e
ct+1 (1

e
ct (1

@a2 rt+1 +

yet
e
kt
1

lt

=w
et ;

lt )1
!1 0

!1

@1

et+1 1
= 1;
e
t+1

yet = e
kt (e
zt lt )1

e
ct + eit = yet ;
41

e
ct
e
ct+1

+ a1 +

t

;

)

yet
lt

1
Et Vt+1

+ zet

lt+1 )1

eit+1
e
kt+1

Rt Et zet

e
ct

= (1

1

1

;

1
Vt+1
1
Et Vt+1

a2
1

!1
eit+1
e
kt+1

1

;
!1

1

113

AA5 ;

0

zet e
kt+1 = (1

)e
kt + @a1 +
w
et = (1

rt =

and

1
1
Et Vt+1
Rt

1

1

1

= Et zet

e
ct+1 (1

1

8.4. Optimizing the Loglikelihood

e
ct (1

1

1
yet
;
lt

)

yet
;
e
kt

zet = exp ( +

Now, we can write:

eit
e
kt

a2

!1

1

1

Ae
kt ;

z "zt )

1

lt+1 )
1

lt )

!1

e
ct
1
Vt+1
e
ct+1

1

1

1

= 1:

t+1

The estimation of the model was done with mixed-programming as follows. Mathematica
computed the analytical derivatives of the value function and decision rules and generated
Fortran 95 code that included those expressions. The derivatives depend on the parameters
as symbolic variables. Then, we link the output into a Fortran 95 code that evaluates the
solution of the model for each parameter value as implied by the maximization algorithm or
by a Markov chain Monte Carlo, as a random-walk Metropolis-Hastings. The Fortran 95
code was compiled in Intel Visual Fortran 10.3 to run on Windows-based machines. We
used a Xeon Processor 5160 EMT64 at 3.00 GHz with 16 GB of RAM.
As we pointed out in the main text, the CMA-ES is an evolutionary algorithm that
approximates the inverse of the Hessian of the loglikelihood function by simulation. We
iterate in the procedure until the change in the objective function is lower than some tolerance
level. In each step g of the routine, m candidate n parameter values are proposed from a
normal distribution:
g
i ~N

where

g 1

2 Rn and C g

1

2 Rn

g 1

; ( g )2 C g

1

, for i = 1; :::; m

Rn are the mean and variance-covariance matrix of the

best candidates for optimal parameter values in step g

1 and

g

is a scaling parameter.

The normal distribution can be truncated to have support only on that part of the parameter
space where the parameters take admissible values (for example, positive discount factors).
To save on notation, we re-order the draws in decreasing relation to the value they attain in

42

the likelihood of the model:
L YT ;

L YT ;

i

i+1

The mean of step g is de…ned as:
g

=

X

wi

g
i

i=1

where the weights wi are de…ned between 0 and 1 (and its sum normalizes to 1) and

is

smaller than m.
The variance-covariance of step g …ts the search distribution to the contour lines of the
likelihood function. To do so, we set:
C g = (1
|

ccov ) C g 1 +
{z
}

term 1

ccov

+
Pcg (Pcg )0 + cc (2
{z
| cov

Hg)

cc ) (1

}

term 2

Pcg

H

and cc ; ccov ;

cov

|

where:

ef f ,

g

and

X wi
( g )2
i=1

1

+ccov 1

g
1:

g 1 0

g
1:

g 1

{z

}

term 3

cc ) Pcg

q
1
+ H g cc (2

(14)

g

cc )

g 1

!

=

(1

=

8
< 1 if p

cov

are constants. Term 1 of (14) is a standard persistence term

kPcg k

1 (1 cg )2g

< 1:5 +

ef f

1
n 0:5

g

E (kN (0; I)k)

: 0 otherwise

from previous variance-covariance matrix to dampen changes in C g . Term 2 captures the
correlation across steps of the algorithm through the evolution of Pcg . Term 3 controls for a
large number of points in the simulation. The term
E (kN (0; I)k) =

p

2

n+1
2

.

n
2

p

n+O

is the expectation of the euclidean norm of a random normal vector.

43

1
n

Finally, the scaling parameter evolves according to:
g

P

g

g 1

=

= (1

exp

c
d

c )P

g

kP g k
1
E (kN (0; I)k)
p
1
+ c (2 c )B g 1 Dg

where c and d are constants and B g
g 2

g

1

1

1

1

B

g 1 0

p

is an orthogonal matrix and Dg

ef f
g
1

g

g 1

a diagonal matrix

g 0

such that C = B g (D ) (B ) :
Standard values for the constants of the algorithm are:
m = 4 + b3 ln nc
= bm=2c
where b c is the integer ‡oor of a real number, and
wi

=

ef f

=

ln ( + 1) ln i
ln j)
j=1 (ln ( + 1)
! 1
X
wi2
cov =

Pn

i=1

c

=

cc

=

ccov

=

+2
n + ef f + 3
4
n+4
ef f

1
cov

d

=

2
p
n+ 2

1 + 2 max 0;

2

1

+ 1

min 1;

cov

r

ef f

n+1

2

!

0

2

ef f
2

1

(n + 2) +

1 max @0:3; 1

ef f

!

n
min g max ;

Lmax
eval
m

1

A+c

where g max is the maximum number of steps and Lmax
eval is the maximum number of likelihood
evaluations. Finally, we can initialize the algorithm by setting
parameters,

0

to 1, Pcg = 0, and

0

0

to some standard calibrated

to an identity matrix. Of all these constants, Hansen and

Kern (2004) recommend only to change m to adapt the algorithm to particular problems. As
one could have guessed, Hansen and Kern show that by increasing the number of simulations
m, the global properties of the search procedure improve.

44

References
[1] Álvarez, F. and U.J. Jermann (2005). “Using Asset Prices to Measure the Persistence of
the Marginal Utility of Wealth.”Econometrica 73, 1977-2016.
[2] Andreasen, M. (2007). “How to Maximize the Likelihood of a DSGE Model.” Mimeo,
Bank of England.
[3] Andreasen, M. and P. Zabczyk (2010). “An E¢ cient Method of Computing Higher Order
Bond Price Perturbation Approximations.”Mimeo, Bank of England.
[4] Ang, A., G. Bekaert, and M. Wei (2008). “The Term Structure of Real Rates and Expected In‡ation.”Journal of Finance, 63, 797-849.
[5] Ang, A., S. Dong, and M. Piazzesi (2007). “No-Arbitrage Taylor Rules.” Mimeo,
Columbia University.
[6] Backus, D.K., B.R. Routledge, and S.E. Zin (2004). “Exotic Preferences for Macroeconomists.”NBER Macroeconomics Annual 2004, 319-390.
[7] Backus, D.K., B.R. Routledge, and S.E. Zin (2007). “Asset Pricing in Business Cycle
Analysis.”Mimeo, New York University.
[8] Bansal, R., Dittman, and D. Kiku (2007). “Cointegration and Consumption Risks in
Asset Returns.”Review of Financial Studies 22, 1343 - 1375.
[9] Bansal, R., R. Gallant, and G. Tauchen (2008). “Rational Pessimism, Rational Exuberance, and Asset Pricing Models.”Review of Economic Studies 74, 1005-1033.
[10] Bansal, R., D. Kiku, and A. Yaron (2007). “Risks for the Long Run: Estimation and
Inference.”Mimeo, Duke University.
[11] Bansal, R. and A. Yaron (2004). “Risks for the Long Run: a Potential Resolution of
Asset Pricing Puzzles.”Journal of Finance 59, 1481-1509.
[12] Beeler, J. and J.Y. Campbell (2009). “The Long-Run Risks Model and Aggregate Asset
Prices: An Empirical Assessment.”NBER Working Paper 14788.
[13] Benigno, P. and M. Woodford (2006). “Linear-Quadratic Approximation of Optimal
Policy Problems.”Mimeo, Columbia University.
[14] Binsbergen, J. van (2009). “Deep Habits and the Cross Section of Expected Returns.”
Mimeo, Stanford University.
[15] Binsbergen, J. van and R.S.J. Koijen (2010). “Likelihood-based Estimation of ExactlySolved Present-Value Models.”Mimeo, University of Chicago and Stanford University.
[16] Caldara, D., J. Fernández-Villaverde, J.F. Rubio-Ramírez, and W. Yao (2010). “Computing Models with Recursive Preferences.”Mimeo, University of Pennsylvania.

45

[17] Campanale, C., R. Castro, and G. Clementi (2010). “Asset Pricing in a Production
Economy with Chew-Dekel Risk Preferences.” Review of Economic Dynamics 13, 379402.
[18] Campbell, J.Y. (1993). “Intertemporal Asset Pricing without Consumption Data.”
American Economic Review 83, 487-512.
[19] Campbell, J.Y. (1996). “Understanding Risk and Return.”Journal of Political Economy
104, 298-345.
[20] Campbell, J.Y. and J. Cochrane (1999). “Force of Habit: A Consumption-Based Explanation of Aggregate Stock Market Behavior.”Journal of Political Economy 107, 205-251.
[21] Campbell, J.Y. and J. Cochrane (2000). “Explaining the Poor Performance of
Consumption-Based Asset Pricing Models.”Journal of Political Economy 55, 2863-78.
[22] Campbell, J.Y. and L. Viceira (2001). “Who Should Buy Long-Term Bonds?”American
Economic Review 91, 99-127.
[23] Chen, X., J. Favilukis and S. Ludvigson (2007). “An Estimation of Economic Models
with Recursive Preferences.”Mimeo, New York University.
[24] Cho, J-O. and T.C. Cooley (2000). “Business Cycle Uncertainty and Economic Welfare.”
Mimeo, New York University.
[25] Christiano, L., M. Eichenbaum, and C.L. Evans (2005). “Nominal Rigidities and the
Dynamic E¤ects of a Shock to Monetary Policy.” Journal of Political Economy 113,
1-45.
[26] Cochrane, J.H. and M. Piazzesi (2005). “Bond Risk Premia.”American Economic Review
95, 138-160.
[27] Cochrane, J.H. and M. Piazzesi (2008). “Decomposing the Yield Curve.” Mimeo, University of Chicago.
[28] Constantinides, G.M. (1990). “Habit Formation: A Resolution of the Equity Premium
Puzzle.”Journal of Political Economy 98 , 519-543.
[29] Croce, M. (2006). “Welfare Costs and Long-Run Consumption Risk in a Production
Economy.”Mimeo, University of North Carolina.
[30] Dai, Q. and K. Singleton (2000). “Speci…cation Analysis of A¢ ne Term Structure Models.” Journal of Finance 55, 1943-1978.
[31] Dai, Q. and K. Singleton (2002). “Expectations Puzzle, Time-varying Risk Premia, and
A¢ ne Models of the Term Structure"” Journal of Financial Economics 63, 415-441.
[32] Del Moral P., and J. Jacod (2002), “The Monte-Carlo Method for Filtering with Discrete
Time Observations. Central Limit Theorems.”in T. J. Lyons and T. S. Salisbury (eds),
Numerical Methods and Stochastics. The Fields Institute Communications, American
Mathematical Society.
46

[33] De Paoli, B., A. Scott, and O. Weeken (2007). “Asset Pricing Implications of a New
Keynesian Model.”Bank of England Working Paper 326.
[34] DeJong, D.N., H. Dharmarajan, R. Liesenfeld, and J.F. Richard (2007). “An E¢ cient Filtering Approach to Likelihood Approximation for State-Space Representations.”Mimeo,
University of Pittsburgh.
[35] Doh, T. (2009). “Yield Curve in an Estimated Nonlinear Macro Model.”Mimeo, Federal
Reserve Bank of Kansas City.
[36] Dolmas, J. (1996). “Balanced-growth-consistent Recursive Utility.”Journal of Economic
Dynamics and Control 204, 657-680.
[37] Du¤ee, G.R. (2002). “Term Premia and Interest Rate Forecasts in A¢ ne Models.”Journal of Finance 57, 405-443
[38] Efron. B. and R.J. Tibshirani (1993). An Introduction to the Bootstrap. Chapman &
Hall.
[39] Epstein, L., and S.E. Zin (1989). “Substitution, Risk Aversion, and the Temporal Behavior of Consumption and Asset Returns: a Theoretical Framework.”Econometrica 57,
937-969.
[40] Epstein, L., and S.E. Zin (1991). “Substitution, Risk Aversion, and the Temporal Behavior of Consumption and Asset Returns: an Empirical Analysis.”Journal of Political
Economy 99, 263-286.
[41] Fermanian, J.D. and B. Salanié (2004). “A Nonparametric Simulated Maximum Likelihood Estimation Method.”Econometric Theory 20, 701-734.
[42] Fernández-Villaverde, J. and J.F. Rubio-Ramírez (2007). “Estimating Macroeconomic
Models: A Likelihood Approach.”Review of Economic Studies 74, 1059-1087.
[43] Fiorentini, G., E. Sentana, and N. Shephard (2004). “Likelihood Estimation of Latent
Generalised ARCH Structures.”Econometrica 72, 1481-1517.
[44] Gabaix, X. (2009). “Variable Rare Disasters: An Exactly Solved Framework for Ten
Puzzles in Macro-Finance.”Mimeo, NYU.
[45] Gomes, F. and A. Michealides (2005). “Optimal Life-Cycle Asset Allocation: Understanding the Empirical Evidence.”Journal of Finance 60, 869-904.
[46] Hall, R.E. (1988). “Intertemporal Substitution in Consumption.” Journal of Political
Economy 96, 339-357.
[47] Hansen, L.P., J. Heaton, and N. Li (2008). “Consumption Strikes Back? Measuring Long
Run Risk.”Journal of Political Economy, 116, 260-302.
[48] Hansen, L.P., J. Heaton, J. Lee, and N. Roussanov (2008). “Intertemporal Substitution
and Risk Aversion.” In J.J. Heckman and E. Leamer (eds), Handbook of Econometrics,
volume 6, 3967-4056. North-Holland.
47

[49] Hansen, L.P., and T.J. Sargent (1995). “Discounted Linear Exponential Quadratic
Gaussian Control.”IEEE Transactions on Automatic Control 40, 968-971.
[50] Hansen, L.P., T.J. Sargent, and T.D. Tallarini (1999). “Robust Permanent Income and
Pricing.”Review of Economic Studies 66, 873-907.
[51] Hansen, N., and S. Kern (2004). “Evaluating the CMA Evolution Strategy on Multimodel
Test Functions.”Springer-Verlag: Eighth International Conference on Parallel Problems
from Nature, 282-291.
[52] Hansen, N., S.D. Müller, and P. Koumoutsakos (2003). “Reducing the Time Complexity
of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMAES).”Evolutionary Computation 11, 1-18.
[53] Jermann, U.J. (1998). “Asset Pricing in Production Economies.” Journal of Monetary
Economics 41, 257-275.
[54] Judd, K.L. (1998). Numerical Methods in Economics. MIT Press.
[55] Kaltenbrunner, G. and L. Lochstoer (2008). “Long-Run Risk through Consumption
Smoothing.”Review of Financial Studies, forthcoming.
[56] Kim, S., N. Shephard, and S. Chib (1998). “Stochastic Volatility: Likelihood Inference
and Comparison with ARCH Models.”Review of Economic Studies 65, 361-93.
[57] Kocherlakota, N.R. (1990). “Disentangling the Coe¢ cient of Relative Risk Aversion from
the Elasticity of Intertemporal Substitution: An Irrelevance Result.”Journal of Finance
45, 175-190.
[58] Koijen, R.S.J., H. Lustig, S. Van Nieuwerburgh, and A. Verdelhan (2010). “Long-Run
Risk, the Wealth-Consumption Ratio, and the Temporal Pricing of Risk.” American
Economic Review, forthcoming.
[59] Kreps, D.M. and E.L. Porteus (1978). “Temporal Resolution of Uncertainty and Dynamic
Choice Theory.”Econometrica 46, 185-200.
[60] Künsch, H.R. (2005). “Recursive Monte Carlo Filters: Algorithms and Theoretical
Analysis.”Annals of Statistics 33, 1983-2021.
[61] Lettau, M. and S. Ludvigson (2001). “Consumption, Aggregate Wealth, and Expected
Stock Returns.”Journal of Finance 56, 815-849.
[62] Lettau, M. and H. Uhlig (2002). “Sharpe Ratios and Preferences: An Analytical Approach.”Macroeconomic Dynamics 6, 242-265.
[63] Levin, A., D. López-Salido, and T. Yun (2007). “Risk-Sensitive Monetary Policy.”
Mimeo, Federal Reserve Board.
[64] Levine, P., J. Pearlman, and R. Pierse (2007). “Linear-Quadratic Approximation, External Habit, and Targeting Rules.”Mimeo, University of Surrey.
48

[65] Lustig, H., S. Van Nieuwerburgh, and A. Verdelhan (2007). “The Wealth-Consumption
Ratio: A Litmus Test for Consumption-based Asset Pricing Models.”Mimeo, UCLA.
[66] McFadden, D.L. (1989). “A Method of Simulated Moments for Estimation of Discrete
Response Models Without Numerical Integration.”Econometrica 57, 995-1026.
[67] Pakes, A. and D. Pollard (1989). “Simulation and the Asymptotics of Optimization
Estimators.”Econometrica 57, 1027-1057.
[68] Piazzesi, M. and M. Schneider (2006). “Equilibrium Yield Curves.” NBER Macroeconomics Annual 2006, 389-442.
[69] Rouwenhost, K.G. (1995). “Asset Pricing Implications of Equilibrium Business Cycle
Models.”In Cooley, T. F. (Eds.), Frontiers of Business Cycle Research. Princeton University Press, Princeton, 293-330.
[70] Rubin, D.B. (1998). “Using the SIR Algorithm to Simulate Posterior Distributions.”
in J.M. Bernardo, M.H. DeGroot, D.V. Lindley, and A.F.M. Smith (eds), Bayesian
Statistics 3, 395-402, Oxford University Press.
[71] Rudebusch, G. and E. Swanson (2008). “The Bond Premium in a DSGE Model with
Long-Run Real and Nominal Risks.” Federal Reserve Bank of San Francisco Working
Paper 2008-31.
[72] Santos, M.S. and A. Peralta-Alva (2005). “Accuracy of Simulations for Stochastic Dynamic Models.”Econometrica 73, 1939-1976.
[73] Schmitt-Grohé, S. and M. Uribe (2005). “Optimal Fiscal and Monetary Policy in a
Medium Scale Macroeconomic Model.”NBER Macroeconomic Annual 2005, 382-425.
[74] Smets, F. and R. Wouters (2007). “Shocks and Frictions in US Business Cycles: A
Bayesian DSGE Approach.”American Economic Review 97, 586-606.
[75] Stock, J.H. and M.W. Watson (2007). “Why Has U.S. In‡ation Become Harder to Forecast?”Journal of Money, Credit and Banking 39, 3-33.
[76] Swanson, E. (2009). “Risk Aversion, the Labor Margin, and Asset Pricing in DSGE
Models.”Federal Reserve Bank of San Francisco Working Paper 2009-26.
[77] Tallarini., T.D. (2000). “Risk-Sensitive Real Business Cycles.”Journal of Monetary Economics 45, 507-532.
[78] Uhlig, H. (2007). “Leisure, Growth, and Long Run Risk.”Mimeo, University of Chicago.
[79] Wang, S. (1993). “The Local Recoverability of Risk Aversion and Intertemporal Substitution.”Journal of Economic Theory 59, 333-363.
[80] Weil, P. (1990). “Nonexpected Utility in Macroeconomics.” Quarterly Journal of Economics 105, 29-42.

49

