NBER WORKING PAPER SERIES

IDENTIFICATION AND INFERENCE IN LINEAR STOCHASTIC DISCOUNT FACTOR
MODELS WITH EXCESS RETURNS
Craig Burnside
Working Paper 16634
http://www.nber.org/papers/w16634
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
December 2010

This is a substantially revised version of an earlier draft entitled “Identification and Inference in Linear
Stochastic Discount Factor Models”. I am grateful to Jeremy Graveline, Cosmin Ilut, Shakeeb Khan,
Frank Kleibergen, Francisco Peñaranda, Cesare Robotti and three anonymous referees for their comments
and suggestions. I am grateful to the National Science Foundation for financial support (SES-0516697).
The views expressed herein are those of the author and do not necessarily reflect the views of the National
Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2010 by Craig Burnside. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.

Identification and Inference in Linear Stochastic Discount Factor Models with Excess Returns
Craig Burnside
NBER Working Paper No. 16634
December 2010, Revised July 2015
JEL No. C3,G12
ABSTRACT
When excess returns are used to estimate linear stochastic discount factor (SDF) models, researchers
often adopt a normalization of the SDF that sets its mean to 1, or one that sets its intercept to 1. These
normalizations are often treated as equivalent, but they are subtly different both in population, and
in finite samples. Standard asymptotic inference relies on rank conditions that differ across the two
normalizations, and which can fail to differing degrees. I first establish that failure of the rank conditions
is a genuine concern for many well known SDF models in the literature. I also describe how failure
of the rank conditions can affect inference, both in population and in finite samples. I propose using
tests of the rank conditions not only as a diagnostic device, but also for model reduction. I show that
this model reduction procedure has desirable size and power properties in a Monte Carlo experiment
with a calibrated model.
Craig Burnside
Department of Economics
Duke University
213 Social Sciences Building
Durham, NC 27708-0097
and NBER
craig.burnside@duke.edu

Standard asset pricing theory implies that if there are no arbitrage opportunities available
in a set of assets, then there exists a stochastic discount factor (SDF), m, such that
E(Re m) = 0,

(1)

where Re is an n ⇥ 1 vector of excess returns on the assets.

In empirical asset pricing, a common approach is to specify the SDF as a linear function

of a k ⇥ 1 vector of risk factors, f , with k < n. To take an example, suppose that we write

m=1

(f

µf )0 , where µf = E(f ) and

is a conformable vector of parameters. Notice

that with this specification of the SDF we can rewrite equation (1) as
E(Re ) = cov(Re , f ) .

(2)

Since E(Re f 0 ) = cov(Re , f ) + E(Re )µ0f we can also rewrite equation (1) as
E(Re ) = E(Re f 0 )

with

⌘

1 + µ0f

.

(3)

Another way we can get to equation (3) directly is to start by writing the SDF as m = 1 f 0
and then substitute this expression into equation (1).
The simple transformation involved in moving from equation (2) to equation (3) seems
innocuous. However, there are important issues involving model validity and identification
that emerge upon closer inspection. These arise in population, and have non-trivial consequences for estimation and inference.
To highlight the issues, consider a candidate model with a scalar risk factor, f . Suppose
that E(Re ) 6= 0 and cov(Re , f ) = 0. Clearly, there is no value of

such that equation (2)

holds. Put di↵erently, not only does the candidate model appear to be false, by reference
to equation (2), but the parameter

is also not identified by equation (2). At the same

time, E(Re f ) = E(Re )µf , so, as long as µf 6= 0, there is a simple solution to equation (3):

= 1/µf . In this case, the candidate model appears to be valid, by reference to equation

(3), and the parameter

is well identified.

The scalar example is also useful for illustrating the symmetric case where E(Re f ) = 0,
combined with E(Re ) 6= 0. Here, there is no value of

such that equation (3) holds. Put

di↵erently, not only does the candidate model appear to be false, by reference to equation (3),
but the parameter

is also not identified by equation (3). At the same time, cov(Re , f ) =

E(Re )µf and, as long as µf 6= 0, there is a simple solution to equation (2):
1

=

1/µf .

In this case, the candidate model appears to be valid, by reference to equation (2), and the
parameter

is well identified.

An obvious question arises in these examples. Is the model valid or not? In each case there
is one representation of the model that correctly assigns a zero price to the vector Re . For
this reason, we will say that, strictly speaking, these representations are valid. Nonetheless,
it is worth digging a little deeper. At first there appears to be perfect symmetry between
the two cases, but there is at least one dimension in which the symmetry is imperfect. In
the second example, with E(Re f ) = 0, the solution to equation (2) implies an SDF with a
positive (unit) mean, m = f /µf , that has non-zero covariance with the return vector. On
the other hand, in the first example, with cov(Re , f ) = 0 the solution to equation (3) implies
an SDF, m = 1

f /µf , with mean zero and no covariance with the return vector. There are

at least three reasons a researcher may find the zero-mean SDF dissatisfying:
1. Because cov(Re , m) = 0 the aggregate measure of risk has a relationship with returns
that is arguably devoid of economic interpretation.
2. Although equation (1) makes reference to excess returns, it is clear that an SDF with a
zero mean cannot price a risk free asset correctly. Therefore, if it prices excess returns
correctly it will price all gross returns incorrectly. This flaw cannot be remedied by a
linear transformation of the SDF since only a proportional transformation would keep
equation (1) intact, and would not change the mean of the SDF.
3. When the SDF is mean zero it implicitly assigns zero or negative prices to contingent
claims on some–presumably many–states of the world. Although these claims may not
be available to agents, this does raise issues as to whether the SDF can accurately
price assets not included in the vector being studied by the researcher. Hansen and
Jagannathan (1997) o↵er a similar reason for sometimes imposing the restriction that
the SDF is nonnegative everywhere.
A central question in this paper is: If we want to exclude (reject) models of this type, how
might we do so?
When f is a vector we have the more general case. Standard methods of estimating and
evaluating linear SDF models rely on cov(Re , f ) and E(Re f 0 ) having full column rank; i.e.
rank k. When cov(Re , f ) has rank less than k,

2

is not identified, and estimators based on

(2) have nonstandard asymptotic distributions. When E(Re f 0 ) has rank less than k,

is not

identified, and estimators based on (3) have nonstandard asymptotic distributions.
Analogous to the scalar case discussed above, when rank[cov(Re , f )] < rank[E(Re f 0 )] I
prove that there is no solution to (2) but, as in the scalar case, there is always at least one
solution to equation (3). Any such solution has the property that E(m) = 0. Additionally,
m is a linear combination of f that has no covariance with Re .
In the symmetric case where rank[cov(Re , f )] > rank[E(Re f 0 )] I prove that there is no
solution to (3) but there is a always a solution to (2) that, as in the scalar case, corresponds
to an SDF with no intercept and a unit mean.
To show that failure of the rank conditions is not simply a theoretical curiosity, but is a
practical reality, I use rank tests proposed by Cragg and Donald (1997) [discussed in Wright
(2003)] and Kleibergen and Paap (2006). Consider Table 1, which shows the results of rank
tests for several well-known models in the literature. For the CAPM and Fama and French
(1993) three factor model, the null hypothesis of reduced rank is strongly rejected. However,
for models with macroeconomic factors, the null hypothesis of reduced rank is not rejected
in many cases. This suggests, at a minimum, that these models are poorly identified. But
it is also noteworthy that in several cases the rank of cov(Re , f ) appears to be less than
the rank of E(Re f 0 ). For example, consider the model from Yogo (2006). This model
includes three factors, but the tests hint that rank[cov(Re , f )] = 1 and rank[E(Re f 0 )] = 2.
For the Lustig and Verdelhan (2007) model, which also has three factors, the tests hint
that rank[cov(Re , f )] = 0 and rank[E(Re f 0 )] = 1. This evidence suggests that if there are
parameterizations of these models that correctly price the assets, it is only because they put
all of their emphasis on linear combinations of the risk factors that are uncorrelated with
returns.
In Section 5 I use Monte Carlo evidence to show that rank tests can be an e↵ective
diagnostic.1 In my calibrated models, the null hypothesis that rank[cov(Re , f )] is less than
its true rank is always rejected. On the other hand, the null hypothesis that rank[cov(Re , f )]
is equal to its true rank is rejected with a probability that converges, as the sample size
grows, to the size of the test. The latter finding is simply confirmation of known properties
of these tests.
1

Equivalently, Kan and Zhang (1999b) suggest that researchers should test for significant spread among
the factor betas, cov(Re , f )⌃f 1 . They point out while Chen, Roll, and Ross (1986) and Ferson and Harvey
(1993) performed such tests, relatively few researchers do so.

3

But what should an applied researcher do if the diagnostics suggest that rank[cov(Re , f )] <
k or rank[E(Re f 0 )] < k? Is it preferable to use the mean-normalization over the interceptnormalization? My Monte-Carlo evidence suggests no simple advice of this type. Inference
with either normalization is a↵ected by identification problems, although often in di↵erent
ways. Instead of favoring one normalization over another, I propose using rank tests as part of
a model reduction procedure. This procedure is designed to eliminate models dominated by
“useless” factors. Described intuitively, if a rank test suggests that rank[cov(Re , f )] = r < k,
the procedure finds r linear combinations of the factors, denoted f˜r , for which the columns
of cov(Re , f˜r ) come closest to spanning the columns of cov(Re , f ). The appropriate linear
combinations are a by-product of the Kleibergen and Paap (2006) rank test. The model
is estimated and tested using the vector f˜r as risk factors. The economic meaning of the
resulting parameter estimates can easily be recovered using the linear transformation that
maps from f to f˜r . My Monte Carlo evidence suggests that this procedure does not induce
a significant size distortion when testing valid models.
An extensive literature, related to this paper, examines the properties of asset pricing tests
in the presence of spurious risk factors; i.e. factors that are uncorrelated with the returns.
Kan and Zhang (1999a) examine the behavior of GMM estimators, when the estimated SDF
includes a spurious factor. In some cases, this factor is added to the true model. In other
cases, it is added to a model that includes only some or none of the true risk factors. Their
results relate to mine for the mean-normalization, because they study risk factors that are
mean zero by construction. Kan and Zhang (1999b) study similar issues in the context of
the two-pass approach to model evaluation. Kleibergen (2009) finds that standard inference
based on the Fama and MacBeth (1973) and GLS two-pass regression methods is inadequate
under partial or weak identification of a factor model. He suggests adoption of alternative
statistics that lead to inference that is centered around the maximum likelihood estimator
of Gibbons (1982). Kleibergen (2009) shows that the basic problem with the traditional
methods is the dependence across the moment conditions that identify the betas and the
moment conditions that identify risk premia. By rewriting the model in terms of betas of
returns relative to a benchmark asset, he gets rid of this dependence, which leads to improved
statistical performance. Kleibergen and Zhan (2013) use these novel statistics to show that
the confidence sets for a number of models based on macroeconomic factors are, e↵ectively,
unbounded. The method proposed here, by contrast, stays within the realm of traditional

4

methods by eliminating spurious linear combinations of factors from the model.
Another closely related paper is Kan and Robotti (2008). They examine the behavior
of the Hansen and Jagannathan (1997) (HJ) distance measure under the two model normalizations discussed here. They show that the traditional HJ statistic can be manipulated
by affine transformations of the factors when the intercept-normalization is adopted. They
suggest a modified statistic that imposes the mean-normalization, and uses the covariance
matrix of returns, not the cross moment matrix, as the weighting matrix. They also discuss how estimation of the SDF representation is a↵ected by the choice of normalization,
including a discussion of misspecification, but their analysis presumes the model is identified. Related papers by Gospodinov, Kan, and Robotti (2012), Gospodinov, Kan, and
Robotti (2013a) develop a unified framework for model estimation, evaluation and model
comparison based on the unconstrained HJ-distance. The e↵ects of model misspecification
on estimation and inference are also discussed by Hou and Kimmel (2006), Shanken and
Zhou (2007), Kan and Robotti (2009), and Kan, Robotti, and Shanken (2013). Some of this
literature proposes using misspecification-robust inference. However, this approach assumes
that models are properly identified. Gospodinov, Kan, and Robotti (2013b) extends the
results regarding HJ distance into the realm of under-identified models. They also propose
a sequential model selection device where individual factors are dropped sequentially based
on robust t-statistics. Bryzgalova (2014) proposes a shrinkage-based framework for eliminating individual (or multiple) spurious factors. In her framework, estimation and model
reduction is done in one step, rather than sequentially. In contrast to these two papers, the
method proposed here focuses on problematic linear combinations of the factors. Gospodinov, Kan, and Robotti (2014) is also related, in that the authors relate model validity and
rank conditions.
The paper is organized as follows. Section 1 uses a simple example with one risk factor
and one asset to lay the groundwork for the rest of the paper. Section 2 discusses issues
of SDF model validity, identification, and misspecification and derives the main theoretical
results. Section 3 discusses the approaches I use to estimate the two model normalizations,
to test for their identification, and to potentially reduce the dimension of the model. Section
4 discusses empirical findings for some models in the literature and links di↵erences (across
normalizations) in qualitative findings to failure of rank conditions. Section 5 performs a
series of Monte Carlo simulation exercises that demonstrate the consequences of failure of

5

the rank conditions in samples similar in size to those being studied in the literature. I
discuss the behavior of parameter estimates, OIR tests, and my model selection procedure.
Section 6 concludes.

1

An Illustrative Scalar Example

Many of the issues discussed in this paper can be understood via the following simple example. In this example, a researcher’s goal is to find an SDF for the excess return on some
asset, denoted Re . To make things concrete, we will assume that E(Re ) > 0 for this single
asset. The researcher proposes the following model of the SDF:
m=a

f b,

(4)

where a and b are scalars and f is a scalar risk factor. The proposed SDF prices the asset if
E(Re m) = E[Re (a

f b)] = 0.

(5)

Clearly, any (a, b) on the locus
E(Re f )b = E(Re )a,

(6)

works. I refer to this locus as the pricing locus. I also refer to any SDF corresponding to a
pair (a, b) on this locus as valid.
Figures 1–4 illustrate this example under di↵erent assumptions about the joint behavior of
f and Re .2 Each figure illustrates possible parameter pairs (a, b) 2 R2 . The parameter space

is divided, for illustration, into two regions. The non-shaded region corresponds to those
values of (a, b) such that E(m) > 0, which we denote M+ . The shaded region corresponds

to those values of (a, b) such that E(m) < 0, which we denote M . Finally, the boundary
between the regions is the locus a = bµf . On this locus, which we denote M0 , E(m) = 0.

Every example assumes that µf > 0 so that this locus is the upward sloping line b = a/µf .

I consider two generic and two special cases of the pricing locus, which is illustrated in
each figure as a solid line:
1. E(Re )/E(Re f ) < 1/µf . In this case, the pricing locus is either upward sloping, but
with a smaller slope than the boundary, or it is downward sloping. The distinction
between these cases is not important, so Figure 1 uses the latter case for clarity.
2

Related graphs appear in the appendix of Peñaranda and Sentana (2014).

6

2. E(Re )/E(Re f ) > 1/µf . In this case, the pricing locus is upward sloping and steeper
than the boundary. This case is illustrated in Figure 2.
3. The knife-edge case where E(Re )/E(Re f ) = 1/µf . This case occurs if cov(Re , f ) = 0,
and implies that the pricing locus and the boundary coincide. This case is illustrated
in Figure 3.
4. Another interesting knife-edge case happens when E(Re f ) = 0. This means the pricing
locus is vertical at a = 0. This case is illustrated in Figure 4.
Given that the pricing locus is not a single point, our researcher adopts a normalization
of the SDF. There are two commonly used alternatives. The first of these, the interceptnormalization, assumes that a = 1. To illustrate this normalization, each figure includes a
dashed line corresponding to the locus (1, b). Alternatively, the mean-normalization assumes
that the mean of the SDF is 1. Consistent with this, each figure includes a dotted line
b = (a

1)/µf along which all candidate SDFs have a unit mean. This line is parallel to

the boundary. Having adopted one or other of these normalizations, there is at most one
solution to the pricing equation.
Case 1. In Figure 1, the pricing line crosses both the dashed and dotted lines (at A and
B). Therefore, there exist valid unit-intercept and unit-mean SDFs, and both lie in M+ .

This is a favorable case, for several reasons. In both cases, the model is identified. When we
turn to finite sample estimation, inference functions as intended. Parameter estimates are
consistent and have standard asymptotic distributions. If multiple assets are available, the
OIR test functions as intended. And the SDFs at A and B are proportional by a positive
constant.
Case 2. In Figure 2 we see that both normalizations are identified. The pricing line crosses
the dotted line (at B), so there exists a valid unit-mean SDF in M+ . It crosses the dashed
line (at A), so the unique valid unit-intercept SDF lies in M .

While, strictly speaking, the SDF at A is valid, because it solves the pricing equation, it

has three potentially undesirable features. First, because it implies E(m) < 0 it obviously
prices risk free assets incorrectly. Second, since the mean of m is negative, this implies that
there are states of the world to which this SDF assigns negative state prices. Third, the
relationship between m and f is positive (b > 0) whereas b < 0 for all valid SDFs that lie
7

in M+ . Obviously a researcher paying attention to the fact that E(m) < 0 could flip the

sign of this SDF, and resolve all of these issues. Of course, doing so would, in e↵ect, imply
adopting a di↵erent normalization.
Case 3. When cov(Re , f ) = 0 we have Figure 3. Here, the pricing line is the same as
the boundary, M0 . Therefore, there are no valid (a, b) pairs in M+ nor in M

. The

pricing line is parallel to the dotted line, so there exists no valid unit-mean SDF in this
case. The pricing/boundary line crosses the dashed line at A (where b = 1/µf ) so the valid
unit-intercept SDF has a zero mean.
The fact that there is no solution to the pricing equation for the mean-normalization
means that the model is not identified. In population the researcher would be able to reject
the model, since nonexistence is an analytic result. However, the identification problem
manifests itself in non-standard inference in finite samples, even asymptotically.
On the other hand, if the researcher adopts the intercept-normalization, the model is
identified, in the sense that there is a unique solution to the pricing equation. Parameter
estimates are consistent and have standard asymptotic distributions. If multiple assets are
used to test the model, it is rejected with probability equal to the size of the test. In other
words, the model is likely to end up being validated by standard inference.
Should the model be rejected in Case 3? This is a matter of interpretation. Clearly,
the model at A is potentially problematic. It implies E(m) = 0, so it prices risk free assets
incorrectly, and must assign negative state-prices to some states of the world. There is also
a sense in which it is economically uninteresting, since it implies zero covariance between the
measure of aggregate risk, m, and the returns being priced. At the same time, the model at
A does price Re correctly.
Case 4. When E(Re f ) = 0 the pricing equation is the vertical line a = 0, as illustrated in
Figure 4. The pricing line crosses the dotted line at B (where b =

1/µf ), so there exists

a valid unit-mean SDF. The pricing line does not cross the dashed line, so there is no valid
unit-intercept SDF.
Econometrically, Case 4 is symmetric to Case 3 with, in this case, point B being defined,
but point A not being defined. In other words, one normalization (in this case the meannormalization) is identified, while the other (the intercept-normalization) is not.
However, Case 4 is economically di↵erent from Case 3. In Case 4, the identified model
8

at B has a positive mean, whereas, in Case 3 the model at A does not. The model at B
in Case 4 does have one perplexing property. Peñaranda and Sentana (2014) argue that an
“SDF that is exactly proportional to an orthogonal factor is not very attractive from an
economic point of view”. They argue that this is because the projection of f onto R with
no constant—in other words, the uncentered factor mimicking portfolio—is constant and
equal to 0. This property follows from equation (5) because a = 0 and m =

f b, implying

that E(Re f ) = 0. This property of the SDF may be unattractive, but it must be kept in
mind that in every case described above (i.e. cases 1—4), the SDF is orthogonal to R so
an uncentered SDF -mimicking portfolio is always constant and equal to 0.3 Additionally,
if the feature that E(Re f ) = 0 is viewed as problematic, it can be remedied by a linear
transformation. On the other hand, in Case 3, linear transformation of the factor cannot
change the fact that the covariance between the factor and the return is zero.
Cases 1—4 illustrate many of the issues dealt with later in this paper, however, the more
general setting of a multi-factor model is more complicated. In a setting with k factors,
the intercept and mean-normalizations are identified if, respectively, rank[E(Re f 0 )] = k and
rank[cov(Re , f )] = k. If both these rank conditions hold, we have a situation analogous to
Cases 1 and 2. When rank[E(Re f 0 )] = k and rank[cov(Re , f )] = k
analogous to Case 3. When rank[E(Re f 0 )] = k

1 we have a situation

1 and rank[cov(Re , f )] = k we have a

situation analogous to Case 4. But we also have an additional possibility that both models are
under-identified. This possibility can only arise in the scalar case if we have cov(Re , f ) = 0
and µf = 0, in which case we also have E(Re f ) = 0.

2

Validity, Misspecification and Identification

Let Rte be an n ⇥ 1 vector whose ith element, Rite , is the excess return at time t to asset i

defined as the di↵erence between the return on asset i and the risk free rate. For notational
convenience I suppress time subscripts unless they are strictly needed.
3

I am grateful to Francisco Peñaranda for pointing out the following fact. It is always possible to
mechanically construct an SDF or risk factor as the residual of the least squares projection of any random
variable x onto Re (without a constant). In fact, any such residual will satisfy the pricing equation for returns.
I would argue that this is not a problem with the mean-normalization, per se. Rather, it is a problem endemic
to working with excess returns. It also does not imply that every factor for which E(Re f 0 ) = 0 is constructed
in an economically meaningless way, because E(Re m) = 0 holds for any valid SDF.

9

Assumption 1. At least one element of the return vector has a non-zero mean; that is
E(Re ) 6= 0.
Under the assumption of no-arbitrage there exists a strictly positive random variable, m,
such that
E(Re m) = 0.

(7)

We consider candidate models of the SDF, m, that take the following form:
m=a

f 0 b,

(8)

where f is a k ⇥ 1 vector of risk factors, a is a scalar constant and b is a k ⇥ 1 vector of
parameters. We will assume, throughout, that n

k. Given the vector of risk factors, we let

M denote the set of all m of the form given in equation (8). We let M+ = {m 2 M|E(m) >

0}, M = {m 2 M|E(m) < 0}, and M0 = {m 2 M|E(m) = 0}.

2.1

Valid Models of the SDF

Definition 1. m is a valid SDF if equation (7) holds for at least one (a, b).
Clearly, if m is valid for some (a, b) so is m for any scalar, , since this transformation
preserves that equation (7) holds. Thus, validity of the SDF does not uniquely determine
(a, b). For this reason, it is common to adopt a normalization of the SDF that reduces the
dimension of the parameter space. We consider two normalizations that appear frequently
in the literature.
To arrive at the first normalization we rewrite equation (8) as

with

m = a(1

f 0 ),

m ⌘1

f0 ,

= b/a. I refer to
(9)

as the intercept-normalization of m since it is a scaled version of m with a unit intercept.
Clearly, equation (7) implies that
E(Re ) = E(Re f 0 )

or

µR = D .

(10)

where µR ⌘ E(Re ) and D ⌘ E(Re f 0 ). Given definition 1, m is a valid SDF if equation (10)

holds for at least one .

10

Alternatively (8) can be rewritten as
m = ⇠[1
where µf = E(f ), ⇠ = a

µ0f b, and

(f

µf )0 ],

= b/⇠. I refer to

m ⌘1

(f

µf )0 ,

(11)

as the mean-normalization since it is a scaled version of m with a unit mean. Given this
normalization equation (7) implies that
E(Re ) = E[Re (f
where C ⌘ E[Re (f

µf )0 ]

or

µR = C ,

(12)

µf )0 ] = cov(Re , f ). Given definition 1, m is a valid SDF if equation

(12) holds for at least one .

2.2

Misspecified Models of the SDF

A model, m, is misspecified if there is no (a, b) such that (7) holds. Because the two normalizations, m and m , are nested within m, it follows that
m misspecified (= m misspecified =) m misspecified.
Notice that the arrows only run in one direction. Case 3, above, is an example in which m
is misspecified, but m and m are valid. Similarly, Case 4, above, is an example in which m
is misspecified, but m and m are valid.
When a model is misspecified, it is useful to consider linear combinations of the moment
restrictions that appear in equations (10) and (12). In particular, we consider the following
equations
D0 W [E(Re )
C 0 W {E(Re )

E[Re (f

E(Re f 0 ) ] = 0,

or

D0 W (µR

D ) = 0,

(13)

µf )0 ] } = 0,

or

C 0 W (µR

C ) = 0.

(14)

Here, W and W are assumed to be n ⇥ n positive definite symmetric matrices. Equations

(13) and (14) are of interest because they are population equivalents of the equations we
use below to define GMM estimators of

and . Solutions to these equations exist even if

solutions to equations (10) and (12) do not, but they may or may not be unique.

11

2.3

Rank Conditions and Identification

In this section, we establish important relationships between identification, which is stated
in terms of the ranks of the matrices C and D, the model’s validity and the mean of the SDF.
We use the following notation: rD ⌘ rank(D) and rC ⌘ rank(C). We let X = ( X1 X2 )

denote a k⇥k orthogonal matrix whose first rD columns, X1 , span the rowspace of D [denoted

R(D)], and whose remaining columns, X2 , span the nullspace of D [denoted N (D)]. We

let Y = ( Y1 Y2 ) denote a k ⇥ k orthogonal matrix whose first rC columns, Y1 , span the

rowspace of C [denoted R(C)], and whose remaining columns, Y2 , span the nullspace of C
[denoted N (C)].

Definition 2. The SDF m is identified if rD = k. The SDF m is identified if rC = k.
Theorem 1. A solution to equation (10), if it exists, is unique if and only if m is identified.
A solution to equation (12), if it exists, is unique if and only if m is identified.
Proof. Let

0

be a solution to (10) so that µR = D 0 . First, assume that rD = k. Assume

there is another parameter vector
this implies that D(

0

1)

1

6=

0

= 0. Because rD = k this implies that

contradiction. Now assume that rD < k with
solution

1

=

0

such that (10) holds. Then µR = D 1 . But

0

1

=

0,

which is a

defined as before. We can construct another

+ x for any x 6= 0 such that Dx = 0. Because rD < k, we know such an x

exists. The same argument applies to the second part of the lemma.

Theorem 2. There exists a solution to equation (13), which is unique if and only if m
is identified. There exists a solution to equation (14), which is unique if and only if m is
identified.
Proof. Because W is positive definite, rD = rank(D0 W D). So when rD = k, standard
arguments imply that

= (D0 W D) 1 D0 W µR is the unique solution to equation (13).

When D has reduced rank, there are many solutions to equation (13). Consider, first, the
case where rD = 0. In this case, D = 0 and any 2 Rk is a solution to (13). If 1  rD < k,
define D̃ = DX1 and let ˜ = (D̃0 W D̃) 1 D̃0 W µR . Then, as shown in the extended proof
in the Appendix,

= X1 ˜ + x is a solution to (13) for any x 2 N (D). A similar argument

applies to the second statement in the theorem.

A consequence of Theorems 1 and 2 is that estimators based on the sample equivalents of
(13) and (14) have standard properties if D and C have full rank. If the underlying moment
12

restrictions, (10) and (12), hold, theorems from Hansen (1982) apply. If these moment
conditions are not satisfied, theorems for misspecified models from Hall and Inoue (2003)
and Hall (2005) apply. I focus, instead, on situations where rD < k or rC < k.
Lemma 1. The matrices C and D di↵er in rank by at most 1, i.e. |rC

rD |  1.

Proof. By definition D = C + µR µ0f . Since µR and µf are vectors, µR µ0f has, at most, unit
rank. Consequently, rD  rC + 1, and rC  rD + 1.
Lemma 2. rC < rD if and only if µf 2
/ R(C) and µf 2 R(D). rC > rD if and only if

/ R(D) and µf 2 R(C). rC = rD if and only if µf 2 R(C) and µf 2 R(D).
µf 2

Proof. Let C = UC SC Y 0 and D = UD SD X 0 denote the singular value decompositions of C
and D. Allowing for the possibility that C and D have less than full rank we can equivalently
write these as
C = UC1 SC1 Y10

D = UD1 SD1 X10

where UC1 represents the first rC columns of UC , SC1 is the upper left rC ⇥ rC corner of SC
and Y1 is defined as above. UD1 , SD1 and X1 are defined similarly. We can then write
D = C + µR µ0f = UC1 SC1 Y10 + µR µ0f
C=D

µR µ0f = UD1 SD1 X10

µR µ0f .

We first establish the ifs. From the first equation it is clear that if µf 2
/ R(C) the columns of
Y1 do not span the rows of D, but (Y1 µf ) does span the rows of D and has full column rank.

/ R(D) the columns of X1 do not span
Therefore, rD > rC and µf 2 R(D). Similarly, if µf 2

the rows of C, but (X1 µf ) does span the rows of C. Therefore, rC > rD and µf 2 R(C).

The first two results imply that the only remaining possibility is that µf 2 R(C) and

µf 2 R(D). Given the above equations this means that the rows of D are in R(C) and

the rows of C are in R(D). This implies that C and D have the same rank. Since we have

considered all possible statements regarding the location of µf the only ifs have also been
established.
Theorem 3. If m is valid and m 2
/ M0 then rC

rD .

Proof. By definition D = C + µR µ0f . When an SDF of the general form (8) is valid and
has a non-zero mean we can always rewrite it in the form (11), so that equation (12) holds.
13

Hence, µR = C and so we can write
D = C + µR µ0f = C(Ik + µ0f ).
It follows that rD  rC .
The following corollary follows directly from Theorem 3:
Corollary 1. If rC < rD any valid m must lie in M0 (has a zero mean).
Theorem 4. If m is valid and a 6= 0 then rC  rD . If, additionally, m 2
/ M0 then rC = rD .
Proof. By definition C = D

µR µ0f . When an SDF of the general form (8) is valid and

a 6= 0 the SDF can be written in the form (9) and equation (10) holds. Hence, µR = D and

so we can write

C=D

µR µ0f = D(Ik

µ0f ).

It follows that rC  rD . The final statement in the theorem follows from Theorem 3.
The following corollary follows directly from Theorem 4:
Corollary 2. If rC > rD any valid m must have a zero intercept.
Clearly, we can also combine the two theorems to get the following corollary:
Corollary 3. If m is valid, m 2
/ M0 and a 6= 0 then rC = rD .
Lemma 3. If rC = rD = 0 and m is valid then m 2 M0 and a = 0.
Proof. When rC = rD = 0 this means C = D = 0. It follows from validity that 0 =
E(Re m) = µR a

Db = µR a, implying a = 0. Similarly, 0 = E(Re m) = µR a

µR µ0f b, implying that µ0f b = 0. Hence, E(m) = a

Cb

µR µ0f b =

µ0f b = 0.

Figure 5 summarizes our results, so far, regarding the ranks of C and D. Each dot represents a combination of ranks that is mathematically possible; i.e. consistent with Lemma 1.
The mean-normalization is identified if rC = k, otherwise it is underidentified. The interceptnormalization is identified if rD = k, otherwise it is underidentified. When rC < rD (the
shaded gray dots), the mean-normalization is underidentified and any valid m is mean zero.
When rC > rD (the striped dots), the intercept-normalization is underidentified and any
valid m has a = 0. If rC = rD = 0 (the gray striped dots), then any valid m is mean zero
and has a = 0. The solid black dots represent combinations of rank potentially consistent
with validity, E(m) 6= 0 and a 6= 0. We turn, next, to an exploration of the consequences of
underidentification.

14

2.4

Under-identification and the existence of valid SDFs

There are three cases to consider: (1) rD = rC

1 with rC  k, (2) rC = rD

and (3) rC = rD < k. We show results for each of these cases in turn.

1 with rD  k

Theorem 5. If rD < rC there exists a valid parameterization of the mean-normalization,
but it has a zero intercept.
rD ) ⇥ 1 vector and choose it so that (µ0f X2 )˜2 =

Proof. Let ˜2 be a (k

1. We know we

/ R(D), so that µ0f X2 6= 0.
can choose ˜2 in this way because Lemma 2 implies that µf 2

Then let

= X2 ˜2 . We then have
µR

C = µR

CX2 ˜2 = µR

(D

µR µ0f )X2 ˜2 .

Recall that DX2 = 0 so that we have
µR

C = µR [1 + (µ0f X2 )˜2 ] = 0.

The second statement in the theorem follows from Corollary 2.
When rC = k the solution for

described in the theorem is unique, otherwise it is not.

There are two ways of getting to this result. First, we already know, from Theorem 1, that
when rC = k any solution to µR = C
implies that rD = k

1 so that ˜2 =

is unique. Second, when rD < rC = k, Lemma 1
1/(µ0f X2 ) is a unique scalar.4

Theorem 6. If rC < rD there exists a valid parameterization of the intercept-normalization,
but it lies in M0 .
Proof. The first part of the proof mimics the proof of Theorem 5. Let ˜2 be a (k rC ) ⇥ 1
vector and choose it so that (µ0f Y2 ) ˜2 = 1. We know we can choose ˜2 in this way because
/ R(C), so that µ0 Y2 6= 0. Then let = Y2 ˜2 . We then have
Lemma 2 implies that µf 2
f

µR

D

= µR

DY2 ˜2 = µR

(C + µR µ0f )Y2 ˜2 .

Recall that CY2 = 0 so that we have
µR

D = µR [1

(µ0f Y2 ) ˜2 ] = 0.

The second statement in the theorem follows from Corollary 1.
4

We can ensure that ˜2 is unique, in this case, by normalizing the first non-zero element of the vector X2
to be positive. Regardless, = X2 ˜2 is always unique, in this case, because a sign change in X2 translates
to a sign change in ˜2 so these sign changes cancel out in .

15

It is worth noting that the solution for

described in the proof has a counterintuitive

economic interpretation: The SDF puts zero weight on the linear combinations of the risk
factors that are correlated with returns, and all its weight on linear combinations of the risk
factors that are uncorrelated with returns:
cov(Re , m ) =

cov(Re , f )Y2 ˜2 =

cov(Re , f ) =

cov(Re , f Y1 )0
{z
}
|
6=0

When rD = k the solution for

cov(Re , f Y2 ) ˜2 = 0.
{z
}
|
=0

described in the theorem is unique, otherwise it is not.

We already know, from Theorem 1, that when rD = k any solution to µR = D is unique.
When rC < rD = k, Lemma 1 implies that rC = k 1 so that ˜2 = 1/(µ0 Y2 ) is a unique
f

scalar.

5

Finally we have the case where rC = rD < k. In this case, neither normalization of
the SDF is identified. If rC = rD = 0 we have C = D = 0, and there are no solutions to
equations (10) and (12). The only valid m has the form m =

f 0 b with µ0f b = 0 so it is

both mean zero and has a zero intercept. When 0 < rC = rD < k there may or may not be
solutions to equations (10) and (12).

3

Estimation and Inference using GMM

3.1

The intercept-normalization

Using the n moment restrictions given by (10), is estimated using GMM. Define ut ( ) =
P
Rte (1 ft0 ) and let gT ( ) = T1 Tt=1 ut ( ) = R̄e DT be an n ⇥ 1 vector of pricing errors,
P
P
where R̄e = T1 Tt=1 Rte , DT = T1 Tt=1 Rte ft0 and T is the sample size.

I consider GMM estimators that set aT gT = 0, where aT is a k ⇥ n matrix and takes the

form aT = DT0 WT , where WT is an n ⇥ n positive definite weighting matrix. It follows that
the GMM estimator of

is

ˆ = D0 W DT
T
T

1

DT0 WT R̄e .

(15)

In the first GMM step I let WT = In . In step j + 1, I let WT = (ST ) 1 where ST =
PT
1
0
e
ft0 ˆj ) and ˆj represents the jth-step estimator of .6
t=1 ûjt ûjt , ûjt = Rt (1
T
Let

T

=

DT . An OIR, or pricing error, test is based on the statistic
J = T gT ( ˆ)0 (V̂g )+ gT ( ˆ),

5
6

Uniqueness requires an argument analogous to the one provided in footnote (4).
By computing ST in this way, I impose the theoretical restriction that Et j (ut ) = 0 for all j

16

(16)
1.

where
V̂g = AT ST (AT )0

with AT = In

T (aT

T)

1

aT ,

(17)

and (V̂g )+ indicates the generalized inverse of the matrix V̂g .

3.2

The mean-normalization

Using the n moment restrictions given by (12) along with the moment condition E(ft µf ) =
µ0f )0 for the combined
P
✓
(✓) = T1 Tt=1 u✓1t (✓) =
parameter vector. Define u✓1t (✓) = Rte [1 (ft µf )0 ] and let g1T
P
✓
(✓) = T1 Tt=1 u✓2t (✓) = f¯ µf , where
R̄e (DT R̄e µ0f ) . Define u✓2t (✓) = ft µf and let g2T
P
f¯ = 1 T ft . Define u✓ = ( u✓0 u✓0 )0 and g ✓ = ( g ✓0 g ✓0 )0 .
and µf are estimated using GMM. I use the notation ✓ = (

0,

T

t=1

t

1t

2t

T

1T

0

2T

I consider GMM estimators that set a✓T gT✓ = 0, where a✓T is a 2k ⇥ (n + k) matrix and

takes the form

a✓T
where CT = DT

=

✓

CT0 WT✓ 0
0
Ik

◆

,

(18)

R̄e f¯0 and WT✓ is an n ⇥ n positive definite weighting matrix. It follows

that the GMM estimators of

and µf are

ˆ = (CT0 WT✓ CT ) 1 CT0 WT✓ R̄e

(19)

µ̂f = f¯.

(20)

In the first GMM step I let WT✓ = In . In step j + 1, I let WT✓ = (PT ST✓ PT0 )

1

where PT =

[ In R̄e ˆj0 ], ˆj represents the jth-step estimator of and ST✓ is a consistent estimator of
P
✓ ✓0
7
✓
S ✓ = +1
j= 1 E(ut ut j ). Because u2t may be serially correlated I use a VARHAC estimator,
described in more detail in the Appendix, to compute ST .8
Let
✓
T

An OIR test is based on

✓

CT R̄e ˆ 0
0
Ik

◆

.

(21)

ˆ 0 (V̂ ✓ )+ g ✓ (✓),
ˆ
J ✓ = T gT✓ (✓)
g
T

(22)

=

7

The first step of the GMM procedure is numerically equivalent (in terms of pricing errors) to using the
two-pass regression method and running the cross-sectional regression with no constant. In the later GMM
steps, Cochrane (2005) suggests using the matrix ( In 0n⇥k ) in place of PT in the expression for WT .
This is less efficient in terms of the covariance matrix of ˆ , but is asymptotically equivalent in terms of the
OIR test.
8
The VARHAC estimator that I use imposes the theoretical restriction that Et j (u✓1t ) = 0 for all j 1.
This restriction does not hold for u✓2t since the risk factors may be serially correlated. The VARHAC
procedure whitens u✓2t using a VAR, but is otherwise identical to a standard HAC procedure.

17

where
V̂g✓ = A✓T ST✓ (A✓T )0

with A✓T = In+k

✓
✓
T (aT

✓
1 ✓
T ) aT .

(23)

Yogo (2006) uses a di↵erent, optimal, GMM procedure in conjunction with the mean✓
with respect to µf is non-zero, he uses a
normalization. Noting that the derivative of g1T

variant of a✓T that is not block diagonal because this improves asymptotic efficiency. In his
case µ̂f does not, in general, equal f¯. As it turns out, in finite samples, the properties of
Yogo’s procedure are quite di↵erent than the properties of the procedure I have outlined here.
Peñaranda and Sentana (2014) discuss Yogo’s procedure and normalizations of the SDF in
the context of continuously-updated (CU)-GMM estimators. I discuss the optimal-GMM
and CU-GMM procedures in more detail in a separately available appendix.

3.3

Asymptotic properties of the estimators

Let rC and rD be defined as in Section 2. If rD = k inference is standard for the interceptnormalization. If there exists a solution, , to equation (10), it is unique. Under regularity
p
a.s.
conditions provided in Hansen (1982), ˆ ! where is that unique solution. Also T ( ˆ
d

) 1 a S a 0 [(a

) ! N (0, V ) with V = (a

limits of aT and

T.

d

Finally, J !

)0 ] 1 , where a and

are the probability

2
n k.

Similarly, if rC = k inference is standard for the mean-normalization. If there exists a
solution, , to equation (12), it is unique. Under regularity conditions provided in Hansen
a.s.
(1982), ✓ˆ ! ✓, where ✓ = ( 0 µ0f )0 and is the unique solution to equation (12). Also
p
d
T (✓ˆ ✓) ! N (0, V✓ ) with V✓ = (a✓ ✓ ) 1 a✓ S ✓ a✓0 [(a✓ ✓ )0 ] 1 , where a✓ and ✓ are the
probability limits of a✓T and

✓
T.

d

Finally, J ✓ !

2
n k.

When the rank conditions are satisfied but there do not exist solutions to equations (10)
and (12) the model is misspecified. We can refer to Hall (2005), Chapter 4 for the asymptotic
properties of ˆ, J , ✓ˆ and J ✓ in this case.
When rD < k the intercept-normalization is under-identified. Therefore, ˆ and J have
non-standard asymptotic distributions. Similarly, when rC < k the mean-normalization is
under-identified. Therefore, ˆ and J ✓ have non-standard asymptotic distributions.
Kan and Zhang (1999a) consider SDF models with factors that are assumed to be mean
zero, so their findings are directly relevant for the mean-normalization. They derive the
asymptotic properties of Wald tests applied to the elements of ˆ and the OIR test for the
case where rC < k. Assume that there is a valid SDF model with risk factors ft . Suppose that
18

a researcher estimates a model with (ft , xt ) as the conjectured factors, where xt is completely
“useless” and mean zero.9 This model is correctly specified, in the sense that it nests the
true model, and it actually doesn’t matter what coefficient ends up attached to the spurious
factor, xt . Because xt is uncorrelated with Rte it has no e↵ect on the pricing equation. On
the other hand, suppose a researcher estimates a model with (f1t , xt ) as the conjectured
factors, where f1t is an incomplete subset of ft . In this case, the model is misspecified. The
asymptotic properties of the tests studied by Kan and Zhang (1999a) depend on whether the
model is correctly specified or misspecified. Additionally, the extent of the misspecification
matters and how many steps are taken over the GMM weighting matrix also matters. For
example, using a model design with two true factors, Kan and Zhang (1999a) show that a
model that uses only the spurious factor xt will be rejected with probability less than the size
of the test when a 2nd-step GMM test is used, but with probability greater than 0.9 when
a 3rd-step GMM test is used. On the other hand, when one of the true factors is included
along with the spurious factor, the power of the 2nd-step GMM test improves, while the
power of the 3rd-step GMM test does not.
One interesting case is when rC = k
rC = k

1 and rD = k. As we saw in Section 2, when

1 and rD = k there is always a solution, , to equation (10). So, in large samples,

tests of the model based on the intercept-normalization will reject it with the same probability
as the size of the test, say 5% of the time. But, in this situation, as discussed in Section 2,
the solution to equation (10) implies a zero-mean SDF. As mentioned above, this solution
puts no weight on economically relevant risk factors, and, instead, puts all its weight on a
“useless” linear combination of the factors. The OIR test is not useful for detecting this
problematic feature of the solution, because, strictly speaking, the zero-mean SDF is valid
in this case. Kan and Zhang’s (1999a) results tell us that the mean-normalization can also
run into trouble, especially when the model is tested in the first or second step of GMM.
Symmetrically when rC = k and rD = k

1 there is always a solution, , to equation

(12). So, in large samples, tests of the model based on the mean-normalization will reject it
with the same probability as the size of the test, say 5% of the time. It is not entirely clear
what the properties of the intercept-normalization will be in this case, but they are likely
non-standard given that

is not identified.

9

Kan and Zhang (1999a) define a useless factor as one that is independent of Rte and ft at all leads and
lags.

19

3.4

Testing identification

In the introduction I discussed tests of the rank conditions proposed by Cragg and Donald
(1997) and Kleibergen and Paap (2006). The Kleibergen and Paap (2006) statistic has a
computational advantage because it does not involve nonlinear optimization whereas the
Cragg and Donald (1997) test does. Instead, it only involves forming the singular value
decomposition (SVD) of a matrix, giving it a computational advantage in Monte Carlo
experiments. For this reason, I focus the rest of my discussion on the Kleibergen and Paap
(2006) statistic.
Suppose we have an n ⇥ k matrix ⇧, and the null hypothesis is that rank(⇧) = r < k.
ˆ Kleibergen and Paap (2006)
Rather than directly test the rank of ⇧ using some estimate, ⇧,
suggest forming the scaled matrix ⇥ = G⇧F 0 where Gn⇥n and Fk⇥k are invertible matrices
that make ⇥ invariant to invertible transformations of the data. When ⇧ = cov(Re , f )
natural choices are G = ⌃R
matrix of Re , and F = ⌃f

1/2

1/2

, the Cholesky decomposition of the inverse of the covariance

, the Cholesky decomposition of the inverse of the covariance

matrix of f .
The next step is to form the SVD ⇥ = U SV 0 , where the upper k ⇥ k block of S is

a diagonal matrix with the singular values of ⇥ arranged from largest to smallest. When
rank(⇧) = rank(⇥) = r < k, the k

r smallest singular values ⇥ are zero. Therefore,
ˆ and finds its k r smallest singular values.
the rank test forms the SVD of an estimate ⇥
The test statistic measures the size of these singular values, in a statistical sense, relative to
zero. The Appendix provides details of the calculation of the statistic, denoted rk(r), which
converges asymptotically to a

3.5

2
(n r)(k r)

distribution.

A procedure for model reduction

Suppose that a researcher has a candidate model based on the k-dimensional risk factor, f . I
propose the following sequential procedure for testing the model’s rank, possibly reducing the
dimension of the model, estimating the model, and performing an OIR test. The procedure
assumes that a researcher has a favored size, denoted ↵, to be used in the OIR test. Model
reduction, estimation and evaluation takes place after a separate procedure determines the
dimension of the model the researcher will actually estimate. The model dimension procedure
can be described as a loop as follows.

20

Step 0. Test whether rC = 0 using the rk(0) statistic.
If the p-value associated with the statistic is greater than ↵, discard the model and
proceed no further. If the p-value associated with the statistic is less than or equal to ↵
continue to step 2.
Step 1. Test whether rC = 1 using the rk(1) statistic.
If the p-value associated with the statistic is greater than ↵, break out of the loop and
invoke the model reduction procedure described below. If the p-value associated with the
statistic is less than or equal to ↵ continue to step 3.
The procedure continues, like this, until ...
Step k

1. Test whether rC = k

1 using the rk(k

1) statistic.

If the p-value associated with the statistic is greater than ↵, break out of the loop
and invoke the model reduction procedure described below. If the p-value associated with
the statistic is less than or equal to ↵ proceed to model estimation without reducing the
dimension of the model.
The model reduction procedure is invoked if the p-value associated with rk(r) is greater
than ↵, and is a by-product of the construction of the statistic. To see how it works consider
the following linear combination of the risk factors: f˜ = Af with A = V 0 F , where V is the
1/2
the matrix from the SVD of ⇥, defined above, and F = ⌃f . The covariance matrix of f˜ is
˜ f = V 0 V = Ik , implying that f˜ is a vector of mutually orthogonal factors with unit variance.
⌃

˜ = GC̃(⌃
˜ 1/2 )0 = GCF 0 V = U SV 0 V = U S
Also C̃ ⌘ cov(Re , f˜) = CF 0 V . It follows that ⇥
˜ and ⇥ have the same singular values. When rC = r, only the first r elements
implying that ⇥
of f˜ are relevant factors. Hence the procedure suggests reducing the model to one in which
f˜r = Ar f is a r ⇥ 1 vector of risk factors. Here Ar is a r ⇥ k matrix representing the first

r rows of the matrix A, or, equivalently, Vr0 F , where Vr represents the first r columns of V .
In the actual procedure all of the matrices are replaced with their sample equivalents.10

Model estimation proceeds in the standard way using either the mean-normalization or
the intercept-normalization. The OIR test is conducted in a standard way but has n

r

degrees of freedom. If the researcher wishes to recover the parameters associated with the
original factors this is straightforward. The SDFs for the two normalizations can be written
10

I discuss the implications of the non-uniqueness of the SVD of ⇥ in the Appendix.

21

as
m

= 1

f˜r0 ˜ = 1

m

= 1

(f˜r

f 0 (A0r ˜),

µ̃f )0 ˜ = 1

(f

so the parameters attached to the original factors are

µf )0 (A0r ˜ ),
= A0r ˜ and

= A0r ˜ .

This is obviously a simple procedure, but one might ask whether it has reasonable size
and power properties. Consider size first. The size calculation depends on the true value of
rC , which I treat as an unknown.
If rC = k the p-value associated with the test in steps 0 through k

1 should converge

to zero 0 in sufficiently large samples. This is because the null hypotheses being tested are
not local to the alternative, which is that cov(Re , f ) has full column rank. Hence, if rC = k
the probability of rejecting the model should be ↵, the size of the OIR test.
By the same argument, if rC = k
through k
rC = k

1 the p-value associated with the test in steps 0

2 should converge to zero 0 in sufficiently large samples. At step k

1, however,

1 will be rejected with a probability that limits to ↵. So, with probability 1

↵ the

model reduction procedure will be invoked, and the probability of rejecting the model based
on the OIR test should be ↵. On the other hand, with probability ↵ the model reduction
procedure is not invoked. In this case, the probability of rejecting the model based on the
OIR test will be some ↵
˜k

1

6= ↵ because inference is non-standard for models with spurious

factors. Therefore, overall, the probability of rejecting the model is ↵k

1

= (1 ↵)↵ + ↵↵
˜k 1.

Propositions 3 and 4 in Kan and Zhang (1999a) suggest that for the mean-normalization the
probability of rejecting a correctly specified model that includes a spurious factor is less than
↵. This suggests that 0  ↵
˜k
the case that ↵k

1

< ↵(2

1

< ↵ and that (1

↵).

↵)↵  ↵k

1

< ↵ but it must certainly be

A similar argument can be used to establish that if rC = r, the probability of invoking
the model reduction procedure at step r is 1

↵, in which case the probability of rejecting

the model based on the OIR test would be ↵. But with probability ↵ it would not be invoked
and the researcher would move on to step r + 1. Again, this suggests that the probability
of rejecting the model is ↵r = (1

↵)↵ + ↵↵
˜ r , where ↵
˜ r is the probability of rejecting the

larger model at a later step. Again, the results in Kan and Zhang (1999a) are suggestive
that 0  ↵
˜ r < ↵ but certainly ↵r < ↵(2

↵).

In terms of power, consider a misspecified model. If the C matrix associated with this

model has full rank this should be revealed with probability one in large samples. Therefore,
22

the model reduction procedure will not be invoked and the model should be rejected with
the same probability as it would be using standard GMM procedures. The main advantage
of the procedure is that it should improve the detection of invalid models for which C has
less than full rank because the model will often be reduced in dimension to one where the
parameters are fully identified and inference is standard. Additionally, the procedure is
designed to discard spurious risk factors, or reject models based entirely on them.
In Section 5 I use Monte Carlo experiments to explore the size and power properties of
the model reduction procedure.

4

Empirical Findings

In Table 1 I present the results of rank tests for a variety of models taken from the literature.
In this section I discuss two of these models in more detail: the Fama and French (1993)
Three Factor model and the Yogo (2006) model. These models are also used as the inspiration
for the Monte-Carlo experiments that I run later in the paper. Estimates of the model
parameters and the results of OIR tests are presented in Table 2. I use quarterly data over
the period 1949Q1–2012Q4 to estimate both models. For Re I use the excess returns of the
25 portfolios of U.S. stocks sorted on size and the book-to-market value ratio introduced by
Fama and French (1993).11

4.1

The Fama-French Three Factor Model

The Fama-French model uses three factors: (1) the excess return on the value-weighted U.S.
stock market (Mkt-Rf), (2) the return di↵erential between a portfolio of small firms and a
portfolio of large firms (SMB) and (3) the return di↵erential between a portfolio of high-value
firms and a portfolio of low-value firms (HML), where a firm’s value is measured as the ratio
of its book value to its market capitalization.
As we saw in Table 1, the rank tests strongly reject the null hypotheses that this model
has reduced rank. It appears that rC = rD = 3. Table 2 indicates that the statistical
significance of

and

and the results of the OIR tests (which are all strong rejections) are

quite similar across the two normalizations. Additionally, Table 2 shows that if the first
step GMM estimates of
11

obtained with intercept-normalization are mapped to equivalent

The data are described in more detail in the Appendix.

23

values of

these estimates are quite similar to the estimates of

obtained with the mean-

normalization. However, this is not true for iterated GMM.

4.2

Yogo (2006)

Yogo (2006) proposes a consumption-based model in which agents have recursive preferences over a consumption bundle of nondurable and durable goods. When this model is
approximated with a linear SDF, the risk factors are the log-growth rate of real per capita
consumption of nondurables and services ( cns ), the log-growth rate of the real per capita
consumption of durables ( cd ) and the return on wealth, which is proxied with the real
return on the value-weighted U.S. stock market (Mkt).
From the results of the rank tests shown in Table 1 it would be tempting to conclude
that rC = 1 and rD = 2 for Yogo’s model. If this were true in population, then any valid
parameterization of the model would have a zero mean, and would put all its weight on
spurious linear combinations of the factors.
As Table 3 shows, if we rely on the intercept-normalization we get statistically significant
estimates of

for

cd (for first step and iterated GMM) and

cns (for iterated GMM). The

model also passes the OIR test with flying colors.
On the other hand, if we use the mean-normalization we draw di↵erent conclusions. For
first step GMM, none of the parameter estimates are statistically significant, but the model
is not rejected by the OIR test. For iterated GMM, the

parameters for

cd and

cns are

statistically insignificant, but the coefficient associated with Mkt is marginally significant.
The model is sharply rejected by the OIR test. It is tempting to attribute the qualitatively
di↵erent inference across normalizations to the fact that rC < rD .
This model appears to be a candidate for reduction to a single factor. Using the procedure
outlined in Section 3.5, we end up with the following single factor: f˜1 = 1.47 · cns + 2.72 ·
cd + 12.1·Mkt. This factor has a correlation of 0.9999 with Mkt.12 The two consumption

factors are, roughly speaking, excluded by the model reduction procedure, and we end up
with a model very similar to the CAPM, except that we have Mkt instead of Mkt-Rf as
the risk factor. As Table 3 indicates, once the model is reduced in dimension the model is
sharply rejected based on the OIR test for both normalizations.
As described above, the model reduction procedure uses V10 F , where V1 is the first column of V in the
0
F , using the remaining columns of V these linear combinations
SVD of ⇥. As it turns out, if we form V2:k
put almost no weight on Mkt, and are almost uncorrelated with it.
12

24

5

A Monte Carlo Experiment

To further demonstrate the sensitivity of empirical results to the choice of normalization in
the presence of failure of the rank conditions, I conduct Monte Carlo experiments. These
experiments also provide evidence on the performance of the model reduction procedure I
proposed in Section 3.5.

5.1

The True Model, Some Misspecified Models, and an Overspecified Model

Here, the true model is calibrated to resemble the Fama-French three-factor model estimated
using U.S. data over the sample period 1949:Q1–2012:Q4. The model is used to generate an
n ⇥ 1 (with n = 25) vector of artificial excess returns Rte with E(Re ) equalling the model-

predicted expected returns in the data. Details on how the simulated data are generated are
provided in the Appendix.
Using the data generated from the model, we estimate several test models in 10000
artificial samples of 256 observations (the size of our U.S. data sample):
1. The true model, which includes the artificial Mkt-Rf, SMB and HML factors. This
model has full rank and is correctly specified, by construction.
2. A model that includes the artificial Mkt-Rf factor. This factor is relevant, in the sense
that it has non-zero covariance with the returns generated within the experiment. So
it satisfies the rank conditions for identification. However, the model is misspecified
because the Mkt-Rf factor, alone, cannot price returns accurately.
3. A model with a single spurious (uncorrelated with returns) factor, St , whose behavior
somewhat mimics durable consumption growth in U.S. data. I assume that St ⇠

N iid(µs ,

2
s)

and set µS and

2
S

equal to the sample mean and variance of durable

consumption growth in the data. In this model, C has reduced rank (0), but D has
full rank (1).
4. A model with two factors: Mkt-Rf and S. In this model, C has reduced rank (1), but
D has full rank (2).
5. The last model is an over-specified model which includes the Mkt-Rf, SMB and HML
factors, as well as the spurious factor, S. In this model C and D are n ⇥ 4 but both
25

have reduced rank (3). The model is not misspecified, because it nests the true model.
The presence of S, however, a↵ects estimation and inference.

5.2

Rank Tests

I first explore the performance of the Kleibergen and Paap (2006) rank tests. Table 4 shows
that the rank tests never understate the rank of C. For models 1 and 2, which have full rank,
the tests reject the null hypothesis of reduced rank 100% of the time. For model 4, where
rC = 1, the test always rejects the null that rC = 0. For model 5, where rC = 3, the test
always rejects the null that rC  2. The tests do sometimes overstate the rank of C in the
sense that for model 3, where rC = 0, the test rejects the null that rC = 0 12.1% of the time

(when size is set at 5%). For model 4, where rC = 1, the test rejects the null that rC = 1
11.9% of the time (when size is set at 5%). For model 5, where rC = 3, the test rejects the
null that rC = 3 11.4% of the time (when size is set at 5%).
For models 1 and 2, where D has full rank, the tests reject the null hypothesis of reduced
rank 100% of the time. For model 3, where D also has full rank the rank test rejects the
null 92.5 of the time (when size is set at 5%). For model 4, where D also has full rank the
rank test rejects the null 69.8% of the time (when size is set at 5%).13 For model 5, where
rD = 3 the rank tests reject the null that rD = 3 10.9% of the time (when size is set at 5%).
In extended results presented in the Appendix, I show that the rank tests become close
to perfect in 10000 simulated samples each of which has 10000 observations. When the
null hypothesis of reduced rank is false, the tests reject the null 100% of the time, as I
conjectured in Section 3.5. When the null hypotheses of reduced rank are true, I find the
tests have almost exactly their asymptotic size.

5.3

Estimating the True Model

Table 5 shows that for both normalizations the parameter estimates are centered near the
true values of the parameters. For the factors that play the biggest role in pricing the
returns in the model (the pseudo Mkt-Rf and HML factors) the parameters are statistically
significant in almost all samples. The parameter associated with the pseudo-SMB factor is
usually not significant, consistent with it playing a very small role in pricing the returns.
The OIR test usually does not reject the model, with size being slightly excessive for the
13

Rank tests on D play no role in the model reduction procedure that I described earlier.

26

mean normalization and very slightly reduced for the intercept normalization. In extended
results presented in the Appendix, I show that OIR tests have almost exactly the correct size
in 10000 simulated samples each of which has 10000 observations. The qualitative di↵erence
between the results for the two normalizations disappears.

5.4

Estimating Model 2

Model 2 is misspecified. The proposed risk factor, Mkt-Rf, is relevant but insufficient to price
the assets. Not surprisingly, as Table 6 shows, for both normalizations we find parameter
estimates to be significant in a very large fraction of the samples. Additionally, for both
normalizations the model is rejected roughly 70% of the time using an OIR test with size
set at 5%. In extended results presented in the Appendix, I show that these rejection rates
rise to 100% in 10000 simulated samples each of which has 10000 observations.

5.5

Estimating Model 3

Model 3 uses a single spurious factor. The intercept and mean-normalizations behave di↵erently because the intercept-normalization is identified and (strictly speaking) valid, while the
mean-normalization is neither. Standard asymptotics apply to the intercept-normalization,
but not to the mean-normalization. This is reflected in the results shown in Table 7.
In the simulations the inverse of the mean of the spurious factor, S, is µS 1 = 102. As
expected, therefore, the estimates of

are centered near this value, and are frequently (in

this case, always) statistically significant. Additionally, because the pricing equations for
the intercept-normalization are satisfied at µS 1 , the model is rejected very infrequently, with
small sample size being close to the asymptotic size of the OIR test.
On the other hand, when the mean-normalization is used, the parameter

S

is statistically

significant in a much smaller fraction of the samples. At the first and second steps of GMM
the OIR test has very weak power to reject the model. Paradoxically, the power of the OIR
tests rises sharply with further iterations over the weighting matrix, while, at the same time,
the tendency of ˆS to be statistically significant rises to 22% (when size is set at 5%). The
remarkably di↵erent behavior of ˆS and its associated t-statistic across the GMM steps can
be better understood with reference to Figure 6. When the identity matrix is used to weight
the pricing equations the distribution of ˆS is very wide and bimodal. This bimodality is
shared by the distribution of the t-statistic but its tails are not that thick. In later GMM

27

steps, where a non-identity weighting matrix is used, the distribution of ˆS narrows and
is unimodal. But with sufficient iterations over the weighting matrix it has very fat tails.
Figure 7 shows that the distribution of ˆS widens when the sample size in the Monte Carlo
experiments is increased to 10000. This reflects the fact that ˆS is unidentified.14 As it turns
out, the behavior of the t-statistic also worsens, with it exhibiting very fat tails at the first
GMM step and after many iterations over the weighting matrix.
The model reduction procedure I described in Section 3.5 suggests discarding the model
if the null hypothesis that rC = 0 cannot be rejected at the 5% level of significance. The
results in Table 10 show that this null is rejected in only 12.1% of the repeated samples.
So the model reduction procedure would yield a 87.9% rejection rate of the model based on
the rank test alone. For the procedure as a whole—in which the model is estimated and
tested if the model passes the rank test—the rejection rate rises to 88.4% for first and second
step GMM and 98.5% for iterated GMM if the mean-normalization is used to estimate the
model. The corresponding rejection rates for the intercept-normalization are 89.2% and
93.2%. When I increase the sample size in the simulations to 10000 observations, these
rejection rates rise to 95.1% and 100% for the mean-normalization, and 98.5% and 99.0%
for the intercept-normalization.

5.6

Estimating Model 4

Model 4 uses the Mkt-Rf factor and the spurious factor, S. Strictly speaking, the interceptnormalization is valid. However it is valid when no weight is put on the relevant factor,
Mkt-Rf, and all the weight in the SDF is on the spurious factor S. The mean normalization
is misspecified.
In Table 8 we see that, as for Model 3, the estimates of

S

are centered near µS 1 = 102,

and are always statistically significant. The typical estimate of

Mkt Rf ,

on the other hand,

is small and significant in a much smaller fraction of the samples. Additionally, because
the pricing equations for the intercept-normalization are satisfied at (0, µS 1 ), the model is
rejected very infrequently, with small sample size being close to the asymptotic size of the
OIR test.
On the other hand, when the mean-normalization is used, ˆMkt

Rf

is typically quite large

(similar to the values we obtained for Model 2) and significant in a large fraction of the
14

T

In an earlier draft I showed that with the data generating process used in my Monte Carlo simulations,
ˆS has an asymptotic distribution so, in a sense, we expect the domain of ˆS to widen at the rate T 1/2 .

1/2

28

samples, while

S

is statistically significant in a smaller fraction of the samples. Once again,

however, at the first and second steps of GMM the OIR test has quite weak power to reject
the model. Paradoxically, the power of the OIR tests rises sharply with further iterations
over the weighting matrix, while, at the same time, the tendency of ˆS to be statistically
significant rises to 16.5% (when size is set at 5%).
The model reduction procedure I described in Section 3.5 suggests rejecting the model if
the null hypothesis that rC = 0 cannot be rejected at the 5% level of significance. However,
as the results in Table 4 show this null is rejected in every repeated sample. The model
reduction procedure also suggests reducing the model to a single factor if the null hypothesis
that rC = 1 cannot be rejected at the 5% level of significance. This occurs in 88.1% of
the samples, as indicated in Table 10. For the procedure as a whole the model rejection
rate rises to 64.3% for first and second step GMM and 70.8% for iterated GMM. This is
an improvement over the 16.8% and 62.4% rejection rates for the procedure without model
reduction. The corresponding rejection rates for the intercept-normalization are 60.7% and
63.8%. When I increase the sample size in the simulations to 10000 observations, these
rejection rates rise to 97.0% and 100% for the mean-normalization, and 98.8% and 99.3%
for the intercept-normalization.
When the model reduction procedure is invoked, it chooses linear combinations of the
two factors that put most of their weight on Mkt-Rf and are highly correlated with it. As
the sample size increases, the procedure limits to choosing Mkt-Rf as the single factor, since
S is uncorrelated with returns, by construction.

5.7

Estimating Model 5

Model 5 uses the factors from the true model (Mkt-Rf, SMB and HML) as well as the
spurious factor, S. This model is not misspecified because it nests the true model, but it has
reduced rank for both normalizations: rC = rD = 3.
In Table 9 we see that adding the spurious factor changes the small sample size of
the OIR test. The under-rejection we observed in Table 5 for the intercept-normalization
becomes more exaggerated here. The over-rejection we observed for the mean-normalization
almost vanishes. But, as in the other cases where a spurious factor is included in the
model, the normalizations di↵er sharply in terms of coefficient estimates. For the interceptnormalization estimates of

Mkt Rf

and

HML

are not centered near their true values, and they

29

are less often found to be statistically significant compared to the case where the spurious
factor is excluded from the model. On the other hand, estimates and inference regarding
Mkt Rf

and

HML

are almost una↵ected by the inclusion of the spurious factor.

The model reduction procedure I described in Section 3.5 suggests reducing the model
to three factors when the null hypothesis that rC = 3 cannot be rejected at the 5% level
of significance. As the results in Table 10 show, this occurs in 88.6% of the samples. In
these samples, the OIR test is performed using the model with three factors instead of four.
As a consequence, for the mean-normalization the model rejection rate rises to 7.0% for
first and second step GMM and 6.9% for iterated GMM. This compares to 5.1% and 5.6%
rejection rates for the procedure without model reduction. The corresponding rejection
rates for the intercept-normalization are 5.5% and 5.4%. When I increase the sample size
in the simulations to 10000 observations, these rejection rates are 5.0% and 5.0% for the
mean-normalization, and 6.1% and 6.2% for the intercept-normalization.

5.8

Indirect Estimates of Model Parameters

If we use the intercept-normalization, GMM produces an estimate, ˆ. This estimate can be
mapped into an indirect estimate of if we use the formula ( ˆ) = ˆ/(1 f¯0 ˆ). Similarly,
if we use the mean-normalization, GMM produces an estimate, ˆ . This estimate can be
mapped into an equivalent estimate of if we use the formula (ˆ ) = ˆ /(1 + f¯0 ˆ ). Is the
asymptotic distribution of ˆ similar to that of (ˆ )? How about ˆ and ( ˆ)?
The Appendix provides extensive evidence on this question for Monte Carlo experiments
with a very large number of observations (10000). These experiments suggest that when
the model is true and identified (i.e., Model 1), the direct and indirect estimates have very
similar distributions in large repeated samples.
When the model is misspecified (Model 2) the direct and indirect estimates, in general,
have di↵erent probability limits. They also have di↵erent distributions around these limits.
This is confirmed by the Monte Carlo evidence, although the distributions look quite similar
at the first GMM step.
When the model includes a spurious factor (Models 3, 4 and 5), or, equivalently, the
mean normalization is poorly identified (both normalizations are underidentified for Model
5), the direct and indirect estimates of

and

have very di↵erent looking distributions, even

for factors that belong in the model. The presence of spurious factors matters.

30

6

Conclusion

When excess returns are used to estimate linear SDFs, GMM estimation requires that a normalization of the SDF be adopted. Two standard normalizations of the SDF, the interceptnormalization and the mean-normalization are equivalent in population, in the sense that
they are proportional up to a constant, when the model is valid and fully identified. However,
the conditions under which these normalizations are identified are, in general, di↵erent.
In practice, di↵erent normalizations sometimes lead to very di↵erent qualitative inferences
about a model. Estimates of the slope coefficients of the SDF can di↵er sharply in terms
of magnitude and statistical significance. OIR tests can di↵er sharply in outcome. I have
demonstrated this, here, for several factor models fit to U.S. data. I interpret these di↵erences
as the consequence of failures of the rank conditions for identification.
It is well known that lack of identification a↵ects inference. But here, there is an additional problem that the rank conditions for identification can fail di↵erentially across model
normalizations. In particular, I establish that when the mean-normalization is less identified
than the intercept-normalization, there is always at least one valid parameterization of the
proposed SDF, and any valid SDF is mean zero and uncorrelated with returns. Also, when
the intercept-normalization is less identified than the mean-normalization, there is always at
least one valid parameterization of the proposed SDF, and any valid SDF has zero intercept.
I propose model diagnostics and a model reduction procedure based on Kleibergen and
Paap (2006)’s rank test. I argue that these tests should have desirable asymptotic size and
power properties. In a Monte Carlo experiment I show that these rank tests are, indeed, a
powerful diagnostic. My preliminary evidence also suggests that the model reduction method
increases the power of asset pricing tests, while having little undesired e↵ect on their size.

31

References
Bryzgalova, Svetlana, 2014, Spurious factors in linear asset pricing models, Working Paper,
London School of Economics.
Burnside, Craig, 2011, The cross section of foreign currency risk premia and consumption
growth risk: Comment, American Economic Review 101, 3456–3476.
Chen, Nai-Fu, Richard Roll, and Stephen A. Ross, 1986, Economic forces and the stock
market, The Journal of Business 59, 383–403.
Cochrane, John H., 2005, Asset Pricing (Princeton University Press) revised edn.
Cragg, John G., and Stephen G. Donald, 1997, Inferring the rank of a matrix, Journal of
Econometrics 76, 223–250.
den Haan, Wouter J., and Andrew T. Levin, 2000, Robust covariance matrix estimation with
data-dependent VAR prewhitening order, NBER Technical Working Paper No. 255.
Fama, Eugene F., and Kenneth R. French, 1993, Common risk factors in the returns on
stocks and bonds, Journal of Financial Economics 33, 3–56.
Fama, Eugene F., and James D. MacBeth, 1973, Risk, return, and equilibrium: Empirical
tests, Journal of Political Economy 81, 607–636.
Ferson, Wayne E., and Campbell R. Harvey, 1993, The risk and predictability of international
equity returns, The Review of Financial Studies 6, 527–566.
Gibbons, Michael R., 1982, Multivariate tests of financial models: A new approach, Journal
of Financial Economics 10, 3–27.
Gospodinov, Nikolay, Raymond Kan, and Cesare Robotti, 2012, Further results on the limiting distribution of gmm sample moment conditions, Journal of Business & Economic
Statistics 30, 494–504.
, 2013a, Chi-squared tests for evaluation and comparison of asset pricing models,
Journal of Econometrics 173, 108–125.
, 2013b, Misspecification-robust inference in linear asset pricing models with irrelevant risk factors, Federal Reserve Bank of Atlanta Working Paper 2013-9.
, 2014, Spurious fit in unidentified asset-pricing models, Working Paper, Imperial
College, London.
Hall, Alastair R., 2005, Generalized Method of Moments (Oxford University Press).
, and Atsushi Inoue, 2003, The large sample behaviour of the generalized method of
moments estimator in misspecified models, Journal of Econometrics 114, 361–394.
Hansen, Lars Peter, 1982, Large sample properties of generalized method of moments estimators, Econometrica 50, 1029–1054.
32

, and Ravi Jagannathan, 1997, Assessing specification errors in stochastic discount
factor models, The Journal of Finance 52, 557–590.
Hou, Kewei, and Robert L. Kimmel, 2006, On estimation of risk premia in linear factor
models, Working Paper, Ohio State University.
Jagannathan, Ravi, and Yong Wang, 2007, Lazy investors, discretionary consumption, and
the cross-section of stock returns, The Journal of Finance 62, 1623–1661.
Kan, Raymond, and Cesare Robotti, 2008, Specification tests of asset pricing models using
excess returns, Journal of Empirical Finance 15, 816–838.
, 2009, Model comparison using the Hansen-Jagannathan distance, The Review of
Financial Studies 22, 3449–3490.
, and Jay A. Shanken, 2013, Pricing model performance and the two-pass crosssectional regression methodology, Journal of Finance 68, 2617–2649.
Kan, Raymond, and Chu Zhang, 1999a, GMM tests of stochastic discount factor models
with useless factors, Journal of Financial Economics 54, 103–127.
, 1999b, Two-pass tests of asset pricing models with useless factors, The Journal of
Finance 54, 203–235.
Kleibergen, Frank, 2009, Tests of risk premia in linear factor models, Journal of Econometrics
149, 149–173.
, and Richard Paap, 2006, Generalized reduced rank tests using the singular value
decomposition, Journal of Econometrics 133, 97–126.
Kleibergen, Frank, and Zhaoguo Zhan, 2013, Unexplained factors and their e↵ects on second
pass r-squared’s and t-tests, Manuscript, Brown University.
Lettau, Martin, and Sydney Ludvigson, 2001, Resurrecting the (C)CAPM: A cross-sectional
test when risk premia are time-varying, Journal of Political Economy 109, 1238–1287.
Lustig, Hanno, and Adrien Verdelhan, 2007, The cross section of foreign currency risk premia
and consumption growth risk, The American Economic Review 97, 89–117.
Peñaranda, Francisco, and Enrique Sentana, 2014, A unifying approach to the empirical
evaluation of asset pricing models, The Review of Economics and Statistics Forthcoming.
Shanken, Jay, and Guofu Zhou, 2007, Estimating and testing beta pricing models: Alternative methods and their performance in simulations, Journal of Financial Economics 84,
40–86.
Wright, Jonathan H., 2003, Detecting lack of identification in GMM, Econometric Theory
19, 322–330.
Yogo, Motohiro, 2006, A consumption-based explanation of expected stock returns, The
Journal of Finance 61, 539–580.
33

TABLE 1: Tests for Failure of Rank Conditions (p-values)
cov(Re , f )

E(Re f 0 )

# of Factors

Test rank

Sample

Model

k

r

CD

KP

CD

KP

period

CAPM

1

0

0.000

0.000

0.000

0.000

49Q1–12Q4

Fama-French 3-Factor

3

2
1
0

0.000
0.000
0.000

0.000
0.000
0.000

0.000
0.000
0.000

0.000
0.000
0.000

49Q1–12Q4

CCAPM

1

0

0.076

0.076

0.000

0.000

49Q1–12Q4

Durables-CCAPM

2

1
0

0.961
0.017

0.958
0.017

0.024
0.000

0.025
0.000

49Q1–12Q4

Yogo (2006)

3

2
1
0

0.977
0.767
0.000

0.931
0.770
0.000

0.521
0.000
0.000

0.513
0.000
0.000

49Q1–12Q4

Lettau and Ludvigson (2001)

3

2
1
0

0.239
0.040
0.000

0.167
0.004
0.000

0.240
0.004
0.000

0.166
0.000
0.000

52Q1–12Q3

Jagannathan and Wang (2007)

1

0

0.005

0.005

0.000

0.000

1949–2012

Lustig and Verdelhan (2007)

3

2
1
0

0.732
0.818
0.758

0.670
0.728
0.758

0.790
0.640
0.000

0.708
0.629
0.000

1953–2002

Note: For eight models from the literature, the table presents p-values for tests of the null hypothesis H0 : rank(M ) = r where M is an n ⇥ k matrix
[cov(Re , f ) or E(Re f 0 )]. CD and KP indicate tests based on Cragg and Donald (1997), and Kleibergen and Paap (2006). Every case, except Lustig
and Verdelhan (2007), uses the real excess returns to the Fama and French (1993) 25 portfolios sorted on the basis of size and value. For risk factors,
the CAPM uses the real market excess return (Mkt-Rf). The Fama-French 3 factor model uses Mkt-Rf, SMB and HML (expressed in real terms).
The CCAPM model uses nondurable & service consumption growth. The Durables-CCAPM case adds durable consumption growth to the CCAPM.
The Yogo (2006) model adds the real market return (Mkt) to the Durables-CCAPM. The Lettau and Ludvigson (2001) model uses consumption
growth, cay, and the product of consumption growth and cay. The Jagannathan and Wang (2007) model uses data on a Q4-Q4 annual basis, and uses
nondurable consumption growth as the risk factor. The Lustig and Verdelhan (2007) case uses a di↵erent set of returns: Eight currency portfolios
sorted by interest rate, and measured on annual basis. The risk factors are the same as the ones from Yogo (2006), measured at the annual frequency.
34

TABLE 2: GMM Estimates of the Fama & French Three Factor Model
First GMM Step
Factor
Mkt-Rf
SMB

( )
3.27

(0.84)

0.17
(1.19)

HML

5.02

(1.04)

3.69

(1.06)

0.19
(1.34)

5.68

(1.34)

Iterated GMM

J
50.3

(0.001)

J
3.67

(1.06)

0.17
(1.34)

5.57

(1.33)

54.9

(0.000)

( )
5.14

(0.81)

1.21
(1.10)

6.87

(0.95)

J

6.16

(1.19)

1.45
(1.32)

8.24

(1.36)

47.9

(0.001)

J
3.79

(1.01)

54.4

(0.000)

0.76
(1.28)

5.98

(1.18)

Note: This table presents first step GMM estimates and GMM estimates after iterating to convergence over the weighting matrix. For the intercept
normalization is the SDF parameter, with standard errors in parentheses [ ( ) is given by = /(1 µ0f ) with standard errors computed using the
delta method]. J is the OIR test statistic with p-values in parentheses. For the mean normalization is the SDF parameter, with standard errors
in parentheses. The returns used to estimate the model are the excess returns of the Fama and French (1993) 25 portfolios sorted on the basis of size
and value. The risk factors, are Mkt-Rf, SMB and HML. Detailed data descriptions are in the Appendix. Sample period is 1949Q1–2012Q4.

35

TABLE 3: GMM Estimates of the Yogo Model
First GMM Step
Factor
cns
cd

( )
23.7

(33.5)

81.1

Mkt

288

(619)

(1881)

0.50

6.04

(0.55)

J
17.2

(0.749)

987

(19.8)

Iterated GMM
J

234

(204)

6.68

(0.999)

249

45.0

(17.4)

67.6

(236)

(9.78)

2.15

(12)

( )

0.41

(2.91)

(0.43)

387

(377)

J
18.0

(0.709)

581

J
43.5

(42.1)

46.9

(0.002)

50.0

(526)

(36.2)

3.55

1.94

(4.76)

(1.03)

Model Reduction
f˜1

0.25

(0.06)

0.27

(0.08)

66.4

(0.000)

0.27

(0.08)

69.9

(0.000)

0.43

(0.06)

0.49

(0.09)

65.6

(0.000)

0.22

(0.07)

70.3

(0.000)

Implied Parameters
cns

0.37

0.40

0.39

0.63

0.71

0.32

cd

0.68

0.74

0.73

1.18

1.33

0.60

Mkt

3.06

3.29

3.26

5.21

5.91

2.69

Note: This table presents first step GMM estimates and GMM estimates after iterating to convergence over the weighting matrix. For the intercept
normalization is the SDF parameter, with standard errors in parentheses [ ( ) is given by = /(1 µ0f ) with standard errors computed using the
delta method]. J is the OIR test statistic with p-values in parentheses. For the mean normalization is the SDF parameter, with standard errors
in parentheses. The returns used to estimate the model are the real excess returns of the Fama and French (1993) 25 portfolios sorted on the basis of
size and value. The risk factors are nondurable & service consumption growth ( cns ), durable consumption growth, cd , and the real market return
(Mkt). Detailed data descriptions are in Appendix. Sample period is 1949Q1–2012Q4.

36

TABLE 4: Rank Tests in the Monte Carlo Experiment

% rejected at
10% level

% rejected at

5% level

10% level

5% level

100%
100%
100%

100%
100%
100%

100%

100%

20.7%

12.1%

20.2%
100%

11.9%
100%

1. True Model (Factors: Mkt-Rf, SMB, HML) rD = rC = 3
H 0 : rD = 2
H 0 : rD = 1
H 0 : rD = 0

100%
100%
100%

100%
100%
100%

H 0 : rC = 2
H 0 : rC = 1
H 0 : rC = 0

2. Single Relevant Factor (Factor: Mkt-Rf) rD = rC = 1
H 0 : rD = 0

100%

100%

H 0 : rC = 0

3. Single Spurious Factor (Factor: S) rD = 1, rC = 0
H 0 : rD = 0

96.3%

92.5%

H 0 : rC = 0

4. Two Factors (Factors: Mkt-Rf, S) rD = 2, rC = 1
H 0 : rD = 1
H 0 : rD = 0

80.0%
100%

69.8%
100%

H 0 : rC = 1
H 0 : rC = 0

5. Over-specified Model (Factors: Mkt-Rf, SMB, HML, S) rD = rC = 3
H0
H0
H0
H0

:
:
:
:

rD
rD
rD
rD

=3
=2
=1
=0

19.0%
100%
100%
100%

10.9%
100%
100%
100%

H0
H0
H0
H0

:
:
:
:

rC
rC
rC
rC

=3
=2
=1
=0

19.5%
100%
100%
100%

11.4%
100%
100%
100%

Notes: The table reports results of Kleibergen and Paap (2006) rank tests from 10000 Monte Carlo experiments with sample size T = 256. The true
risk factors are synthetic mimics of the Mkt-Rf, SMB and HML factors in U.S. data. Rank tests are performed for five test models that use di↵erent
factors. rC = rank[cov(Re , f )] and rD = rank[E(Re f 0 )], where Re are the returns generated in the experiment, and f is the conjectured vector of
factors. The table reports the fraction of the samples in which these tests reject the null hypothesis when the size of the test is set to 10% and 5%.
Details of the Monte Carlo experiments are provided in the Appendix and main text. Results for the case where T = 10000 are in Appendix Table 1.

37

TABLE 5: Monte Carlo Experiment: Estimation of Model 1 (The True Model)
GMM Step 1

GMM Step 2

% Significant
True

Mean

Iterated GMM

% Significant

% Significant

Median

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

3.28
-0.17
4.97

3.29
-0.17
4.99

99.2
11.3
99.8
9.6

98.3
5.9
99.4
4.3

3.67
-0.14
5.54

3.68
-0.16
5.55

99.7
17.0
99.9
9.6

99.3
10.3
99.7
4.3

3.74
-0.14
5.63

3.75
-0.15
5.65

99.7
18.0
99.9
9.3

99.3
11.4
99.7
4.0

3.77
-0.19
5.70

3.75
-0.19
5.65

99.2
10.4
99.8
13.5

98.1
5.1
99.4
7.1

3.77
-0.19
5.71

3.74
-0.19
5.65

99.4
12.1
99.8
13.5

98.5
6.3
99.4
7.1

3.79
-0.19
5.74

3.75
-0.19
5.68

99.4
12.3
99.8
13.4

98.6
6.6
99.4
7.0

Intercept Normalization
Mkt Rf
SMB
HML

3.25
-0.15
4.92

OIR test
Mean Normalization
Mkt Rf
SMB
HML

OIR test

3.70
-0.17
5.61

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 256. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The true values of the parameters of the SDF are indicated in the first column. The table reports mean
and median estimates of the parameters of the intercept and mean-normalizations, as well as the frequency with which the estimated parameters are
found to be statistically significant at the 10% and 5% levels. The table also reports the frequency with which the model is rejected at the 10% and
5% levels based on the OIR test. The test statistic is numerically identical at the first and second GMM steps. Details of the Monte Carlo experiments
are provided in the Appendix and main text. Results for the case where T = 10000 are in Appendix Table 2.

38

TABLE 6: Monte Carlo Experiment: Estimation of Model 2

GMM Step 1

GMM Step 2

% Significant
Mean

Median

Iterated GMM

% Significant

% Significant

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

3.07

99.0
79.1

98.0
67.7

3.45

3.46

99.3
79.1

98.8
67.7

3.60

3.62

99.3
78.8

98.9
66.9

3.25

98.8
81.2

97.6
70.3

2.85

2.82

97.8
81.2

95.3
70.3

2.78

2.76

97.3
82.2

94.5
71.6

Intercept Normalization
Mkt Rf

3.06

OIR test
Mean Normalization
Mkt Rf

OIR test

3.27

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 256. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The test model includes only the Mkt-Rf factor. The table reports mean and median estimates of
the parameters of the intercept and mean-normalizations, as well as the frequency with which the estimated parameters are found to be statistically
significant at the 10% and 5% levels. The table also reports the frequency with which the model is rejected at the 10% and 5% levels based on the
OIR test. The test statistic is numerically identical at the first and second GMM steps. Details of the Monte Carlo experiments are provided in the
Appendix and main text. Results for the case where T = 10000 are in Appendix Table 3.

39

TABLE 7: Monte Carlo Experiment: Estimation of Model 3

GMM Step 1

GMM Step 2

% Significant
Mean

Median

Iterated GMM

% Significant

% Significant

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

101

100.0
3.6

100.0
1.3

90.4

90.2

100.0
3.6

100.0
1.3

88.7

88.5

100.0
12.4

100.0
5.6

43.3

11.9
3.5

4.4
2.8

0.24

0.23

3.3
3.5

1.7
2.8

0.47

0.85

30.3
92.2

21.7
86.3

Intercept Normalization
S

103

OIR test
Mean Normalization
S

OIR test

5.63

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 256. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The test model includes only a spurious factor, S, that is uncorrelated with returns. The table reports
mean and median estimates of the parameters of the intercept and mean-normalizations, as well as the frequency with which the estimated parameters
are found to be statistically significant at the 10% and 5% levels. The table also reports the frequency with which the model is rejected at the 10%
and 5% levels based on the OIR test. The test statistic is numerically identical at the first and second GMM steps. Details of the Monte Carlo
experiments are provided in the Appendix and main text. Results for the case where T = 10000 are in Appendix Table 4.

40

TABLE 8: Monte Carlo Experiment: Estimation of Model 4
GMM Step 1

GMM Step 2

% Significant
Mean

Median

Iterated GMM

% Significant

% Significant

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

0.11
98.1

10.3
100.0
3.2

5.4
100.0
1.1

0.39
87.1

0.39
86.9

26.2
100.0
3.2

17.8
100.0
1.1

0.43
85.5

0.44
85.3

34.7
100.0
11.5

25.5
100.0
5.3

3.14
6.77

73.8
54.7
22.3

64.3
29.1
16.8

2.81
0.19

2.79
0.60

77.1
10.1
22.3

65.6
4.9
16.8

2.79
0.00

2.76
0.79

95.6
24.8
74.6

91.7
16.5
62.4

Intercept Normalization
Mkt Rf
S

0.08
99.3

OIR test
Mean Normalization
Mkt Rf
S

OIR test

3.14
4.68

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 256. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The test model includes Mkt-Rf and a spurious factor, S, that is uncorrelated with returns. The table
reports mean and median estimates of the parameters of the intercept and mean-normalizations, as well as the frequency with which the estimated
parameters are found to be statistically significant at the 10% and 5% levels. The table also reports the frequency with which the model is rejected
at the 10% and 5% levels based on the OIR test. The test statistic is numerically identical at the first and second GMM steps. Details of the Monte
Carlo experiments are provided in the Appendix and main text. Results for the case where T = 10000 are in Appendix Table 5.

41

TABLE 9: Monte Carlo Experiment: Estimation of Model 5 (The Over-specified Model)

GMM Step 1

GMM Step 2

% Significant
True

Mean

Iterated GMM

% Significant

% Significant

Median

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

0.89
-0.04
1.35
74.2

0.89
-0.05
1.34
74.3

44.0
8.8
47.2
99.7
5.5

32.2
4.2
35.2
99.4
2.2

0.89
-0.03
1.35
74.9

0.90
-0.03
1.35
74.9

55.2
14.0
58.7
100.0
5.5

44.3
8.1
47.8
100.0
2.2

0.89
-0.03
1.34
75.0

0.90
-0.03
1.35
75.0

55.8
15.3
59.4
100.0
6.0

45.7
9.0
48.9
100.0
2.4

3.77
-0.19
5.70
-0.06

3.74
-0.20
5.64
0.47

98.4
9.1
99.2
5.7
10.0

96.2
4.3
98.6
1.8
5.1

3.77
-0.19
5.71
-0.11

3.74
-0.20
5.66
0.23

99.0
11.2
99.4
10.6
10.0

97.6
5.9
98.9
5.3
5.1

3.79
-0.19
5.74
-0.19

3.76
-0.18
5.69
-0.14

99.1
11.7
99.6
11.3
11.2

97.7
6.3
99.0
5.9
5.6

Intercept Normalization
Mkt Rf
SMB
HML

3.25
-0.15
4.92

S

OIR test
Mean Normalization
Mkt Rf
SMB
HML
S

OIR test

3.70
-0.17
5.61

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 256. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data, but the test model also includes a spurious factor, S, that is uncorrelated with returns. The true values
of the parameters of the SDF are indicated in the first column. The table reports mean and median estimates of the parameters of the intercept
and mean-normalizations, as well as the frequency with which the estimated parameters are found to be statistically significant at the 10% and 5%
levels. The table also reports the frequency with which the model is rejected at the 10% and 5% levels based on the OIR test. The test statistic is
numerically identical at the first and second GMM steps. Results for the case where T = 10000 are in Appendix Table 6.

42

TABLE 10: Model Reduction using the Mean-Normalization in the Monte Carlo Experiment

Model 3

Model 4

Model 5

1st/2nd step

Iterated

1st/2nd step

Iterated

1st/2nd step

Iterated

Frequency rank is not reduced

0.121

0.121

0.119

0.119

0.114

0.114

Rejection rate in these cases

3.7%

87.1%

19.5%

64.5%

5.6%

6.0%

0.879

0.879

0.881

0.881

0.886

0.886

100%

100%

70.3%

71.7%

7.1%

7.1%

Overall rejection rate with model reduction

88.4%

98.5%

64.3%

70.8%

7.0%

6.9%

Rejection rate without model reduction

2.8%

86.3%

16.8%

62.4%

5.1%

5.6%

Frequency rank is reduced
Rejection rate in these cases

Notes: The table reports results of the rank reduction procedure from 10000 Monte Carlo experiments with sample size T = 256. The nominal size
of the OIR tests for the procedure is set to 5%. Details of the Monte Carlo experiments are provided in the Appendix and main text. Results for
sample size T = 10000 are presented in Appendix Table 7.

43

FIGURE 1

Simple Example with E(Re )/E(Re f ) < 1/µf

a

1

b

b

a

a

B
b

a−1
f

A

Notes: The shaded area denotes the region in which E(m) < 0. The heavy solid line
denotes the pricing line, b = [E(Re )/E(Re f )]a. The dashed line denotes all possible
parameter pairs where the SDF has a unit intercept. The dotted line denotes all possible
parameter pairs where E(m) = 1. Point A lies at the intersection of the solid and dashed
lines indicating that it corresponds to the unique unit-intercept SDF consistent with the
pricing equation. Point B lies at the intersection of the solid and dotted lines indicating
that it is the unique unit-mean SDF consistent with the pricing equation.
44

f

FIGURE 2

Simple Example with E(Re )/E(Re f ) > 1/µf

b

a

1

A
b

a
f

a
b

a−1

B

f

Notes: The shaded area denotes the region in which E(m) < 0. The heavy solid line
denotes the pricing line, b = [E(Re )/E(Re f )]a. The dashed line denotes all possible
parameter pairs where the SDF has a unit intercept. The dotted line denotes all possible
parameter pairs where E(m) = 1. Point A lies at the intersection of the solid and dashed
lines indicating that it corresponds to the unique unit-intercept SDF consistent with the
pricing equation. It implies an SDF with a negative mean. Point B lies at the intersection
of the solid and dotted lines indicating that it is the unique unit-mean SDF consistent with
the pricing equation.

45

FIGURE 3

Simple Example with E(Re )/E(Re f ) = 1/µf or cov(Re , f ) = 0

a

1

b

b

A

a
f

a
b

a−1
f

Notes: The shaded area denotes the region in which E(m) < 0. The heavy solid line
denotes the pricing line, b = [E(Re )/E(Re f )]a, which is also the locus where E(m) = 0.
The dashed line denotes all possible parameter pairs where the SDF has a unit intercept.
The dotted line denotes all possible parameter pairs where E(m) = 1. Point A lies at the
intersection of the solid and dashed lines indicating that it corresponds to the unique
unit-intercept SDF consistent with the pricing equation. It implies an SDF with E(m) = 0.

46

FIGURE 4

Simple Example with E(Re f ) = 0

a

1

b

b

a
f

a

B
b

a−1
f

Notes: The shaded area denotes the region in which E(m) < 0. The heavy solid line
denotes the pricing line, b = [E(Re )/E(Re f )]a, which is also the vertical axis in this case.
The dashed line denotes all possible parameter pairs where the SDF has a unit intercept.
The dotted line denotes all possible parameter pairs where E(m) = 1. Point B lies at the
intersection of the solid and dotted lines indicating that it is the unique unit-mean SDF
consistent with the pricing equation.

47

FIGURE 5

Model Identification and Validity

rD

intercept-normalization
identified

k
k−1
mean-normalization
identified

2

1
0

1

k−1

2

k

rC

Any valid m is mean zero
Any valid m has a=0
Any valid m is mean zero and has a=0

Notes: Here rC = rank(C) and rD = rank(D).

48

FIGURE 6

Small Sample Distributions of ˆS and its Associated t-statistic

−4

8

x 10

GMM Step 1

GMM Step 1
0.5
0.4
f[t( S)

S

f( )

6
4
2
0

0.3
0.2
0.1

−2000 −1000

0

1000

0
−4

2000

−2

0
t( S)

S
−3

6

x 10

GMM Step 2

2

4

2

4

GMM Step 2
0.8

5
0.6
f[t( S)

S

f( )

4
3

0.4

2
0.2
1
0
−300 −200 −100

0

100

200

0
−4

300

−2

0
t( S)

S
−3

8

x 10

Iterated GMM

Iterated GMM
0.25
0.2
S

f[t( )

S

f( )

6
4
2
0
−300 −200 −100

0.15
0.1
0.05

0

100

200

300

S

0
−6

−4

−2

0
t( S)

2

4

6

Note: The figure reports results from 10000 Monte Carlo experiments with sample size
T = 256. The true risk factors are synthetic mimics of the Mkt-Rf, SMB and HML factors
in U.S. data. The test model includes only a spurious factor, S, that is uncorrelated with
returns. The graphs report distributions of parameter estimates for the
mean-normalization, and the associated t-statistics. Details of the Monte Carlo
experiments are provided in the Appendix and main text. Results for the case where
T = 10000 are in Figure 7.
49

FIGURE 7

Large Sample Distributions of ˆS and its Associated t-statistic

−4

1.5

GMM Step 1

x 10

GMM Step 1
0.5
0.4

f( S)

f[t( S)

1

0.5

0.3
0.2
0.1

0
−1.5

−1

−0.5

0

0.5

−3

0
−4

1.5

−2

4

x 10

S

1.5

1

GMM Step 2

x 10

0
t( S)

2

4

2

4

GMM Step 2
1.5

1

f( S)

f[t( S)

1

0.5

0

0.5

−1000

−500

0

500

0
−4

1000

−2

S
−3

1.5

x 10

Iterated GMM

0
t( )
S
Iterated GMM

0.2
0.15

f( S)

f[t( S)

1
0.1

0.5
0.05
0

−1000

−500

0

500

0

1000

S

−5

0
t( S)

5

Note: The figure reports results from 10000 Monte Carlo experiments with sample size
T = 10000. The true risk factors are synthetic mimics of the Mkt-Rf, SMB and HML
factors in U.S. data. The test model includes only a spurious factor, S, that is uncorrelated
with returns. The graphs report distributions of parameter estimates for the
mean-normalization, and the associated t-statistics. Details of the Monte Carlo
experiments are provided in the Appendix and main text.
50

7

Appendix

7.1

Extended Proof of Theorem 2

As in Section 2.3 we have X = ( X1 X2 ) with the columns of X1 lying in R(D) and the
columns of X2 lying in N (D). The matrix X is invertible and X 0 = X

1

. We also have

D̃ = DX1 .

First we write
D0 W (µR

D ) = XX 0 D0 W (µR D )
✓ 0 ◆
X1
D0 W (µR D )
= X
X20
✓ 0
◆
D̃ W
= X
(µR D ).
0

Then we note that
µR

D

= µR
= µR

DXX 0
D

= µR
= µR
Now let

✓

X1 X 2
✓ 0
X1
D̃ 0
X20

D̃X10

◆

= X1 ˜ + x for any x 2 N (D). Then we have
µR

D = µR

Therefore,
0

D W (µR

D )=X

✓

D̃ ˜.

D̃0 W (µR
0

Clearly this is zero if and only if ˜ = (D̃0 W D̃) 1 D̃0 W µR .⌅

7.2

X10
X20
◆

D̃ ˜)

◆

.

Estimating Long-Run Covariance Matrices

The intercept-normalization
P
I define ST = T1 Tt=1 ût ût 0 . When the model is true Et 1 [ut ( )] = 0 when evaluated at the
true value of . Therefore E[ut ( )ut j ( )0 ] = 0 for j 6= 0. It follows that ST is a consistent
estimate of S .

51

The mean-normalization
To compute ST✓ I use a VARHAC procedure proposed by den Haan and Levin (2000). The
pricing equations for the assets imply that Et 1 [u✓1t (✓)] = 0 when evaluated at the true value
of ✓. Therefore E[u✓1t ( )u✓t j ( )0 ] = 0 for j > 0 and E[u✓1t ( )u✓1t j ( )0 ] = 0 for j < 0. When I
use the VARHAC procedure I make the more restrictive assumption that lagged variables do
not appear in the equations for u✓1t (the errors corresponding to the asset pricing conditions)
but allow for lags in the equations for u✓2t (the GMM errors corresponding to ft

7.3

µ).

Data

Fama-French 25 Portfolios
Each Fama and French (1993) portfolio represents the intersection of one of 5 groups of stocks
sorted according to their market capitalization with one of 5 groups of stocks sorted according
to their book equity to market capitalization ratio. The returns are equally weighted. I obtained raw monthly returns from Kenneth French’s website http://mba.tuck.dartmouth.edu/
pages/ faculty/ ken.french/ data library.html. To obtain quarterly returns I compounded
monthly returns within each quarter. To obtain quarterly excess returns I subtract the
quarterly risk free rate defined as the compounded monthly risk free rate from Fama/French
Research Data Factor file. Real excess returns are defined by dividing the nominal excess
return by one plus the inflation rate, which I define below.
Consumption Data
To compute real consumption of nondurables and services I proceed as follows. Let CtN be
the consumption of nondurables and CtS be the consumption of services in nominal dollars,
S
and let cN
t and ct be the corresponding series in constant chained dollars, as published by the
Bureau of Economic Analysis. To obtain nominal consumption of nondurables and services I
simply set Ct = CtN + CtS . However, because real chained series are not summable, I proceed
as follows to create real consumption of nondurables and services, which I denote ct . First
N
define st = (CtN /Ct + CtN 1 /Ct 1 ), gtN = cN
1 and gtS = cSt /cSt 1 1. Then define
t /ct 1
the growth rate of ct as gt = st gtN + (1 st )gtS . Notice that a real levels series can then be
generated by forward and backward induction relative to a base period. I convert the real
levels series into per capita terms by dividing by the quarterly population series published in
the National Income and Product Accounts by the BEA. I construct an inflation series using
a similar method. Letting ⇡tN and ⇡tS be the inflation rates for nondurables and services, I
let the combined inflation rate be ⇡t = st ⇡tN + (1 st )⇡tS .

52

I assume that households derive utility in quarter t + 1 from the stock of durables at
the end of quarter t. To compute the real quarterly stock of durable goods I proceeded
as follows. The Bureau of Economic Analysis publishes end-of-year real stocks of durables
goods. Let kt denote the real stock of durables at the end of some year, and let kt+4 be the
same stock a year (four quarters) later. We observe quarterly real purchases of consumer
durables, which I denote cD
t . I assume that within each year the model
kt+1 = cD
t+1 + (1

)kt

(24)

holds, with allowed to vary by year. I solve for the value of such that the beginning and
end-of-year stocks are rationalized by purchases series. This is the such that
kt+4 = cD
t+4 + (1
Once I identify the value of

)cD
t+3 + (1

) 2 cD
t+2 + (1

) 3 cD
t+1 + (1

) 4 kt .

(25)

that applies within a year using (25), I use (24) to calculates

the within year stocks. I convert the real stocks to per capita terms by dividing by the same
population series used for the consumption series.
Fama and French Factors
These series are taken from the Fama/French Research Data Factor file. I define the monthly
market return as the sum of the market premium series (Mkt-Rf) and the risk free rate series
(Rf). I convert this to a quarterly return by compounding the monthly series geometrically
within each quarter. Denoting the resulting series, RtM , I convert it to a real return as
follows: rtM = (RtM ⇡t )/(1 + ⇡t ).
To create quarterly versions of the Fama-French factors (Mkt-Rf, SMB and HML) I
proceed as described in Burnside (2011). To convert them to real excess returns I divide
these series by 1 + ⇡t .
Lettau and Ludvigson Factors
Lettau and Ludvigson (2001) propose a scaled CCAPM model, which uses three factors:
consumption growth, the cay factor (a cointegrating residual between the logarithms of
consumption, asset wealth and labor income), and the product of consumption growth and
cay. I downloaded the factor data directly from the Martin Lettau’s web page for the sample
period 1952Q1–2012Q3.
Jagannathan and Wang Factors
Jagannathan and Wang (2007) propose a Q4–Q4 CCAPM model. This is simply the CCAPM
estimated using annual, rather than quarterly, equity returns, and using annual consumption
53

growth measured from the fourth quarter of one year to the fourth quarter of the year in
which the returns are realized. I construct the relevant series from the quarterly data set
described above, while constructing annual real excess returns for the FF25 portfolios in
similar fashion as to what was described above for quarterly data.

7.4

Lustig and Verdelhan Portfolios and Factors

Lustig and Verdelhan (2007) consider the annual real US dollar excess returns to portfolios
of short-term foreign government securities denominated in foreign currency. The sample
period is 1953–2002. They form these portfolios on the basis of the interest rates on the
underlying securities. In particular the real excess returns on a large number of countries’
treasury securities are sorted into eight bins in each period according to the nominal interest
rates on the securities, from lowest to highest. The returns to holding equally-weighted
portfolios of each bin are then calculated. Lustig and Verdelhan use three risk factors to
explain these returns: consumption growth, durables growth and the market return [their
model is equivalent to Yogo (2006)’s model]. I take the data for the returns and factors
directly from the AEA data repository for their paper.

7.5

Rank Tests

Cragg and Donald (1997)
The Cragg and Donald (1997) and Wright (2003) test for whether B has rank r < k is based
on measuring the distance between B and the set of matrices of the same dimension with
p
d
rank r. Let B̂ be a consistent estimator for B and assume that T vec(B̂ B0 ) ! N (0, VB ),
where B0 is the true value of B. Let V̂B be a consistent estimator for VB . To test the null
hypothesis that rank(B) = r < k I form the statistic
L(r) = min T vec(B̂
P 2⌦r

P )0 V (B̂)

1

vec(B̂

P)
d

where ⌦r is the set of all n ⇥ k matrices with rank r. If the true rank of B0 is r, L(r) !
2
(n r)(k r) . I construct tests of the rank conditions for the two normalizations by letting B
be C or D and estimating the elements of these matrices by GMM.
To take an example, when the null hypothesis is that r = 0, the test is analogous to a
simple F-test for Bij = 0 for all i, j. In the case the test statistic can be computed with
ease and has a chi-squared distribution with nk degrees of freedom. However, when the null
hypothesis is that rC = 1, computing L(1) can be computationally burdensome, especially
when n and k are large. It involves optimization over nk

54

(n

1)(k

1) = k + n

1

free parameters since the single vector that forms the basis for the rows of B has k

1 free

parameters and there are n rows.
Kleibergen and Paap (2006)
We start with an n ⇥ k matrix ⇧, and the null hypothesis that rank(⇧) = r < k. We

also define ⇡ = vec(⇧) and assume that we have some estimate ⇡
ˆ with the property that
p
d
T (ˆ
⇡ ⇡) ! N (0, V⇡ ) as well as a consistent estimate of V⇡ , denoted V̂⇡ .
We form the scaled matrix ⇥ = G⇧F 0 where Gn⇥n and Fk⇥k are invertible matrices

that make ⇥ invariant to invertible transformations of the data. As described in the main
1/2
1/2
text, when ⇧ = cov(Re , f ) natural choices are G = ⌃Re and F = ⌃f , the Cholesky
decompositions of the inverses of the covariance matrices of Re and f .
ˆ = Ĝ⇧
ˆ F̂ 0 where Ĝ and F̂ are
If we define ✓ = vec(⇥) then ✓ = (F ⌦ G)⇡. Now let ⇥
ˆ has
standard sample analogs of G and F . Then, under standard assumptions, ✓ˆ = vec(⇥)
p
d
the property that T (✓ˆ ✓) ! N (0, V✓ ) with V✓ = (F ⌦ G)V⇡ (F ⌦ G)0 .
Now let U SV 0 = ⇥ be the SVD of ⇥ and let Un⇥n , Sn⇥k and Vk⇥k be partitioned as
follows:
✓
◆
✓
◆
✓
◆
U11 U12
S1 0
V11 V12
U=
S=
V =
0 S2
U21 U22
V21 V22
where U12 is r ⇥ (n r), U22 is (n r) ⇥ (n r), S2 is (n r) ⇥ (k r), V12 is r ⇥ (k
ˆ be the SVD of ⇥.
ˆ Let
and V22 is (k r) ⇥ (k r). Similarly let Û Ŝ V̂ 0 = ⇥
0
ˆ r = (Û22 Û22
) 1/2 Û22 Ŝ2 V̂220 (V̂22 V̂220 )
⇤
◆
✓
U12
0 1/2
Ar =
U221 (U22 U22
) ,
U22

1/2

r)

,

Br = (V22 V220 )1/2 (V220 ) 1 ( V120 V220 ),

ˆ r ).
and ˆ r = vec(⇤
p
Under the null hypothesis that rank(⇧) = r, T ˆ r converges in distribution to a normal
distribution with mean 0 and covariance matrix ⌦r = (Br ⌦ A0r )V✓ (Br ⌦ A0r )0 . Therefore,
ˆ r = (B̂r ⌦
T ˆ 0r ⌦r 1 ˆ r converges to a 2(n r)(k r) . In practice, a consistent estimator for ⌦r , ⌦
ˆ 1 ˆ r with Âr and B̂r being
Â0 )V̂✓ (B̂r ⌦ Â0 )0 , is used to form the test statistic rk(r) = T ˆ 0 ⌦
r

r

r

r

sample analogs of Ar and Br .

7.6

Monte Carlo Experiment Details

The true SDF is given by mt = 1 (ft µf )0 with µf and being 3 ⇥ 1 vectors. The
vector of risk factors, ft , is assumed to follow the law of motion ft ⇠ N iid(µf , ⌃f ). I set
0.17 5.61 )0 . This parameter vector corresponds to first step GMM estimates
= ( 3.70
55

of the Fama-French three factor model obtained using the mean-normalization using real,
rather than nominal, excess returns and factors to estimate the model. I estimate the model
in real terms because some of the test models in the Monte Carlo experiments use synthetic
consumption factors. I set µf and ⌃f equal to the sample mean and covariance matrix of
the Mkt-Rf, SMB and HML factors.
I generate an n ⇥ 1 (with n = 25) vector of artificial excess returns Rte = µR + (ft
µf ) + ⇠t where µR is an n ⇥ 1 vector, is an n ⇥ k matrix, is an n ⇥ n lower triangular
matrix, ⇠t ⇠ N iid(0, In ) and is independent of ft . Given this law of motion for Rte , it follows

that the covariance matrix of Rte is ⌃R = ⌃f

0

+

0

.

So that the model shares some characteristics of actual data, I set ⌃R equal to its sample
equivalent for the Fama-French 25 portfolios sorted on size and value. I set equal to the
matrix of factor betas for these returns regressed on Mkt-Rf, SMB and HML. I set equal to
the Cholesky decomposition of the covariance matrix of the residuals from those regressions.
Given the assumptions above we have
E(Rte mt ) = E {[µR + (ft
= µR

µf ) + ⇠t ][1

⌃f .

(ft

µf )0 ]}
(26)

To ensure that the right hand side of equation (26) is zero, I set µR = ⌃f . This means
that expected returns of the model correspond to the model-predicted expected returns for
the GMM estimates described above.

7.7

Non-Uniqueness of the SVD and the Model Reduction Procedure

As mentioned in footnote 10, the model reduction procedure relies on identifying linear
combinations of factors using the SVD of the matrix ⇥ = U SV 0 . This might be viewed as
problematic given that V in the SVD is sometimes non-unique.
There are two types of non-uniqueness. Neither causes difficulty with the procedure as
long as the researcher’s primary interest is in recovering the parameters associated with the
original factors. Recall that the procedure identifies f˜r = Ar f as the reduced set of risk
factors, where Vr0 F and Vr represents the first r columns of V .
The first type of non-uniqueness is that it is usually possible to swap the signs of columns
in V and corresponding columns in U . So, we could swap the signs of the elements of Vr . Of
course, this reverses the signs of the factors, and the resulting coefficients, ˜ and ˜ . Therefore,
if one interested in backing out the implied and one ends up with the same values as
before. This is because the transformations = F 0 Vr ˜ and = F 0 Vr ˜ negate the change of
sign.
56

The second type of non-uniqueness arises if there are repeat singular values. In this
the columns of V associated with these repeat singular values can be rotated using any
orthogonal matrix. So, now, imagine that this issue applies to two (or more) of the columns
in Vr . This means that we could find one SVD and define f˜r = (Vr0 F )f and, using an
arbitrary unitary rotation matrix Qr⇥r define V̄r = Vr Q and let f¯r = (V̄ 0 F )f .
r

For the model based on the f˜r we have
D̃ = E(Re f˜r0 ) = E(Re f 0 )F 0 Vr = DF 0 Vr
C̃ = cov(Re , f˜r ) = cov(Re , f )F 0 Vr = CF 0 Vr .
For the model based on the f¯r we have
D̄ = E(Re f¯r0 ) = E(Re f 0 )F 0 V̄r = DF 0 Vr Q = D̃Q
C̄ = cov(Re , f¯r ) = cov(Re , f )F 0 V̄r = CF 0 Vr Q = C̃Q.
Now consider the population version of first step GMM with the mean normalization for
these reduced factors. We have
¯ = (C̄ 0 C̄) 1 (C̄ 0 µR ) = (Q0 C̃ 0 C̃Q) 1 (Q0 C̃ 0 µR ) = Q0 (C̃ 0 C̃) 1 (C̃ 0 µR ) = Q0 ˜ .
Similarly
¯ = Q0 ˜.
Thus the parameters associated with the original factors are the same regardless of which
rotation we use. That is ⌘ A0r ˜ is the same as
⌘ Ā0r ¯ = (V̄r0 F )0 (Q0 ˜ ) = F 0 V̄r Q0 ˜ = F 0 Vr ˜ = A0r ˜ ,
⌘ A0r ˜ is the same as ⌘ Ā0r ¯.
Since the parameters associated with the di↵erent rotations are equivalent for the original
factors, the GMM errors at the first step are the same. So we get identical weighting matrices
at the next step of GMM. This makes the estimators at all GMM steps equivalent, regardless
of the rotation used.
and

57

APPENDIX TABLE 1: Rank Tests in the Monte Carlo Experiment

% rejected at
10% level 5% level

% rejected at
10% level 5% level

True Model (Factors: Mkt-Rf, SMB, HML) rD = rC = 3
H 0 : rD = 2
H 0 : rD = 1
H 0 : rD = 0

100%
100%
100%

100%
100%
100%

H 0 : rC = 2
H 0 : rC = 1
H 0 : rC = 0

100%
100%
100%

100%
100%
100%

100%

100%

10.4%

5.1%

10.3%
100%

4.9%
100%

Single Relevant Factor (Factor: Mkt-Rf) rD = rC = 1
H 0 : rD = 0

100%

100%

H 0 : rC = 0

Single Spurious Factor (Factor: S) rD = 1, rC = 0
H 0 : rD = 0

100%

100%

H 0 : rC = 0

Two Factors (Factors: Mkt-Rf, S) rD = 2, rC = 1
H 0 : rD = 1
H 0 : rD = 0

100%
100%

100%
100%

5. Over-specified Model (Factors: Mkt-Rf,
H 0 : rD = 3
10.1%
5.3%
H 0 : rD = 2
100%
100%
100%
100%
H 0 : rD = 1
100%
100%
H 0 : rD = 0

H 0 : rC = 1
H 0 : rC = 0
SMB, HML, S) rD
H 0 : rC = 3
H 0 : rC = 2
H 0 : rC = 1
H 0 : rC = 0

= rC = 3
10.2%
100%
100%
100%

5.2%
100%
100%
100%

Notes: The table reports results of Kleibergen and Paap (2006) rank tests from 10000 Monte Carlo experiments with sample size T = 10000. The
true risk factors are synthetic mimics of the Mkt-Rf, SMB and HML factors in U.S. data. Rank tests are performed for five test models that use
di↵erent factors. rC = rank[cov(Re , f )] and rD = rank[E(Re f 0 )], where Re are the returns generated in the experiment, and f is the conjectured
vector of factors. The table reports the fraction of the samples in which these tests reject the null hypothesis when the size of the test is set to 10%
and 5%. Details of the Monte Carlo experiments are provided in the Appendix and main text.

58

APPENDIX TABLE 2: Monte Carlo Experiment: Estimation of Model 1 (The True Model)
GMM Step 1

GMM Step 2

% Significant
True

Mean

Iterated GMM

% Significant

% Significant

Median

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

3.28
-0.15
4.97

3.27
-0.15
4.97

100.0
18.4
100.0
9.7

100.0
10.8
100.0
5.0

3.29
-0.15
4.98

3.29
-0.15
4.98

100.0
18.9
100.0
9.7

100.0
10.9
100.0
5.0

3.29
-0.15
4.98

3.29
-0.15
4.98

100.0
18.9
100.0
9.7

100.0
10.9
100.0
5.0

3.70
-0.17
5.62

3.70
-0.17
5.62

100.0
18.4
100.0
9.7

100.0
10.8
100.0
5.0

3.70
-0.17
5.62

3.70
-0.17
5.62

100.0
18.8
100.0
9.7

100.0
10.9
100.0
5.0

3.70
-0.17
5.62

3.70
-0.17
5.62

100.0
18.8
100.0
9.7

100.0
10.9
100.0
5.1

Intercept Normalization
Mkt Rf
SMB
HML

3.28
-0.15
4.97

OIR test
Mean Normalization
Mkt Rf
SMB
HML

OIR test

3.70
-0.17
5.61

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The true values of the parameters of the SDF are indicated in the first column. The table reports mean
and median estimates of the parameters of the intercept and mean-normalizations, as well as the frequency with which the estimated parameters are
found to be statistically significant at the 10% and 5% levels. The table also reports the frequency with which the model is rejected at the 10% and
5% levels based on the OIR test. The test statistic is numerically identical at the first and second GMM steps. Details of the Monte Carlo experiments
are provided in the Appendix and main text.

59

APPENDIX TABLE 3: Monte Carlo Experiment: Estimation of Model 2

GMM Step 1

GMM Step 2

% Significant
Mean

Median

Iterated GMM

% Significant

% Significant

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

3.06

100.0
100.0

100.0
100.0

3.03

3.04

100.0
100.0

100.0
100.0

3.03

3.03

100.0
100.0

100.0
100.0

3.24

100.0
100.0

100.0
100.0

2.78

2.78

100.0
100.0

100.0
100.0

2.74

2.74

100.0
100.0

100.0
100.0

Intercept Normalization
Mkt Rf

3.06

OIR test
Mean Normalization
Mkt Rf

OIR test

3.24

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The test model includes only the Mkt-Rf factor. The table reports mean and median estimates of
the parameters of the intercept and mean-normalizations, as well as the frequency with which the estimated parameters are found to be statistically
significant at the 10% and 5% levels. The table also reports the frequency with which the model is rejected at the 10% and 5% levels based on the
OIR test. The test statistic is numerically identical at the first and second GMM steps. Details of the Monte Carlo experiments are provided in the
Appendix and main text.

60

APPENDIX TABLE 4: Monte Carlo Experiment: Estimation of Model 3

GMM Step 1

GMM Step 2

% Significant
Mean

Median

Iterated GMM

% Significant

% Significant

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

102

100.0
9.4

100.0
4.4

101

101

100.0
9.4

100.0
4.4

101

101

100.0
10.5

100.0
5.0

-185

18.7
3.6

10.4
3.4

-0.39

-0.47

2.2
3.6

1.6
3.4

-0.52

-1.71

82.3
99.9

78.4
99.8

Intercept Normalization
S

102

OIR test
Mean Normalization
S

OIR test

9.22

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of
the Mkt-Rf, SMB and HML factors in U.S. data. The test model includes only a spurious factor, S, that is uncorrelated with returns. The table
reports mean and median estimates of the parameters of the intercept and mean-normalizations, as well as the frequency with which the estimated
parameters are found to be statistically significant at the 10% and 5% levels. The table also reports the frequency with which the model is rejected
at the 10% and 5% levels based on the OIR test. The test statistic is numerically identical at the first and second GMM steps. Details of the Monte
Carlo experiments are provided in the Appendix and main text.

61

APPENDIX TABLE 5: Monte Carlo Experiment: Estimation of Model 4
GMM Step 1

GMM Step 2

% Significant
Mean

Median

Iterated GMM

% Significant

% Significant

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

0.00
102

9.9
100.0
9.1

4.9
100.0
4.1

0.03
101

0.03
101

11.6
100.0
9.1

6.1
100.0
4.1

0.03
101

0.03
101

11.9
100.0
10.9

6.3
100.0
5.4

3.18
-39.7

83.3
91.5
31.5

77.1
76.8
30.0

2.75
-1.17

2.75
-4.35

91.1
4.6
31.5

79.1
2.2
30.0

2.74
-1.19

2.74
-3.55

100.0
77.4
99.8

100.0
72.4
99.8

Intercept Normalization
Mkt Rf
S

0.00
102

OIR test
Mean Normalization
Mkt Rf
S

OIR test

3.11
-18.1

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The test model includes Mkt-Rf and a spurious factor, S, that is uncorrelated with returns. The table
reports mean and median estimates of the parameters of the intercept and mean-normalizations, as well as the frequency with which the estimated
parameters are found to be statistically significant at the 10% and 5% levels. The table also reports the frequency with which the model is rejected
at the 10% and 5% levels based on the OIR test. The test statistic is numerically identical at the first and second GMM steps. Details of the Monte
Carlo experiments are provided in the Appendix and main text.

62

APPENDIX TABLE 6: Monte Carlo Experiment: Estimation of Model 5 (The Over-specified Model)
GMM Step 1

GMM Step 2

% Significant
True

Mean

Iterated GMM

% Significant

% Significant

Median

10%

5%

Mean

Median

10%

5%

Mean

Median

10%

5%

0.89
-0.04
1.35
74.1

0.89
-0.04
1.36
74.1

69.4
10.5
69.7
99.8
7.0

58.9
5.7
59.1
99.5
3.1

0.89
-0.04
1.35
74.2

0.89
-0.04
1.35
74.2

83.6
10.8
83.8
100.0
7.0

75.2
5.9
75.8
100.0
3.1

0.89
-0.04
1.35
74.2

0.89
-0.04
1.35
74.2

84.1
11.2
84.2
100.0
8.2

76.0
6.2
76.7
100.0
3.7

3.70
-0.17
5.62
-0.21

3.70
-0.17
5.62
0.21

100.0
15.9
100.0
6.6
6.5

100.0
9.0
100.0
1.9
2.8

3.70
-0.17
5.62
-0.36

3.70
-0.17
5.62
-0.59

100.0
16.6
100.0
7.3
6.5

100.0
9.4
100.0
3.1
2.8

3.70
-0.16
5.62
-0.36

3.70
-0.17
5.62
-0.55

100.0
17.1
100.0
7.8
7.6

100.0
9.8
100.0
3.0
3.4

Intercept Normalization
Mkt Rf
SMB
HML

3.25
-0.15
4.92

S

OIR test
Mean Normalization
Mkt Rf
SMB
HML
S

OIR test

3.70
-0.17
5.61

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data, but the test model also includes a spurious factor, S, that is uncorrelated with returns. The true values
of the parameters of the SDF are indicated in the first column. The table reports mean and median estimates of the parameters of the intercept
and mean-normalizations, as well as the frequency with which the estimated parameters are found to be statistically significant at the 10% and 5%
levels. The table also reports the frequency with which the model is rejected at the 10% and 5% levels based on the OIR test. The test statistic is
numerically identical at the first and second GMM steps.

63

APPENDIX TABLE 7: Model Reduction using the Mean-Normalization in the Monte Carlo Experiment

Model 3

Model 4

Model 5

1st/2nd step

Iterated

1st/2nd step

Iterated

1st/2nd step

Iterated

Frequency rank is not reduced

0.051

0.051

0.049

0.049

0.052

0.052

Rejection rate in these cases

3.2%

100%

38.1%

100%

3.5%

4.3%

0.949

0.949

0.951

0.951

0.948

0.948

100%

100%

100%

100%

5.1%

5.1%

Overall rejection rate with model reduction

95.1%

100%

97.0%

100%

5.0%

5.0%

Rejection rate without model reduction

3.4%

99.8%

30.0%

99.8%

3.4%

3.8%

Frequency rank is reduced
Rejection rate in these cases

Notes: The table reports results of the rank reduction procedure from 10000 Monte Carlo experiments with sample size T = 10000. The nominal size
of the OIR tests for the procedure is set to 5%. Details of the Monte Carlo experiments are provided in the Appendix and main text.

64

APPENDIX TABLE 8: Distributions of Direct and Indirect Parameter Estimates: Model 1

GMM Step 1
P10

Median

GMM Step 2

Iterated GMM

P90

P10

Median

P90

P10

Median

P90

Intercept Normalization

HML

3.12
-0.41
4.76

3.27
-0.15
4.97

3.43
0.12
5.18

3.13
-0.40
4.78

3.29
-0.15
4.98

3.44
0.11
5.20

3.13
-0.40
4.78

3.29
-0.15
4.98

3.44
0.11
5.20

( )Mkt Rf
( )SMB
( )HML

3.12
-0.41
4.76

3.27
-0.15
4.97

3.43
0.12
5.18

3.12
-0.40
4.76

3.27
-0.15
4.97

3.43
0.11
5.18

3.12
-0.40
4.76

3.27
-0.15
4.97

3.43
0.11
5.18

HML

3.51
-0.47
5.34

3.70
-0.17
5.62

3.90
0.13
5.89

3.51
-0.45
5.35

3.70
-0.17
5.62

3.89
0.12
5.89

3.51
-0.45
5.35

3.70
-0.17
5.62

3.89
0.12
5.89

( )Mkt Rf
( )SMB
( )HML

3.51
-0.47
5.34

3.70
-0.17
5.62

3.90
0.13
5.89

3.52
-0.45
5.37

3.72
-0.17
5.64

3.91
0.12
5.91

3.52
-0.45
5.37

3.72
-0.17
5.64

3.91
0.12
5.91

Mkt Rf
SMB

Mean Normalization
Mkt Rf
SMB

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The table reports 10th percentiles, medians, and 90th percentiles of the distributions of the parameter
estimates for the true model. Details of the Monte Carlo experiments are provided in the Appendix and main text.

65

APPENDIX TABLE 9: Distributions of Direct and Indirect Parameter Estimates: Model 2

GMM Step 1
P10

Median

GMM Step 2

Iterated GMM

P90

P10

Median

P90

P10

Median

P90

Intercept Normalization
Mkt Rf

( )Mkt

Rf

2.91

3.06

3.21

2.87

3.04

3.20

2.87

3.03

3.19

2.90

3.05

3.20

2.50

2.64

2.78

2.46

2.61

2.75

3.06

3.24

3.41

2.61

2.78

2.95

2.58

2.74

2.91

3.07

3.25

3.43

3.03

3.22

3.41

3.02

3.22

3.41

Mean Normalization
Mkt Rf

( )Mkt

Rf

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The table reports 10th percentiles, medians, and 90th percentiles of the distributions of the parameter
estimates for Model 2, which includes only Mkt-Rf as a risk factor. Details of the Monte Carlo experiments are provided in the Appendix and main
text.

66

APPENDIX TABLE 10: Distributions of Direct and Indirect Parameter Estimates: Model 3

GMM Step 1
P10

Median

GMM Step 2

Iterated GMM

P90

P10

Median

P90

P10

Median

P90

Intercept Normalization
S

99.0

102

105

99.1

101

103

99.1

101

103

( )S

97.9

102

106

-43.2

75.7

236

-42.1

75.7

235

Mean Normalization
S

( )S

-5617

-185

5617

-354

0

357

-355

-1.71

356

-19720

-1502

17535

-22426

4657

24897

-22385

4674

24884

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The table reports 10th percentiles, medians, and 90th percentiles of the distributions of the parameter
estimates for Model 3, which includes only a spurious risk factor, S. Details of the Monte Carlo experiments are provided in the Appendix and main
text.

67

APPENDIX TABLE 11: Distributions of Direct and Indirect Parameter Estimates: Model 4

GMM Step 1
P10

Median

GMM Step 2

Iterated GMM

P90

P10

Median

P90

P10

Median

P90

Intercept Normalization
Mkt Rf
S

( )Mkt
( )S

Rf

-0.12
99.0

0.00
102

0.12
105

-0.09
98.2

0.03
101

0.14
104

-0.09
98.2

0.03
101

0.14
104

-0.58
83.4

0.03
98.3

0.56
121

-4.01
-82.3

0.89
67.3

4.70
261

-4.01
-87.5

0.89
66.8

4.86
259

1.74
-2000

3.18
-39.7

4.34
1991

2.50
-292

2.75
-4.35

2.99
294

2.50
-292

2.74
-3.55

2.98
292

-9.03
-20218

2.77
2166

13.9
20421

-7.93
-16466

2.70
3847

12.5
20273

-7.92
-16438

2.70
3849

12.5
20288

Mean Normalization
Mkt Rf
S

( )Mkt
( )S

Rf

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The table reports 10th percentiles, medians, and 90th percentiles of the distributions of the parameter
estimates for Model 4, which includes Mkt-Rf and a spurious risk factor, S. Details of the Monte Carlo experiments are provided in the Appendix
and main text.

68

APPENDIX TABLE 12: Distributions of Direct and Indirect Parameter Estimates: Model 5

GMM Step 1
P10

Median

GMM Step 2

Iterated GMM

P90

P10

Median

P90

P10

Median

P90

Intercept Normalization
Mkt Rf
SMB
HML
S

(
(
(
(

)Mkt Rf
)SMB
)HML
)S

0.36
-0.18
0.55
57.9

0.89
-0.04
1.36
74.1

1.42
0.10
2.15
90.2

0.47
-0.18
0.71
61.3

0.89
-0.04
1.35
74.2

1.32
0.10
1.99
86.9

0.47
-0.18
0.71
61.3

0.89
-0.04
1.35
74.2

1.32
0.10
1.99
86.9

2.01
-0.55
3.04
-119

3.21
-0.14
4.85
2.08

7.06
0.14
10.8
39.1

2.22
-0.50
3.37
-85.0

3.28
-0.15
4.97
-0.10

6.03
0.12
9.13
32.7

2.22
-0.50
3.37
-85.0

3.28
-0.15
4.97
0.03

6.02
0.12
9.12
32.7

3.50
-0.47
5.34
-67.4

3.70
-0.17
5.61
0.21

3.91
0.14
5.90
66.0

3.50
-0.46
5.34
-53.7

3.70
-0.17
5.62
-0.59

3.90
0.13
5.90
53.2

3.50
-0.46
5.34
-53.7

3.70
-0.17
5.62
-0.55

3.90
0.13
5.90
53.1

3.24
-0.87
4.99
143

3.70
-0.17
5.62
299

4.15
0.55
6.24
797

3.30
-0.81
5.05
171

3.71
-0.16
5.62
307

4.12
0.47
6.20
650

3.29
-0.81
5.05
172

3.71
-0.16
5.62
307

4.12
0.48
6.20
650

Mean Normalization
Mkt Rf
SMB
HML
S

(
(
(
(

)Mkt Rf
)SMB
)HML
)S

Note: The table reports results from 10000 Monte Carlo experiments with sample size T = 10000. The true risk factors are synthetic mimics of the
Mkt-Rf, SMB and HML factors in U.S. data. The table reports 10th percentiles, medians, and 90th percentiles of the distributions of the parameter
estimates for Model 5, which includes the true risk factors and a spurious risk factor, S. Details of the Monte Carlo experiments are provided in the
Appendix and main text.

69

