NBER WORKING PAPER SERIES

ONE-NODE QUADRATURE BEATS MONTE CARLO:
A GENERALIZED STOCHASTIC SIMULATION ALGORITHM
Kenneth Judd
Lilia Maliar
Serguei Maliar
Working Paper 16708
http://www.nber.org/papers/w16708

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
January 2011

Lilia Maliar and Serguei Maliar acknowledge support from the Hoover Institution at Stanford University,
the Ivie, the Ministerio de Ciencia e Innovación and FEDER funds under the project SEJ-2007-62656.
The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2011 by Kenneth Judd, Lilia Maliar, and Serguei Maliar. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.

One-node Quadrature Beats Monte Carlo: A Generalized Stochastic Simulation Algorithm
Kenneth Judd, Lilia Maliar, and Serguei Maliar
NBER Working Paper No. 16708
January 2011
JEL No. C63
ABSTRACT
In conventional stochastic simulation algorithms, Monte Carlo integration and curve fitting are merged
together and implemented by means of regression. We perform a decomposition of the solution error
and show that regression does a good job in curve fitting but a poor job in integration, which leads
to low accuracy of solutions. We propose a generalized notion of stochastic simulation approach in
which integration and curve fitting are separated. We specifically allow for the use of deterministic
(quadrature and monomial) integration methods which are more accurate than the conventional Monte
Carlo method. We achieve accuracy of solutions that is orders of magnitude higher than that of the
conventional stochastic simulation algorithms.

Kenneth Judd
Hoover Institution
Stanford University
Stanford, CA 94305-6010
and NBER
kennethjudd@mac.com
Lilia Maliar
Department of Economics
University of Alicante
Campus San Vicente del Raspeig
Ap. Correos 99, 03080 Alicante, Spain
maliarl@merlin.fae.ua.es

Serguei Maliar
Department of Economics
University of Alicante
Campus San Vicente del Raspeig
Ap. Correos 99, 03080 Alicante, Spain
maliars@merlin.fae.ua.es

1

Introduction

Numerical methods for solving dynamic economic models diﬀer substantially
in their accuracy and speed. For example, in the comparison studies by Den
Haan (2010), and Kollmann, Maliar, Malin and Pichler (2011) (henceforth,
KMMP), these diﬀerences amount to several orders of magnitude. To develop eﬃcient solution methods, we must understand what accounts for the
diﬀerences across methods and identify computational techniques that lead
to best results. This is not easy to do because existing solution methods differ in many dimensions (solution domain, interpolation method, integration
method, iterative procedure, etc.) and contain steps that are not directly
comparable.
In the present paper, we assess computational techniques that are part
of the basis of two broad classes of solution methods, the stochastic simulation and projection.1 The first class approximates solutions on a set of
simulated points using Monte Carlo integration, while the second class approximates solutions on a fixed grid of points using deterministic numerical
integration. Furthermore, in stochastic simulation methods, the integration
and curve-fitting steps are merged into one by means of regression, whereas in
projection methods, these steps are performed separately. Finally, stochastic
simulation methods are simpler to implement and are relatively less expensive in high-dimensional problems than projection methods, but they are also
less accurate; see Maliar, Maliar and Judd (2011) and KMMP (2011).
We introduce a notion of generalized stochastic simulation algorithm
() that breaks down the fusion of integration and curve-fitting that
is present in the conventional stochastic simulation algorithm. We specifically allow for the use of deterministic (quadrature and monomial) integration methods which are characteristic for the projection class of algorithms.
 eﬀectively lies between a pure stochastic simulation and pure projection algorithms and includes both of these algorithms as limiting cases. By
separating the integration and curve-fitting steps, we are able to decompose
the total solution error into errors associated with integration, curve-fitting
and the choice of a solution domain.
Within our generalized framework, we show that errors associated with
1

Examples of stochastic simulation methods are Den Haan and Marcet (1990), Smith
(1991), Maliar and Maliar (2005), Judd, Maliar and Maliar (2010b) (henceforth, JMM),
and examples of projection methods are Judd (1992), Christiano and Fisher (2000),
Krueger and Kubler (2004), and JMM (2010a).

2

curve-fitting decrease rapidly with the degree of the approximating polynomial function. Furthermore, errors associated with a stochastically generated solution domain decrease rapidly with the simulation length. Finally,
the integration errors also decrease with the simulation length but slowly.2
Under the conventional one-node Monte Carlo method, the integration errors are considerably larger than other kinds of errors and restrict the overall
accuracy of solutions. After we substitute the conventional one-node MonteCarlo integration method with a one-node Gauss-Hermite quadrature integration method, the integration errors decrease dramatically. As a result,
 combines high accuracy of projection methods and low cost in highdimensional problems of stochastic simulation methods.3
We first test the performance of  on the example of the standard
representative-agent growth model. We find that for a wide range of parameters, replacing conventional one-node Monte Carlo integration with one-node
Gauss-Hermite quadrature integration reduces the solution errors by about
2 − 3 orders of magnitude, while replacing it with several-node quadrature
integration reduces such errors by up to 5 orders of magnitude. For example,
for Monte Carlo integration with with 1, 30 and 3000 nodes, the solution
errors (measured by the size of Euler equation errors on a stochastic simulation) are at most 51 · 10−4 , 31 · 10−4 and 12 · 10−5 , respectively, while for
quadrature integration with 1, 2 and 10 nodes, the solution errors are at most
63 · 10−7 , 17 · 10−9 and 16 · 10−9 , respectively. Surprisingly, the quadrature
integration method with just one node leads to more accurate solutions than
a Monte Carlo method with thousands of nodes.
We next study the performance of  in the context of a heterogeneousagent growth model, namely, we consider a multi-country model with up to
thirty heterogeneous countries. We find that for a second-degree polynomial,
the one-node Gauss-Hermite quadrature integration method produces errors
that are up to two orders of magnitude smaller than those produced by the
conventional one-node Monte Carlo integration method. For example, after
we substitute one-node Monte Carlo integration by one-node quadrature in2

Galant and Nychka (1987) analyze a relation between the choice of an approximating
polynomial function and the sample size in the context of semi-parametric maximum
likelihood estimation.
3
Stochastic simulation algorithms operate on the ergodic set realized in equilibrium.
This allows to avoid costs associated with finding a solution in areas of state space that
are never realized in equilibrium. The higher is the dimensionality of the problem, the
larger is the gain from focusing on the ergodic set; see JMM (2010b) for a discussion.

3

tegration, the solution error in a thirty-country model goes down from 5·10−3
to 25 · 10−5 .
We finally point out that the use of numerically stable approximation
methods is crucial for the successful performance of . The problem of
recovering policy functions from simulated data is often ill-conditioned, and
standard least-squares methods (such as ordinary least-squares and GaussNewton methods) work only under low-degree polynomial approximations.
JMM (2009, 2010b) describe a variety of approximation methods that can
handle ill-conditioned problems in the context of stochastic simulation algorithms. The numerically stable approximation methods include the leastsquares method based on SVD, Tikhonov regularization, least absolute deviation methods and principal components method. These methods help restore
numerical stability under high-degree polynomial approximations and allow
us to achieve high accuracy of solutions.
The rest of the paper is as follows: In Section 2, we describe the representative agent model (the heterogeneous agents model is outlined in Appendix
A). In Section 3, we present . In Section 4, we discuss the determinants
of accuracy of . In Section 5, we describe the numerical experiments
performed for both the representative- and heterogeneous-agent models. Finally, in Section 6, we conclude.

2

The model

We study the standard representative-agent neoclassical stochastic growth
model:
∞
X
0
   ( )
(1)
max
{ +1 }=0∞

=0

s.t.  + +1 = (1 − )  +   ( ) 
¡
¢
+1 ∼ N 0  2 
ln +1 =  ln  + +1 

(2)

(3)

where initial condition (0  0 ) is given. Here,  is the operator of conditional expectation;  ,  and  are, respectively, consumption, capital and
productivity level;  ∈ (0 1) is the discount factor;  ∈ (0 1] is the depreciation rate of capital;  ∈ (−1 1) is the autocorrelation coeﬃcient; and  ≥ 0
is the standard deviation. The utility and production functions,  and  , respectively, are assumed to be strictly increasing, continuously diﬀerentiable
and concave.
4

An interior solution to problem (1) − (3) satisfies the following Euler
equation:
0 ( ) =  {0 (+1 ) [1 −  + +1  0 (+1 )]} 
(4)

where 0 and  0 are the first derivatives of the utility and production functions, respectively. In this paper, we look for a solution to problem (1) − (3)
in the form of capital policy function, +1 =  (   ), satisfying conditions
(2) − (4). To approximate the capital policy function, we use the following
representation of Euler equation (4),
+1 =  [ (    +1 )] 

(5)

where  (    +1 ) is given by
 (    +1 ) ≡ 

0 (+1 )
[1 −  + +1  0 (+1 )] +1 
0 ( )

(6)

with  and +1 being determined by conditions (2) and (3) and by the
capital policy function,  (   ). Condition (5) holds because 0 ( ) 6= 0
and because +1 is -measurable.4

3

Generalized stochastic simulation algorithm

We parameterize end-of-period capital, +1 , in the left side of the Euler
equation (5) with a flexible functional form,
+1 = Ψ (   ; ) 

(7)

where  is a vector of coeﬃcients. To find , the generalized stochastic
simulation algorithm () proceeds as follows:
Choose a simulation length,  . Fix initial condition (0  0 ). Draw a
sequence of productivity shocks, { }=1 . Compute a sequence of productivity levels, { }=0 , using (3), and fix it for all simulations. For initial
iteration  = 1, fix a vector of coeﬃcients  (1) .
4

In a similar way, one can use Euler equation (4) to express other -measurable variables,
e.g., ln (+1 ),  and 0 ( ). JMM (2009) show that the choice of a policy function to
approximate can aﬀect the numerical stability of stochastic simulation methods.

5

´
³
• Step 1. On iteration , use the assumed policy function Ψ    ;  ()
in (7) to compute forward the capital path, {+1 }=0 , for the given
sequence { }=0 . Calculate the consumption path, { }=0 , from
budget constraint (2).
• Step 2. Approximate conditional expectation (integral)  [ (    +1 )]
in (5) for  = 0   − 1 as a weighted sum of the integrand, , evaluated in  nodes:
 ≡


X

 (    +1 )  

(8)

=1

where  is defined in (6) and {+1 }=1 and { }=1 are
integration nodes and weights, respectively; +1 is given by (7); 
follows directly from (2); and +1 is determined by (2) conditional
on future productivity shock +1 with +1 =  exp (+1 ) and
+2 ≡ Ψ (Ψ (   ; )   exp (+1 ) ; ).
b that minimizes the distance
• Step 3. Find a vector of coeﬃcients 
between  and Ψ (   ; ) by running a regression.

• Step 4. Compute the vector of coeﬃcients to be used on the nextiteration  (+1) using fixed-point iteration, namely,
b
 (+1) = (1 − )  () + 

(9)

where  ∈ (0 1) is a damping parameter.

Iterate on Steps 1-4 until convergence is achieved,
¯
¯

()
(+1)
1 X ¯¯ +1 − +1 ¯¯
(10)
¯
¯  10− 
()
¯
 =1 ¯
+1
n
o
o
n
()
(+1)
where +1
and +1
are the capital paths obtained
=1

=1

on iterations  and  + 1, respectively, and parameter   0 determines
the convergence criterion. (Note that convergence of the capital path
implies convergence of the polynomial coeﬃcients, ).

6

 is similar to the stochastic simulation algorithm () in JMM
(2009, 2010b) except for the integration procedure in Step 2.  relies on
a specific type of Monte Carlo integration, whereas  can use any integration methods including those unrelated to the estimated density function.
Formula (8) represents two alternative integration methods, Monte Carlo and
Gauss-Hermite quadrature ones, however other methods can be used as well.
Monte Carlo integration method For each period  and each node ,
an -node Monte Carlo integration method,  (), draws  shocks for
the next period, {+1 }=1 , and computes (8) by assigning equal weights
to all nodes, i.e.  = 1 for all  and . We refer to  using the
-node Monte Carlo integration method as  − (). One-node
Monte Carlo integration,  (1), is the conventional integration procedure
used by stochastic simulation methods; see Den Haan and Marcet (1990) and
JMM (2009, 2010b). It approximates the conditional expectation in (5) by
the value of integrand (6) realized in period  + 1, i.e.,  = 1, +11 = +1
and 1 = 1 (this implies that +11 =  exp (+1 )).
Gauss-Hermite quadrature integration method An -node GaussHermite quadrature integration method,  (), evaluates (8) using nodes
and weights that are constructed by Gauss-Hermite quadrature integration.
For example, two-node Gauss-Hermite quadrature,  (2), uses +11 = −,
+12 =  and 1 = 2 = 12 , and three-node Gauss-Hermite quadrature,
q
q
√
 (3), uses +11 = 0, +12 = 32 , +13 = − 32 and 1 = 2 3  , 2 =
√

3 = 6 for all ; see Judd (1998 p.261). We refer to  using -node
Gauss-Hermite quadrature integration as −(). A one-node GaussHermite quadrature integration method,  (1), approximates the conditional
expectation in (5) by the value of integrand (6) under the assumption of zero
shock in period  + 1, i.e.,  = 1, +11 = 0 and 1 = 1 (this implies that
+11 =  ).

4

Determinants of accuracy

We now explore factors that determine the accuracy of . First, since
the integration method used in Step 2 is not exact, we have an integration

7

error  , namely,
 =  [·] −  

(11)

where  [·] is a compact notation for the exact conditional expectation in
(5), and  is the approximation given by (8).
Second, since we use a finite-degree polynomial for approximation, there
is an error from omitting high-degree polynomial terms (curve-fitting error).
Let Ψ (   ; ) =    be a complete
polynomial ¢of degree 
¡ ordinary

2
composed of +1 terms, where  = 1          2   
∈ R1×(+1)




 0
and  = ( 0   1     ) ∈ R(+1)×1 . Furthermore, let ∞  ∞ be an
infinite-degree polynomial, which is equivalent to the true policy function
 (   ). Then, the error from omitting high-degree polynomial terms is
 ∞−  ∞− ≡  ∞  ∞ −     
(12)
¡
¡
¢0
¢0
where  ∞ ≡ 0∞   ∞−1 ∈ R ×∞ ,   ≡ 0   −1 ∈ R ×(+1) ,
0
∞
∞×1
and   is the initial segment of  ∞ = ( ∞
.
0   1  ) ∈ R
If we use an approximating polynomial of an infinite degree,  ∞  ∞ , but
our integration method produces errors of type (11), the regression model in
Step 3 is
 =  ∞  ∞ + 
(13)
where  ≡ (0 [·]    −1 [·])0 ∈ R ×1 and  ≡ (0    −1 )0 ∈ R ×1 . Let us
assume that the coeﬃcients in regression equation (13) are estimated using
the ordinary least-squares (OLS) method,
£
£
¤
¤
b∞ = ( ∞ )0  ∞ −1 ( ∞ )0  =  ∞ + ( ∞ )0  ∞ −1 ( ∞ )0 


(14)

∞

b denotes the OLS estimator of  ∞ .5 The more accurate is the
where 
b∞
integration rule, the smaller is the integration error, , and the closer is 
to  ∞ .
Since, in practice, we use a finite- rather than infinite-degree polynomial,
the regression model in Step 3 is misspecified,
 =     + 
5

(15)

OLS is used in the discussion of this section for the sake of expository convenience.
In the context of the stochastic simulation class of methods, OLS is numerically unstable.
See JMM (2009, 2010b) for a description of numerically stable approaches that can be
used in the approximation step of a stochastic simulation algorithm.

8

where  ≡ (0    −1 )0 ∈ R ×1 is an error term. Substituting the true
regression model, (13), into the OLS estimator corresponding to (15) and
using (12), we get
£
¤
b = (  )0   −1 (  )0 ( ∞  ∞ + ) =

£
£
¤−1  0
¤−1  0 ∞− ∞−
( )  + (  )0  
( ) 

 (16)
  + (  )0  

b , contains errors of both types, (11) and
As a result, the OLS estimator, 
(12). To measure accuracy of a solution, we compute the diﬀerence in capital
allocations produced by the true policy function,  ∞  ∞ , and our approxib ,
mate policy function,   


b = −   − [  −  ]  ∞−  ∞− 
 ≡  ∞ ∞ −  

(17)

£
¤−1  0
where  is a  ×  identity matrix, and   ≡   (  )0  
( ) is
a matrix known in econometrics as a projection matrix.
Below, we assess the magnitude of errors of types (11) and (12) in our
model. We do not intend to give formal proofs concerning bounds on the
errors but lay out a series of arguments that helps us expose what we find
later to be quantitatively important.
Integration errors Let us consider an infinitely dimensional polynomial
approximation  ∞  ∞ leading to estimator (14), and let us assume that the
integration error,  , is  with zero mean and constant variance,  2 (i.e.,
we neglect a bias resulting from omitting the high-degree polynomial terms).
Under these assumptions, we have the standard version of the Central Limit
Theorem, namely, the asymptotic distributions of OLS estimator (14) and
the capital-allocation errors in (17) are given, respectively, by
´
³ £
´
√ ³ 
¤
b −   ∼ N 0 ( ∞ )0  ∞ −1  2 
(18)
 

√
¡
¢
  ∼ N 0  ∞  2 
(19)
Thus,
√ a rate of convergence of our approximate solution to the true solution
is  . This means that to increase accuracy of the solution by an order
of magnitude, we must increase the simulation length,  , by two orders of
magnitude.

9

Consider the conventional one-node Monte Carlo integration method,
 (1), which approximates the expectation in (5) with the next-period
realization of the integrand, so that the integration error is
 =  [ (    +1 )] −  (    +1 ) 

(20)

where  (    +1 ) is given by (6). In a typical real business cycle model,
fluctuations in variables like  (    +1 ) amount to several percents. If integration error (20) is on average 1%, (i.e.,   = 001), then regression model
(13) with  = 10 000 observations allows
√ us to approximate the conditional
expectation with accuracy of about     = 10−4 . To achieve accuracy of
10−5 , we need to increase the simulation length to  = 1 000 000. Thus,
a very long simulation is needed to approximate the conditional expectation with a high degree of accuracy if the integration method used is not
suﬃciently accurate.6
We now present an integration theorem regarding our second integration
method, the Gauss-Hermite quadrature one; see, e.g., Judd (1998, p. 261).
Theorem 1 Under an -node Gauss-Hermite quadrature integration method,
the integration error is equal to
√
!  (2)

 =  [ (    +1 )] −  = 
(    ) 
(21)
2 (2)! 3
(2)

where  (    +1 ) and  are given by (6) and (8), respectively; 3
denotes a 2-th order partial derivative of  with respect to the third argument;
and  is some real number,  ∈ (−∞ ∞).
For a function  that is smooth and has little curvature, integration error
with the number of quadrature nodes, . This is because
 decreases rapidly
√

the term 2!(2)!
goes rapidly to zero with . For example, for  equal to 1,
2, 3 and 10, this term is 044, 0037, 00018 and 3 × 10−15 , respectively. In
particular, Gauss-Hermite quadrature integration is exact for functions that
(2)
are linear in  since for such functions, we have 3 (    ) = 0 for all
 ≥ 1.
6

The√Monte Carlo integration method with  nodes,  (), has the convergence
rate of   and has the cost proportional to  . Roughly speaking, an increase in the
number of integration nodes from  = 1 to   1 under   () has the same eﬀect on
accuracy and cost as an increase in the simulation length from  to   under  (1).

10

The properties of the integration errors produced by Gauss-Hermite quadrature are in general unknown. If  has a∞ non-zero expected value, the Central
b does not converge to  ∞ asymptotLimit Theorem does not apply, and 
ically. However, provided that the integration errors are very small in size,
our approximation to the conditional expectation will be still very accurate.
Error from omitting high-degree polynomial terms (curve-fitting
error) Suppose now that the only source of errors in (16) and (17) is the
omission of high-degree polynomial terms,  ∞−  ∞− (i.e., we assume that
the integration procedure is exact so that integration errors are absent). Below, we evaluate errors resulting from omitting high-degree polynomial terms
in a one-dimensional case under the Chebyshev polynomial representation;
see Judd (1998, p. 209). Chebyshev polynomial is defined with the following
recursive formula: 0 () = 1, 1 () =  and  () = 2−1 ()−−2 ()
for  ≥ 2.
Theorem 2 Let a function  : [−1 1] → R be  times diﬀerentiable and
have a Chebyshev polynomial representation
∞
X
1
 () =  0 +
   () 
2
=1

(22)

where { }=0∞ are the Chebyshev polynomial coeﬃcients. Then, there
exists a constant  such that | | ≤  ,  ≥ 1.
Thus, the Chebyshev polynomial coeﬃcients decline rapidly with the
polynomial degree  if function  () is suﬃciently smooth. Provided that
the coeﬃcients on high-degree polynomial terms are small in size, the error from omitting such terms,  ∞−  ∞− , is also small in size. Other
polynomial representations (e.g., ordinary polynomials, Hermite polynomials, Legendre polynomials) are linear combinations of Chebyshev polynomials
so that result (22) applies to them as well.
Errors associated with stochastically generated solution domain
In the above discussion, we treated the dependent variables,   , in the regression as being exogenous. However, such variables are constructed by
 endogenously as a part of the solution procedure. It might happen
that the simulated points produced by  do not adequately represent
11

the relevant solution domain. First, the simulation length,  , might be not
suﬃciently large. Second, the solution might depend on a specific random
sequence of productivity levels, { }=0 . Third, the capital series prob , might diﬀer from those
duced by our approximate policy function,   
implied by the true policy function,  ∞  ∞ . Finally, stochastic simulation
overrepresents the center of the ergodic distribution and underrepresents the
tails of the ergodic distribution. The above factors can be also important for
accuracy of solutions.

5

Numerical experiments

In this section, we discuss the implementation details of  and describe
the results of our numerical experiments. We first study the representativeagent model of Section 2, and we later extend this model to include heterogeneous agents.

5.1

Representative-agent model

We investigate how the simulation length, polynomial degree and integration method aﬀect accuracy of the  solutions in the context of the
representative-agent model.
5.1.1

Implementation

We assume a constant relative risk aversion (CRRA) utility function,  ( ) =
1−
−1

, with risk-aversion coeﬃcient  ∈ (0 ∞) and a Cobb-Douglas produc1−
tion function,  ( ) =  , with capital share  = 036. We set the discount
factor and depreciation rate at  = 099 and  = 0025, respectively. We
set the damping parameter for fixed-point iteration in (9) at  = 01, and
we set the convergence parameter in (10) at  = 9 (i.e., we target nine-digit
precision).
We implement one- and -node Monte Carlo and quadrature integration
methods as discussed in Section 3. For a detailed description of quadrature
and monomial formulas and examples of their use, see JMM (2010a). Our
regression method in Step 3 is the numerically stable least-squares method
based on singular value decomposition, SVD; see JMM (2009, 2010b).

12

For each experiment, we report the time necessary for computing a solution (in seconds) and unit-free Euler equation errors (defined as the diﬀerence
between the left and right sides of (5) divided by +1 ) on a stochastic simulation of 1,000 observations. To compute conditional expectation in the
test, we use an accurate ten-node Gauss-Hermite quadrature rule. We run
the computational experiments on a desktop computer ASUS with Intel(R)
Core(TM)2 Quad CPU Q9400 (2.66 GHz). Our programs are written in
Matlab, version 7.6.0.324 (R2008a).
5.1.2

Monte Carlo integration

We first solve model (1) − (3) using  − (). We parameterize the
CRRA utility function by  = 1, and we parameterize the process for shocks
(3) by  = 095 and  = 001. We implement Monte Carlo integration using
 randomly drawn nodes in each simulated point with  ∈ {1 30 3000}; we
vary the simulation length by  ∈ {30 300 3000 10000}, and we compute
polynomial approximations up to degree five,  ∈ {1  5}. −(1)
corresponds to  as is used in JMM (2009). The results are reported in
Table 1.
When the simulation length,  , and the number of integration nodes, ,
are small,  − () fails to deliver high-degree polynomial approximations and the accuracy of solutions is low. Increasing  and  helps
restore numerical stability and increase accuracy. The highest accuracy is
achieved in the experiment with the largest values of  and  considered
(i.e.,  = 10 000 and  = 3 000) under the third-degree polynomial,  = 3,
namely, the Euler equation errors are of size 10−6 .
5.1.3

Gauss-Hermite quadrature integration

We next solve model (1) − (3) using  − (). We specifically repeat
the experiments reported in Section 5.1.2 using Gauss-Hermite quadrature
integration with  ∈ {1 2 10} nodes instead of the Monte Carlo integration
with  ∈ {1 30 3000} nodes. We report the results in Table 2.
The accuracy of the solutions increases with the polynomial degree, the
sample size and the number of integration nodes. Any of these three factors
can restrict the overall accuracy of solutions. Specifically, if we use a rigid
first-degree polynomial, the maximum error is of order 10−4 ; if we use a
small sample size of  = 30, it is of order 10−5 ; and if we use the least
13

accurate one-node integration rule, it is of order 10−7 . In turn, under our
most accurate case of fifth-degree polynomial,  = 10 000 and the most
accurate ten-node integration, the maximum error is of order 10−9 . Under
quadrature integration, the simulation length plays a less important role
than under Monte Carlo integration. This is because in the former case, the
simulation length aﬀects only the solution domain, while in the latter case, it
aﬀects both the solution domain and the number of integration nodes. The
key result of Table 2 is that the change in the integration procedure leads
to a large increase in accuracy. The least accurate solution, produced by
 − () under  = 30 and  = 1, is still more accurate than the
most accurate solution, delivered by −() under  = 10 000 and
 = 3000. In order  − () had a comparable degree of accuracy
as  − (), its running time must be of orders of magnitude larger.
5.1.4

Sensitivity experiments

In Table 3, we present the results of the sensitivity experiments, namely,
we consider  ∈ {02 1 5},  ∈ {095 099} and  ∈ {001 003}. We use
 = 10 000, polynomial approximations up to degree five and two alternative
integration methods such as  (1) and  () with  = {1 2 10}.
As is seen from the table, the tendencies, observed in the sensitivity experiments, are similar to those observed in Tables 1 and 2. Under some parameterizations, the diﬀerence between the mean and maximum errors is large
(up to two orders of magnitude), which indicates that the approximation obtained is not uniformly accurate. Since stochastic simulation overrepresents
the center and underrepresents the tails of the ergodic distribution, the accuracy of solutions on tails might be low. To achieve more uniform accuracy,
we should use an approximation method that minimizes the maximum error
instead of the least-squares error. Finally, we find that the results are not
visibly aﬀected by a specific random draw of productivity shocks.

5.2

Heterogeneous-agent economy

In this section, we apply  for solving a multi-country (heterogeneousagent) variant of representative-agent model (1) − (3). A formal description
of the heterogeneous-agent model, the parameters choice and the implementation details are provided in Appendix A. We compute the second-degree
polynomial approximations, and we employ two alternative integration meth14

ods, namely, the conventional (one-node) Monte Carlo integration method,
 (1), and the one-node Gauss-Hermite quadrature integration method,
 (1). The results are shown in Table 4.
Under  (1), the accuracy of solutions visibly depends on the sample
size,  , relative to the dimensionality of the problem,  (the number of the
regression coeﬃcients,  + 1). For a given  , an increase in  leads to a lower
degree of accuracy because there are more regression coeﬃcients to identify;
for example, under  = 10 000, going from  = 2 to  = 30 increases
the mean solution error by about a factor of 10, namely, from 83 · 10−5 to
93·10−4 . For a given , an increase in  leads to a higher degree of accuracy;
for example, under  = 8, going from  = 10 000 to  = 100 000 decreases
the mean solution error by about a factor√of 3, namely, from 24 · 10−4 to
74 · 10−5 . These results are in line with a  -rate of convergence predicted
by the Central Limit Theorem.
Replacing  (1) with  (1) increases the accuracy of solutions between
one and two orders of magnitude. A reduction in the simulation length from
 = 10 000 to  = 3 000 has virtually no eﬀect on the accuracy of solutions.
With a small number of countries, we can reduce  even further without a
visible decrease in accuracy (these experiments are not reported), however,
we must have at least as many observations as the polynomial coeﬃcients to
identify; for example, for the model with  = 30, we cannot reduce  below
 + 1 = 1 891.
The computational time for  (1) and  (1) is similar: it ranges from 3
minutes to 3 hours depending on the number of countries, , and simulation
length,  . This time is quite modest given that we use a standard desktop
computer and solve computationally costly high-dimensional problems.
Finally, in the studied models,  −  (1) delivers accuracy that is
comparable to the highest accuracy achieved in the literature; see KMMP
(2011) for accuracy comparison of diﬀerent solution methods. If  (1) happens to be not suﬃciently accurate in other applications, we can extend
 to include more accurate integration methods such as low-cost nonproduct monomial integration rules; see JMM (2010a).

6

Conclusion

In this paper, we generalize the stochastic simulation approach to make it
compatible with both Monte Carlo and deterministic (quadrature and mono15

mial) integration methods. At the conceptual level, our analysis provides a
link between the stochastic simulation and projection classes of algorithms
and makes it possible to assess diﬀerent kinds of approximation errors. At
the practical level, our generalized framework allows us to select the integration method most suitable for a given application. Any existing stochastic
simulation algorithm can be easily extended to include deterministic integration. A version of  based on one-node Gauss-Hermite quadrature
integration is particularly easy to implement and is as simple and intuitive
as the conventional stochastic simulation method based on one-node Monte
Carlo integration.  can solve high-dimensional problems that are computationally demanding and even intractable for earlier solution procedures,
and it delivers accuracy comparable to the highest accuracy attained in the
literature.

References
[1] Christiano, L. and D. Fisher, (2000). Algorithms for solving dynamic
models with occasionally binding constraints. Journal of Economic Dynamics and Control 24, 1179-1232.
[2] Den Haan, W. and A. Marcet, (1990). Solving the stochastic growth
model by parameterizing expectations. Journal of Business and Economic Statistics 8, 31-34.
[3] Gallant, R. and D. Nychka (1987). Semi-Nonparametric maximum likelihood estimation. Econometrica 55, 363-390.
[4] Judd, K., (1992). Projection methods for solving aggregate growth models. Journal of Economic Theory 58, 410-452.
[5] Judd, K., (1998). Numerical Methods in Economics. London, England:
The MIT Press, Cambridge Massachusetts.
[6] Judd, K., L. Maliar and S. Maliar, (2009). Numerically stable stochastic simulation approaches for solving dynamic economic models, NBER
working paper 15296.
[7] Judd, K., L. Maliar and S. Maliar, (2010a). A cluster-grid projection
method: solving problems with high dimensionality, NBER working paper 15965.
16

[8] Judd, K., L. Maliar and S. Maliar, (2010b). Numerically stable stochastic simulation approaches for solving dynamic economic models. Manuscript.
[9] Kollmann, R., S. Maliar, B. Malin and P. Pichler, (2011). Comparison
of solutions to the multi-country real business cycle model. Journal of
Economic Dynamics and Control 35, 186-202.
[10] Krueger, D. and F. Kubler, (2004). Computing equilibrium in OLG
models with production. Journal of Economic Dynamics and Control
28, 1411-1436.
[11] Maliar, L. and S. Maliar, (2005). Solving nonlinear stochastic growth
models: iterating on value function by simulations. Economics Letters
87, 135-140.
[12] Maliar, S., L. Maliar and K. Judd, (2011). Solving the multi-country real
business cycle model using ergodic set methods. Journal of Economic
Dynamic and Control 35, 207-228.
[13] Smith, A., (1991). Solving stochastic dynamic programming problems
using rules of thumb. Queen’s University. Economics Department. Discussion Paper 816.

7

Appendix A

In this section, we describe the heterogeneous-agent model studied in Section
5.2. For the sake of comparison, we consider the same setup as the one used
in JMM (2010a) and JMM (2010b) for testing the performance of the clustergrid and stochastic simulation approaches, respectively (the notation in this
section corresponds to that in JMM, 2010b).
A world economy consists of  countries. Each country is populated by
a representative consumer. The social planner’s problem is
"∞
#

X
X
¡
¢
max
0

   
(23)
=1
 
{ +1 }=0∞ =1
=0

17

subject to

X
=1



+


X
=1


+1

=


X
=1



(1 − ) +


X
=1

¡ ¢
    

(24)

¡
¢
ln +1 =  ln  + +1 +  +1 ,  = 1  
(25)
©   ª=1
where initial condition 0  0
is given, and  ∼ N (0  2 ),   ∼

N (0 2 ). We denote by  , +1
, +1 and   a country’s  consumption,
end-of-period capital, productivity level and welfare weight, respectively. A
common productivity shock, +1 , and a country-specific technology shock,
 +1 , determines country’s  productivity level. The utility and production
functions,  and   , are both increasing, continuously diﬀerentiable and
concave.

We assume that  =  and
P =  for  = 1  , which implies that


 = 1 and that  =  ≡ 1 =1  for  = 1  . We parameterize the
³©
ª=1 ´

, by a
=     
capital policy functions of each country, +1
³©
´
ª

polynomial Ψ    =1 ;   using a set of Euler equations:

½ 0
¾
³©
´
¡  ¢¤ 
ª
 (+1 ) £

0
1 −  + +1  +1 +1 ≈ Ψ    =1 ;   
=   0
 ( )
(26)
for  = 1  . Note that since the countries are identical in their fundamentals, we could have computed just one capital policy function for all countries.
However, we treat the countries as fully heterogeneous and compute a separate policy function for each country considered. This approach allows us
to make a judgement regarding accuracy and costs in models with heterogeneous fundamentals.  for solving the heterogeneous-agent model is
similar to the one presented in Section 3 for the case of the representativeagent model, however, instead of doing each step just once, we do it for each
country  = 1  .
We parameterize the model by  = 1;  = 095 and  = 001. We set
the damping parameter for the first- and second-degree polynomial approximations at  = 01 and  = 005, respectively; we use  = 6 (i.e., six-digit
precision) for all experiments. To estimate the regression coeﬃcients, we
normalize the data and use the OLS method. In the studied model, data
normalization is suﬃcient for stabilizing the stochastic simulation approach

+1

18

under low-degree polynomial approximations; see JMM (2009, 2010b) for
a discussion. An important advantage of linear approximation methods is
that they can be vectorized in multi-dimensional applications; this enables
us to iterate on policy functions of all heterogeneous countries simultaneously
which reduces the costs considerably. To compute the Euler equation errors,
we evaluate the conditional expectation in (26) using a 2-node monomial
rule; JMM (2010a) shows that this rule is suﬃciently accurate for the purpose
of accuracy checks.

19

Table 1. Solving the benchmark model using Monte Carlo integration.
T=30

emean

emax

T=300
CPU

emean

Monte Carlo integration with P=1 draw
1st degree 8,98(-4) 2,16(-3)
1 2,97(-4)
2nd degree
3,80(-4)
3rd degree
4th degree
5th degree
Monte Carlo integration with P=30 draws
1st degree 1,28(-4) 7,10(-4)
4 6,00(-5)
2nd degree
1,84(-4)
3rd degree
2,27(-4)
4th degree
5,79(-4)
5th degree
Monte Carlo with integration with P=3000 draws
1st degree 1,28(-4) 5,26(-4) 4(2) 5,41(-5)
2nd degree 7,28(-4) 5,39(-3) 3(2) 1,39(-5)
3rd degree 2,08(-3) 9,51(-2) 6(2) 3,07(-5)
4th degree
1,32(-4)
5th degree
7,66(-4)

emax

T=3000
CPU

T=10000

emean

emax

CPU

emean

emax

CPU

1,36(-3)
2,03(-3)
-

4
3
-

5,38(-5)
9,26(-5)
1,03(-4)
1,38(-4)
1,53(-4)

3,05(-4)
3,94(-4)
5,91(-4)
1,01(-3)
1,64(-3)

1(1)
1(1)
1(1)
1(1)
1(1)

5,5(-5)
4,0(-5)
5,1(-5)
5,4(-5)
7,1(-5)

3,0(-4)
2,0(-4)
3,3(-4)
9,0(-4)
5,1(-4)

1(1)
3(1)
2(1)
3(1)
3(1)

3,15(-4)
1,02(-3)
2,61(-3)
1,02(-2)
-

1(1)
1(1)
1(1)
5(1)
-

4,40(-5)
1,60(-5)
1,89(-5)
2,04(-5)
2,96(-5)

2,58(-4)
6,68(-5)
8,87(-5)
2,59(-4)
3,35(-4)

6(1)
4(1)
5(1)
5(1)
6(1)

4,7(-5)
8,6(-6)
1,5(-5)
1,6(-5)
1,7(-5)

3,0(-4)
4,1(-5)
1,4(-4)
1,6(-4)
3,1(-4)

2(2)
2(2)
1(2)
1(2)
1(2)

3,82(-4)
1,14(-4)
3,55(-4)
3,19(-3)
2,68(-2)

6(2)
6(2)
5(2)
4(2)
5(2)

4,36(-5)
1,74(-6)
2,29(-6)
2,45(-6)
2,51(-6)

2,86(-4)
1,92(-5)
4,00(-5)
5,58(-5)
2,73(-5)

3(3)
1(3)
1(3)
1(3)
1(3)

4,9(-5)
1,2(-6)
1,1(-6)
1,1(-6)
1,2(-6)

3,1(-4)
1,0(-5)
9,5(-6)
1,0(-5)
1,2(-5)

7(3)
5(3)
2(3)
3(3)
2(3)

Remark: emean and emax are the average and maximum Euler equation errors, respectively; CPU is computational time in
seconds; and the notation x(m) means x·10m.

Table 2. Solving the benchmark model using the Gauss-Hermite quadrature integration method.
T=30

emean

emax

T=300
CPU

emean

Gauss-Hermite quadrature with P=1 node
1st degree 4,49(-5) 2,66(-4)
2 5,19(-5)
2nd degree 1,05(-6) 1,12(-5)
2 1,18(-6)
3rd degree 1,92(-6) 3,25(-5) 8(-2) 4,84(-7)
4th degree 8,05(-7) 1,78(-5) 1(-1) 4,82(-7)
5th degree 5,57(-7) 7,61(-6) 2(-1) 4,90(-7)
Gauss-Hermite quadrature with P=2 nodes
1st degree 4,68(-5) 2,71(-4)
2 5,19(-5)
2nd degree 9,08(-7) 1,15(-5)
2 1,03(-6)
3rd degree 1,34(-6) 2,00(-5) 1(-1) 4,77(-8)
4th degree 9,25(-7) 1,71(-5) 1(-1) 3,27(-9)
5th degree 3,74(-7) 1,08(-5) 2(-1) 1,57(-8)
Gauss-Hermite quadrature with P=10 nodes
1st degree 4,68(-5) 2,71(-4)
1 5,19(-5)
2nd degree 9,08(-7) 1,15(-5)
2 1,03(-6)
3rd degree 1,34(-6) 2,00(-5) 1(-1) 4,78(-8)
4th degree 9,25(-7) 1,71(-5) 1(-1) 3,28(-9)
5th degree 3,03(-7) 6,79(-6) 2(-1) 1,57(-8)

T=3000

T=10000

emax

CPU

emean

emax

CPU

emean

emax

CPU

3,62(-4)
1,32(-5)
1,45(-6)
6,41(-7)
6,63(-7)

5
4
1
6(-1)
9(-2)

4,50(-5)
9,11(-7)
4,83(-7)
4,86(-7)
4,86(-7)

2,90(-4)
9,29(-6)
7,07(-7)
6,30(-7)
6,26(-7)

1(1)
1(1)
8
3
1

5,0(-5)
1,1(-6)
4,8(-7)
4,9(-7)
4,9(-7)

3,0(-4)
1,0(-5)
9,5(-7)
6,3(-7)
6,3(-7)

5(1)
4(1)
3(1)
2(1)
9

3,68(-4)
1,33(-5)
1,15(-6)
8,65(-8)
4,21(-7)

4
2
2
1
6(-2)

4,36(-5)
7,62(-7)
2,46(-8)
1,24(-9)
5,21(-10)

2,85(-4)
9,60(-6)
3,56(-7)
2,23(-8)
7,22(-9)

1(1)
1(1)
8
4
2

4,8(-5)
8,6(-7)
3,3(-8)
2,1(-9)
1,1(-10)

3,1(-4)
1,0(-5)
6,5(-7)
4,1(-8)
1,7(-9)

5(1)
3(1)
2(1)
1(1)
9

3,68(-4)
1,33(-5)
1,15(-6)
8,65(-8)
4,22(-7)

8
8
2
7(-1)
9(-2)

4,36(-5)
7,62(-7)
2,46(-8)
1,25(-9)
5,24(-10)

2,85(-4)
9,60(-6)
3,56(-7)
2,25(-8)
7,00(-9)

4(1)
3(1)
2(1)
1(1)
4

4,8(-5)
8,6(-7)
3,3(-8)
2,1(-9)
1,1(-10)

3,1(-4)
1,0(-5)
6,5(-7)
4,1(-8)
1,6(-9)

2(2)
1(2)
5(1)
4(1)
2(1)

Remark: emean and emax are the average and maximum Euler equation errors, respectively; CPU is computational time in
seconds; and the notation x(m) means x·10m.

Table 3. Comparison of the Monte Carlo and Gauss-Hermite quadrature integration methods: sensitivity results.
γ = 1/5

emean

emax

γ=5
CPU

emean

emax

Monte Carlo integration with P=1 random draw
1st degree
1,9(-5)
1,9(-4)
8 4,5(-4) 3,1(-3)
2nd degree
1,2(-5)
1,2(-4) 1(1) 2,2(-4) 3,8(-4)
3rd degree
1,5(-5)
1,3(-4) 1(1) 2,7(-4) 2,3(-3)
4th degree
1,9(-5)
4,6(-4) 1(1) 3,4(-4) 3,4(-3)
5th degree
2,2(-5)
4,2(-4) 2(1)
Gauss-Hermite quadrature with P=1 node
1st degree
1,5(-5)
1,4(-4) 2(1) 4,3(-4) 3,2(-3)
2nd degree
2,2(-6)
4,1(-6) 2(1) 1,1(-4) 2,3(-4)
3rd degree
2,1(-6)
2,3(-6) 2(1) 1,1(-4) 1,4(-4)
4th degree
2,1(-6)
2,2(-6) 1(1) 1,1(-4) 1,3(-4)
5th degree
2,1(-6)
2,2(-6)
4 1,1(-4) 1,3(-4)
Gauss-Hermite quadrature with P=2 nodes
1st degree
1,4(-5)
1,5(-4) 2(1) 4,5(-4) 3,2(-3)
2nd degree
2,6(-7)
3,2(-6) 2(1) 1,3(-5) 1,6(-4)
3rd degree
1,0(-8)
1,9(-7) 1(1) 7,0(-7) 1,5(-5)
4th degree 4,1(-10)
7,6(-9)
6 6,0(-8) 1,5(-6)
5th degree 8,8(-11) 5,4(-10)
2 5,7(-9) 1,5(-7)
Gauss-Hermite quadrature with P=10 nodes
1st degree
1,4(-5)
1,5(-4) 8(1) 4,5(-4) 3,2(-3)
2nd degree
2,6(-7)
3,2(-6) 5(1) 1,3(-5) 1,6(-4)
3rd degree
1,0(-8)
1,9(-7) 3(1) 7,0(-7) 1,5(-5)
4th degree 4,2(-10)
7,6(-9) 1(1) 6,0(-8) 1,5(-6)
5th degree 6,5(-11) 5,0(-10)
5 5,4(-9) 1,5(-7)
Remark: emean and emax are the average and maximum Euler
seconds; and the notation x(m) means x·10m.

ρ = 0.99

σ = 0.03

CPU

emean

emax

CPU

emean

emax

CPU

8(1)
2(2)
2(2)
3(2)
-

1,1(-4)
6,3(-5)
1,1(-4)
1,1(-4)
1,4(-4)

7,1(-4)
1,4(-4)
7,9(-4)
9,6(-4)
1,3(-3)

8
4(1)
3(1)
4(1)
4(1)

4,50(-4)
1,20(-4)
1,57(-4)
1,65(-4)
2,11(-4)

3,12(-3)
5,39(-4)
1,12(-3)
2,51(-3)
2,72(-3)

1(1)
5(1)
4(1)
5(1)
6(1)

3(2)
3(2)
2(2)
2(2)
1(2)

1,2(-4)
6,0(-6)
5,1(-6)
5,1(-6)
5,1(-6)

7,2(-4)
3,0(-5)
6,8(-6)
5,7(-6)
5,7(-6)

5(1)
4(1)
3(1)
2(1)
2(1)

5,03(-4)
2,94(-5)
5,44(-6)
4,35(-6)
4,36(-6)

3,21(-3)
3,14(-4)
6,43(-5)
1,24(-5)
8,17(-6)

5(1)
5(1)
4(1)
3(1)
2(1)

3(2)
3(2)
2(2)
1(2)
1(2)

1,2(-4)
3,5(-6)
2,9(-7)
3,8(-8)
5,3(-9)

7,3(-4)
3,3(-5)
4,8(-6)
5,6(-7)
6,6(-8)

4(1)
4(1)
3(1)
2(1)
2(1)

4,91(-4)
2,67(-5)
3,18(-6)
5,54(-7)
9,18(-8)

3,22(-3)
3,23(-4)
6,61(-5)
1,33(-5)
2,30(-6)

5(1)
5(1)
4(1)
3(1)
2(1)

9(2) 1,2(-4) 7,3(-4) 2(2) 4,91(-4) 3,22(-3)
7(2) 3,5(-6) 3,3(-5) 2(2) 2,67(-5) 3,23(-4)
5(2) 2,9(-7) 4,8(-6) 1(2) 3,18(-6) 6,61(-5)
3(2) 3,8(-8) 5,6(-7) 7(1) 5,55(-7) 1,33(-5)
3(2) 5,3(-9) 6,6(-8) 6(1) 9,18(-8) 2,31(-6)
equation errors, respectively; CPU is computational

2(2)
2(2)
1(2)
8(1)
6(1)
time in

Table 4. Solving the multi-country model using Monte-Carlo and Gauss-Hermite quadrature integration: comparison results.
J

n 1

J=2
J=4
J=6
J=8
J=10
J=12
J=16
J=20
J=30

15
45
91
153
231
325
561
861
1891

MC(1), T = 10,000
emax CPU
8.3(-5) 1.1(-3) 8(2)
1.4(-4) 1.3(-3) 1(3)
2.0(-4) 1.6(-3) 2(3)
2.4(-4) 1.4(-3) 3(3)
2.9(-4) 1.7(-3) 5(3)
3.6(-4) 2.1(-3) 6(3)
4.4(-4) 2.4(-3) 2(4)
5.6(-4) 3.1(-3) 2(4)
9.3(-4) 5.0(-3) 1(5)

emean

MC(1), T = 100,000
emax CPU
2.6(-5) 2.4(-4) 9(3)
4.5(-5) 2.7(-4) 1(4)
5.7(-5) 3.5(-4) 2(4)
7.4(-5) 4.5(-4) 2(4)
-

emean

Q(1), T=10,000

emean

emax

7,19(-6)
9,56(-6)
1,04(-5)
1,09(-5)
1,10(-5)
1,12(-5)
1,13(-5)
1,14(-5)
1,15(-5)

4,06(-5)
4,54(-5)
3,55(-5)
3,20(-5)
3,01(-5)
2,97(-5)
3,17(-5)
2,93(-5)
2,54(-5)

Q(1), T=3000
CPU
2(3)
4(3)
5(3)
9(3)
1(4)
2(4)
3(4)
5(4)
1(5)

emean

emax

7,31(-6)
9,60(-6)
1,02(-5)
1,06(-5)
1,08(-5)
1,09(-5)
1,11(-5)
1,12(-5)
1,10(-5)

3,59(-5)
4,59(-5)
3,53(-5)
3,22(-5)
3,09(-5)
3,01(-5)
3,56(-5)
3,75(-5)
3,81(-5)

CPU
6(2)
1(3)
1(3)
2(3)
5(3)
2(3)
5(3)
6(3)
2(4)

Remark: J is the number of countries; n 1 is the number of polynomial coefficients in the parameterized decision rule of
one country; emean and emax are the average and maximum Euler equation errors, respectively; T is the simulation length;
CPU is computational time in seconds; and the notation x(m) means x·10m.

