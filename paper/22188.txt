NBER WORKING PAPER SERIES

DECOMPOSING DURATION DEPENDENCE IN A STOPPING TIME MODEL
Fernando E. Alvarez
Katarína Borovičková
Robert Shimer
Working Paper 22188
http://www.nber.org/papers/w22188

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
April 2016

We are grateful for comments from Jaap Abbring, Bo Honoré, Stephane Bonhomme, Jan Eberly,
Rasmus Lentz, Andreas Mueller, Emi Nakamura, Pedro Portugal, Andrea Rotnitzky, Jon
Steinsson, Quang Vuong, and Josef Zweimüller, as well as participants in numerous seminars.
The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.
At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w22188.ack
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2016 by Fernando E. Alvarez, Katarína Borovičková, and Robert Shimer. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.

Decomposing Duration Dependence in a Stopping Time Model
Fernando E. Alvarez, Katarína Borovičková, and Robert
Shimer NBER Working Paper No. 22188
April 2016
JEL No. E24,J64
ABSTRACT
We develop a dynamic model of transitions in and out of employment. A worker finds a job at an
optimal stopping time, when a Brownian motion with drift hits a barrier. This implies that the
duration of each worker's jobless spells has an inverse Gaussian distribution. We allow for
arbitrary heterogeneity across workers in the parameters of this distribution and prove that the
distribution of these parameters is identified from the duration of two spells. We use social
security data for Austrian workers to estimate the model. We conclude that dynamic selection is
a critical source of duration dependence.
Fernando E. Alvarez
University of Chicago
Department of Economics
1126 East 59th Street
Chicago, IL 60637
and NBER
f-alvarez1@uchicago.edu
Katarína Borovičková
New York University
Department of Economics
19 West 4th Street
New York, NY 10003
kb103@nyu.edu

Robert Shimer
Department of Economics
University of Chicago
1126 East 59th Street
Chicago, IL 60637
and NBER
shimer@uchicago.edu

1

Introduction

The hazard rate of finding a job is higher for workers who have just exited employment than
for workers who have been out of work for a long time. Economists and statisticians have long
understood that this reflects a combination of two factors: structural duration dependence
in the job finding rate for each individual worker, and changes in the composition of workers
at different non-employment durations (Cox, 1972). The goal of this paper is to explore a
flexible but testable model of the job finding rate for any individual worker, allow for arbitrary
heterogeneity across workers, and decompose these two factors. To do this, we develop a
structural economic model which views finding a job as an optimal stopping problem for
each worker. On top of this, we layer an arbitrary degree of individual heterogeneity. Thus,
while the structure of our economic model is pinned down by economic theory, we treat
heterogeneity flexibly, since here economic theory offers us no guidance. In the end, we
attribute about twice as much of the observed decline in the job finding rate to changes in
the composition of jobless workers at different durations, compared to what we would have
obtained from a more standard statistical approach to the decomposition.
One interpretation of our structural model is a classical theory of employment. All
individuals always have two options, working at some time-varying wage or not working and
receiving some time-varying income and utility from leisure. These values are persistent but
change over time. If there were no cost of switching employment status, an individual would
work if and only if the wage were sufficiently high relative to the value of not working. We
add a switching cost to this simple model, so a worker starts working when the difference
between the wage and non-employment income is sufficiently large and stops working when
the difference is sufficiently small. Given a specification of the individual’s preferences, a level
of the switching cost, and the stochastic process for the wage and non-employment income,
this theory generates a structural model of duration dependence for any individual worker.
For instance, the model allows that workers gradually accumulate skills while employed and
lose them while out of work, as in Ljungqvist and Sargent (1998).
An alternative interpretation of our structural model is a classical theory of unemployment. According to this interpretation, a worker’s productivity and her wage follow a
stochastic process. Again, the difference is persistent but changes over time. If the worker
is unemployed, a monopsonist has the option of paying a fixed cost to employ the worker,
then earning flow profits equal to the difference between productivity and the wage. Given
a specification of the hiring cost and the stochastic process for productivity and the wage,
the theory generates the same structural duration dependence for any individual worker.
We also allow for arbitrary individual heterogeneity in the parameters describing pref-

1

erences, fixed costs, and stochastic processes. For example, some individuals may expect
their labor market productivity to increase the longer they stay out of work while others
may expect it to fall. We maintain two key restrictions: for each individual, the evolution
of a latent variable, the net benefit from employment, follows a geometric Brownian motion
with drift during a non-employment spell; and each individual starts working when the net
benefit exceeds some fixed threshold and stops working when it falls below some (weakly)
lower threshold. In the first interpretation of our structural model, these thresholds are determined by the worker while in the second interpretation they are determined by the firm.
These assumptions imply that the duration of a non-employment spell is given by the first
passage time of a Brownian motion with drift, a random variable with an inverse Gaussian
distribution. The parameters of the inverse Gaussian distribution are fixed over time for
each individual but we allow that they vary arbitrarily across individuals.
Given this environment, we ask four key questions. First, we ask whether the distribution
of unobserved heterogeneity is identified. We prove that an economist armed with data on
the joint distribution of the duration of two completed non-employment spells can identify
the population distribution of the parameters of the inverse Gaussian distribution, except for
the sign of the drift in the underlying Brownian motion. We discuss this important limitation
to identification and show how information on incomplete spells can help overcome this.
Second we ask whether the model has testable implications. We show that an economist
armed with the same data on the joint distribution of the completed duration of two spells
can potentially reject the model. Moreover, the test has power against competing models.
We prove that if the true data generating process is one in which each individual has a
constant hazard of finding a job, the economist will always reject our model. Similarly, we
prove that if the true data generating process is one in which each individual has a log-normal
distribution for duration, the economist will always reject our model. The same result holds
if the data generating process is a finite mixture of such models.
Third, we ask whether we can use the partial identification of the model parameters to
decompose the observed evolution of the hazard of exiting non-employment into the portion
attributable to structural duration dependence and the portion attributable to unobserved
heterogeneity. We propose a simple multiplicative decomposition. With a mixed proportional
hazard model (Lancaster, 1979), this decomposition would recover the baseline hazard, so
this generalizes that well-known approach.
Finally, we show that we can use duration data as well as information about wage dynamics to infer the size of the fixed cost of switching employment status. Even small fixed
costs give rise to a large region of inaction, which in turn affects the duration of job search
spells. We show how to invert this relationship to recover the fixed costs.
2

We then use data from the Austrian social security registry from 1986 to 2007 to test
our model, estimate the distribution of unobserved parameters, evaluate the decomposition,
and bound the size of fixed costs. Using data on over 750,000 individuals who experience at
least two non-employment spells, we find that we cannot reject our model and we uncover
substantial heterogeneity across individuals. Although the raw hazard rate is hump-shaped
with a peak at around 10 weeks, the hazard rate for the average individual increases until
about 20 weeks and then declines by much less. Overall, changes in the composition of
jobless workers accounts for about a 74 percent reduction in the hazard rate during the first
two years of non-employment. We also estimate tiny fixed costs. For the median individual,
the total cost of taking a job and later leaving it are approximately equal to six minutes of
leisure time. As a result, the median newly employed worker leaves her job if she experiences
a 1.6 percent drop in the wage. Previous work has shown that small fixed costs can generate
large regions of inaction (Dixit, 1991; Abel and Eberly, 1994). We find that not only are the
fixed costs small, but so is the region of inaction.
There are a few other papers that use the first passage time of a Brownian motion to
model duration dependence. Lancaster (1972) examines whether such a model does a good
job of describing the duration of strikes in the United Kingdom. He creates 8 industry
groups and observes between 54 and 225 strikes per industry group. He then estimates
the parameters of the first passage time under the assumption that they are fixed within
industry group but allowed to vary arbitrarily across groups. He concludes that the model
does a good job of describing the duration of strikes, although subsequent research armed
with better data reached a different conclusion (Newby and Winterton, 1983). In contrast,
our testing and identification results require only two observations per individual and allow
for arbitrary heterogeneity across individuals.
Shimer (2008) assumes that the duration of an unemployment spell is given by the first
passage time of a Brownian motion but does not allow for any heterogeneity across individuals. The first passage time model has also been adopted in medical statistics, where
the latent variable is a patient’s health and the outcome of interest is mortality (Aalen and
Gjessing, 2001; Lee and Whitmore, 2006, 2010). For obvious reasons, such data do not allow
for multiple observations per individual, and so bio-statistics researchers have so far not
introduced unobserved individual heterogeneity into the model. These papers have also not
explored either testing or identification of the model.
Abbring (2012) considers a more general model than ours, allowing that the latent net
benefit from employment is spectrally negative Lévy process, e.g. the sum of a Brownian
motion with drift and a Poisson process with negative increments. On the other hand, he
assumes that individuals differ only along a single dimension, the distance between the barrier
3

for stopping and starting an employment spell. In contrast, we allow for two dimensions of
heterogeneity, and so our approach to identification is completely different. We also go
beyond Abbring (2012) by confronting the model with real-world data.
Within economics, the mixed proportional hazard model has received far more attention
than the first passage time model. This model assumes that the probability of finding a job
at duration t is the product of three terms: a baseline hazard rate that varies depending
on the duration of non-employment, a function of observable characteristics of individuals,
and an unobservable characteristic. Our model neither nests the mixed proportional hazard
model nor is it nested by that model. Relaxing the mixed proportional hazard assumption,
which is feasible because of our large data set, is important for our finding that heterogeneity
plays a critical role in the evolution of the hazard rate. We show that if we would impose
the mixed proportional hazard assumption on our data, we would find that heterogeneity
accounts for much less of the observed duration dependence.
Despite the difference in our conclusions, our work harkens back to an older literature
on identification of the mixed proportional hazard model. Elbers and Ridder (1982) and
Heckman and Singer (1984a) show that such a model is identified using a single spell of
non-employment and appropriate variation in the observable characteristics of individuals.
Heckman and Singer (1984b) illustrates the perils of parametric identification strategies in
this context. Even closer to the spirit of our paper, Honoré (1993) shows that the mixed
proportional hazard model is also identified with data on the duration of at least two nonemployment spells for each individual.
Finally, some recent papers analyze duration dependence using models that are identified through assumptions on the extent of unobserved heterogeneity. For example, Krueger,
Cramer, and Cho (2014) argue that observed heterogeneity is not important in accounting
for duration dependence and so conclude that unobserved heterogeneity must also be unimportant. Hornstein (2012) and Ahn and Hamilton (2015) both assume there are two types of
workers with different job finding hazards at all durations. We show that in our model and
with our data set, identification through assumptions on the number of unobserved types
leads us to understate the role of unobserved heterogeneity.
The remainder of the paper proceeds as follows. In Section 2, we describe our structural
model, show that the model generates an inverse Gaussian distribution of duration for each
worker, and discuss how we can use duration data to infer the magnitude of switching costs.
Section 3 contains our main theoretical results on duration analysis. We prove that a subset
of the parameters is identified if we observe at least two non-employment spells for each
individual, discuss how information on incomplete spells can provide additional information
that helps identify the model, and mention the limitations of any analysis that relies on single4

spell data. In Section 4, we propose a multiplicative decomposition of the aggregate hazard
rate into the portion attributable to structural duration dependence and the proportion
attributable to heterogeneity. Section 5 summarizes the Austrian social security registry
data. Section 6 presents our empirical results, including tests and estimates of the model,
decomposition of hazard rates, comparison to the mixed proportional hazard model, and
inference of the distribution of fixed costs. Finally, Section 7 briefly concludes.

2
2.1

Theory
Structural Model

We consider the problem of a risk-neutral, infinitely-lived worker with discount rate r who
can either be employed, s(t) = e, or non-employed, s(t) = n, at each instant in continuous
time t. The worker earns a wage ew(t) when employed and gets flow utility eb(t) when nonemployed. Both w(t) and b(t) follow correlated Brownian motions with drift, but the drift
and standard deviation of each may depend on the worker’s employment status. If the worker
is non-employed at t, she can become employed by paying a fixed cost ψe eb(t) for a constant
ψe ≥ 0. Likewise, the worker can switch from employment to non-employment by paying a
cost ψn eb(t) for a constant ψn ≥ 0. The worker must decide optimally when to change her
employment status s(t).
It is convenient to define ω(t) ≡ w(t) − b(t), the worker’s (log) net benefit from employment. This inherits the properties of w and b, following a random walk with state-dependent
drift and volatility given by:
dω(t) = µs(t) dt + σs(t) dB(t),

(1)

where B(t) is a standard Brownian motion and µs(t) and σs(t) are the drift and instantaneous
standard deviation when the worker is in state s(t).
Appendix A describes and solves the worker’s problem fully. There we impose restrictions
on the drift and volatility of w(t) and b(t) both while employed and non-employed to ensure
that the worker’s value is finite. We then prove that the worker’s employment decision
depends only on her employment status s(t) and her net benefit from employment ω(t).
In particular, the worker’s optimal policy involves a pair of thresholds. If s(t) = e and
ω(t) ≥ ω, the worker remains employed, while she stops working the first time ω(t) < ω. If
s(t) = n and ω(t) ≤ ω̄, the worker remains non-employed, while she takes a job the first time
ω(t) > ω̄. Assuming the sum of the fixed costs ψe + ψn is strictly positive, the thresholds
satisfy ω̄ > ω, while the thresholds are equal if both fixed costs are zero.
5

We have so far described a model of voluntary non-employment, in the sense that a
worker optimally chooses when to work. But a simple reinterpretation of the objects in the
model turns it into a model of involuntary unemployment. In this interpretation, the wage
is eb(t) , while a worker’s productivity is ew(t) . If the worker is employed by a monopsonist, it
earns flow profits ew(t) − eb(t) . If the worker is unemployed, the firm may hire her by paying
a fixed cost ψe eb(t) , and similarly the firm must pay ψn eb(t) to fire the worker. In this case,
the firm’s optimal policy involves the same pair of thresholds. If s(t) = e and ω(t) ≥ ω, the
firm retains the worker, while she is fired the first time ω(t) < ω. If s(t) = n and ω(t) ≤ ω̄,
the worker remains unemployed, while a firm hires her the first time ω(t) > ω̄.
Proposition 4 in Appendix A provides an approximate characterization of the distance
between the thresholds, ω̄ − ω, as a function of the fixed costs when the fixed costs are
small for arbitrary parameter values. Here we consider a special case, where the utility from
unemployment is constant, b(t) = b̄ for all t. We still allow the stochastic process for wages
to depend on a worker’s employment status. Then
(ω̄ − ω)3 ≈

12rσe2 σn2
p
p
(ψe + ψn ).
(µe + µ2e + 2rσe2 )(−µn + µ2n + 2rσn2 )

(2)

An increase in the fixed costs increases the distance between the thresholds ω̄ − ω, as one
would expect. An increase in the volatility of the net benefit from employment, σn or σe ,
has the same effect because it raises the option value of delay. An increase in the drift in
the net benefit from employment while out of work, µn , or a decrease in the drift in the
net benefit from employment while employed, µe , also increases the distance between the
thresholds. Intuitively, an increase in µn or a reduction in µe reduces the amount of time it
takes to go between any fixed thresholds. The worker optimally responds by increasing the
distance between the thresholds.
This structural model is similar to the one in Alvarez and Shimer (2011) and Shimer
(2008). In particular, setting the switching cost to zero (ψe = ψn = 0) gives a decision rule
with ω̄ = ω, as in the version of Alvarez and Shimer (2011) with only rest unemployment, and
with the same implication for non-employment duration as Shimer (2008). Another difference
is that here we allow the process for wages to depend on a worker’s employment status,
(µe , σe ) 6= (µn , σn ). The difference in the drift µe and µn allows us to capture structural
features such as those emphasized by Ljungqvist and Sargent (1998), who explain the high
duration of European unemployment using “...a search model where workers accumulate
skills on the job and lose skills during unemployment.”
The most important difference is that this paper allows for arbitrary time-invariant worker
heterogeneity. An individual worker is described by a large number of structural parameters,
6

including her discount rate r, her fixed costs ψe , and ψn , and all the parameters governing
the joint stochastic processes for her wage and benefit, both while the worker is employed
and while she is non-employed. We allow for arbitrary distributions of these structural
parameters in the population, subject only to the constraint that the utility is finite.

2.2

Duration Distribution

We use the structural model to determine the distribution of non-employment duration for
any single individual. All non-employment spells start when an employed worker’s log net
benefit from employment hits the lower threshold ω. The log net benefit from employment then follows the stochastic process dω(t) = µn dt + σn dB(t) and the non-employment
spell ends when it hits the upper threshold ω̄. Therefore the duration of a completed nonemployment spell is given by the first passage time of a Brownian motion with drift. This
random variable has an inverse Gaussian distribution with density function at duration t
(αt−β)2
β
e− 2t ,
f (t; α, β) = √
2π t3/2

(3)

where α ≡ µn /σn and β ≡ (ω̄ − ω)/σn . Hence, even though each worker is described by a
large number of structural parameters, only two reduced-form parameters α and β determine
how long a worker stays without a job. Note β is nonnegative by assumption, while α may
R∞
be positive or negative. If α is nonnegative, 0 f (t; α, β)dt = 1, so a worker almost surely
returns to work. But if α is negative, the probability of eventually returning to work is
e2αβ < 1, so there is a probability the worker has a defective spell and never finds a job.
Thus a non-employed worker with α negative faces a risk of a severe form of long term
non-employment, since with probability 1 − e2αβ she stays forever non-employed.
The inverse Gaussian is flexible, but the model still imposes some restrictions on behavior.
Figure 1 shows hazard rates for different values of α and β. It reveals that for the most part,
β controls the shape of the hazard rate and α controls its level. Assuming β is strictly
positive, the hazard rate of exiting non-employment always starts at 0 when t = 0, achieves
a maximum value at some finite time t which depends on both α and β, and then declines to
a long run limit of α2 /2 if α is positive and 0 otherwise. If β = 0, the hazard rate is initially
infinite and declines monotonically towards its long-run limit.
If α is positive, the expected duration of a completed non-employment spell is β/α and
the variance of duration is β/α3 . As a spell progresses, the expected residual duration
converges to 2/α2 , which may be bigger or smaller than the initial expected duration. The
model is therefore consistent with both positive and negative duration dependence in the
structural exit rate from non-employment.
7

small β
medium β
large β

hazard rates

0.04

0.04

0.02

0

small α
medium α
large α

0.02

0

100
200
300
duration in weeks

0

400

0

100
200
300
duration in weeks

400

Figure 1: Hazard rates implied by the inverse Gaussian distribution for different values of
α > 0 and β. The left panel shows hazard rates for α = 0.1 and three different values of β,
1, 10, and 30. The right panel shows hazard rates for three different values of α, 0.10, 0.18,
and 0.27. We also adjust the value of β to keep the peak of the hazard rate at the same
duration, which gives β = 10, 9.5, and 9.2, respectively.
Our analysis explicitly assumes that there is no time-varying heterogeneity beyond that
captured by the model. For example, a worker’s experience cannot affect the stochastic
process for the net benefit from employment, (µs , σs ), nor can it affect the switching costs
ψs , s ∈ {e, n}. However, our model does allow for learning-by-doing, since a worker’s
wage may increase faster on average when employed than when non-employed, µe > µn .
Our maintained assumption is that the parameters α and β are constant for each worker
throughout her lifetime.
On the other hand, we allow for an arbitrary population distribution of these parameters, which we label G(α, β). A major goal of this paper is to recover G from data. This
heterogeneity exacerbates the structural duration dependence. Take two types of workers
characterized by reduced-form parameters (α1 , β1 ) and (α2 , β2 ) and suppose α1 ≤ α2 and
β1 ≥ β2 , with at least one inequality strict. Then type 2 workers have a higher hazard
rate of finding a job at all durations t and so the population of long-term non-employed
workers is increasingly populated by type 1 workers, those with a lower hazard of exiting
non-employment.

2.3

Magnitude of the Switching Costs

Non-employment duration is determined by two reduced-form parameters, α and β, the
distribution of which we will estimate. Although we will not recover the underlying structural
parameters, we show in this section that we can use this distribution and a small amount of
8

additional information to bound the magnitude of workers’ switching costs.
We focus on the special case highlighted in equation (2), where the utility from nonemployment is constant, b(t) = b̄ for all t. Suppose we observe worker’s type (α, β), as well
as the parameters of the wage process when working (µe , σe ), the drift of the wage when not
working µn and the discount rate r. Assuming that µe > 0, we find that

p
√
µ µ2
3 2
2
2
2
(µe + µe + 2rσe )(−α + α + 2r)β µn  6eσe2n
∼
(ψe + ψn ) ≈
 µe µ2n
12 r α2 σe2
2
3 σe

β3
|α|3
β3

if α > 0
α2

(4)

if α < 0

|α|3 r

Equation (4) expresses the fixed costs as a function of four parameters, µe , σe , µn , and r, as
well as α and β.1 Since the discount rate r is typically small, in (4) we derive two expressions
for the limit as r → 0, one for positive and one for negative α.2
To back out the magnitude of switching costs, we need to choose values for the parameters
µn , µe , σe , and r. Since we expect the estimated fixed costs to be small, our strategy will be
to choose their values to make the fixed costs as large as possible while still staying within
a range that can be supported empirically. We use the second part of equation (4) to guide
our choice, since it tells us whether a given structural parameter increases or decreases fixed
costs. In Section 6.6, we use estimated distribution of α and β to calculate the distribution
of the fixed costs.
We can also use a simple calculation to deduce whether switching costs are necessarily
positive. If switching costs were zero, the distance between the barriers would be zero as
well, i.e. β = 0. In that limit, the duration density (13) is ill-behaved. Nevertheless, we can
compute the density conditional on durations lying in some interval [t, t̄]:
f (t; α, β|t ∈ [t, t̄]) = R t̄
t

t−3/2 e−
τ −3/2 e−

α2 t
2
α2 τ
2

dτ

.

The expected value of a random draw from this distribution is
1
1 
Φ(αt̄ 2 ) − Φ(αt 2 ) /α

 ≤ t t̄
Φ0 (αt )/t − Φ0 (αt̄ )/t̄ − α Φ(αt̄ ) − Φ(αt ) )
1
2

1
2

1
2

1
2

1
2

1
2

 21

,

with the inequality binding when α = 0. Thus viewed through the lens of our model, if we
observe that the average duration of all spells with duration in the interval [t, t̄] exceeds the
geometric mean of t and t̄, we can conclude that switching costs must be positive for at least
1

The sense in which we use the approximation ≈ in expression (4), as well as its derivation for the general
model, is in Proposition 4 in Appendix A.
2
In 4, we use ∼ to mean that the ratio of the two functions converges to one as r converges to 0.

9

some individuals. We show below that this is the case in our data set.

3

Duration Analysis

This section examines how we can use duration data to evaluate this model. We start by
showing that the joint distribution of the reduced-form parameters α and β, G(α, β), is
identified using data on the completed duration of two spells for each individual and a sign
restriction on the drift in the net benefit from employment while non-employed. We then
show that incorporating information on the frequency of defective spells allows us to relax the
sign restriction. We also show that the model is in fact overidentified and develop testable
implications using data on the completed duration of two spells. Finally, we show that the
model is identified with a single spell only under strong auxiliary assumptions, such as that
there is a known, finite number of types in the population.
Our analysis builds on the structure of our economic model. We assume that each
individual is characterized by a pair of parameters (α, β) and that density of each completed
spell length is an inverse Gaussian f (t; α, β) for that individual. In particular, we impose that
α and β are fixed over time for each individual, although the parameters may vary arbitrarily
across individuals, reflecting some time-invariant observed or unobserved heterogeneity.

3.1

Intuition for Identification

Consider the following two data generating processes. In the first, there is a single type of
worker (α, β), giving rise to the duration density f (t; α, β) in equation (3). In the second,
there are many types of workers. A worker who takes d periods to find a job has σn = 0 and
µn = (ω̄−ω)/d, which implies that both α and β converge to infinity with β/α = d. Moreover,
the distribution of this ratio differs across workers so as to generate the same population
duration density as the first data generating process. There is no way to distinguish these
two data generating processes using a single non-employment spell.
With two completed spells for each individual, however, distinguishing these two models
is trivial. In the first model without any heterogeneity, the duration of an individual’s first
spell tells us nothing about the duration of her second spell. In particular, the correlation
between the durations of the two spells is zero. In the second model without any uncertainty
in the duration of a spell for each individual, the duration of an individual’s two spells is
identical. In particular, the correlation between the durations of the two spells is one.
This simple example suggests that the distribution of the duration of the second spell
conditional on the duration of the first spell should provide some information on the under-

10

lying type distribution. Our main result is that this information, together with some prior
information about the sign of drift in the net benefit from employment while non-employed,
identifies the type distribution.

3.2

Proof of Identification

Let G(α, β) denote the distribution of (α, β) in some population. Assume that all members
of this population completed their first spell and started their second spell. The second spell
may be completed or defective. The assumption that we observe (at least) two spells for
each individual lies at the heart of our identification, and hence our results apply only to
this population.
For some individuals in our population, the first two spells have duration (t1 , t2 ) ∈ T 2 ,
where T ⊆ R+ is a set with non-empty interior.3 Let φ : T 2 → R+ denote the joint
distribution of the durations for this subpopulation. According to the model,
R

φ(t1 , t2 ) = R
T2

R

f (t1 ; α, β)f (t2 ; α, β)dG(α, β)
.
f (t01 ; α, β)f (t02 ; α, β)dG(α, β) d(t01 , t02 )

(5)

Our main identification result, Theorem 1 below, is that the joint density of spell lengths φ
identifies the joint distribution of characteristics G if we know the sign of α, i.e. the drift in
the net benefit from employment while non-employed.
We prove this result through a series of Propositions. The first shows that the partial
derivatives of φ exist at all points where t1 6= t2 :
Proposition 1 Take any (t1 , t2 ) ∈ T 2 with t1 > 0, t2 > 0 and t1 6= t2 . For any G, the
density φ is infinitely many times differentiable at (t1 , t2 ).
We prove this proposition in Appendix B. The proof verifies the conditions under which the
Leibniz formula for differentiation under the integral is valid. This requires us to bound the
derivatives in appropriate ways, which we accomplish by characterizing the structure of the
partial derivatives of the product of two inverse Gaussian densities. Our bound uses that
t1 6= t2 , and indeed an example shows that this condition is indispensable:
Example 1 Assume that β is distributed Pareto with parameter θ while α = dβ for some
constant d, equal to the common mean duration of all individuals’ spells. Solving equation (5)
3

We allow for the possibility that T is a subset of the positive reals to prove that our model is identified
even if we do not observe spells of certain durations.

11

implies that the joint density of two spells is
φ(t1 , t2 ) =

θ∆θ/2−1


Γ
1
−
θ/2,
∆
3/2 3/2
4πt1 t2

(6)

R∞
2
2
where ∆ ≡ 12 (t1 /d−1)
+ (t2 /d−1)
and Γ(1 − θ/2, ∆) ≡ ∆ z −θ/2 e−z dz is the incomplete
t1
t2
Gamma function.
When either t1 6= d or t2 6= d or both, ∆ is strictly positive and hence φ(t1 , t2 ) is infinitely
θ
differentiable. But when t1 = t2 = d, ∆ = 0 and so both the Gamma function and ∆ 2 −1 can
either diverge or be non-differentiable. In particular, for θ ∈ (0, 2), limt→d φ(t, t) = ∞. For
θ ∈ [2, 4), the density is finite but non-differentiable at t1 = t2 = d. For higher values of θ,
the density can only be differentiated a finite number of times at this critical point.
The source of the non-differentiability is that when β converges to infinity, the volatility
of the Brownian motion vanishes, and thus the spells end with certainty at duration d. Equivalently the corresponding distribution tends to a Dirac measure concentrated at t1 = t2 = d.
For a distribution with a sufficiently thick right tail of β, the same phenomenon happens,
but only at points with t1 = t2 , since individuals with vanishingly small volatility in their
Brownian motion almost never have durations t1 6= t2 . Instead, for values of t1 6= t2 , the
density φ is well-behaved because randomness from the Brownian motion smooths out the
duration distribution, regardless of the underlying type distribution.
For the next Proposition, we look at the conditional distribution of (α, β) among individuals whose two spells last exactly (t1 , t2 ) periods:
G̃(α, β|t1 , t2 ) ≡ R

f (t1 , α, β) f (t2 , α, β) dG(α, β)
,
f (t1 , α0 , β 0 ) f (t2 , α0 , β 0 ) dG(α0 , β 0 )

(7)

We prove that the partial derivatives of φ uniquely identify all the even moments of G̃ for
any t1 6= t2 :
Proposition 2 Take any (t1 , t2 ) ∈ T 2 with t1 > 0, t2 > 0 and t1 6= t2 , and any strictly
positive integer m. The set of partial derivatives ∂ i+j φ(t1 , t2 )/∂ti1 ∂tj2 for all i ∈ {0, 1, . . . , m}
and j ∈ {0, 1, . . . , m − i} uniquely identifies the set of moments
2i 2j

E(α β |t1 , t2 ) ≡

Z

α2i β 2j dG̃(α, β|t1 , t2 )

(8)

for all i ∈ {0, 1, . . . , m} and j ∈ {0, 1, . . . , m − i}.
Note that the statement of the proposition suggests a recursive structure, which we follow
in our proof in appendix B. In the first step, set m = 1. The two first partial deriva12

tives ∂φ(t1 , t2 )/∂t1 and ∂φ(t1 , t2 )/∂t2 determine the two first even moments, E(α2 |t1 , t2 )
and E(β 2 |t1 , t2 ). In the second step, set m = 2. The three second partial derivatives
and the results from first step then determine the three second even moments, E(α4 |t1 , t2 ),
E(α2 β 2 |t1 , t2 ), and E(β 4 |t1 , t2 ). In the mth step, the m + 1 mth partial derivatives and the
results from the previous steps determine the m + 1 mth even moments of G̃.
In the third Proposition, we recover the joint distribution G̃(α, β|t1 , t2 ) from the moments
of (α2 , β 2 ) among individuals who find jobs at durations (t1 , t2 ). There are two pieces to
this. First, we need to know the sign of α; here we assume this is either always positive or
always negative, although other assumptions would work. We show in Section 3.3 that the
sign of α is not identified using completed spell data alone. Second, we need to ensure that
the moments uniquely determine the distribution function. A sufficient condition is that the
moments not grow too fast; our proof verifies that this is the case.
Proposition 3 Assume that α ≥ 0 with G-probability 1 or that α ≤ 0 with G-probability
1. Take any (t1 , t2 ) ∈ T 2 with t1 > 0, t2 > 0 and t1 6= t2 . The set of conditional moments E(α2i β 2j |t1 , t2 ) for (i, j) ∈ {0, 1, . . .}2 , defined in equation (8), uniquely identifies the
conditional distribution G̃(α, β|t1 , t2 ).
The proof of this proposition in Appendix B.
Our main identification result follows immediately from these three propositions:
Theorem 1 Assume that α ≥ 0 with G-probability 1 or that α ≤ 0 with G-probability 1.
Take any density function φ : T 2 → R+ . There is at most one distribution function G such
that equation (5) holds.
Proof. Proposition 1 shows that for any G, φ is infinitely many times differentiable. Proposition 2 shows that for any (t1 , t2 ) ∈ T 2 , t1 6= t2 , t1 > 0, and t2 > 0, there is one solution for
the moments of (α2 , β 2 ) conditional on durations (t1 , t2 ), given all the partial derivatives of
φ at (t1 , t2 ). Proposition 3 shows that these moments uniquely determine the distribution
function G̃(α, β|t1 , t2 ) with the additional assumption that α ≥ 0 with G-probability 1 or
α ≤ 0 with G-probability 1. Finally, given the conditional distribution G̃(·, ·|t1 , t2 ), we can
recover G(·, ·) using equation (7) and the known functional form of the inverse Gaussian
density f :
dG̃(α, β|t1 , t2 ) f (t1 ; α0 , β 0 )f (t2 ; α0 , β 0 )
dG(α, β)
=
(9)
dG(α0 , β 0 )
dG̃(α0 , β 0 |t1 , t2 ) f (t1 ; α, β)f (t2 ; α, β)
Our theorem states that the density φ is sufficient to recover the joint distribution G if
we know the sign of α. Our proof uses all the derivatives of φ evaluated at a point (t1 , t2 )
13

to recover all the moments of the conditional distribution G̃(·, ·|t1 , t2 ). Intuitively, if one
thinks of a Taylor expansion around (t1 , t2 ), we are using the entire empirical density φ for
(t1 , t2 ) ∈ T 2 to recover the distribution function G.
We comment briefly on an alternative but ultimately unsuccessful proof strategy. Proposition 2 establishes that we can measure E(α2i β 2j |t1 , t2 ) at almost all (t1 , t2 ) and all i and j. It
might seem we could therefore integrate the conditional moments using the density φ(t1 , t2 )
to compute the unconditional (2i, 2j)th moment of G. This strategy might fail, however,
because the integral need not converge; indeed, this is the case whenever the appropriate
moment of G does not exist. We continue Example 1 to illustrate this possibility:
Example 2 Assume that β is distributed Pareto with parameter θ while α = dβ for some
constant d. The distribution G thus does not have all its moments. Nevertheless, we find
that the conditional moments are well-defined:
m

E(β |t1 , t2 ) = ∆

−m
2

, ∆)
Γ(1 + m−θ
2
,
Γ(1 − 2θ , ∆)


R ∞ s−1 −z
2
(t2 /d−1)2
where again ∆ ≡ 21 (t1 /d−1)
+
and
Γ(s,
x)
≡
z e dz is the incomplete Gamma
t1
t2
x
function; this follows from equation (7).
If ∆ > 0, all conditional moments Mm ≡ E(β m |t1 , t2 ) exist and are finite. Moreover,
the moments do not grow too fast, so we can use the D’Alembert criterium (see for example
Theorem A.5 in Coelho, Alberto, and Grilo (2005)) to prove that the conditional moments
uniquely determine the conditional distribution G̃(α, β|t1 , t2 ). Finally, we can use Bayes rule
to recover the unconditional distribution G, even though that distribution only has finitely
many moments.

3.3

Share of Population with Negative Drift

Our theoretical model makes no predictions about the sign of the drift in the net benefit
from employment while an individual is non-employed, i.e. about the sign of α. Completed
spell data alone also cannot identify the sign of this reduced-form parameter. This is a
consequence of the functional form of the inverse Gaussian distribution, which implies
f (t; α, β) = e2αβ f (t; −α, β)
for all α, β, and t; see equation (3). Proportionality of f (t; α, β) and f (t; −α, β) implies
that, once we condition on an individual having n ≥ 1 completed spells, the probability
distribution over completed durations (t1 , . . . , tn ) is the same if the individual is described
14

by reduced-form parameters (α, β) or (−α, β).
On the other hand, the possibility that individuals have a negative drift in the net benefit
from employment is economically important because it affects the hazard rate of exiting
non-employment, particularly at long durations. This insight motivates our approach to
identifying the distribution of the sign of α using data on incomplete spells. Recall that our
population of interest consists of individuals with at least two spells, the first of which is
completed within an interval T . We proceed in two steps. In the first step, we use φ, the
distribution of the duration of two completed spells, together with the auxiliary assumption
that α ≥ 0 with G-probability 1, in order to identify a candidate type distribution, which
we call G+ (α, β). Theorem 1 tells us that this is feasible.
In the second step, we let c denote the fraction of the population whose second spell also
has duration t2 ∈ T . The candidate type distribution G+ provides an upper bound on c:
c̄ ≡

Z Z

f (t2 ; α, β)dG+ (α, β) dt2 .

T

The model implies that c̄ ≥ c, with equality if and only if the true type distribution is
given by the candidate, G = G+ . Any other type distribution which gives rise to the same
completed spell distribution φ must have some negative values of α and so must generate a
smaller fraction of completed second spells.
We aim to construct a distribution of types which is consistent with both φ and c. To do
this, take any individual of type (α, β), α > 0, and replace her with e4αβ individuals of type
(−α, β). These individuals have the same duration distribution conditional on a completed
spell, but they have a lower probability of completing a spell. The choice of e4αβ ensures that
the same number of individuals complete two spells. By flipping the sign of α for enough
individuals, we generate the observed value of c from the model.
There are many ways of selecting individual types from G+ for flipping the sign of α. We
focus on two versions of this exercise, choosing alternatively the largest and smallest possible
fraction of individuals. Assume T = [0, t̄] and define
p(α, β) ≡ c(1 − e−4αβ ) − (e−2αβ − e−4αβ )F (t̄; α, β).
We first choose individuals from G+ with the smallest value of p(α, β), flip the sign of α
for these individuals, and augment their share by e4|α|β , so as to keep the completed spell
distribution unchanged. We do this until we achieve the desired value of the fraction of
completed spells c.4 We prove in Online Appendix OA.B that the resulting type distribution,
4

In theory, we might flip the sign of everyone’s drift without achieving the desired completed spell share
c. In that case, we would reject the model.

15

which we label Ḡ, has the largest fraction of individuals with negative drift consistent with
our data. We then choose individuals with the largest value of p(α, β) to construct the
distribution G, which has the smallest fraction of individuals with negative drift. We view our
model as partially identified, with Ḡ and G providing bounds on the numbers of individuals
with negative drift.

3.4

Overidentifying Restrictions

This model has many overidentifying restrictions. First, Proposition 1 tells us that the joint
density of two completed spells φ is infinitely differentiable at any (t1 , t2 ) ∈ T 2 with t1 > 0,
t2 > 0, and t1 6= t2 . We can reject the model if this is not the case. This test is not
useful in practice, however, since φ is never differentiable in any finite data set. Second,
Proposition 2 tells us how to construct the even-powered moments E(α2i β 2j |t1 , t2 ) for all
{i, j} ∈ {0, 1, . . .}2 . Even-powered moments must all be nonnegative, and so this prediction
yields additional tests of the model. Third, Proposition 3 tells us that we can use the
moments to reconstruct the distribution function G̃. These moments must satisfy certain
restrictions in order for them to be generated from a valid CDF. For example, Jensen’s
inequality implies that
E(α2i |t1 , t2 )1/i ≤ E(α2j |t1 , t2 )1/j
for all integers 0 < i < j. Any completed spell distribution φ that satisfies these three types
of restrictions could have been generated by some type distribution G.
In practice, measuring higher moments can be difficult and so we focus on the simplest
overidentifying test that comes from the model, E(α2 |t1 , t2 ) ≥ 0 and E(β 2 |t1 , t2 ) ≥ 0 for all
t1 6= t2 . Following the proof of Proposition 2, our model implies that these moments satisfy
E(α2 |t1 , t2 ) =
and E(β 2 |t1 , t2 ) =

2 t22 ∂φ(t∂t12,t2 ) − t21 ∂φ(t∂t11,t2 )


−

3

≥0

φ(t1 , t2 )(t21 − t22 )
t1 + t2
∂φ(t1 ,t2 ) 
∂φ(t1 ,t2 )
2t1 t2 ∂t2 − ∂t1
t1 t2
+
φ(t1 , t2 )(t21 − t22 )
t1

3
+ t2

(10)
!
≥ 0.

(11)

These inequality tests have considerable power against alternative theories, as some simple
examples illustrate.
Example 3 Consider the canonical search model where the hazard of finding a job is a constant θ and so the density of completed spells is φ(t1 , t2 ) = θ2 e−θ(t1 +t2 ) . Then conditions (10)

16

and (11) impose
E(α2 |t1 , t2 ) = 2θ −

3t1 t2
3
≥ 0 and E(β 2 |t1 , t2 ) =
≥ 0.
t1 + t2
t1 + t2

3
The first inequality is violated whenever t1 + t2 < 2θ
. Weighting this by the density φ, we
5 −3/2
≈ 44% of individuals experience these short durations. We conclude
find that 1 − 2 e
that our model cannot generate this density of completed spells for any joint distribution of
parameters.
More generally, suppose the constant hazard θ has a population distribution G, with some
R
abuse of notation. The density of completed spells is φ(t1 , t2 ) = θ2 e−θ(t1 +t2 ) dG(θ). Then

R 3 −θ(t +t )
θ e 1 2 dG(θ)
3
≥ 0,
E(α |t1 , t2 ) = 2 R 2 −θ(t1 +t2 )
−
θ e
dG(θ) t1 + t2
2

while E(β 2 |t1 , t2 ) is unchanged. If the ratio of the third moment of θ to the second moment is
finite—for example, if the support of the distribution G is bounded—this is always negative
for sufficiently small t1 + t2 and hence the more general model is rejected.
One might think that the constant hazard model is rejected because the implied density
φ is decreasing, while the density of a random variable with an inverse Gaussian distribution
is hump-shaped. This is not the case. The next two examples illustrate this. The first looks
at a log-normal distribution.
Example 4 Suppose that the density of durations is log-normally distributed with mean µ
and standard deviation σ. For each individual, we observe two draws from this distribution
and test the model using conditions (10) and (11). Then our approach implies



2
t1 log t1 − t2 log t2
1 2
E(α |t1 , t2 ) = 2
− µ + 2σ
≥0
σ (t1 + t2 )
t1 − t2



2t1 t2
t2 log t1 − t1 log t2
2
1 2
and E(β |t1 , t2 ) = 2
+ µ + 2σ
≥ 0.
σ (t1 + t2 )
t1 − t2
2

−t2 log t2
One can prove that t1 log tt11 −t
is increasing in (t1 , t2 ), converging to minus infinity when
2
t1 and t2 are sufficiently close to zero; therefore the first condition is violated at small values
−t1 log t2
of (t1 , t2 ). Similarly, t2 log tt11 −t
is decreasing in (t1 , t2 ), converging to minus infinity when
2
t1 and t2 are sufficiently large; therefore the second condition is violated at large values of
(t1 , t2 ).
The same logic implies that any mixture of log-normally distributed random variables
generates a joint density φ that is inconsistent with our model, as long as the support of

17

the mixing distribution is compact. Thus even though the log normal distribution generates
hump-shaped densities, the test implied by conditions (10) and (11) would never confuse a
mixture of log normal distributions with a mixture of inverse Gaussian distributions.
The final example relates our results to data generated from the proportional hazard
model, a common statistical model in duration analysis
Example 5 Suppose that each individual has a hazard rate equal to θ hb (t) at times t ≥ 0,
where the baseline hazard hb (·) is unrestricted and θ is an individual characteristic with distribution function again denoted by G. If hb (t) and |hb0 (t)/hb (t)| are both bounded as t converges
to 0, the test implies E(α2 |t1 , t2 ) < 0 for (t1 , t2 ) sufficiently small. Online Appendix OA.C
gives a more detailed description and proves this result.

3.5

Single-Spell Data

The distribution of reduced-form parameters (α, β) is also identified using the duration of
a single completed spell and auxiliary assumptions on the distribution function G. For
example, we prove in Appendix C that the model is identified if every individual has the
same expected duration d = β/α. We also prove the model is identified if there are no
switching costs, ψe = ψn = 0. Both of these economically-motivated restrictions reduce the
unobserved type distribution to a single dimension.
Another approach would be to impose a known number of types. A finite mixture of
inverse Gaussian distributions is identified by the distribution of the duration of a single
spell. Conversely, a finite mixture model is sufficiently flexible so as to fit many realized
single-spell duration distributions quite well. We show in Section 6.5 that a model with
three types can capture the distribution of the duration of a single non-employment spell in
our data set. The fundamental problem with this approach is that there is no good economic
justification for the finite type assumption.5 That is, the fact that we can fit the single-spell
duration distribution does not imply that we have estimated an object of interest. We also
show in Section 6.5 that estimates using single-spell data miss much of the heterogeneity that
we uncover using multiple-spell data. Such estimates therefore understate the contribution
of heterogeneity in explaining the shape of the hazard rate.

4

Decomposition of the Hazard Rate

Suppose we know the type distribution G(α, β). This section discusses how to use that
information, together with the known functional form of the duration density f (t; α, β), to
5

Heckman and Singer (1984b) pointed out a similar issue in the mixed proportional hazard model.

18

understand the relative importance of structural duration dependence and dynamic selection
of heterogeneous individuals for the evolution of the hazard rate of exiting non-employment.
We propose a multiplicative decomposition of the aggregate hazard rate conditional on
two completed spells into two components, one measuring a “structural” hazard rate and
another measuring an “average type” among workers still nonemployed at a given duration.
We interpret the structural hazard rate as the aggregate hazard rate that would prevail in
the population if there were no heterogeneity.
The decomposition is based on a Divisia index. Let h(t; α, β) denote the hazard rate for
type (α, β) at duration t,
f (t; α, β)
,
(12)
h(t; α, β) ≡
1 − F (t; α, β)
where F (t; α, β) is the cumulative distribution function associated with the duration density
f . With some abuse of notation, let G(α, β; t) denote the type distribution among individuals
whose duration exceeds t periods. This depends on the initial type distribution and the
functional form of the duration distribution for each type:
dG(α, β; t) ≡ R

(1 − F (t; α, β))dG(α, β)
.
(1 − F (t; α0 , β 0 ))dG(α0 , β 0 )

(13)

The aggregate hazard rate at duration t, H(t), is an average of individual hazard rates
weighted by their share among workers with duration t,
R

f (t; α, β)dG(α, β)
H(t) = R
=
(1 − F (t; α0 , β 0 ))dG(α0 , β 0 )

Z
h(t; α, β)dG(α, β; t),

as can be confirmed directly from the definitions of h(t; α, β) and dG(α, β; t). For example,
if a positive measure of individuals have α ≤ 0, the aggregate hazard rate converges to zero
at sufficiently long durations, since those individuals dominate the population.
We propose an exact multiplicative decomposition of the aggregate hazard rate, H(t) =
H s (t)H h (t), where
Rt

H s (t) ≡ H(0)e

0

d log H s (t0 )

Rt

and H h (t) ≡ e

0

d log H h (t0 )

and
d log H s (t)
≡
dt

R

ḣ(t; α, β)dG(α, β; t)
d log H h (t)
and
≡
H(t)
dt

19

R

h(t; α, β)dĠ(α, β; t)
.
H(t)

That this is an exact decomposition follows immediately from the product rule:
Ḣ(t)
=
H(t)

R

ḣ(t; α, β)dG(α, β; t)
+
H(t)

R

h(t; α, β)dĠ(α, β; t)
d log H s (t) d log H h (t)
=
+
.
H(t)
dt
dt

We interpret the term H s (t) as the contribution of structural duration dependence, since it
is based on the change in the hazard rates of individual worker types. If each individual had
a constant hazard rate, there would be no structural duration dependence, and so this term
would be constant. The remaining term H h (t) represents the role of heterogeneity because
it captures how the hazard rate changes due to changes in the distribution of workers in the
non-employed population. We normalize this to equal 1 at duration 0.
One attractive feature of the multiplicative decomposition is that it nests the usual decomposition of the mixed proportional hazard model. That is, suppose it were the case that
for each type (α, β), there is a constant θ such that h(t; α, β) ≡ θhb (t) for some function
hb (t). Normalizing the population average value of θ to one, our multiplicative decomposition would uncover that the structural hazard rate H s (t) is equal to the baseline hazard rate
hb (t) and the heterogeneity portion H h (t) is equal to the average value of θ in the population
of individuals whose spell lasts at least t periods.
The structural hazard rate H s (t) can either increase or decrease with duration, but the
contribution of heterogeneity H h (t) is always decreasing with duration. It turns out that the
change in the contribution of heterogeneity equals the minus the ratio of the cross-sectional
variance and mean of the hazard rates:6
V ar(h(t; α, β))
d log H h (t)
=−
< 0.
dt
E(h(t; α, β))

(14)

This result is a version of the fundamental theorem of natural selection (Fisher, 1930), which
states that “The rate of increase in fitness of any organism at any time is equal to its genetic
variance in fitness at that time.”7 Intuitively, types with a higher than average hazard rate
6

To prove this, first take logs and time-differentiate dG(α, β; t):
R
f (t; α0 , β 0 )dG(α0 , β 0 ; t)
dĠ(α, β; t)
f (t; α, β)
R
=−
+
= −h(t; α, β) + H(t).
dG(α, β; t)
1 − F (t; α, β)
(1 − F (t; α0 , β 0 ))dG(α0 , β 0 ; t)

Substituting this result into the expression for d log H h (t)/dt gives
R
− h(t; α, β)(h(t; α, β) − H(t))dG(α, β; t)
d log H h (t)
=
.
dt
H(t)
R
Since (h(t; α, β) − H(t))dG(α, β; t) = 0, we can add H(t) times this to the numerator of the previous
expression to get the formula in equation (14).
7
We are grateful to Jörgen Weibull for pointing out this connection to us.

20

are always declining as a share of the population.

5

Austrian Data

We test our theory, estimate our model, and evaluate the role of structural duration dependence using data from the Austrian social security registry (Zweimuller, Winter-Ebmer,
Lalive, Kuhn, Wuellrich, Ruf, and Buchi, 2009). The data set covers the universe of private
sector workers over the years 1986–2007. It contains information on individual’s employment, registered unemployment, maternity leave, and retirement, with the exact begin and
end date of each spell.8

5.1

Characteristics of the Austrian Labor Market

Austrian data are appropriate for our purposes. The Austrian labor market is flexible despite
institutional regulations. Almost all private sector jobs are covered by collective agreements
between unions and employer associations at the region and industry level. The agreements
typically determine the minimum wage and wage increases on the job, but do not directly
restrict the hiring or firing decisions of employers. The main firing restriction is a severance
payment, with size and eligibility determined by law. A worker becomes eligible for severance
pay after three years of tenure if she does not quit voluntarily. The pay starts at two months
salary and increases gradually with tenure. Depending on the details of how wages are set,
this need not have any impact on employment or nonemployment duration (Lazear, 1990).
The unemployment insurance system in Austria is similar to the one in the United States.
The potential duration of unemployment benefits depends on previous work history and age.
If a worker has been employed for more than a year during the two years before a layoff,
she is eligible for 20 weeks of the unemployment benefits. The potential duration of benefits
increases to 30, 39, and 52 weeks for older workers with longer work histories.
Temporary separations and recalls are prevalent in Austria. Around 40 percent of nonemployment spells end with an individual returning to the previous employer. Our structural
model already incorporates this possibility and so we do not break out recalls in our analysis.
Finally, we show in Online Appendix OA.D that the Austrian business cycle is moderate
during this period. For example, the mean duration of in-progress non-employment spells
varies very little over time. We therefore treat the Austrian labor market as a stationary
environment and use pooled data for our analysis.
8

We have data available back to 1972, but can only measure registered unemployment after 1986.

21

5.2

Sample Selection and Definition of Duration

Our data set contains the complete labor market histories of workers over a 21 year period,
which allows us to observe multiple non-employment spells for many individuals. We use
complete and incomplete non-employment spells. We define complete non-employment spells
as the time from the end of one full-time job to the start of the following full-time job. We
further impose that a worker has to be registered as unemployed for at least one day during
the non-employment spell. We drop spells involving a maternity leave. Although in principle
we could measure non-employment duration in days, disproportionately many jobs start on
Mondays and end on Fridays, and so we focus on weekly data.9
A non-employment spell is incomplete if it does not end by a worker taking another job.
Instead, one of the following can happen: 1) the non-employment spell is still in progress
when the data set ends, 2) the worker retires, 3) the worker goes on a maternity leave, 4) the
worker disappears from the sample or dies. We consider any of these as incomplete spells.
We consider only individuals who were younger than 46 in 1986 and older than 39 in 2007,
and have at least one non-employment spell which started after the age of 25.10 Imposing the
age criteria guarantees that each individual has at least 15 years when she could potentially
be at work. To estimate the distribution G+ (α, β), we will use information on two complete
spells shorter than 260 weeks, which means that we are setting T = [0, 260]. This choice
leads to further restrictions on the sample. We consider only spells which started before year
2003 to allow workers to have at least 260 weeks to complete their non-employment spell
before the data set ends in 2007. Incomplete spells which end with retirement or death are
included in our sample only if they are longer than 260 weeks.
We further restrict our population to workers who have at least two spells, with the
first spell completed within 260 weeks and the second spell possibly incomplete. There are
751,125 individuals in this population. For estimating the type distribution G+ , we focus
on the 704,794 individuals whose first two complete spells each have duration shorter than
260 weeks; however, we use the other workers to discipline the fraction of individuals with
a negative drift in the net benefit from employment. Our final sample contains 59 percent
of all workers who were younger than 46 in 1986, older than 39 in 2007, and started a
non-employment spell between 1986 and 2003.
9

We measure spells in calendar weeks. A calendar week starts on Monday and ends on Sunday. If a
worker starts and ends a spell in the same calendar week, we code it as duration of 0 weeks. The duration
of 1 week means that the spell ended in the calendar week following the calendar week it has started, and
so on.
10
We do this because older individuals in 1986 or younger individuals in 2007 are less likely to experience
two such spells in the data set we have available. Moreover, Theorem 1 tells us that we can identify the
type distribution G using the duration density φ(t1 , t2 ) on any subset of durations (t1 , t2 ) ∈ T 2 . Here we set
T = [0, 260].

22

first spell
second spell
model

density

10−2

10−3

10−4
0

20

40

60

80 100 120 140 160 180 200 220 240 260
duration in weeks

Figure 2: Marginal distribution of the first two non-employment spells, conditional on duration less than or equal to 260 weeks. First and second spell in data versus model.
In the subpopulation with two complete spells shorter than 260 weeks, the average duration of a completed non-employment spell is 25.9 weeks, and the average employment
duration between these two spells is 85.7 weeks. Figure 2 depicts the marginal densities of
the duration of the first two completed non-employment spells for all workers who experience at least two spells. The two distributions are very similar. They rise sharply during the
first five weeks, hover near four percent for the next ten weeks, and then gradually start to
decline. The first spells lasts 0.9 weeks longer than the second spell, a difference we suppress
in our analysis.
Figure 3 depicts the joint density φ(t1 , t2 ) for (t1 , t2 ) ∈ {0, . . . , 80}2 . Several features of
the joint density are notable. First, it has a noticeable ridge at values of t1 ≈ t2 . Many
workers experience two spells of similar durations. Second, the joint density is noisy, even
with more than 700,000 observations. This does not appear to be primarily due to sampling
variation, but rather reflects the fact that many jobs start during the first week of the month
and end during the last one. There are spikes in the marginal distribution of nonemployment
spells every fourth or fifth week and, as Figure 2 shows, these spikes persist even at long
durations.

23

number of individuals

103
102
101
0

0

10

0

20

duratio
n

40

in week
s

60

60
80 80

ion
rat

s

eek
w
n

40

20

i

du

Figure 3: Non-employment exit joint density during the first two non-employment spells,
conditional on duration less than or equal to 80 weeks.

6
6.1

Results
Test of the Model

We propose a test of the model inspired by the overidentifying restrictions in Section 3.4.
We make three changes to accommodate the reality of our data. The first is that the data
are only available in discrete time, and so we cannot measure the partial derivatives of the
reemployment density φ. Instead, we propose a discrete time analog of equations (10)–(11):
a(t1 , t2 ) ≡

t22 log

and b(t1 , t2 ) ≡ t1 t2


φ(t1 ,t2 +1) 
1 +1,t2 )
− t21 log φ(t
3
φ(t1 ,t2 −1)
φ(t1 −1,t2 )
−
≥0
2
2
t1 − t2
t1 + t2
!
φ(t1 ,t2 +1) φ(t1 −1,t2 ) 
t1 t2 log φ(t
3
,t
−1)
φ(t
+1,t
)
1 2
1
2
+
≥ 0,
t21 − t22
t1 + t2

(15)
(16)

where we have approximated partial derivatives using
∂φ(t1 , t2 )/∂t1
1
≈ log
φ(t1 , t2 )
2



φ(t1 + 1, t2 )
φ(t1 − 1, t2 )



∂φ(t1 , t2 )/∂t2
1
and
≈ log
φ(t1 , t2 )
2



φ(t1 , t2 + 1)
φ(t1 , t2 − 1)


.

The second is that the density φ is not exactly symmetric in real world data, as seen in
Figure 2. We instead measure φ as 12 (φ(t1 , t2 ) + φ(t2 , t1 )). The third is that the raw measure
24

0.4

failure percentage

data
95% confidence interval
0.3

0.2

0.1

0

0

10

20

30

40
50
60
70
smoothing parameter

80

90

100

Figure 4: Nonparametric test of model. The blue circles show the percent of observations
in the data with a(t1 , t2 ) < 0 or b(t1 , t2 ) < 0, weighted by share of workers with realized
durations (t1 , t2 ), for different values of the smoothing parameter. The red lines show a
bootstrapped 95% confidence interval.
of φ is noisy, as we discussed in the previous section. This noise is amplified when we estimate


1 ,t2 +1)
1 +1,t2 )
and log φ(t
. In principle, we could address this by explicitly
the slope log φ(t
φ(t1 −1,t2 )
φ(t1 ,t2 −1)
modeling calendar dependence in the net benefit from employment, but we believe this issue
is secondary to our main analysis. Instead, we smooth the symmetric empirical density φ
using a multidimensional Hodrick-Prescott filter and run the test on the trend φ̄.11 Since
Proposition 1 establishes that φ should be differentiable at all points except possibly along
the diagonal, we also do not impose that φ̄ is differentiable on the diagonal. See Appendix E
for more details on our filter.
Figure 4 displays our test results. We report the weighted fraction of points (t1 , t2 ) with
0 ≤ t1 < t2 ≤ 260 for which we compute either a(t1 , t2 ) < 0 or b(t1 , t2 ) < 0, weighting
duration pairs by the density φ(t1 , t2 ). Without any smoothing, we reject the model for 32
percent of workers in our sample. Setting the smoothing parameter to at least 7 reduces
the rejection rate to thirteen percent, although further increases in the parameter do not
significantly affect the rejection rate. The fact that the rejection rate declines with the
smoothing parameter is not a trivial observation. In the limit as the smoothing parameter
11

In practice we smooth the function log(φ(t1 , t2 ) + 1/n), rather than φ, where n ≈ 705, 000 is the number
of individuals with two completed spells. This avoids taking log 0.

25

becomes unboundedly large, the smoothed density converges to the one for the exponential
hazard. In that limit, we reject the model at 44 percent of pairs; see Example 3 in Section 3.4.
To interpret the magnitude of the rejection rates, we show the bootstrapped 95% confidence interval in red in Figure 4.12 The confidence interval is narrow and our test statistic
lies above the upper bound for all values of λ. We think there are three reasons for this
finding. First, the measured distribution of spells is not smooth both because of the finite
sample of individuals and because of the role of months in measured durations. The modelgenerated data do not recognize the role of months. Second, the signal-noise ratio is low in
our data at higher values of (t1 , t2 ) because the number of observations declines quickly with
duration. Third, our model does not describe the joint distribution of spells well at long
durations. When we consider only pairs (t1 , t2 ) such that 0 ≤ t1 < t2 ≤ 80, the rejection rate
in the data drops to 0.087 when λ = 100 and lies close to the corresponding 95% confidence
interval, [0.060, 0.073]. We thus conclude that our data could have been generated by the
proposed model at short durations, but we should be cautious in interpreting our results at
long durations, where the inverse Gaussian assumption appears to break down. For example,
the assumption that the net benefit from employment is a Brownian motion may not be valid
for the longest spells.

6.2

Estimation

We estimate our model in several steps. To start, we assume that α ≥ 0 with G-probability
1, so all types have a positive drift in the net benefit from employment while non-employed.
Using data on individuals with two completed spells, we obtain an estimate of the distribution
function G+ . At the end, we allow for the possibility that some individuals have a negative
drift and so use data on incomplete spells to bound the type distribution.
We start with estimating G+ . For a given type distribution G(α, β), the probability that
any individual has duration (t1 , t2 ) ∈ T 2 is
R

R

R
T2

f (t1 ; α, β)f (t2 ; α, β)dG(α, β)
.
f (t1 ; α, β)f (t2 ; α, β) dG(α, β) d(t1 , t2 )

We can therefore compute the likelihood function by taking the product of this object across
all the individuals in the economy. Combining individuals with the same realized duration
12
To compute this statistic, we assume that the data generating process is a mixture of inverse Gaussians
with the distribution G, which we estimate later in this section. We draw 500 samples of two non-employment
spells for 850,000 individuals and keep individuals with two completed spells with duration between 0 and
260 weeks. We then proceed as in the data: we construct the empirical distribution φ(t1 , t2 ), smooth it with
our 2-dimensional HP filter for different values of the smoothing parameter λ, and apply our test. Confidence
intervals for each λ is then the range of values which contain 95% of the rejection rates across samples.

26

α
β
µn
ω̄−ω
σn
ω̄−ω

minimum distance estimate
EM estimate
mean median st. dev.
min
mean median
st. dev
min
1.543
0.332 12.444 0.003 508.770
0.138 3123.989 0.096
16.878
4.768 94.010 0.667 3512.460
5.735 18390.764 1.465
0.045
0.037
0.043 0.005
0.052
0.043
0.033 0.018
0.228
0.210
0.143 0.001
0.231
0.174
0.155 10−5
Table 1: Summary statistics from estimation.

into a single term, we obtain that the log-likelihood of the data φ(t1 , t2 ) is equal to
X
(t1 ,t2

)∈T 2


φ(t1 , t2 ) log

R

f (t1 ; α, β)f (t2 ; α, β)dG(α, β)
R R
f (t1 ; α, β)f (t2 ; α, β) dG(α, β) d(t1 , t2 )
T2


.

Our basic approach to estimation chooses the distribution function G+ to maximize this
objective. More precisely, we follow a two-step procedure. In the first step, we constrain α
and β to lie on a discrete nonnegative grid and use a minimum distance estimator to obtain
an initial estimate. In the second step, we allow α and β to take nonnegative values off
of the grid and optimize using the expectation-maximization (EM) algorithm. See Online
Appendix OA.E for more details.
Our parameter estimates place a positive weight on 152 different types (α, β).13 Table 1
summarizes our estimates. We report the mean, median, minimum, and standard deviation
of α and β, as well as the drift and standard deviation of the net benefit from employment
relative to the width of the inaction region, µn /(ω̄ − ω) = α/β and σn /(ω̄ − ω) = 1/β. The
first four columns summarize the estimates from the first estimation step, while the last 4
columns show results after refining the initial estimates using the EM algorithm. The mean
and standard deviation of µn /(ω̄ − ω) and σn /(ω̄ − ω) are similar in the two estimates, but
the moments for α and β differ substantially. This is because the EM algorithm uncovers
several types with a small σn /(ω̄ − ω) (nearly deterministic duration), which implies a large
value of α and β. The median values of α and β change by much less.
Our estimates uncover a considerable amount of heterogeneity. For example the crosssectional standard deviation of α is six times its mean, while the cross-sectional standard
deviation of β is around five times its mean. Moreover, α and β are positively correlated in
the cross-section, with correlation 0.91 in the initial stage and 0.80 in the EM stage.
The smooth red line in Figure 2 shows the fitted marginal distribution of the duration
13

A potential concern is that our model is over-parameterized, leading us to recover more types than are
in the data. In the Online Appendix OA.F, we find that 77 types minimizes Akaike information criterion,
but also document that our results are virtually identical with 77 and 152 types.

27

20

40

weeks

60

60

40

log deviation

density

10−3
10−4
10−5

20

eks
we

0
−2
20

40

weeks

60

60

40

20

ek
we

s

Figure 5: Nonemployment exit density: model (left) and log ratio of model to data (right)
of a single non-employment spell conditional on completed duration t ∈ [0, 260]. The model
matches the initial increase in the density during the first ten weeks, as well as the gradual
decline the subsequent five years. We miss the distribution at very long durations. There are
two reasons for this. First, there are very few observations at long durations, so our procedure
does not try to fit this data. Second, our model does not capture long durations very well,
as we saw from the test results in Section 6.1. We could fit the marginal distribution better
at long durations, but this would worsen the fit of the joint distribution φ.
Of course, it is not surprising that we can match the univariate hazard rate, since it
is theoretically possible to match any univariate hazard rate with a mixture of (possibly
degenerate) inverse Gaussian distributions. More interesting is that we can also match the
joint density of the duration of the first two spells. The first panel in Figure 5 shows the
theoretical analog of the joint density in Figure 3. The second panel shows the log of the
ratio of the empirical density to the theoretical density. The root mean squared error is
about 0.17 times the average value of the density φ, with the model able to match the major
features of the empirical joint density, leaving primarily the high frequency fluctuations that
we previously indicated we would not attempt to match.
In the last step, we build on Section 3.3 to infer bounds on the fraction of the population
with negative drift in the net benefit from employment while non-employed, and hence
negative α. Our estimates of the distribution function G+ , which imposes that all individuals
have a positive value of α, imply that 99 percent of individuals should have the second spell
completed with duration less than 260 weeks. The corresponding number in the data is 94
percent. To fit this fact, we flip the sign of the drift for the smallest and largest possible
fraction of individuals, constructing the distribution functions G and Ḡ. Both of these
distributions have identical predictions for completed spell data and are also able to match
the prevalence of incomplete spells in our data set. The share of workers with negative α

28

lies between 6 and 16 percent.

6.3

Decomposition of the Hazard Rate

We now use our estimated type distribution to evaluate importance of heterogeneity in
shaping the aggregate hazard rate. We consider three type distributions: G+ , where α is
nonnegative with G-probability 1; and Ḡ and G, where a positive fraction of types have
negative α. The choice of the type distribution affects the hazard rate decomposition for two
reasons. First, the weight we attribute to a type (|α|, β) depends on the sign of α. Ignoring
the possibility that α is negative, we underestimate the number of individuals who start
non-employment spells and so overestimate the hazard rate at long durations. Second, the
hazard rate itself depends on the sign of α. The hazard rate for negative α is lower than for
positive α, and only the former converges to zero at long durations. The structural hazard
rate will thus be lower for Ḡ and G than for G+ , but so will be the aggregate hazard rate.
In general, there is no a priori reason to think that one distribution will attribute a bigger
role to heterogeneity than another.
We are interested in a decomposition of the hazard rate of the second spell in our population. The measured hazard rate of the first spell in our sample is biased because we constrain
our population to be those workers whose first spell lasts ends within 260 weeks; we do not
impose this constraint on the second spell. The type distributions Ḡ and G are suitable for
the analysis of this hazard rate because they are consistent with the fraction of incomplete
second spells in the sample. We focus on the distribution Ḡ but also compare our results to
those obtained with G and G+ .
The purple line in the left panel of Figure 6 shows the raw hazard rate implied by the
type distribution Ḡ.14 This peaks at 5.1 percent after 10 weeks, declines to 1.6 percent after
a year and to 0.7 percent after two years. In contrast, the blue line shows the corresponding
structural hazard rate H s (t). Most individuals have an increasing hazard for about 20 weeks.
The structural hazard peaks at 7.6 percent, falls to 4.9 percent after a year and further
declines to 2.8 percent after two years. The non-employment duration of an individual
worker thus has a significant effect on her future prospects for finding a job, but less than
the raw data indicates. After a worker is out of work for half a year, her chances of returning
to work start to decline, possibly due to the loss of human capital. After two years of
non-employment, the hazard of finding a job is only a third of what it was at the peak.
The ratio of the structural and aggregate hazard rate is attributed to heterogeneity and
dynamic selection, measured by H h (t) ≡ H(t)/H s (t) in the right panel of Figure 6. Recall
14

The dotted lines in Figure 6 show bootstrapped 95 percent confidence intervals. See Appendix D for
details on their construction.

29

1

0.06

“average type”

weeekly hazard rate

0.08

H s (t)

0.04
0.02

0.6
0.5
0.4

H h (t)

0.3

H(t)
0

0

20
40
60
80
duration in weeks

0.2

100

0

20
40
60
80
duration in weeks

100

Figure 6: Hazard rate decomposition under Ḡ. The purple line in the first figure shows the
aggregate hazard rate H(t). The blue line in the first figure shows the structural hazard rate
H s (t). The ratio of them is the contribution of heterogeneity, plotted as the red line in the
second figure. The dotted lines show bootstrapped 95 percent confidence bands. Note that
these hazard rates do not condition on the spell ending within 260 weeks.
that selection necessarily pushes the hazard rate down, since high hazard individuals always
find jobs faster than those with low hazard rates. We find very strong sorting during the
first year of non-employment. The average type declines sharply, and after 39 weeks of nonemployment it is only 36 percent of its initial value. The sorting continues even after a year,
but at a slower rate than within the first year. After two years, the average type is only 26
percent as high as at the start of a spell.
Figure 7 replicates the decomposition using the type distributions G+ and G. The left
hand panel confirms that the level of aggregate and structural hazard rate is lower with
G and Ḡ than with G+ , a consequence of having types whose hazard rates are lower and
converge to zero. The right hand panel indicates that the role of heterogeneity is similar
during the first year under all three distributions; however, while with G+ , there is virtually
no sorting after a year, dynamic selection continues to play an important role during the
second year once we recognize that some spells will be defective. Under G, dynamic selection
reduces the hazard rate by 78 percent during the first two years of non-employment.

6.4

Comparison to the Mixed Proportional Hazard Model

A large literature assumes that the hazard rate of an individual has the form h(t; θ) = θhb (t),
where θ is an individual characteristic with an unknown distribution and hb (t) is the unknown
baseline hazard of a spell ending at duration t. Heterogeneity is captured by the parameter
30

1
+

G
Ḡ
G

0.08

“average type”

weekly hazard rate

0.1

0.06
H s (t)

0.04
0.02
0

0

H(t)
20
40
60
80
duration in weeks

0.6
0.5
0.4

H h (t)

0.3
0.2

100

G+
Ḡ
G

0

20
40
60
80
duration in weeks

100

Figure 7: Decomposition of the hazard rate for distribution G+ , G, Ḡ. The blue lines show
the structural hazard rate H s (t). The red lines show the contribution of heterogeneity, H h (t).
The sum of the two is the raw hazard rate H(t), shown as purple lines. Solid lines correspond
to distribution G+ , dashed lines to Ḡ and dotted lines to G.
θ, which may be partially unobserved, while structural duration dependence appears in the
baseline hazard hb (t) and is assumed to behave identically across individuals. This is the
mixed proportional hazard (MPH) model.
While we view the MPH model as a convenient reduced-form representation of the data,
we argue in Alvarez, Borovičková, and Shimer (2015) that this specification is restrictive.
In particular, the assumption that heterogeneity enters as a multiplicative constant on the
common baseline hazard hb (t) can be rejected. Here we show that restricting heterogeneity
in this way leads us to underestimate its importance.
We start by noting two restrictions imposed by the MPH model. First, the MPH model
implies that the hazard rate for each type peaks at the same duration. We find that this is
not the case in our estimated stopping time model. The hazard rate peaks within eighteen
weeks for the most workers, but for more than ten percent of workers, the peak hazard rate
does not occur until after one year. The ratio of the timing of the peak hazard for workers
at the 90th and 10th percentile of this statistic is 182.
Second, the MPH model implies that the ratio of the hazard rate at any two durations
(t1 , t2 ) should be the same for all types. We analyze this ratio at t1 = 13 and t2 = 52. Again,
there is considerable dispersion in this outcome. 85 percent of workers have a lower hazard
rate after one year than after one quarter, but for nearly five percent of workers, the hazard
rate has increased by a factor of ten.
Imposing homogeneity along these important dimensions leads us to underestimate the

31

1
“average type”

weekly hazard rate

0.08
0.06
H s (t) = hb (t)
0.04
0.02

0.6
0.5
0.4

H h (t)

0.3

MPH
stopping time

H(t)
0

0

20
40
60
80
duration in weeks

0.2

100

0

20
40
60
80
duration in weeks

100

Figure 8: Decomposition of the hazard rate for the mixed proportional hazard model. In
the left panel, the purple line shows the raw hazard rate, H(t), and the blue line shows the
baseline hazard rate, hb (t), which here is the same as the structural hazard rate, H s (t). The
right panel shows the contribution of heterogeneity, H h (t). The green line shows the MPH
model and the red line shows the stopping time model with distribution Ḡ.
role of heterogeneity in determining duration dependence. To show this, we estimate the
MPH model using maximum likelihood with a nonparametric baseline hazard.15 The hazard
rate decomposition is particularly simple in the MPH model: the structural hazard is the
baseline hazard, H s (t) = hb (t), and the contribution of heterogeneity H h (t) is the mean of θ
among those still non-employed at duration t. Figure 8 shows our results. Our stopping time
model implies much more dynamic selection. For example, we find that that the average
type is only 35 percent as high after one year and 28 percent as high after two years as at
the start of the spell. The comparable numbers in the MPH model are almost twice as large,
61 and 52 percent, severely understating the importance of heterogeneity.

6.5

Single-Spell Data

We comment briefly on what happens if we estimate the model using single-spell data. To do
this, we construct a new data set consisting of all individuals with a single completed spell.
15
We first modify the data set in a manner that makes it amenable to estimating the MPH model. We
assume that the baseline hazard rate in the first and second spell is the same and that it integrates to infinity
(i.e. there are no defective spells). Under these assumptions, we can extend our data set to include first
spells that do not end within 260 weeks: for every worker in our sample whose second spell is longer than 260
weeks, we create a new observation that flips the durations of the first and second spells. This avoids biasing
the estimated hazard rate by selecting a sample of individuals whose first spell ends within 260 weeks.
We then use the Stata command streg for estimation. We specify that the distribution of unobserved θ is
gamma, but obtain similar results when we assume θ has an inverse Gaussian distirbution. We include a full
set of dummy variables for each week of duration, which permits us to estimate a flexible baseline hazard.

32

data
two spell
single spell

density

10−2

10−3

10−4
0

20

40

60

80 100 120 140 160 180 200 220 240 260
duration in weeks

Figure 9: Distribution of nonemployment spells in the data and in the model estimated using
the joint distribution of two spells (red, 152 types) and single-spell data (green, 5 types).
We assume that the data is generated by a mixture of five types of workers, each with an
inverse Gaussian distribution. We estimate the mixing distribution using the EM algorithm
to minimize the distance between the empirical and theoretical distribution of the duration
of a single spell. Figure 9 shows that we can fit the data very well with only five types;
indeed, we fit the single-spell density better with five types than we do in our preferred
estimate with 152 types. Unsurprisingly, the single-spell estimates do worse at fitting joint
density of the two-spell data. The question is whether this matters for our results.
We find that the five-type model substantially understates the importance of heterogeneity in duration dependence. The right hand panel of Figure 10 indicates that, after a
steady 33 percent decline in the quality of job searchers during the first year, there is little
subsequent change in the composition, so that after two years, the average type remains at
64 percent of its original value, more than twice as high as implied by our estimates with
two-spell data.
The choice of the number of types is arbitrary and indeed our results are sensitive to
it. With more than ten types, our single-spell estimates actually assign a bigger role to
heterogeneity than our preferred estimates. Identification through prior knowledge of the
number of unobserved types is tenuous. In any case, the fact that a finite mixture of inverse
Gaussians can match the distribution of duration of a single spell does not imply that it

33

0.06
0.04

“average type”

weekly hazard rate

1

H s (t)

0.02

0.6
0.5
0.4
0.3

single spell
two spell

H(t)
0

0

20
40
60
80
duration in weeks

0.2

100

0

20
40
60
80
duration in weeks

100

Figure 10: Decomposition of the hazard rate using single-spell data and an assumption that
there are five types. In the left panel, the purple line shows the raw hazard rate, H(t),
and the blue line shows the structural hazard rate, H s (t). In the right panel, the green line
shows the contribution of heterogeneity, H h (t). The red line corresponds to the contribution
of heterogeneity in the stopping time model with distribution Ḡ.
captures the real-world heterogeneity.

6.6

Estimated Switching Costs

In Section 2.3, we argued that knowledge of α and β, together with other four parameters of
the model, pins down the magnitude of the fixed costs of switching employment status. Here
we use the estimated distribution function G to find an upper bound on the distribution
of the fixed costs in the population. We assume that there are no costs of switching from
employment to non-employment, ψn = 0, and we focus on costs of switching from nonemployment to employment, ψe .16
Equation (4) implies that for given value of α and β, higher µe and |µn | increase the
implied fixed costs, while higher σe and r reduce the implied fixed cost. With that in mind,
we calibrate these parameters to find an upper bound on the fixed costs. First, we set the
drift in employed workers’ wages at µe = 0.01 at an annual frequency. Estimates of the
average wage growth of employed workers are often higher than one percent, but this is for
workers who stay employed, a selected sample. The parameter µe governs wage growth for
all workers without selection, and thus we view µe = 0.01 as a large number. We set the
standard deviation of log wages at σe = 0.05, again at an annual frequency. This is lower
than typical estimates in the literature, which are closer to ten percent.
16

Note that this is always expressed relative to the value of leisure.

34

mean

std

10th per.

50th per.

90th per.

0.017%

0.024%

0.001%

0.005%

0.062%

Table 2: Summary statistics for the estimated switching costs, ψe , expressed as a percent
of the annual flow value of leisure. Calculations assume µe = 0.01, σe = 0.05, |µn | = 0.033,
r = 0.02, and the type distribution Ḡ.
We cannot observe the drift of latent wages when non-employed, µn , but we can infer
its value relative to µe from the duration of completed spells. The model implies that
the expected duration of completed employment and non-employment spells are given by
(ω̄ −ω)/µe and (ω̄ −ω)/|µn |, respectively, and thus |µn |/µe determines their relative expected
duration. In our sample, the average duration of non-employment spells is 25.9 weeks,
while the average duration between two non-employment spells is 85.7 weeks, implying that
|µn | = 3.3µe . Finally, we choose a low value for r. Since agents in the model are infinitely
lived, we think of this as the sum of workers’ discount rate and their death probability. A
lower bound on this is 0.02, consistent with no discounting and a fifty year working lifetime.
Given this calibration, we estimate the distribution of fixed costs for the type distribution
Ḡ. Since α and µn have the same sign, we assume that workers with α positive have
µn = 0.033, while those with α negative have µn = −0.033.
Table 6.6 summarizes our results. We again focus on the estimated costs for the distribution Ḡ, but the costs for other two distributions are virtually the same. The median value of
the switching costs is only 0.005 percent of the annual non-employment flow value, or about
6 minutes of time, assuming a 2000 hours of work per year. The costs vary across types,
but even at the top decile, it amounts to 75 minutes of time. This is an order of magnitude
smaller than the cost estimates in Silva and Toledo (2009).
Even though the magnitudes are very small, strictly positive switching costs are important
for our results. If the switching cost were zero for someone, their region of inaction would be
degenerate. And if the region of inaction were degenerate for everyone, the mean duration of
√
spells in the interval [1, 260] could not exceed 260 ≈ 16 weeks, as discussed in Section 2.3.
Instead, we find that the mean duration of these spells is 26 weeks.
Previous work by Mankiw (1985), Dixit (1991), Abel and Eberly (1994), and others has
shown that even small fixed costs can generate large regions of inaction. In our model,
however, not only are the fixed costs small, but so is the region of inaction. The mean and
median width of the inaction region are 1.2 and 1.6 log points, respectively. That is, the
median worker who has just started working will quit if she experiences a 1.6 percent decline
in her wage, holding fixed the value of nonemployment. A similar wage increase will induce
her to return to work.
35

We are unaware of other papers that study the cost of switching between employment
and non-employment at the level of an individual worker. In other areas, empirical results
on the size of fixed costs are mixed. Cooper and Haltiwanger (2006) find a large fixed cost
of capital adjustment, around 4 percent of the average plant-level capital stock. Nakamura
and Steinsson (2010) estimate a multisector model of menu costs and find that the annual
cost of adjusting prices is less than 1 percent of firms’ revenue. In a model of house selling,
Merlo, Ortalo-Magne, and Rust (2013) find a very small fixed cost of changing the listing
price of a house, around 0.01 percent of the house value.

7

Conclusion

We develop a dynamic model of a worker’s transitions in and out of employment. Our model
features structural duration dependence in the job finding rate, in the sense that the hazard
rate of finding a job changes during a non-employment spell for a given worker. Moreover,
the job finding rate as a function of duration varies across workers. We use the model to
answer two questions: what is the relative importance of heterogeneity versus structural
duration dependence for explaining the evolution of the aggregate job finding rate; and how
big are the fixed costs of switching between employment and non-employment. We find that
the decline in the job-finding rate is mostly driven by changes in the composition of the
pool of non-employed workers, rather than by declines in the job-finding rate for the typical
worker. Workers differ not only in the average value of their job finding rate, but also in
the timing of its peak. Finally, we find that fixed costs of switching employment status are
small, but can also soundly reject any version of the model without fixed costs.
Our result that heterogeneity is an important driving force for duration dependence is in
part a consequence of the stopping time model and its implied inverse Gaussian distribution.
Our model allows for two dimensions of heterogeneity, while the MPH model, a common
statistical model in applied research, allows for only a single dimension. Other statistical
models, such as a mixture of log-normal distributions, has similar flexibility to the mixture
of inverse Gaussian distributions. In fact, we have estimated a mixture of log-normals and
found that it implies a similarly important role for heterogeneity; however, the mixture
of log-normals has no structural interpretation and thus cannot be used to estimate the
distribution of switching costs. The bottom line is that large data sets like the Austrian social
security panel allow for a flexible treatment of heterogeneity; and that a flexible treatment
of heterogeneity indicates an important role for dynamic selection of heterogeneous workers
in driving the aggregate hazard rate.

36

References
Aalen, Odd O., and Håkon K. Gjessing, 2001. “Understanding the shape of the hazard rate:
A process point of view (with comments and a rejoinder by the authors).” Statistical
Science. 16 (1): 1–22.
Abbring, Jaap H., 2012. “Mixed Hitting-Time Models.” Econometrica. 80 (2): 783–819.
Abel, Andrew B., and Janice C. Eberly, 1994. “A Unified Model of Investment Under Uncertainty.” American Economic Review. 84 (5): 1369–1384.
Ahn, Hie Joo, and James D. Hamilton, 2015. “Heterogeneity and Unemployment DYnamics.”
UCSD Mimeo.
Alvarez, Fernando, Katarı́na Borovičková, and Robert Shimer, 2015. “Testing the Mixed
Proportional Hazard Model.” University of Chicago Mimeo.
Alvarez, Fernando, and Robert Shimer, 2011. “Search and Rest Unemployment.” Econometrica. 79 (1): 75–122.
Coelho, Carlos A., Rui P. Alberto, and Luis P. Grilo, 2005. “When Do the Moments
Uniquely Identify a Distribution.” Preprint 17/2005, Mathematics Department, Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa.
Cooper, Russell W, and John C Haltiwanger, 2006. “On the nature of capital adjustment
costs.” The Review of Economic Studies. 73 (3): 611–633.
Cox, David R., 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical
Society. Series B (Methodological). 34 (2): 187–220.
Dixit, Avinash, 1991. “Irreversible Investment with Price Ceilings.” Journal of Political
Economy. 99 (3): 541–557.
Elbers, Chris, and Geert Ridder, 1982. “True and Spurious Duration Dependence: The
Identifiability of the Proportional Hazard Model.” Review of Economic Studies. 49 (3):
403–409.
Engl, Heinz W., Martin Hanke, and Andreas Neubauer, 1996. Regularization of Inverse
Problems, Kluwer Academic Publishers.
Feller, William, 1966. An introduction to probability theory and its applications. Vol. II, John
Wiley & Sons Inc., New York, second edn.
37

Fisher, R.A., 1930. The Genetical Theory of Natural Selection, Oxford University Press.
Heckman, James J., and Burton Singer, 1984a. “The Identifiability of the Proportional
Hazard Model.” Review of Economic Studies. 51 (2): 231–241.
Heckman, James J., and Burton Singer, 1984b. “A Method for Minimizing the Impact of
Distributional Assumptions in Econometric Models for Duration Data.” Econometrica. 52
(2): 271–320.
Hodrick, Robert James, and Edward C Prescott, 1997. “Postwar US Business Cycles: An
Empirical Investigation.” Journal of Money, Credit and Banking. 29 (1): 1–16.
Honoré, Bo E., 1993. “Identification Results for Duration Models with Multiple Spells.”
Review of Economic Studies. 60 (1): 241–246.
Hornstein, Andreas, 2012. “Accounting for Unemployment: The Long and Short of It.”
Federal Reserve Bank of Richmond Working Paper 12-07.
Krueger, Alan B, Judd Cramer, and David Cho, 2014. “Are the Long-Term Unemployed on
the Margins of the Labor ¡arket?.” Brookings Papers on Economic Activity. pp. 229–280.
Lancaster, Tony, 1972. “A Stochastic Model for the Duration of a Strike.” Journal of the
Royal Statistical Society. Series A (General). 135 (2): 257–271.
Lancaster, Tony, 1979. “Econometric Methods for the Duration of Unemployment.” Econometrica. 47 (4): 939–956.
Lazear, Edward P., 1990. “Job Security Provisions and Employment.” Quarterly Journal of
Economics. 105 (3): 699–726.
Lee, Mei-Ling Ting, and G. A. Whitmore, 2006. “Threshold Regression for Survival Analysis:
Modeling Event Times by a Stochastic Process Reaching a Boundary.” Statistical Science.
21 (4): 501–513.
Lee, Mei-Ling Ting, and G. A. Whitmore, 2010. “Proportional Hazards and Threshold Regression: Their Theoretical and Practical Connections.” Lifetime data analysis. 16 (2):
196–214.
Lemeshko, Boris Yu, Stanislav B. Lemeshko, Kseniya A. Akushkina, Mikhail S. Nikulin, and
Noureddine Saaidia, 2010. “Inverse Gaussian Model and Its Applications in Reliability
and Survival Analysis.” in Mathematical and Statistical Models and Methods in ReliabilityStatistics for Industry and Technology, pp. 433–453. Birkhäuser Boston.
38

Ljungqvist, Lars, and Thomas J Sargent, 1998. “The European Unemployment Dilemma.”
Journal of Political Economy. 106 (3): 514–550.
Loeve, Michel, 1977. Probability Theory I . , vol. 45, Graduate texts in mathematics.
Mankiw, N Gregory, 1985. “Small menu costs and large business cycles: A macroeconomic
model of monopoly.” The Quarterly Journal of Economics. 100 (2): 529–537.
Merlo, Antonio, Francois Ortalo-Magne, and John Rust, 2013. “The home selling problem:
Theory and evidence.” PIER Working Paper.
Nakamura, Emi, and Jón Steinsson, 2010. “Monetary Non-neutrality in a Multisector Menu
Cost Model.” The Quarterly Journal of Economics. 125 (3): 961–1013.
Newby, Martin, and Jonathan Winterton, 1983. “The Duration of Industrial Stoppages.”
Journal of the Royal Statistical Society, Series A (General). 146 (1): 62–70.
Shimer, Robert, 2008. “The Probability of Finding a Job.” American Economic Review. 98
(2): 268–273.
Silva, José Ignacio, and Manuel Toledo, 2009. “Labor Turnover Costs and the Cyclical
Behavior of Vacancies and Unemployment.” Macroeconomic Dynamics. 13 (S1): 76–96.
Zweimuller, Josef, Rudolf Winter-Ebmer, Rafael Lalive, Andreas Kuhn, Jean-Philipe Wuellrich, Oliver Ruf, and Simon Buchi, 2009. “Austrian Social Security Database.” Mimeo.

39

Appendix
A

Structural Model

In this section, we describe the structural model used in Section 2.1 in detail and show
that the optimal worker’s switching decision is described by two thresholds, ω < ω̄. We
characterize these thresholds in the online appendix OA.A.
Let s ∈ {e, n} denote employment status of a worker. We assume that b(t) and w(t)
follow state-contingent Brownian motions:

db(t) =


µ

b,e

dt + σb,e dBb (t)

if worker is employed, s = e,

µ dt + σ dB (t) if worker is non-employed, s = n,
b,n
b,n
b

µ dt + σ dB (t) if worker is employed, s = e,
w,e
w,e
w
dw(t) =
µ dt + σ dB (t) if worker is non-employed, s = n.
w,n
w,n
w
Bb (t) and Bw (t) are correlated Brownian motions, and we use ρs ∈ [−1, 1] to denote the
instantaneous correlation between dw and db in state s,

E [dw(t) db(t)] =


σ

ρe dt

if worker is employed, s = e

σ

ρn dt

if worker is non-employed, s = n.

w,e σb,e
w,n σb,n

The state of worker’s problem is a triplet (s, w, b). Let Ẽ(w, b) and Ñ (w, b) be the value
functions of an employed and non-employed worker with state (w, b), respectively. It is
technically convenient to denote the flow value of non-employment by b0 eb(t) ; in the text we
normalize b0 = 1. The value functions satisfy
Z

τe

Ẽ(w, b) = max E
τe

e
0

Z
Ñ (w, b) = max E
τn

−rt w(t)

0

τn

e

−rτe

dt + e

Ñ (w(τe ), b(τe )) − ψn e

e−rt b0 eb(t) dt + e−rτn

b(τe )


|w(0) = w, b(0) = b



(17)


b(τn )
Ẽ(w(τn ), b(τn )) − ψe e
|w(0) = w, b(0) = b .
(18)

An employed worker chooses the stopping time τe at which to switch to non-employment,
described by equation (17). Similarly in equation (18), a non-employed worker chooses the
first time τn at which to change her status to employment. The expectation in equations

40

(17) and (18) is taken with respect of the law of motion for w(t) and b(t) between 0 ≤ t ≤ τs .
For the problem to be well-defined, we require that
2
,
r > µw,s + 12 σw,s
2
r > µb,s + 12 σb,s
,

for s ∈ {e, n}

(19)

for s ∈ {e, n}

(20)

The conditions in (19) guarantee that the value of being employed (non-employed) forever
is finite. Moreover, if the conditions (20) hold, then being non-employed (employed) for T
periods and then switching to employment (non-employment) forever is also finite in the
limit as T converges to infinity.
Equations (17) and (18) imply that we can restrict our attention to functions that satisfy
the following homogeneity property. For any pair (w, b) and any constant a,
Ẽ(w + a, b + a) = ea Ẽ(w, b),
Ñ (w + a, b + a) = ea Ñ (w, b).
By choosing a = −b, we get
Ẽ(w, b) = eb Ẽ(w − b, 0) ≡ eb E(w − b),

Ñ (w, b) = eb Ñ (w − b, 0) ≡ eb N (w − b),
which implicitly defines E(·) and N (·) as a function of the scalar w − b. We define ω(t), the
log net benefit to work, as ω(t) ≡ w(t) − b(t). It also follows a state-contingent Brownian
motion,
dω(t) = µs dt + σs dB(t),
where {B} is a standard Brownian motion defined in terms of {Bb , Bw }, and the drift and
the diffusion coefficient are given by
2
2
µs = µw,s − µb,s and σs2 = σw,s
− 2σw,s σb,s ρs + σb,s
.

The optimal decision of switching from employment to non-employment and vice versa
is described by thresholds ω and ω̄ such that a non-employed worker chooses to become
employed if the net benefit from working is sufficiently high, ω(t) > ω̄, and an employed
worker switches to non-employment if the benefit is sufficiently low, ω(t) < ω.
Figure 11 depicts value functions E(·) and N (·) for the case ψn = 0.
We characterize the thresholds ω, ω̄ in terms of parameters in of the model in the Online

41

60

value

50

employed value E(ω)
always employed
non-employed value N (ω)
never employed

40

30

20
−1.2

ω

ω̄
−0.8
−0.4
net benefit from employment ω

Figure 11: Value functions E(ω) and N (ω), together with thresholds ω < ω̄. The solid red
line shows the value of being employed E(ω) for ω ∈ [ω, ∞). The solid blue line shows
N (ω) for ω ∈ (−∞, ω̄]. The dotted red line indicates the value of being employed forever,
eω /(re − µe − σe2 /2), while the blue dotted line the value of being non-employed forever,
b0 /rn . The parameter values are r = 0.04, µe = 0.02, σe = 0.1, µn = 0.01, σn = 0.04, b0 = 1,
µb,s = σb,s = 0, ψe = 2, and ψn = 0.

42

Appendix OA.A, here we only state an approximation for the distance between them. We
use this result to infer the size of the fixed costs from the distance between the barriers and
known values of the other parameters.
Proposition 4 The distance between the barriers is approximately proportional to the cube
root of the size of fixed costs. More precisely,

ψe + ψn
λe λn (ω̄ − ω)3
=−
+ o (ω̄ − ω)3 ,
b0
12rn
where λe , λn and rn are defined as
p

µ2e + 2re σe2
λe =
< −1,
σe2
1 2
.
rn = r − µb,n − σb,n
2
−µe −

λn =

−µn +

p
µ2n + 2rn σn2
> 1,
σn2

Numerical simulations indicate that this approximation is very accurate at empirically plausible values of ω̄ − ω.

B

Proof of Identification

We start by proving a preliminary lemma that describes the structure of the partial derivatives of the product of two inverse Gaussian distributions.
Lemma 1 Let m be a nonnegative integer and i = 0, . . . , m. The partial derivative of the
product of two inverse Gaussian distributions at (t1 , t2 ) is:

∂ m f (t1 ; α, β) f (t2 ; α, β)
= f (t1 ; α, β) f (t2 ; α, β)
∂ti1 ∂tm−i
2

r+s≤m
X
r,s=0

!
κr,s (t1 , t2 ; i, m − i)α2r β 2s

(21)

where κr,s (t1 , t2 ; i, m − i) are polynomials functions of (t1 , t2 ),
κr,s (t1 , t2 ; i, m − i) =

2i 2(m−i)
X
X
k=0

`=0

−`
θk,`,r,s (i, m − i)t−k
1 t2 ,

(22)

and the coefficients θk,`,r,s (i, m − i) are independent of t1 , t2 , α, and β.
Proof of Lemma 1. The lemma holds trivially when m = i = 0, with κ0,0 (t1 , t2 , 0, 0) = 1.
We now proceed by induction. Assume equation (21) holds for some m ≥ 0 and all i ∈
43

{0, . . . , m}. We first prove that it holds for m + 1 and all i + 1 ∈ {1, . . . , m + 1}, then verify
that it also holds for i = 0. We start by differentiating the key equation:

∂ m+1 f (t1 ; α, β) f (t2 ; α, β)
∂ti+1
∂tm−i
1
2
!
∂ m f (t1 ; α, β) f (t2 ; α, β)
∂ti1 ∂tm−i
2
 2

β
3
α2
= f (t1 ; α, β) f (t2 ; α, β)
−
−
2t21 2t1
2
∂
=
∂t1

+ f (t1 ; α, β) f (t2 ; α, β)

r+s≤m
X
r,s=0

r+s≤m
X
r,s=0

!
2r

κr,s (t1 , t2 ; i, m − i)α β

∂κr,s (t1 , t2 ; i, m − i) 2r 2s
α β
∂t1

2s

!

or
∂ m+1 f (t1 ; α, β) f (t2 ; α, β)
1
f (t1 ; α, β) f (t2 ; α, β)
∂ti+1
∂tm−i
1
2
r+s≤m
1 X
=−
κr,s (t1 , t2 ; i, m − i)α2(r+1) β 2s
2 r,s=0



r+s≤m
1 X
κr,s (t1 , t2 ; i, m − i)α2r β 2(s+1)
+ 2
2t1 r,s=0

r+s≤m
X  3
∂κr,s (t1 , t2 ; i, m − i)
+
− κr,s (t1 , t2 ; i, m − i) +
α2r β 2s .
2t
∂t
1
1
r,s=0

This expression defines the new functions κr,s (t1 , t2 ; i, m + 1 − i), and it can be verified that
they are polynomial functions by induction. Finally, an analogous expression obtained by
differentiating with respect to t2 gives the result for m + 1 and i = 0.
Proof of Proposition 1. We seek conditions under which we can apply Leibniz’s rule
and differentiate equation (5) under the integral sign:
∂ m φ(t1 , t2 )
=
∂ti1 ∂tm−i
2

Z


∂ m f (t1 ; α, β) f (t2 ; α, β)
dG(α, β)
∂ti1 ∂tm−i
2

for m > 0 and i ∈ {0, . . . , m}. Let B represent a bounded, non-empty open neighborhood
of (t1 , t2 ) and let B̄ denote its closure. Assume that there are no points of the form (t, t),
(t1 , 0), or (0, t2 ) in B̄. In order to apply Leibniz’s rule, we must check two conditions:

1. The partial derivative ∂ m f (t1 ; α, β) f (t2 ; α, β) /∂ti1 ∂tm−i
exists and is a continuous
2
44

function of (t01 , t02 ) for every (t01 , t02 ) ∈ B and G-almost every (α, β); and
2. There is a G−integrable function hi,m−i : R2+ → R+ , i.e. a function satisfying
Z

hi,m−i (α, β) dG(α, β) < ∞

such that for every (t01 , t02 ) ∈ B and G-almost every (α, β)

∂ m f (t1 ; α, β) f (t2 ; α, β)
≤ hi,m−i (α, β) .
∂ti1 ∂tm−i
2
Existence of the partial derivatives follows from Lemma 1. The bulk of our proof establishes
that the constant
hi,m−i ≡ max

(t1 ,t2 )∈B̄

r+s≤m
2i 2(m−i)
X X
X
r,s=0 k=0

`=0

θk,`,r,s (i, m − i) −k− 32 −`− 32
t1
t2
2π



r+s+1
τ (t1 , t2 )

r+s+1

e−(r+s+1) ,
(23)

where
τ (t1 , t2 ) =

(t1 − t2 )2
,
2 t1 (1 + t2 )2 + t2 (1 + t1 )2

(24)

is a suitable bound. Note that hi,m−i is well-defined and finite since it is the maximum of
a continuous function on a compact set; the exclusion of points of the form (t, t), (t1 , 0),
or (0, t2 ) is important for this continuity. This bound on the (i, m − i) partial derivatives
ensures that the lower order partial derivatives are continuous.
We now prove that hi,m−i is an upper bound on the magnitude of the partial derivative. Using Lemma 1, the partial derivative is the product of a polynomial function and an
exponential function:
∂

m


 r+s≤m 2i 2(m−i)
X
X
X
3
3
f (t1 ; α, β) f (t2 ; α, β)
θk,`,r,s (i, m − i) −k− 2 −`− 2 2r 2(s+1) 
=
t1
t2
α β
m−i
i
2π
∂t1 ∂t2
r,s=0 k=0 `=0


(αt1 − β)2 (αt2 − β)2
× exp −
−
.
2t1
2t2

Only the constant terms θ may be negative.
To bound the partial derivative, first note that for any nonnegative numbers α and β, r,
and s,
(α + β)2(r+s+1) ≥ α2r β 2(s+1) .
(25)

45

To prove this, observe that the inequality holds when r = s = 0, and the difference between
the right hand side and left hand side is increasing in r and s whenever the two sides are
equal; therefore it holds at all nonnegative r and s. Next note that


(αt1 − β)2 (αt2 − β)2
−
.
exp −(α + β) τ (t1 , t2 ) ≥ exp −
2t1
2t2
2



(26)

This can be verified by finding a maximum of the right hand side of (26) with respect to
α, β subject to the constraint that α + β = K for some K > 0. Next, consider the function
ax exp(−ay) for a and x nonnegative and y strictly positive. This is a single-peaked function
of a for fixed x and y, achieving its maximum value at a = x/y. Letting (α + β)2 play the
role of a, this implies in particular that


r+s+1
τ (t1 , t2 )

r+s+1


e−(r+s+1) ≥ (α + β)2(r+s+1) exp −(α + β)2 τ (t1 , t2 )

(27)

for all nonnegative r, s, α, and β, as long as τ (t1 , t2 ) 6= 0, i.e. t1 6= t2 . Finally, combine
inequalities (25)–(27) to verify the bound on the partial derivative,
hi,m−i


∂ m f (t1 ; α, β) f (t2 ; α, β)
≥
,
∂ti1 ∂tm−i
2

where hi,m−i is defined in equation (23).
Proof of Proposition 2. Start with m = 1. Using the functional form of f (t; α, β) in
equation (3), the partial derivatives satisfy
∂φ(t1 , t2 )
=
∂ti
or

R  β2


α2
3
−
−
f (t1 ; α, β)f (t2 ; α, β)dG(α, β)
2t
2
2t2
R iR i
f (t1 ; α, β)f (t2 ; α, β)dG(α, β) d(t1 , t2 )
T2

2t2i ∂φ(t1 , t2 )
= E(β 2 |t1 , t2 ) − 3ti − t2i E(α2 |t1 , t2 ),
φ(t1 , t2 )
∂ti

where
2

E(α |t1 , t2 ) ≡

Z

2

2

α dG̃(α, β|t1 , t2 ) and E(β |t1 , t2 ) ≡

Z

β 2 dG̃(α, β|t1 , t2 ).

For any t1 6= t2 , we can solve these equations for these two expected values as functions of
φ(t1 , t2 ) and its first partial derivatives.
For higher moments, the approach is conceptually unchanged. First express the (i, j)th
46

partial derivatives of φ(t1 , t2 ) as
2j i+j

2i+j t2i
φ(t1 , t2 )
1 t2 ∂
2
2 2 i
2
2 2 j
=
E
(β
−
α
t
)
(β
−
α
t
)
|t
,
t
+ vij (t1 , t2 )
1
2
1
2
φ(t1 , t2 )
∂ti1 ∂tj2

=

i+j
X

min{x,i}

X

x=0 y=max{0,x−j}

i!j!(−t1 )y (−t2 )x−y E(α2x β 2(i+j−x) |t1 , t2 )
+ vij (t1 , t2 ), (28)
y!(x − y)!(i − y)!(j − x + y)!

where vij depends only on lower moments of the conditional expectation. The first line can be
i+j
1 ,t2 )
from the first line and differentiate with respect
established by induction. Express ∂ ∂tφ(t
i ∂tj
1 2
to t1 . One can realize that all terms except one contain conditional expected moments of
order lower than i + j and thus could be grouped into the term vi+1,j . The only term of

order m + 1 has a form E (β 2 − α2 t21 )i+1 (β 2 − α2 t22 )j |t1 , t2 which follows directly from the
derivative of f (t1 , α, β) with respect to t1 . The second line of (28) follows from the first by
expanding the power functions.
Now let i = {0, . . . , m} and j = m − i. As we vary i, equation (28) gives a system of
m + 1 equations in the m + 1 mth moments of the joint distribution of α2 and β 2 among
workers who find jobs at durations (t1 , t2 ), as well as lower moments of the joint distribution.
These functions are linearly independent, which we show by expressing them using an LU
decomposition:


2m t12m ∂ m φ(t1 ,t2 )
φ(t1 ,t2 )
∂tm
1












2m t1
t22 ∂ m φ(t1 ,t2 )
φ(t1 ,t2 )
∂tm−1
∂t2
1

2(m−1)

2(m−2)

2m t1
t42 ∂ m φ(t1 ,t2 )
φ(t1 ,t2 )
∂tm−2
∂t22
1

..
.

2m t22m ∂ m φ(t1 ,t2 )
φ(t1 ,t2 )
∂tm
2





E(α2m |t1 , t2 )




 E(α2(m−1) β 2 |t , t )


1 2



 = L(t1 , t2 ) · U (t1 , t2 ) · 
 E(α2(m−2) β 4 |t1 , t2 )



..


.


E(β 2m |t1 , t2 )






 + vm (t1 , t2 ), (29)





where L(t1 , t2 ) is a (m + 1) × (m + 1) lower triangular matrix with element (i + 1, j + 1)
equal to
(m−j)!
Lij (t1 , t2 ) = (m−i)!(i−j)!
(−t2 )2(i−j) (t22 − t21 )j/2
for 0 ≤ j ≤ i ≤ m and Lij (t1 , t2 ) = 0 for 0 ≤ i < j ≤ m; U (t1 , t2 ) is a (m + 1) × (m + 1)
upper triangular matrix with element (i + 1, j + 1) equal to
Uij (t1 , t2 ) =

j!
(t2
i!(j−i)! 2

− t21 )i/2

for 0 ≤ i ≤ j ≤ m and Uij (t1 , t2 ) = 0 for 0 ≤ j < i ≤ m; and vm (t1 , t2 ) is a vector that
depends only on (m − 1)st and lower moments of the joint distribution, each of which we
47

have found in previous steps.17 It is easy to verify that the diagonal elements of L and U are
nonzero if and only if t1 6= t2 . This proves that the mth moments of the joint distribution
are uniquely determined by the mth and lower partial derivatives. The result follows by
induction.
Before proving Proposition 3, we state a preliminary lemma, which establishes sufficient
conditions for the moments of a function of two variables to uniquely identify the function.
Our proof of Proposition 3 shows that these conditions hold in our environment.
Lemma 2 Let Ĝ(α, β) denote the cumulative distribution of a pair of nonnegative random
R
variables and let E(α2i β 2j ) ≡ α2i β 2j dĜ(α, β) denote its (i, j)th even moment. For any
m ∈ {1, 2, . . .}, define
Mm = max E(α2i β 2(m−i) ).
(30)
i=0,...,m

Assume that

1

[ Mm ] 2m
= λ < ∞.
lim
m→∞
2m

(31)

Then all the moments of the form E (α2i β 2j ), (i, j) ∈ {0, 1, . . .}2 uniquely determine Ĝ.
Proof of Lemma 2. First recall the sufficient condition for uniqueness in the Hamburger
moment problem. For a random variable u ∈ R, its distribution is uniquely determined by
its moments {E [ |u|m ]}∞
m=1 if the following condition holds:
1

( E [ |u|m ] ) m
lim sup
≡ λ0 < ∞,
m
m→∞

(32)

as shown in the Appendix of Feller (1966) chapter XV.4. We will, however, use an analogous
condition but for even moments only,
1

( E [ u2m ] ) 2m
lim
≡ λ < ∞.
m→∞
2m

(33)

Note that if condition (33) holds, then condition (32) holds as well. To prove this, assume
that λ0 = ∞ and λ < ∞. Then there must be an odd integer m which is very large, and in
particular
1

(E [ |u|m ] ) m
m+1
> (1 + ε)
λ,
m
m
17

(34)

If t2 > t1 , the elements of L and U are real, while if t1 > t2 , some elements are imaginary. Nevertheless,
L.U is always a real matrix. Moreover, we can write a similar real-valued LU decomposition for the case
where t1 > t2 . Alternatively, we can observe that G̃(α, β|t1 , t2 ) = G̃(α, β|t2 , t1 ) for all (t1 , t2 ), and so we
may without loss of generality assume t2 ≥ t1 throughout this proof.

48

where ε > 0 is any number. For any positive number m, as shown in Loeve (1977) Section
1
1
9.3.e0 , it holds that (E [ |u|m ] ) m < ( E [ |u|m+1 ] ) m+1 , and thus
1

1

(E [ |u|m ] ) m
m + 1 ( E [ |u|m+1 ] ) m+1
m+1
≤
≤
λ(1 + ε),
m
m
m+1
m

(35)

which is a contradiction with (34).
We combine this result with the Cramér-Wold theorem, stating that the distribution of a
random vector, say (α, β), is determined by all its one-dimensional projections. In particular,
the distribution of the sequence of random vectors (αm , βm ) converges to the distribution of
the random vector (α∗ , β∗ ) if and only if the distribution of the scalar x1 αm + x2 βm converges
to the distribution of the scalar x1 α∗ + x2 β∗ for all vectors (x1 , x2 ) ∈ R2 .
Thus we want to ensure that for any (x1 , x2 ) the distribution of (x1 α +x2 β) is determined
by its moments. For this we want to check the condition in equation (33) for u(x) ≡
(x1 α + x2 β). We note that:
2m


 X
E u(x)2m = E (x1 α + x2 β)2m =
i=0

≤

2m
X
i=0


2m!
xi1 x2m−i
E α2i β 2(m−i)
2
i!(2m − i)!
m

X

2m!
m!
|x1 |i |x2 |2m−i E α2i β 2(m−i) ≤ Mm
|x1 |i |x2 |m−i
i!(2m − i)!
i!(m − i)!
i=0

= Mm (|x1 | + |x2 |)2m
where we use that (α, β) are non-negative random variables, and Mm is defined in equation
(30).
Now we check that the limit in equation (33) is satisfied given the assumptions in equations (30) and (31):
1

1

(E [u(x)2m ]) 2 m
[ Mm ] 2m
≤
2m
2m
Hence, since the distribution of each linear combination is determined, the joint distribution
is determined.
Proof of Proposition 3. Write the conditional moments as
2i 2(m−i)

E(α β

R
|t1 , t2 ) = R

q(α, β, i, m; t1 , t2 )dG(α, β)
,
f (t1 ; α, β)f (t2 ; α, β)dG(α, β)

49

where
q(α, β, i, m; t1 , t2 ) ≡ α2i β 2(m−i) f (t1 ; α, β)f (t2 ; α, β).
Using the definition of f , we have
α2i β 2(m+1−i)



(αt1 − β)2 (αt2 − β)2
q(α, β, i, m; t1 , t2 ) =
−
exp −
3/2 3/2
2t1
2t2
2πt1 t2

m+1
1
m+1
≤
exp(−(m + 1)),
3/2 3/2
τ (t1 , t2 )
2πt1 t2
where τ (t1 , t2 ) is defined in equation (24) and the inequality uses the same steps as the proof
of Proposition 1 to bound the function. In the language of Lemma 2, this implies
Mm =

((m + 1)/τ (t1 , t2 ))m+1 exp(−(m + 1))
.
3/2 3/2 R
f (t1 ; α, β)f (t2 ; α, β)dG(α, β)
2πt1 t2

(36)

We use this to verify condition (31) in Lemma 2.
Taking the log transformation of (1/2m) (Mm )1/2m and using the expression (36) we get:
1

log

[Mm ] 2m
2m

!
=

1
ϕ(t1 , t2 )
2m

−

1+m
2m

log (τ (t1 , t2 ))

+ 1+m
log (m + 1) −
2m

1+m
2m

− log(m)

where ϕ is independent of m. We argue that the limit of this expression as m → ∞ diverges
to −∞, or that (1/2m) (Mm )1/2m → 0 as m → ∞. To see this
1

log

[Mm ] 2m
2m

!
=

1
1+m
ϕ(t1 , t2 ) −
log (τ (t1 , t2 ))
2m
2m

+

1
1+m 1
1
log (m + 1) + [log(m + 1) − log(m)] −
− log(2m)
2m
2
2m
2

Note that log(1 + m) ≤ log(m) + 1/m and log(1 + m) ≤ m thus taking limits we obtain the
desired result.

C

Identification with One Spell

Special cases of our model are identified with one spell. We discuss two of them. First, we
consider an economy where every worker has the same expected duration of unemployment
1/µ. Second, we consider the case of no switching costs ψe = ψn = 0. These special cases
50

reduce the dimensionality of the unknown parameters. In the first case, the distribution of α
is just a scaled version of the distribution of β. In the second case, the distribution of β = 0
and we are after recovering the distribution of α.

C.1

Identifying the Distribution of β with a Fixed µ

Consider the case where every individual has the same expected unemployment duration
and thus the same value of µn , µin = µn for all i, and σn is distributed according to some
non-degenerate distribution. In our notation, we have that α = µn β for some fixed µn and
β is distributed according to g(β). We argue that we can identify µn and all moments of the
distribution g from data on one spell. The distribution of spells in the population is then
given by
Z
φ(t) =

f (t; µn β, β) g(β) dβ.

(37)

Since the expected duration is 1/µ,
1
=
µn

Z

∞

Z
tf (t; µn β, β) g (β) dβdt =

0

∞

tφ(t)dt,

0

which we can use to identify µn .
Let’s now identify the moments of g. Our approach is based on relating the k th moment
of the distribution φ(t) to the expected values of β 2k . Let M (k) and m(k, µn β, β) be the k th
moment of the distribution φ(t) and f (t; µn β, β), respectively,
Z

∞

tk f (t; µn β, β)dt

Z0 ∞
Z Z ∞
k
k
M (k) ≡
t φ(t)dt =
t f (t; µn β, β) dt g(β) dβ
0
0
Z
=
m(k, µn β, β)g(β) dβ

m(k, µn β, β) ≡

(38)
(39)
(40)

Lemeshko, Lemeshko, Akushkina, Nikulin, and Saaidia (2010) show that the k th moment
of the inverse Gaussian distribution m(k, α, β) can be written as
 k X
k−1
β
(k − 1 + i)!
(2αβ)−i .
m (k, α, β) =
α
i!
(k
−
1
−
i)!
i=0

51

Specialize it to our case with α = µn β to get
m (k, µn β, β) =

k−1
X

a (k, i, µn ) β −2i

i=0

(k − 1 + i)!
a (k, i, µn ) ≡ 2
i! (k − 1 − i)!
−i



1
µn

k+i

Then the k th moment of the distribution φ is
M (k) =
=

Z X
k−1

a (k, i, µn ) β −2i g (β) dβ

i=0
k−1
X



a (k, i, µn ) E β −2i

(41)

i=0

Note that since µn is known, the values of a(k, i, µn ) are known for all k, i ≥ 0. For
k = 2, equation (41) can be solved to find E [β −2 ]. By induction, if E [β −2i ] are known for


i = 1, . . . , k − 1, then equation (41) for M (k) can be used to find E β −2k .

C.2

The Case of Zero Switching Costs

Consider now the special case of no switching costs, ψe = ψn = 0. The region of inaction
is degenerate, ω̄ = ω and hence β = 0. The distribution of spells for any given type is
described by a single parameter α distributed according to density g(α). For any given α,
the distribution of spells is again given by the inverse Gaussian distribution


1
1 2
f (t; α, 0) = √
exp − α t .
2
σn 2πt3/2

(42)

and thus the distribution of spells in the population is
Z
φ(t) =

f (t; α, 0) g(α)dα.

(43)

We argue that the derivatives of φ can be used to identify even moments of the distribution
of g.
Let start by deriving the k th derivative of f (t; α, 0). Use the Leibniz formula for the
derivative of a product to get


m  
∂ m f (t; α, 0)
1 X m ∂ s −3/2  ∂ m−s
1 2
=√
t
exp − α t .
∂tm
∂tm−s
2
2π s=0 s ∂ts
52

Observe that

∂ m−s
∂tm−s


s 
Y
3
∂ s −3/2 
−3/2
= t
− −i
t
∂ts
2
i=0
r−s




1 2
1 2
1 2
,
= exp − α t
− α
exp − α t
2
2
2

and thus we can write an equation for the mth derivative of φ,
∂ m φ(t)
=
∂tm

∂ m f (t; α, 0)
g (α) dα
∂tm
r−s

Z
s 
m  
X
m −s Y
3
1 2
=
f (t; α, 0)
t
− −i
− α
g (α) dα.
s
2
2
s=0
i=0
Z

Finally, rearrange the terms
  r−s h
m  
s 
i
X

m −s Y
1
∂ m φ(t)
3
2 r−s
=
t
−
− −i
E α
|t ,
s
∂tm
2
2
s=0
i=0
to find the mth derivative of φ as a sum of mth and lower moments of α2 .

D

Standard Errors

We use a non-parametric resampling bootstrap to compute the standard errors for the hazard
rate decomposition. We use our data to draw B samples of size N with replacement, where
N = 751, 125 is the number of people in our dataset. We treat each sample b = 1, . . . B as our
original data. That is, we select workers with two completed spells shorter than 260 weeks
to construct the density φb (t1 , t2 ). We smooth the density with a two-dimensional HP filter
and use it to estimate the distribution G+
b (α, β). We construct Ḡb (α, β) and Gb (α, β) using
the share of completed spells in the sample b. Finally, we use Ḡb (α, β), Gb (α, β), G+
b (α, β) to
conduct the decomposition.
Let Hb (t), Hbs (t), Hbh (t) be the aggregate hazard, structural hazard and average type at
duration t under the distribution Ḡb (α, β). To construct the standard errors for aggregate
hazard H(t), we choose b(t) and b̄(t) for each duration t, such that
P rob[Hb (t) ≤ Hb(t) (t) = 0.025],

P rob[Hb (t) ≥ Hb̄(t) (t) = 0.975].

Then, Hb(t) (t) and Hb̄(t) (t) represent error bands on our estimate of the hazard rate H(t).
We proceed in the same way to construct error bands on Hbs (t), Hbh (t).
53

We choose B = 200. Figure 6 shows the decomposition together with the standard
errors. The standard errors are tiny. We find similarly small standard errors when using
non-parametric subsampling bootstrap or parametric bootstrap as well.

E

Multidimensional Smoothing

We start with a data set that defines the density on a subset of the nonnegative integers,
say ψ : {0, 1, . . . , T }2 7→ R. We treat this data set as the sum of two terms, ψ(t1 , t2 ) ≡
ψ̄(t1 , t2 ) + ψ̃(t1 , t2 ), where ψ̄ is a smooth “trend” and ψ̃ is the residual. According to our
model, the trend is smooth except possibly at points with t1 = t2 (Proposition 1). We
therefore define a separate trend on each side of this “diagonal.”
The spirit of our definition of the trend follows Hodrick and Prescott (1997), but extended
to a two dimensional space. For any value of the smoothing parameter λ, we find ψ̄(t1 , t2 )
at t2 ≥ t1 to solve
min

{ψ̄(t1 ,t2 )}

T X
T
X
t1 =1 t2 =t1

(ψ(t1 , t2 ) − ψ̄(t1 , t2 ))2 +

T tX
2 −1
X
(ψ̄(t1 + 1, t2 ) − 2ψ̄(t1 , t2 ) + ψ̄(t1 − 1, t2 ))2 +
λ
t2 =3 t1 =2

λ

T −2 X
T −1
X

!
(ψ̄(t1 , t2 + 1) − 2ψ̄(t1 , t2 ) + ψ̄(t1 , t2 − 1))2 .

t1 =1 t2 =t1 +1

The first line penalizes the deviation between ψ and its trend at all points with t2 ≥ t1 . The
remaining lines penalize changes in the trend along both dimensions, with weight λ attached
to the penalty. If λ = 0, the trend is equal to the original series, while as λ converges to
infinity, the trend is a plane in (t1 , t2 ) space. More generally, the first order conditions to
this problem define ψ̄ as a linear function of ψ and so can be readily solved.
The optimization problem for (t1 , t2 ) with t1 ≤ t2 is analogous. If ψ is symmetric,
ψ(t1 , t2 ) = ψ(t2 , t1 ) for all (t1 , t2 ), the trend will be symmetric as well.

54

Online Appendix
OA.A

Characterization of the Thresholds

To characterize the thresholds ω, ω̄, it is useful to go back to value functions Ẽ(w, b) and
Ñ (w, b) and formulate the Hamilton-Jacobi-Bellman (HJB) equation for the worker’s problem. If a worker is employed, then for all w and b such that w − b ≥ ω, the HJB for Ẽ(w, b)
is
2
2
σb,e
σw,e
+Ẽ22 (w, b)
+Ẽ12 (w, b)σw,e σb,e ρe .
rẼ(w, b) = e +Ẽ1 (w, b)µw,e +Ẽ2 (w, b)µb,e +Ẽ11 (w, b)
2
2
w

Similarly, if a worker is non-employed, then
rÑ (w, b) = b0 eb +N1 (w, b)µw,n +Ñ2 (w, b)µb,n +Ñ11 (w, b)

2
2
σb,n
σw,n
+Ñ22 (w, b)
+Ñ12 (w, b)σw,n σb,n ρn
2
2

for all w and b with w − b ≤ ω̄. At the thresholds, a worker has to be indifferent between
staying with her current status or switching. Using the homogeneity Ẽ, Ñ , the boundary
conditions are
Ẽ(ω, 0) = Ñ (ω, 0) − ψn , Ẽ1 (ω, 0) = Ñ1 (ω, 0) , Ẽ2 (ω, 0) = Ñ2 (ω, 0)
Ñ (ω̄, 0) = Ẽ(ω̄, 0) − ψe , Ẽ1 (ω̄, 0) = Ñ1 (ω̄, 0) , Ẽ2 (ω̄, 0) = Ñ2 (ω̄, 0)
Thus, worker’s problem leads to two partial differential equations. These are difficult to solve
in general, and therefore we use the homogeneity property and rewrite them as a system of
second-order ordinary differential equations for E (ω) and N (ω).
We write the the derivatives of E and N in terms of Ẽ and Ñ :
Ẽ1 (w, b) = eb E 0 (w − b) and Ẽ2 (w, b) = eb E(w − b) − eb E 0 (w − b).
Differentiate again to obtain the second derivatives. The expressions for the derivatives of
N are analogous. Use these to get a second-order ODE for E(ω) and N (ω):
re E(ω) = eω + µ̂e E 0 (ω) + 21 σe2 E 00 (ω)

(OA.1)

rn N (ω) = b0 + µ̂n N 0 (ω) + 21 σn2 N 00 (ω)

(OA.2)

1

where the parameters are
2
rs ≡ r − µb,s − 12 σb,s

2
+ σw,s σb,s ρs
µ̂s ≡ µw,s − µb,s − σb,s

2
2
− 2σw,s σb,s ρs
+ σb,s
σs2 ≡ σw,s

for s ∈ {e, n}. Conditions (19) and (20) reduce to
rs > µ̂s + 21 σs2 and rs > 0 for s ∈ {e, n}.

(OA.3)

We can also rewrite the boundary conditions as
E(ω) = N (ω) − ψn and E 0 (ω) = N 0 (ω)

N (ω̄) = E(ω̄) − ψe and N 0 (ω̄) = E 0 (ω̄).

(OA.4)
(OA.5)

The solution to equation (OA.1) and equation (OA.2) with boundary conditions equation (OA.4) and equation (OA.5) is of a form
eω
+ ce,1 eλe,1 ω + ce,2 eλe2 ω
2
re − µ̂e − σe /2
b0
+ cn,1 eλn,1 ω + cn,2 eλn,2 ω
N (ω) =
rn
E(ω) =

(OA.6)
(OA.7)

where
λe,1 < 0 < λe,2 and λn,1 < 0 < λn,2
are the roots of the equations re = λe (µ̂e + λe σe2 /2) and rn = λn (µ̂n + λn σn2 /2). Hence we
have six equations, (OA.4)–(OA.7), in six unknowns, (ce,1 , ce,2 , cn,1 , cn,2 , ω, ω̄). We turn to
their solution.
Two non-bubble conditions require that
b0
and
ω→−∞
rn
E(ω)
1
lim
=
ω
ω→+∞ e
re − µ̂e − σe2 /2
lim N (ω) =

(OA.8)
(OA.9)

Equation (OA.8) requires that the value function for arbitrarily small ω converges to the value
of being non-employed forever. Likewise equation (OA.9) requires that for an arbitrarily
large ω the value function converges to the value of being employed forever. These nobubble conditions imply that ce,2 = cn,1 = 0. Simplifying the notation, we let ce = ce,1 > 0,
2

λe = λe,1 < 0, cn = cn,2 > 0, and λn = λn,2 > 0. Using this, we rewrite the value
functions (OA.6) and (OA.7) as:
eω
+ ce eλe ω for all ω ≥ ω
2
re − µ̂e − σe /2
b0
+ cn eλn ω for all ω ≤ ω̄
N (ω) =
rn
E(ω) =

(OA.10)
(OA.11)

with
λe =

−µe −

p

µ2e + 2re σe2
−µn +
< −1 and λn =
2
σe

p
µ2n + 2rn σn2
> 1.
σn2

(OA.12)

Condition (OA.3) ensures that the roots are real and satisfy the specified inequalities.
We now have four equations, two value matching and two smooth pasting, in four unknowns (ce , cn , ω, ω̄). Rewrite these as
eω
+ ce eλe ω
2
re − µe − σe /2
eω̄
−ψe +
+ ce eλe ω̄
2
re − µe − σe /2
eω
+ ce λe eλe ω
2
re − µe − σe /2
eω̄
+ ce λe eλe ω̄
2
re − µe − σe /2
ψn +

b0
+ cn eλn ω
rn
b0
=
+ cn eλn ω̄
rn

=

(OA.13)
(OA.14)

= cn λn eλn ω

(OA.15)

= cn λn eλn ω̄

(OA.16)

Note that the values of ce and cn have to be positive, since it is feasible to choose to be either
employed forever or non-employed forever, and since the value of being employed forever
and non-employed forever are the obtained in equations (OA.10) and equations (OA.11) by
setting ce = 0 and cn = 0 respectively.
We conclude this section with a lemma on the units of the switching costs.
Lemma 3 Fix λn > 1, λe < −1 and re − µe − σe2 /2 > 0. Suppose that (ce , cn , ω, ω̄) solve
the value functions for fixed cost and flow utility of non-employment (ψe , ψn , b0 ). Then for
any k > 0, (e0 , n0 , ω 0 , ω̄ 0 ) solve the value function for flow utility of non-employment b00 = kb0
and fixed cost ψe0 = kψe , ψn0 = kψn with ω̄ 0 = ω̄ + log k, ω 0 = ω + log k, c0e = ce k 1−λe , and
n0 = nk 1−λn .
To prove Lemma 3, multiply the appropriate objects in equations (OA.13) and (OA.14) by
k and then simplifying those equations as well as equations (OA.15) and (OA.16) using the
expressions in the statement of the proof. The lemma implies that the size of the region of
inaction, ω̄ − ω, depends only on (ψe + ψn )/b0 . In the main text, we normalized b0 = 1.
3

Thus, the switching costs we calculate in Section 6.6 are in units of the flow value of nonemployment.

OA.B

Constructing Distributions Ḡ and G

We construct distributions Ḡ and G by flipping sign of α for the largest and smallest number
of types from the distribution G+ . We have two restrictions: the distribution of the completed
spells has to be the same as under G+ , and the share of workers with the second spell shorter
than t̄ has to equal c. The first restriction implies that the weight of a type with (−α, β)
has to be given by e4αβ dG+ (α, β).
Let x (α, β) ∈ [0, 1] be a fraction of the type (α, β) from G+ that we will flip into the
type (−α, β). Then we can formulate a maximization problem to construct Ḡ as follows:

Z

x(α, β)e4αβ dG+ (α, β)

R
x (α, β) e4αβ F (t̄; −α, β) + (1 − x (α, β)) F (t̄; α, β) dG+ (α, β)
R
s.t. c =
(1 − x (α, β) + x (α, β) e4αβ ) dG+ (α, β)
max

x(α,β)

x(α, β) ≥ 0,

x(α, β) ≤ 1

Assigning a Lagrange multipliers λ, λ0 (α, β) and λ1 (α, β) to the constraints, the first
order condition for a given (α, β) is



 

0 = 1 + λ c 1 − e−4αβ − e−2αβ − e−4αβ F (t̄; αβ) + λ0 (α, β) − λ1 (α, β) .
It then follows that there will be at most one type (α, β) for which we have an interior solution
x(α, β) ∈ (0, 1), while for other types either x(α, β) = 1 or x(α, β) = 0. The first order


condition also defines the selection rule p(α, β) = c 1 − e−4αβ − e−2αβ − e−4αβ F (t̄; α, β)
which we used in Section 3.3.

OA.C

Power of the First Moment Test

We consider two interesting specifications of the data generating mechanism which fail our
test for the first moments of (α2 , β 2 ). Both cases are elaborations around examples introduced in Section 3.4. We find that in both cases the test for E[α2 |t1 , t2 ] fails for t2 = 0 and t1
sufficiently small. We note that the first example has the property that φ is not differentiable
at points where t1 = t2 .
4

OA.C.1

A Model of Delayed Search

Consider an extension of a canonical search model where a non-employed individual starts
actively searching for a job only after τ periods, and finds a job at the rate θ. Thus, the
hazard rate of exiting non-employment is zero for t ≤ τ , and θ for t ≥ τ . Each worker is
described by a pair (θ, τ ) which is distributed in the population according to a cumulative
distribution function G. The joint density of two spells is given by
∞

Z
φ(t1 , t2 ) =

min {t1 ,t2 }

Z

0

θ2 e−θ (t1 +t2 −2τ ) dG(θ, τ ).

0

We now apply our test to this model. If t1 > t2 , then
R 2 −θ(t −t )
R 3 −θ(t +t −2τ )
dG(θ, τ )
θ e 1 2 dG(θ|t2 )
θ e 1 2
3
2 t22
R
R
+
2
−
,
E(α |t1 , t2 ) = 2
2
2
−θ(t
+t
−2τ
)
2
−θ(t
+t
−2τ
)
t1 − t2 θ e 1 2
dG(θ, τ )
θ e 1 2
dG(θ, τ ) t1 + t2
R 2 −θ(t −t )


θ e 1 2 dG(θ|t2 )
2t1 t2
3
2
R
+
E(β |t1 , t2 ) = t1 t2
≥ 0.
(t21 − t22 ) θ2 e−θ(t1 +t2 −2τ ) dG(θ, τ ) t1 + t2
2

Assume that the following regularity conditions hold:
R
R

θ3 e−θ(t1 −2τ ) dG̃(θ|t2 )
θ2 e−θ(t1 −2τ ) dG̃(θ, τ )

R
< ∞ and R

θ2 e−θt1 dG̃(θ|0)

θ2 e−θ(t1 −2τ ) dG̃(θ, τ )

< ∞.

Setting t2 = 0, the term E(α2 |t1 , t2 ) becomes
R 3 −θ(t −2τ )
θ e 1
dG(θ, τ )
3
− .
E(α |t1 , t2 ) = 2 R 2 −θ(t1 −2τ )
θ e
dG(θ, τ ) t1
2

For t1 small enough, the negative term

OA.C.2

3
t1

will dominate and the test fails, i.e. E(α2 |t1 , 0) < 0.

Mixed Proportional Hazard Model

We consider a mixed proportional hazard (MPH) model, which specifies that worker’s hazard
of finding a job at duration t is given by θh(t) where h(t) is a so-called baseline hazard, and
θ is an unobserved individual characteristic distributed according to G. The distribution of
two spells t1 , t2 is given by
Z
φ(t1 , t2 ) =

∞

R t1

θ2 h(t1 ) h(t2 ) e−θ (

0

5

0

Rt
h(s)ds+ 0 2 h(s)ds)

dG(θ)

(OA.17)

Differentiate with respect to ith spell
Z
φi (t1 , t2 ) =
0

∞




R t2
R t1
h0 (ti )
− θ h(ti ) θ2 h(t1 ) h(t2 ) e−θ ( 0 h(s)ds+ 0 h(s)ds) dG(θ)
h(ti )

and thus:
φi (t1 , t2 )
h0 (ti )
=
− h(ti ) E [θ | t1 , t2 ] where
φ(t1 , t2 )
h(ti )
Rt
Rt
R∞ 3
−θ ( 0 1 h(s)ds+ 0 2 h(s)ds)
dG(θ)
θ
h(t
)
h(t
)
e
1
2
E [θ | t1 , t2 ] ≡ R 0∞
R t2
R t1
θ2 h(t ) h(t ) e−θ ( 0 h(s)ds+ 0 h(s)ds) dG(θ)
0

1

2

We thus have:


0
0
2

2
3
2 h (t1 )
2
2 h (t2 )
E(α |t1 , t2 ) = 2
− t1
+ E [θ | t1 , t2 ] t1 h(t1 ) − t2 h(t2 ) −
,
t2
2
t1 − t2
h(t2 )
h(t1 )
t1 + t2




2 t1 t2 h0 (t2 ) h0 (t1 )
3
2
E(β |t1 , t2 ) = t1 t2 2
−
+ E [θ | t1 , t2 ] [h(t1 ) − h(t2 )] +
≥ 0.
t1 − t22 h(t2 )
h(t1 )
t1 + t2
2

Assume that the baseline hazard rate h is bounded and has bounded derivative around
t = 0, so that |h0 (t1 )/h(t1 )| ≤ b and |h(t1 )| < B for two constants B, b. Set t2 = 0 in which
case we have:

 0
3
h (t1 )
2
+ E [θ | t1 , 0] h(t1 ) − ≥ 0
E(α |t1 , 0) = 2 −
h(t1 )
t1
Then the test fails, i.e. E(α2 |t1 , 0) < 0, for t1 small enough because the negative term −3/t1
will dominate.

OA.D

Selected Properties of the Austrian Data

In this section, we show business cycle properties of the Austrian data, and analyze the
impact of our selection criteria on the final sample.
We start with the business cycle properties. During the analyzed period, Austrian annual
GDP growth varied between 0.5 and 3.6 percent per year and never slipped negative. In
Figure 12 we plot the mean duration of all in-progress nonemployment spells that are shorter
than 5 years. We plot this between 1991 and 2006, where the delayed start date ensures that
we have not artificially truncated the duration of any spells. There is virtually no cyclical
variation. We smooth the data before we depict them but this is to remove weekly variation
in the duration. In particular, there is always a peak in 4, 8, etc weeks which is due to the
6

mean in-progress non-employment, in weeks

60
50
40
30
20
10
0

1992

1994

1996

1998 2000
time

2002

2004

2006

Figure 12: Mean duration of all in-progress non-employment spells with duration shorter
than 260 weeks, smoothed using an HP filter with a smoothing parameter 14,400.
fact that employment spells tend to start in the first week of the month and end in the last
week of the month.
We next analyze the impact of the selection criteria on our sample. To be included in
our sample, a worker has to meet certain age criteria, and has to have at least two nonemployment spells, with the first one longer than 260 weeks. We compare this sample to a
sample which has the same age criteria but does not require a worker to have at least one
spell.
Figure 13 shows a comparison of the distribution of non-employment spells in the full
sample and in the sample we selected for estimation. For comparability, we condition on
spells lasting no more than 260 weeks even in the full sample. We see that our selected
sample has fewer very short spells and more longer spells.

OA.E

Estimation

The link between the model and the data is given by equation (13). Our goal is to find the
distribution G(α, β) using the data on the distribution of two non-employment spells. We
do so in two steps. In the first step, we discretize equation (13) and solve it by minimizing
the sum of squared errors between the data and the model-implied distribution of spells. In
7

multi-spell data
single-spell data

density

10−2

10−3

10−4
0

20

40

60

80 100 120 140 160 180 200 220 240 260
duration in weeks

Figure 13: Comparison of the marginal distributions of non-employment spells with and
without imposing restrictions of the number and duration of spells. The red line shows the
distribution in our selected sample. The blue line shows the distribution of the single spell
data where we impose the same age criteria as in our sample, but do not require a worker
having at least two non-employment spells.

8

the second step, we refine these estimates by applying the expectation-maximization (EM)
algorithm.
Each method has advantages and disadvantages. The advantage of the first step is that
it is a global optimizer. The disadvantage is that we optimize only on a fixed grid for (α, β).
The EM method does not require to specify bounds on the parameter space, but needs a
good initial guess because it is a local method.
It also turns out that the maximum likelihood method suffers from two potential biases:
one inherited from inverse Gaussian distribution, and one from working with discrete rather
than continuous durations. We elaborate on these issues in our detailed discussion of the
EM algorithm below.

OA.E.1

Step 1: Minimum Distance Estimator

To discretize equation (13), we view φ(t1 , t2 ) and g(α, β) as vectors in finite dimensional
spaces. We consider a set T ⊂ R2+ of duration pairs (t1 , t2 ), and refer to its typical elements
as (t1 (i), t2 (i)) ∈ T with i = 1, . . . , I. Guided by our data selection and the fact that
the model is symmetric, we choose T to be the set of all integer pairs (t1 , t2 ) satisfying
0 ≤ t1 ≤ t2 ≤ 260. We also replace φ(t1 , t2 ) with the average of φ(t1 , t2 ) and φ(t2 , t1 ) to take
advantage of the fact that our model is symmetric.
For the pairs of (α, β) we choose a set Θ ⊂ R2+ and again refer to its typical element
(α(k), β(k)) ∈ Θ with k = 1, . . . , K.18 The distribution of types is then represented by
P
g(k), k = 1, . . . K such that g(k) ≥ 0 and K
k=1 g(k) = 1. Naturally, β(k) > 0 for all k.
Given the limitation of our identification, we choose α(k) > 0 for all k.
Equation (13) in the discretized form is
φ=

Fg
,
H0 g

where φ is a vector φ(t1 , t2 ), (t1 , t2 ) ∈ T, g is K × 1 vector of g(k), F is a T × K matrix with
elements Fi,j , and H is a K × 1 vector with element Hj defined below
Fi,j = f (t1 (i), α(j), β(j))f (t2 (i), α(j), β(j))
X
Hj =
f (t1 , α(j), β(j))f (t2 , α(j), β(j)).
(t1 ,t2 )∈T2
18

We experiment with different square grids on Θ both on equally spaced values on levels and in logs. We
also set the grid in terms of σn /(ω̄ − ω) = 1/β and µn /(ω̄ − ω) = α/β.

9

In the first stage, we solve the minimization problem
0

min ((F − φH0 ) g) ((F − φH0 ) g)
g

s.t. g ≥ 0,

K
X

g(k) = 1.

k=1

In practice, this problem is ill-posed. The kernel f (t1 ; α, β)f (t2 ; α, β) which maps the distribution g(α, β) into the joint distribution of spells φ(t1 , t2 ) is very smooth, and dampens any
high-frequency components of g. Thus, when solving the inverted problem of going from the
data φ to the distribution g, high-frequency components of φ get amplified. This is particularly problematic when data are noisy, as is our case, since standard numerical methods
lead to an extremely noisy estimate of g. Moreover, the solution of ill-posed problems is
very sensitive to small perturbation in φ. In order to stabilize the solution and eliminate
the noise, we do two things: first, we use smoothed rather than raw data as a vector φ,
and second, we stabilize the solution by replacing F̃ ≡ F − φH0 with F̃ + λI where λ is a
parameter of choice. This effectively adds a penalty λ on the norm of g, and one minimizes
||F̃g||2 + λ||g||2 subject to the same constraints as above. We use the so-called L-curve to
determine the optimal choice of λ.19 This is called Tikhonov regularization method, see for
example Engl, Hanke, and Neubauer (1996).

OA.E.2

Step 2: Maximum Likelihood using the EM algorithm

We apply the EM method in the second stage. This is an iterative method for finding
maximum likelihood estimates of parameters α, β
log `(t; α, β, g) =

X
(t1 ,t2 )∈T

φ(t1 , t2 ) log

K
X
f (t1 ; α(k), β(k))f (t2 ; α(k), β(k))
k=1

(1 − F (t̄; α(k), β(k)))2

!
g(k) ,

where F (t; α, β is the cumulative distribution function of the inverse Gaussian distribution
with parameters α, β, and t̄ = 260 is the maximum measured duration. The mth iteration
step of the EM has two parts. In the first part, the E step, we use estimates from (m − 1)st
iteration to calculate probabilities that ith pair of spells t1 (i), t2 (i) comes from each of the
type k. In the second part, the M step, we use these probabilities to find new values of
α(k), β(k), g(k) from the first order conditions of the maximum likelihood problem.
19
The L-curve is a graphical representation of the tradeoff between || (Fg − φ) ||2 and |g||2 . When plotted
in the log-log scale, it has the L shape, hence its name. We choose value of λ which corresponds to the
”corner” of the L-curve because it is a compromise between fitting the data and smoothing the solution.

10

The EM algorithm is an iterative procedure to solve a maximum likelihood problem. To
simplify the notation, denote data as xi = (t1i , t2i ) , i = 1, . . . N and parameter θk = (αk , βk )
K
K
and gk for k = 1, . . . K. Also, let x = {xi }N
i=1 , θ = {θk }k=1 , g = {gk }k=1 . The likelihood is
l(x; θ, g) =

" K
N
Y
X
i=1

#
h(xi , θk ) gk

k=1

where we use the following notation
h(xi , θk ) =

f (t1i , αk , βk )f (t2i , αk , βk )
.
(F (t̄, αk , βk ) − F (t, αk , βk ))2

Here, t and t̄ are the bounds on t. In our case, t = 0 and t̄ = 260. The log-likelihood is then
given by

log `(x; θ, g) =

N
X

log

i=1

K
X

!
h(xi , θk ) gk

,

(OA.18)

k=1

which we want to maximize by choosing θ, g.
This problem has first order conditions:
N

∂ log h(xi , θk )
h(xi , θk ) gk
∂ log `(x; θ, g) X
=
0=
PK
∂θk
∂θk
k0 =1 h(xi , θk0 ) gk0
i=1
N

0=

∂ log `(x; θ, g) X
h(xi , θk )
=
PK
∂gk
k0 =1 h(xi , θk0 ) gk0
i=1

Define zk,i as the probability that the ith pair of spells comes from the type k, for all i =
1, ..., N and k = 1, ..., K, as
h(xi , θk ) gk
zk,i (xi ; θ, g) ≡ PK
.
0
k0 =1 h(xi , θk0 ) gk
Notice that for all i = 1, ..., N , we have

PK

k=1 zk,i

11

(OA.19)

= 1. We can write the first order conditions

using z as follows:
0=

N
X

zk,i (xi ; θ, g)

i=1

∂ log h(xi , θk )
∂θk

(OA.20)

PN

i=1 zk,i (xi ; θ, g)
PN
k0 =1
i=1 zk0 ,i (xi ; θ, g)

gk = PK

(OA.21)

This is a system of (3 + N ) K equations in (3 + N ) K unknowns, namely {αk , βk , gk } and
{zk,i }. These equations are not recursive; for instance g enters in all of them.
The EM algorithm is a way of computing the solution to the above system iteratively.
It can be shown that this procedure converges to a local maximum of the log-likelihood
function. Given {θ m , gm } we obtain new values {θ m+1 , gm+1 } as follows:
m
1. (E-step) For each i = 1, .., N compute the weights zk,i
as :

h(xi , θkm ) gkm
m
for all k = 1, ..., K .
zk,i
= PK
m
m
k0 =1 h(xi , θk0 ) gk0

(OA.22)

2. (M-step) For each k = 1, ..., K define θkm+1 as the solution to:
0=

N
X

m
zk,i

i=1

∂ log h(xi , θkm+1 )
,
∂θk

(OA.23)

for all k = 1, ..., K.
3. (M-step) For each k = 1, ..., K let gkm+1 as :
PN

gkm+1

OA.E.3

=

m
i=1 zk,i
PK PN m
k0 =1
i=1 zk0 ,i

.

(OA.24)

Potential Biases in ML Estimation

There are two biases in the maximum likelihood estimation, one related to estimation of µ,
and one related to estimation of σ. These then lead to biases in estimation of α and β.
It is instructive to derive the maximum likelihood estimators for µ and σ in a simple
case, where data on (single spell) duration t(i), i = 1, . . . N come from an inverse Gaussian
distribution. A straightforward algebra leads to
µ̂M LE =

N
1 X
t(i) = E[t],
N i=1

N
N
1 X 1
1 X
1
σˆ2 M LE =
−
t(i) = E[ ] − E[t].
N i=1 t(i) N i=1
t

12

(OA.25)

1.3

1.3
σ
σ
σ
σ

ratio µ̂/µ

1.2

= 0.02
= 0.22
= 0.42
= 0.62

1.2

1.1

1.1

1

1

0.9

0.02

0.04
µ

0.06

µ = 0.01
µ = 0.0.3
µ = 0.05
µ = 0.07

0.9

0.08

0.2

0.4

0.6
σ

0.8

1

1.2

Figure 14: Maximum likelihood estimates of µ̂, relative to the true value of µ. The figure
shows the ML estimate of µ using data on 1 million spells generated from a single type
inverse Gaussian distribution with parameters (µ, σ) ∈ [0.01, 0.08] × [0.02, 1.2]. We show
estimated µ̂ relative to the true µ as a function of µ in the left panel, or as a function of σ
in the right panel.
Notice that E[t] and E[1/t] are sufficient statistics.
The bias in µ is inherited from the inverse Gaussian distribution. In particular, it is
very difficult to estimate µ precisely if µ is close to zero, which can be seen from the Fisher
information matrix. This is given by, see for example Lemeshko, Lemeshko, Akushkina,
Nikulin, and Saaidia (2010),
I(µ, σ) =

µ3 /σ 2
0

0
1 4
σ
2

!

and thus the lower bound on any unbiased estimate of µ is proportional to 1/µ3 . This
diverges to infinity as µ approaches zero. Therefore, any estimate of µ, and thus also any
estimate of α, will have a high variance for small µ (α). To illustrate this point, we generate
1,000,000 unemployment spells from a single inverse Gaussian distribution with parameters
µ, σ, assuming that µ ∈ [0.01, 0.08] and σ ∈ [0.02, 1.2]. For different combinations of µ and
σ, we find the maximum likelihood estimates µ̂ and σ̂, and plot µ̂ relative to true value of µ
in Figure 14. The left panel shows this ratio as a function of µ, the right panel as a function
of σ. The estimate of µ has a high variance for small µ and thus is likely to be further away
from the true value. This bias is somewhat worse for a larger value of σ, in line with the
lower bound on the variance σ 2 /µ3 , which is higher for smaller µ and larger σ.
To illustrate the performance of the ML estimator, we worked with continuous data. The

13

ˆ
ratio of E[1/t]/E[1/t]

ratio σ̂/σ

1

0.8

0.6

µ = 0.01
µ = 0.0.3
µ = 0.05
µ = 0.07
0.2

0.4

0.6
σ

0.8

1

1

0.8

0.6

1.2

µ = 0.01
µ = 0.03
µ = 0.05
µ = 0.07
0.2

0.4

0.6
σ

0.8

1

1.2

Figure 15: Maximum likelihood estimates of σ (left panel) and of the mean of 1/t (right
panel) using discretized data, relative to their true values. We used 1,000,000 spells simulated
from a single inverse Gaussian distribution with parameters (µ, σ) ∈ [0.01, 0.08] × [0.02, 1.2].
The ratios are plotted as a function of the true σ. Each line corresponds one value of
µ ∈ [0.01, 0.08].
real-world data differ from simulated in terms of measurement, as these can be measured
only in discrete times. In particular, anybody with duration between, say 12 and 13 weeks,
will be used in our estimation as having duration of 12.5 weeks. We study what bias this
measurement introduces by treating the simulated data as if they were measured in discrete
times. We find that this measurement affects estimates of σ, see the left panel of Figure
15, and the bias comes through the bias in estimating E[1/t], see the right panel of the
same figure. The bias in estimation of µ is small for values of σ < 0.6, which the range
we estimate in the Austrian data. The magnitude of the bias for estimation of σ does not
depend on the value of µ, but is larger for larger values of σ, see the right panel of Figure 15.
Discretization affects the mean of t only very mildly, and thus it does not affect estimation of
µ. However, the mean of 1/t is sensitive to discretization. Since the mean of t is very similar
for discretized and real values of t, this suggests that the distribution of spells between t
and t + 1 is not very different from symmetric. If this distribution was uniform, the bias in
E[1/t] can be mitigated by using a different estimator for E[1/t]. For example, noticing that
R t+1
log(t + 1) − log(t) = t (1/t)dt, one can use the sample average of log(t + 1) − log(t) to
measure E[1/t]. In practice, we find that this estimator reduces the bias in E[1/t] if spells
are measured at some starting duration t larger than 0, say 2 weeks. However, if spells are
measured starting at zero, the bias is worse.

14

α
β
µn
ω̄−ω
σn
ω̄−ω

EM with 152 types
EM with 77 types
mean
median
st.dev.
min
mean
median
st.dev
min
508.770
0.138
3123.989 0.096 527.173
0.139
3146.488 0.097
3512.460 5.735 18390.764 1.465 3724.065 5.744 18916.972 1.466
0.052
0.043
0.033
0.018
0.052
0.044
0.033
0.018
−5
0.231
0.174
0.155
10
0.230
0.174
0.155
10−5
Table 3: Comparison of summary statistics with 152 and 77 types.

OA.F

Akaike Information Criterion

One might worry that our model is over-parametrized and hence recovers too many types.
We address this concern by using Akaike information criterion to reduce the number of types
and compare the decomposition with fewer types to our main results.
We proceed as follows. Starting from the our estimated distribution G+ with K = 152
types, characterized by pairs (αk , βk ) and weights gk , for k = 1, . . . K, we construct a new
distribution with fewer types by eliminating m > 0 types with the lowest weights gk . We
use this new distribution as an initial guess for the EM algorithm, which further refines the
estimates. We compute the Akaike information criterion for these estimates. We repeat this
exercise for m = 1, . . . K − 1 and choose the value of m which minimizes the criterion. The
minimum corresponds to a distribution with 77 types.
Table 3 and Figure 16 show compare summary statistics for the distribution and the
decomposition using the distirbution with 152 or 77 types. The take away from this exercise
is that the big role of heterogeneity which we recover does not hinge on the large number of
types.

15

1

0.06

“average type”

weekly hazard rate

0.08

H s (t)

0.04
0.02
0

H(t)
0

20
40
60
80
duration in weeks

0.6
0.5
0.4
0.3
0.2

100

H h (t)
77 types
152 types
0

20
40
60
80
duration in weeks

100

Figure 16: Decomposition of the hazard rate with 77 types. In the left panel, the purple line
shows the raw hazard rate, H(t), and the blue line shows the structural hazard rate, H s (t).
The right panel shows the contribution of heterogeneity, H h (t). The green line shows the
model with 77 types and the red line shows the model with 152 types, in both cases for the
distribution Ḡ.

16

