NBER WORKING PAPER SERIES

LONG-RUN RISK IS THE WORST-CASE SCENARIO
Rhys Bidder
Ian Dew-Becker
Working Paper 22416
http://www.nber.org/papers/w22416

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
July 2016

We appreciate helpful comments and discussions from Harjoat Bhamra, Stefano Giglio, Valentin
Haddad, Lars Hansen, Stavros Panageas, Costis Skiadas, Matt Smith, and seminar participants at
the Federal Reserve Bank of San Francisco, Duke Fuqua, the Asset Pricing Retreat in Tilburg,
Kellogg, UC Santa Cruz, the NBER Asset Pricing meeting, the NYU Conference on Robustness
and Ambiguity, Stanford GSB, the SED, the WFA, and CMU Tepper. Ben Pyle provided
excellent research assistance. The views expressed in this paper are those of the authors and not
necessarily those of the Federal Reserve Bank of San Francisco, the Federal Reserve Board of
Governors, the Federal Reserve System, or the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2016 American Economic Association, circulated with permission. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.

Long-Run Risk is the Worst-Case Scenario
Rhys Bidder and Ian Dew-Becker
NBER Working Paper No. 22416
July 2016
JEL No. C14,D83,D84,G12
ABSTRACT
We study an investor who is unsure of the dynamics of the economy. Not only are parameters
unknown, but the investor does not even know what order model to estimate. She estimates her
consumption process nonparametrically – allowing potentially infinite-order dynamics – and
prices assets using a pessimistic model that minimizes lifetime utility subject to a constraint on
statistical plausibility. The equilibrium is exactly solvable and we show that the pricing model
always includes long-run risks. With risk aversion of 4.7, the model matches major facts about
asset prices, consumption, and dividends. The paper provides a novel link between ambiguity
aversion and non-parametric estimation.

Rhys Bidder
Federal Reserve Bank of San Francisco
101 Market Street
San Francisco
CA 94105
rhys.bidder@sf.frb.org
Ian Dew-Becker
Kellogg School of Management
Northwestern University
2001 Sheridan Road
Evanston, IL 60208
and NBER
ian.dewbecker@gmail.com

A data appendix is available at http://www.nber.org/data-appendix/w22416

1

Introduction

Economists do not agree on the dynamic properties of the economy. There has been a long debate
in the finance literature over how risky consumption growth is in the long-run (e.g. Bansal, Kiku,
and Yaron (2012) and Beeler and Campbell (2012)), and it is well known that long-run forecasting
is econometrically difficult (Müller and Watson (2013)). It is likely that the average investor is also
unsure of the true model driving the world. This paper studies the behavior of such an investor.
With exactly solved results, we show that a model in which investors have Epstein–Zin preferences and uncertainty about consumption dynamics generates high and volatile risk premia, excess
volatility in stock returns, a large degree of predictability in stock returns, low and stable interest
rates, and an estimated elasticity of intertemporal substitution from interest rate regressions of zero
as measured in Hall (1988) and Campbell and Mankiw (1989). Moreover, variation over time in
risk or model uncertainty is not required to generate predictability in returns.
We argue that investors consider a set of models of the economy that is only weakly constrained. People face pervasive ambiguity: no one can say they know the exact specification to
estimate when forecasting economic activity. So rather than just allowing uncertainty about the
parameters in a specific model, or putting positive probability on a handful of models, we treat
investors as considering an infinite-dimensional space of autoregressive moving average (ARMA)
specifications of unrestricted order. They therefore face a nonparametric problem in the sense that
the number of parameters to be estimated is potentially infinite.1
Infinite-dimensional estimation problems are well known to be difficult or impossible to approach with standard likelihood-based methods.2 But people must estimate some model. So,
following the literature on nonparametric time series estimation, we assume that in a finite sample,
they estimate a relatively small-scale model, which they view as an approximation to the truth. But
the true model may be infinite-dimensional. So investors face a highly non-standard estimation
problem, and, as Sims (1972) shows, the consequences for welfare of using a misspecified model
can be severe.
The uncertainty due to estimating consumption dynamics when the true model is unknown
and potentially of infinite order is clearly very different from that due to standard sources of risk,
such as future innovations to the consumption process; in many cases a valid posterior distribution
cannot be placed on the space of models. It is simply not always possible to be a Bayesian (in
particular, when the model order is unknown). It is reasonable to think that people view model
1

This is a typical definition of nonparametric estimation – see, e.g., Chen (2007)
Diaconis and Freedman (1986) note that Doob’s (1948) theorem on the consistency of Bayesian estimates only
applies to finite-dimensional parameter spaces. In infinite-dimensional settings, Bayesian estimators need not be
consistent for even apparently reasonable priors. Sims (1971, 1972) shows that it is generally impossible to create
accurate confidence intervals in such settings. See also Chow and Grenander (1985), Faust (1999), and Hansen and
Sargent (2007) (who note the links between Sims (1971), Diaconis and Freedman (1986), and robust control theory).
2

2

uncertainty fundamentally differently from other sources of risk, in the sense of Knight (1921) and
Ellsberg (1961).
There are multiple potential ways of modeling attitudes towards such ambiguity. We draw
upon the insights of Gilboa and Schmeidler’s (1989) work on choice under ambiguity in modeling
people as making choices under a “worst-case” process for consumption that is chosen to minimize
lifetime utility.3 Such analysis is highly tractable, it is a plausible and parsimonious description of
how people might approach ambiguity, and it yields results that have a natural economic interpretation because they point directly to the specific model that is most painful to agents.
Our headline theoretical result is that for an ambiguity-averse agent whose point estimate is that
consumption growth is white noise, the worst-case model used for decision-making, chosen from
the entire space of ARMA models, is an ARMA(1,1) with a highly persistent trend – literally the
homoskedastic version of Bansal and Yaron’s (2004) long-run risk model. More generally, whatever the investor’s point estimate, the worst-case model always adds a long-run risk component to
it.
The low-frequency fluctuations that people in our model fear die out at precisely the rate of time
preference. In a sense, then, they are shocks that effectively last the rest of the investor’s life. So
a way of interpreting our results is that they say that what people fear most, and what makes them
averse to investing in equities, is that growth rates or asset returns are going to be persistently lower
over the rest of their lives than they have been on average in the past. Our specific formulation of
model uncertainty allows us to formalize that intuition.
Our results are derived in the frequency domain, which allows strikingly clear conceptual and
analytical insights. Two factors determine the behavior of the worst-case model at each frequency:
estimation uncertainty and how utility is affected by fluctuations at that frequency. Growth under
the worst-case model has larger fluctuations at frequencies about which there is more uncertainty
or that are more painful. Quantitatively, we find that differences in estimation uncertainty across
frequencies are relatively small. Instead, since people with Epstein–Zin preferences are highly
averse to low-frequency fluctuations (for parameterizations such as ours where they prefer an early
resolution of uncertainty), persistent shocks play the largest role in robust decision-making.
A criticism of the long-run risk model has always been that it depends on a process for consumption growth that is difficult to test for.4 We turn that idea on its head and argue that it is
the difficulty of testing for and rejecting long-run risk that actually makes it a sensible model for
investors to focus on. If anything, our result is more extreme than that of Bansal and Yaron (2004):
whereas they posit a consumption growth trend with shocks that have a half-life of 3 years, the
3

Alternative models of ambiguity include Hansen and Sargent (2001), Epstein and Schneider (2003), Klibanoff,
Marinacci, and Mukerji (2005) and Maccheroni, Marinacci, and Rustichini (2006).
4
See Beeler and Campbell (2012) and Marakani (2009).

3

endogenous worst-case model that we derive features trend shocks with a half-life of 70 years.
In a calibration of the model, we show that it explains a wide range of features of financial
markets that have been previously viewed as puzzling. Similar to the intuition from Bansal and
Yaron (2004), equities earn high average returns in our model because low-frequency fluctuations
in consumption growth induce large movements in both marginal utility and equity prices. In our
setup, though, long-run risk need not actually exist – it only needs to be plausible.
The results that we obtain on excess volatility, forecasting, and interest rate regressions all
follow from the fact that the pricing model that our investor uses always involves more persistence
than her point estimate (i.e. the model used by an econometrician with access to the same data
sample as the investor). Since the pricing model has excess persistence, investors overextrapolate
recent news relative to what the point estimate would imply. Following positive shocks, then, stock
prices are relatively high and econometric forecasts of returns are low. We are thus able to generate
predictability without any changes in risk or risk aversion over time. Significantly, we obtain this
result in a model in which agents are rational, if uncertain. The lower case of the ‘r’ in rational here
is important. Agents behave as if optimizing under a “worst case” distribution that is different from
the true data generating process, putting us outside the standard Rational Expectations paradigm.
As discussed in Hansen and Sargent (2007), this is characteristic of models of ambiguity.
In generating all these results we have no more free parameters than other standard models
of consumption and asset prices. We link the parameter that determines how the agent penalizes
deviations from her point estimate for consumption dynamics directly to the coefficient of relative
risk aversion. There is thus a single free parameter that determines risk preferences, and it corresponds to a coefficient of relative risk aversion of only 4.7. We also take no extreme liberties with
beliefs – the investor’s pricing model is essentially impossible to distinguish from the true model
in a 100-year sample. Using a correctly specified likelihood ratio test, the null hypothesis that the
pricing model is true is rejected at the five-percent level in, at most, only 6.8 percent of samples.
Our analysis directly builds on a number of important areas of research. First, the focus on a
single worst-case outcome is closely related to Gilboa and Schmeidler’s (1989) work on ambiguity aversion that provides an axiomatic basis for decision making under a worst-case distribution
(though here we do not start from an axiomatic foundation). Second, we build on the analysis
of generalized recursive preferences to allow for the consideration of multiple models, especially
Hansen and Sargent (2010) and Ju and Miao (2012).5 The work of Hansen and Sargent (2010) is
5

See, e.g., Kreps and Porteus (1978); Weil (1989); Epstein and Zin (1991); Maccheroni, Marinacci, and Rustichini
(2006); and Hansen and Sargent (2005), among many others. There is also a large recent literature in finance that
specializes models of ambiguity aversion to answer particularly interesting economic questions, such as Liu Pan and
Wang (2004) and Drechsler’s (2013) work with tail risk and the work of Uppal and Wang (2003), Maenhout (2004),
Sbuelz and Trojani (2008), and Routledge and Zin (2009) on portfolio choice. Recent papers on asset pricing with
learning and ambiguity aversion include Veronesi (2000), Brennan and Xia (2001), Epstein and Schneider (2007),
Cogley and Sargent (2008), Leippold, Trojani, and Vanini (2008), Ju and Miao (2012), and Collin-Dufresne, and

4

perhaps most comparable to ours, in that they study an investor who puts positive probability on
both a white-noise model and a parameterized long-run risk model for consumption growth. The
key difference here is that we consider a nonparametric setting in which the agent considers all
ARMA models, instead of only two. The emergence of the long-run risk model as the one that she
focuses on is entirely endogenous.6 We also obtain analytic results that provide economic insights
into precisely what model is most painful to agents, whereas Hansen and Sargent’s (2010) analysis
is numerically solved.
Since the worst-case model is more persistent than the point estimate, pricing behavior is similar to the extrapolation implied by the “natural expectations” studied by Fuster, Hebert, and Laibson (2011). Our results differ from theirs, though, in that we always obtain excess extrapolation,
whereas in their setting it results from the interaction of suboptimal estimation on the part of
investors with a specific data-generating process. Cecchetti, Lam, and Mark (2000) examine a
setting in which investors use a rule-of-thumb estimation technique and are subject to random belief shocks to obtain excess volatility and predictability. In our analysis, there is no assumption
that the estimation method is suboptimal, and excess volatility and predictability are natural consequences of the pessimistic model that agents choose. Our paper complements the literature on
belief distortions and extrapolative expectations by deriving them as a natural response to model
uncertainty.7
The remainder of the paper is organized as follows. Section 2 discusses the agent’s estimation
method. Section 3 describes the basic structure of the agent’s preferences, and section 4 then
derives the worst-case model. We examine asset prices in general under the preferences in section
5. Section 6 then discusses the calibration and section 7 analyzes the quantitative implications of
the model. Section 8 concludes.

2

Measuring model plausibility

We begin by describing the set of possible models that investors consider and the estimation method
they use to measure the relative plausibility of different models.

2.1

Economic environment

We study a pure endowment economy.
Lochstoer (2013).
6
Bidder and Smith (2015) also develop a model in which the worst-case process of an agent with multiplier preferences also features extra peristence that arises from the interaction of ambiguity aversion and stochastic volatility.
7
See also Barsky and Delong (1993), Abel (2002), Brandt, Zheng, and Zhang (2004), and Hirshleifer and Yu
(2013).

5

Assumption 1. Investors form expectations for future log consumption growth,
models of the form
ct =
"t

+ a (L) ( ct
N 0;

) + "t

1

2

c, using

(1)
(2)

where is mean consumption growth, a (L) is a power series in L, the lag operator, with roots
inside the unit circle, and "t is an innovation.
The change in log consumption on date t, ct , is a function of past consumption growth and
a shock. We restrict our attention to models with purely linear feedback from past to current consumption growth. It seems reasonable to assume that people use linear models for forecasting,
even if consumption dynamics are not truly linear, given that the economics literature focuses almost exclusively on linear models. For our purposes, the restriction to the class of linear processes
is a description of the agent’s modeling method, rather than an assumption about the true process
driving consumption growth. We make the further assumption that "t is i.i.d. normal. While the
assumption of normality is not necessary, it simplifies the exposition; the key assumption is that "t
is serially independent.8
In much of what follows, it will be more convenient to work with the moving average (MA)
representation of the consumption process (1),
ct =

+ b (L) "t

where b (L)

(1
= 1+

La (L))
1
X

bj L j

(3)
1

(4)
(5)

j=1

We can thus express the dynamics of consumption equivalently as depending on a or just on the
power series b (L), with coefficients bj .9 The two different representations are each more convenient than the other in certain settings, so we will refer to both in what follows. They are directly
linked to each other through equation (4), so that a particular choice of a is always associated with
a distinct value of b and vice versa (as long as b is invertible, which we impose).
There are no latent state variables. When a model a (L) has infinite order we assume that
the agent knows all the necessary lagged values of consumption growth for forecasting (or has
8

It is straightforward to solve the model when "t has an arbitrary distribution but remains serially independent.
While time varying volatility in innovations is an important area of analysis (e.g. Drechsler and Yaron (2011)) we
avoid it here for simplicity.
9
In working with finite order regressive or moving average representations, a and b can be regarded as polynomials.
When, as will be necessary below, we deal with potentially infinite order representations, we continue to use the term
polynomial for ease of expression, though in that case b (L) and a (L) are, formally, power series.

6

dogmatic beliefs about them) so that no filtering is required. We discuss necessary constraints on
the models below. For now simply assume that they are sufficiently constrained that any quantities
we derive exist.
We set the notation
fb; ; 2 g to represent the set of parameters that defines a model of
consumption growth. Note that induces a joint Gaussian density over sequences of consumption
growth realizations.

2.2

Model distance

For the purpose of forecasting consumption growth, the agent in our model chooses among specifications for consumption growth, , partly based on their statistical plausibility. As is common
in the literature, the plausibility of a model, or its distance from the agent’s point estimate, is measured by the Kullback–Leibler divergence, or relative entropy, which is the expected value of a log
likelihood ratio statistic comparing a pair of models.10
We denote the divergence between two models by g ; , where
is the benchmark or
point estimate, and is some alternative model. Suppose the agent has a point estimate and she
compares it to an alternative model based on their relative log likelihoods. Then as the number
of observations grows to infinity, g ;
is the limit of the expectation of that likelihood ratio
statistic if the data is generated by the model . The likelihood ratio is a natural choice to measure
the difference between a pair of models because the Neyman–Pearson lemma shows that such a
test is the most powerful way to discriminate between a pair of models.
Any sample of observed consumption growth of length T from the model (3) has a multivariate
normal distribution with mean and a covariance matrix determined by fb; g, denoted ;T . The
log likelihood for a sample of length T is then
LLT ( ) =

1
log j
2

;T j

1
( c1;:::;T
2

)0

1
;T

( c1;:::;T

)

(6)

where c1;:::;T denotes a column vector containing the sample of observed consumption growth
between dates 1 and T .
Our analysis of the model takes place in the frequency domain because it will allow us to obtain
a tractable and interpretable solution. The analysis centers on the transfer function,
b ei!

B (!)
for i

p

(7)

1. The transfer function measures how the filter b (L) transfers power at each frequency,

10

See Hansen and Sargent (2001), Maccheroni, Marinacci, and Rusticini (2006), Sbuelz and Trojani (2008), Strzalecki (2011), Drechsler (2013), among others.

7

!, from the white-noise innovations, ", to consumption growth.
Now suppose consumption growth is generated by the model . One may show that as T ! 1,
the expected difference between the log likelihoods for the models and converges to
lim T

T !1

1

E

LLT

LLT ( )

=

1
2

2
2

Z

B (!)

B (!)

B (!)

1 (
1
)2
+
2
2 2 B (0)
2

2

2

d!
2

log

2

2

2
2

(8)

(where j j denotes the norm of a complex number, the notation E indicates an unconditional
expectation taken over the probability measure induced by , and the integral sign with no limits
R
denotes 21
). This result is a simple application of Whittle’s (1953) limiting formula for the log
likelihood (see, e.g., Dahlhaus (1996)).
The relative entropy between and depends on three terms. The first measures the difference
between the dynamics (in terms of autocorrelations) implied by the two models. A simple way to
2
interpret it is that B (!) measures the uncertainty about the size of fluctuations at frequency !,
2
so the entropy distance penalizes squared deviations between the models, B (!) B (!) , less
strongly at frequencies at which there is more uncertainty. This is the major term that will drive
our results – it determines how the agent quantifies deviations of dynamics from the benchmark.
The second term incorporates the differences in the means of the distributions under the two
models. We obtain the typical result that the uncertainty about the mean depends on the spectrum at
frequency zero.11 In other words, deviations in the mean of consumption growth between the two
models are penalized with a similar scaling to deviations in the dynamics, but they are essentially
infinitely low frequency differences – i.e. infinitely long lived shocks to consumption growth – so
2
they are scaled by 2 B (0) .
Finally, there is a contribution from the deviation of the innovation variance, 2 , from the
benchmark, which we find to have quantitatively minimal effects in our calibration.
As a divergence measure, (8) is precisely the limit of the Kullback–Leibler (KL) divergence
between the models and – the relative entropy between the joint distributions for consumption
growth implied by and as the sample length grows to infinity. We therefore define the agent’s
measure of model plausibility in the following assumption:
Assumption 2. Given a point estimate , investors measure the statistical plausibility of an
2

2
f (!)
jB (!)j is the spectral density (or spectrum) of consumption growth under the model . The
spectral density decomposes the total variance of a time series into components coming from fluctuations at different
frequencies.
11

8

alternative model

through the divergence measure,
g

;

1
2

2
2

Z

B (!)

B (!)

B (!)

1 (
)2
+
2 2 B (0) 2

1
2

2

2

d!
2

log

2

2

2
2

(9)

While the KL divergence is widely studied and has a prominent place in the ambiguity literature, one could ask how an investor, in reality, might adopt such a measure of statistical discrimination. Is it reasonable to think that an investor armed with a simple estimation toolkit would behave
as if she were using this distance measure?
As noted above, it is the first term in the definition of g ;
that drives our results. That
component also arises if the investor uses the nonparametric estimation methods for AR and MA
models described by Berk (1984) and Brockwell and Davis (1988). Specifically, one may model
agents as estimating AR or MA models whose lag orders grow with the sample size. They thus
have nonparametric confidence intervals. We show in the online appendix that if they follow such
an estimation process, then as the sample size grows, a Wald test of the difference in the MA
R 2 jB(!) B(!)j2
d!. So
coefficients between the point estimate and any alternative approaches 21
2
2
jB(!)j
the key part of the KL divergence that addresses dynamics may also be derived from an explicit
nonparametric estimation method.12

3

Preferences

Given a particular model of consumption dynamics, the agent has Epstein–Zin (1991) preferences.
We augment those preferences with a desire for a robustness against alternative models. The
desire for robustness induces the agent to form expectations, and hence calculate utility and asset
prices, under a pessimistic but plausible model, where plausibility is quantified using the estimation
approach described above.

3.1

Utility given a model

Assumption 3. Given a forecasting model
fb; ; 2 g, the investor’s utility is described by
Epstein–Zin (1991) preferences. The coefficient of relative risk aversion is , the time discount
parameter is , and the elasticity of intertemporal substitution (EIS) is equal to 1. Lifetime utility,
12

Intuitively, this result is related to the fact that Wald and likelihood ratio tests are asymptotically equivalent. We
thank Lars Hansen for pointing out the connection between the method based on the Wald test – which is how we
originally derived our results – and the KL divergence.

9

v, for a fixed model
v

ct ;

, is
= (1

) ct +

= ct +

1
X
k=1

k

1

log Et exp v

Et [ ct+k j ] +

ct+1 ;

(1

2

b ( )2

1
1

2

) j

(10)
(11)

where Et [ j ] denotes the expectation operator conditional on the history of consumption growth
up to date t, ct , assuming that consumption is driven by the model .
1
2
b ( )2 is an adjustment to utility for risk. The investor’s utility is lower when risk
1
2
aversion or the riskiness of the endowment is higher. The relevant measure of the risk of the
endowment is 2 b ( )2 , which measures the variance of the shocks to lifetime utility in each period.
b ( ) measures the total discounted effect of a unit innovation to "t+1 on consumption growth, and
hence utility, in the future. It is the term involving b ( ) that causes people with Epstein–Zin
preferences to be averse to long-run risk.13 b ( ) can be written in terms of the transfer function as
b( ) =
where Z (!)

Z

Z (!) B (!) d!

1
X

j i!j

e

(12)
(13)

j=0

and denotes a complex conjugate. The parameter enters through the discounted expectation of
.
future consumption growth, and hence appears as 1

3.2

Robustness over dynamics

Equation (11) gives lifetime utility conditional on consumption dynamics. We now discuss the
investor’s consideration of alternative models of dynamics.
The investor entertains a set of possible values for
and can associate with any model a
measure of its plausibility, g (from assumption 2). Seeking robustness, the investor makes decisions
that are optimal in an unfavorable world – specifically, as though consumption growth is driven
by worst-case dynamics, denoted w
fbw ; 2w ; w g. These dynamics are not the worst in an
unrestricted sense but, rather, are the worst among statistically plausible models. So the investor
does not fear completely arbitrary models – she focuses on models that are not too far from her
point estimate in terms of KL distance (expected log likelihood).
13

We focus on the case of a unit EIS to ensure that we can derive analytic results. The precise behavior of interest
rates is not our primary concern, so a unit EIS is not particularly restrictive. The unit EIS also allows us to retain the
result that Epstein–Zin preferences are observationally equivalent to a robust control model, as in Barillas, Hansen,
and Sargent (2009), which will be helpful in our calibration below

10

Assumption 4. Investors use a worst-case model to form expectations – for both calculating
utility and pricing assets – that is obtained as the solution to a penalized minimization problem:
w

= arg min E v

ct ; b;

2

;

j

+ g

;

(14)

fbw ; 2w ; w g is the model that gives the agent the lowest unconditional expected lifetime utility,
subject to the penalty g.14 is a parameter that determines how much weight the penalty receives.
As usual, can either be interpreted directly as a parameter or as a Lagrange multiplier on a
constraint on the KL divergence g. The KL divergence can be large for three reasons: deviations
in the dynamics, b; deviations in the mean, ; and deviations in the conditional variance, 2 .
The agent’s assessment of plausibility is based on our statistical measure of distance and controlled by . We are modeling the agent’s beliefs about potential models by assuming that she
compares possible models to a point estimate . The role of g in our analysis is similar to that
of the KL divergence used in the robust control model of Hansen and Sargent (2001), in that it
imposes discipline on what models the investor considers.
There are three important differences between our analysis and the multiplier preferences of
Hansen and Sargent (2001). First, in Hansen and Sargent’s (2001) model, the deviation between
the worst-case model and the benchmark is purely in the distribution of innovations, "t+1 . Here
we explicitly focus on uncertainty about dynamics. As we showed above, shifts in the mean of the
distribution of "t+1 (as obtained in Barillas, Hansen, and Sargent (2009)) represent deviations in the
model only at frequency zero. Our nonparametric analysis allows for deviations at all frequencies.
The allowance of models with alternative dynamics is central to our results – it is what generates
predictability in returns and excess volatility in asset prices.
Second, Hansen and Sargent (2001) model agents as having log utility over fixed models, while
we start from the assumption that agents have Epstein–Zin (1991) preferences over fixed models.
Because Epstein–Zin preferences imply that the timing of resolution of uncertainty affects utility,
our agents view more persistent processes as less favorable (for > 1).
Finally, under multiplier preferences, the state variables in the worst-case model are the same
as the state variables in the benchmark model. So, for example, if consumption growth is an
AR(1) under the benchmark model, then lagged consumption growth is also the only state variable
under the worst-case model. In our setting, though, the state variables in the worst-case model
are typically different from those under the benchmark (and in general in fact include the entire
history of consumption growth). Our setting thus allows agents to consider potentially infinitely
richer economic dynamics.
14

Since consumption can be non-stationary, this expectation does not always exist. In that case, we simply rescale
lifetime utility by the level of consumption yielding E [v ( ct ; b) ct j ], which does exist. Scaling by consumption
is a normalization that has no effect other than to ensure the expectation exists.

11

A natural question is why we analyze a worst case instead of allowing the agent to average as a
Bayesian across all the possible models. Our answer is that people may not actually be Bayesians,
or they may not be able to assign priors to all models. Machina and Siniscalchi (2014) discuss the
extensive experimental evidence that people make choices consistent with ambiguity aversion.
Ambiguity aversion is particularly compelling in our context because, as we will see, it is ultimately dynamics at the very lowest frequencies that drive our results. And direct estimation of,
say, 50- or 100-year autocorrelations is, for practical purposes with realistic data sources, impossible. So investors face a situation where they simply do not have data that directly measures all
features of consumption dynamics. They do not face a standard estimation problem – rather, they
must make decisions in the face of model uncertainty that cannot be resolved by the data at hand,
a problem akin to that discussed by Knight (1921) and Ellsberg (1961). Moreover, as Hansen and
Sargent (2007) note, almost any model is necessarily just an approximation, even a high-order one.
If the true model has infinite order, then it can never be fully characterized in any finite sample.
Finally, (again, following Hansen and Sargent (2007)), it is well understood that when the
parameter space is infinite, likelihood-based methods are difficult to implement at best, and often
impossible. Specifically, frequentist methods with an infinite-dimensional parameter space lead to
degenerate estimates (in the context of spectral estimation, for example, see Chow and Grenander
(1985)), while constructing priors over such a space that lead to accurate posterior confidence
intervals is difficult or impossible (Sims (1971) and Diaconis and Freedman (1986)). So when
an investor does not know the true order of the model driving the endowment process, she may
literally not be able to assign likelihoods to different specifications. Instead, she uses g ;
to measure the “plausibility” of potential models, even though she has no probabilities on models
over which to integrate.
Ultimately, our ambiguity-averse investor’s utility takes the form of that of an Epstein–Zin
agent but using w to form expectations about future consumption growth,15
v

w

t

c

=v

t

c;

w

= ct +

1
1

2

2 w
wb

2

( ) +

1
X
k=1

k

Et [ ct+k j

w

]

(15)

In modeling investors as choosing a single worst-case w , we obtain a setup similar to Gilboa
and Schmeidler (1989), Maccheroni, Marinacci, and Rustichini (2006), and Epstein and Schneider
(2007) in the limited sense that we are essentially constructing a set of models and minimizing
over that set. Our worst-case model is, however, chosen once and for all and is not state- or choicedependent. The choice of w is timeless – it is invariant to the time-series evolution of consumption
15

Note that since utility is recursive, the agent’s preferences are time-consistent, but under a pessimistic probability
measure. Furthermore, the assumption that bw is chosen unconditionally means that bw is essentially unaffected by
the length of a time period, so the finding in Skiadas (2013) that certain types of ambiguity aversion become irrelevant
in continuous time does not apply here.

12

– so what it represents is an unconditional worst-case model: if an agent had to choose a worst-case
model to experience prior to being born into the world, it would be w . The worst-case analysis
is certainly not the only way to model behavior under ambiguity, but it is plausible and generates
economically interpretable results.
Unlike in some recent related papers, the investor in this model does not change her probability
weights every day or adjust the worst case according to a learning process.16 She chooses a single
pessimistic model to protect against. An added benefit of the assumption that the worst-case model
is chosen timelessly is that it ensures time-consistency. The agent sets the model used for forming
expectations once and for all, and then uses a recursive utility specification conditional on that
single probability measure. In other words, since the model is chosen a single time, our agent’s
preferences inherit the time-consistency of Epstein–Zin preferences.

4

The worst-case scenario

Our analysis above leads us to a simple quadratic optimization problem. The solution is summarized in the following proposition.
Proposition 1 Under assumptions 1–4, for an agent who chooses a model w fbw ; w ; 2w g to
minimize the unconditional expectation of lifetime utility subject to the loss function g ; , that
is,
1
2
w
b ( )2 +
+ g ;
(16)
= arg min
1
2
1
the worst-case model is determined by the set of equations
Dynamics:

2
w

+
Mean:

w

jB w (!)j2 = f (!)
1

=

(1 + ) (
1

1

1)

(17)
2 w
wb

f (0)

( )2 f (!) jZ (!)j2

(18)
(19)

P1 j i!j 2 w
where B (!) = b (ei! ), Z (!)
e , w jB (!)j2 is the spectrum under the worst-case
j=0
model, and f is the benchmark spectrum. The time-domain model bw (L) has coefficients bw
j that
2
w
are obtained as the Wold representation associated with jB (!)j (see Priestley (1981) section
10.1.1).
We thus have a closed form expression for the worst-case model. The spectral density under the
worst case, f w (!) = 2w jB w (!)j2 , in (17) is equal to the point estimate plus a term that depends
16

See, for example, Hansen and Sargent (2010), Ju and Miao (2012), and Collin-Dufresne, Johannes, and Lochstoer
(2015).

13

Figure 1 about here.
on three factors. First, 1 (1 + ) (
1) w bw ( ) represents the ratio of the utility losses from
a marginal increase in w bw ( ) to the cost of deviations from the point estimate, . When risk
aversion, , is higher or the cost of deviating from the point estimate, , is lower, the worst-case
model is farther from the point estimate.
Second, f (!) represents the amount of uncertainty the agent has about consumption dynamics
at frequency !. Where the spectral density, f (!), is high, there is relatively more uncertainty and
the worst-case model is farther from the point estimate.
Finally, jZ (!)j2 determines how much weight the lifetime utility function places on frequency
!. The top panel of figure 1 plots jZ (!)j2 for = 0:99 , a standard annual calibration. It is strikingly peaked near frequency zero; in fact, the x-axis does not even show frequencies corresponding
to cycles lasting less than 10 years because they carry essentially zero weight. Since the mass of
jZ (!)j2 lies very close to frequency 0, the worst case shifts power to very low frequencies. In that
sense, the worst-case model always includes long-run risk.
The mean of consumption growth also differs from the benchmark in a natural way: when
people are willing to consider more extreme models, they are more risk averse, or they have more
uncertainty about the mean (through f (0)), w is farther below .
When = 1, so that the investor’s preferences reduce to time-separable log utility, the shift in
the dynamics is set to zero: f w (!) = f (!). That is because under time separable preferences, the
dynamics of consumption do not affect average utility. But the mean growth rate obviously still
matters, so even for = 1, w < .
Finally, people in principle also consider deviations of 2w from the point estimate 2 . We find
below that this effect is extremely small in a typical calibration.
An important feature of the results is that the worst-case model depends on preferences. In
particular, when is higher – the investor is more patient – the worst-case model is more persistent.
At the same time, when the investor is more risk averse – is higher – the amount of long-run risk
in the worst-case model is higher. The model thus has implications for disagreement across agents
that is tightly linked to preferences or investment horizons.
Proposition 1 represents the completion of the solution to the model. To summarize, given a
point estimate, (estimated from a finite-order model that we need not specify here), the agent
selects a worst-case model w . She then uses the worst-case model when calculating expectations
and pricing assets.

14

4.1

Long-run risk is the worst-case scenario

Corollary 2 Suppose the agent’s point estimate is that consumption growth is white noise, with
b (L) = 1. The worst-case model is then an ARMA(1,1), and consumption growth has the following
representation under the worst-case dynamics,
ct = (1
"t

)

N 0;

w

+

ct

1

+ "t

"t

1

2
w

(20)
(21)

The above ARMA(1,1) process also has an equivalent state-space representation,
ct =

w

+ xt

xt =

xt

1

1

+

(22)

t

+ vt

(23)

where
t

'

N 0;
1

2

and

(1 + ) (

t

N 0;

2

'

1) bw ( )2

(24)
2
w

(25)

The state-space form in equations (22–23) is observationally equivalent to the process (20–21) in
the sense that they have identical autocovariances for consumption growth, and (22–23) is exactly
case I from Bansal and Yaron (2004), the homoskedastic long-run risk model.
The worst-case process exhibits a small but highly persistent trend component, and the persistence is exactly equal to the time discount factor. Intuitively, since j determines how much
weight in lifetime utility is placed on consumption j periods in the future, a shock that decays with
spreads its effects as evenly as possible across future dates, scaled by their weight in utility. And
spreading out the effects of the shock over time minimizes its detectability. The worst-case/longrun risk model is thus the departure from pure white noise that generates the largest increase in risk
prices (and decrease in lifetime utility) for a given level of statistical distinguishability.17
There is a difference between the worst-case model derived here and the long-run risk model,
which is that in this setting, the long-run trend, xt , is unobservable. Asset prices therefore carry
no ability to forecast future consumption growth beyond what is available in the history of lagged
consumption. However, one may show that the volatility of the pricing kernel, and hence the price
17
This exact balance results from the quadratic nature of the minimization problem. All the lag coefficients bj
contribute to the KL divergence symmetrically, but their effects on utility decline with j , which yields our result.
2
2
Specifically, the contribution of dynamics to lifetime utility is proportional to bw ( ) , and dbdj b ( ) / j . Similarly
P1
dg ( ; )
the contribution to the KL divergence from the fbj g is j=1 b2j , so dbj / bj . So we end up with the result that

bj /

j

.

15

of risk, is actually higher under the process (20–21) than under (22–23) (due to the later arrival of
information). The key intuition behind the long-run risk model is that a small persistent component
in consumption growth can induce large fluctuations in the pricing kernel, and that force (or fear
of it) is also present in our model.
The composite parameter ' determines the degree to which the worst-case dynamics differ
from the point estimate and also the volatility of the trend shock . When the worst-case model
differs from the benchmark by a greater degree, the worst-case model displays more volatile trendtype shocks, t , and the white-noise shocks, t , become relatively smaller.
The bottom panel of figure 1 plots the spectrum for the white-noise benchmark and the worstcase model. The spectrum for white noise is totally flat, while the worst case has substantial power
at the very lowest frequencies, exactly as we would expect from the top panel of figure 1.
Hansen and Sargent (2010) also study a setting in which investors price assets as though the
long-run risk model might be driving the consumption process. The key difference between their
analysis and ours, though, is that they start from the assumption that agents put a non-zero probability on possibility that the long-run risk model is actually true. We, on the other hand, start from
a simple white-noise benchmark and obtain the long-run risk model entirely endogenously.

5

The behavior of asset prices

The investor’s Euler equation is calculated under the worst-case dynamics. For any available return
Rt+1 ,
1 = Et [Rt+1 Mt+1 j
where Mt+1

exp (

ct+1 )

w

]

(26)
t+1

exp (v ( c ;
Et [exp (v ( ct+1 ;

w

)

w)

(1
(1

))
)) j

w]

(27)

Mt+1 is the stochastic discount factor (SDF). The SDF is identical to what is obtained under
Epstein–Zin preferences, except that here expectations are calculated under w . The key implication of that change is that expected shocks to v ( ct+1 ; w ) have a larger standard deviation
since the worst-case model features highly persistent shocks that affect lifetime utility much more
strongly than the less persistent point estimate.

5.1

Consumption and dividend claims

It is straightforward, given that log consumption follows a linear Gaussian process, to derive approximate expressions for prices and returns on levered consumption claims. We consider an asset
whose dividend is Ct in every period, where represents leverage. Denote the return on that asset
16

on date t + 1 as rt+1 and the real risk-free rate as rf;t+1 . We will often refer to the levered consumption claim as an equity claim, and we view it as a simple way to model equity returns (Abel
(1999)).18
From the perspective of an econometrician who has the same point estimate for consumption
dynamics as the investor, , the expected excess log return on the levered consumption claim is

Et rt+1

rf;t+1 j

1
+ vart (rt+1 ) =
2

1
cov w (rt+1 ; mt+1 ) + (vart (rt+1 ) vartw (rt+1 ))
2
"
#
w
w
(1 aw (1)) (
)+
a ( )
+
(28)
1
aw ( ) (a (L) aw (L)) ( ct
)

where is a linearization parameter from the Campbell–Shiller approximation that depends on the
steady-state price/dividend ratio. The addition of the variance is a correction to make the left-hand
side approximately the arithmetic mean return.
The first term, 21 covtw (rt+1 ; log Mt+1 ) (i.e. the conditional covariance of log returns with
the log SDF measured under the worst-case dynamics), is the standard risk premium, and it is
calculated under the worst-case model. The primary way that the model increases risk premia
compared to standard Epstein–Zin preferences is that the covariance of the return with the SDF is
more negative. That covariance, in turn, is more negative for two reasons. First, since the agent
behaves as though shocks to consumption growth are highly persistent, they have large effects on
lifetime utility, thus making the SDF very volatile. Second, again because of the persistence of
consumption growth under the worst case, shocks to consumption have large effects on expected
long-run dividend growth, so the return on the levered consumption claim is also very sensitive to
shocks.19 These two effects cause the consumption claim to strongly negatively covary with the
SDF and generate a high risk premium.
The second term is a quantitatively trivial (under our benchmark calibration) adjustment to the
arithmetic mean due to the difference between the variance of returns under the benchmark and
worst-case models.
The second line is the part of the expected return that comes from differences in forecasts
of consumption under the models of the econometrician and investors. As long as > aw ( )
w
(which is always satisfied if > 1), the coefficients on (
) and (a (L) aw (L)) ( ct
)
18

The model of dividends here is very simplistic and does not match the dynamics of dividend growth particularly
well. The online appendix examines a more realistic model of dividends that matches the standard deviation of
dividend growth and the correlation between dividend and consumption growth in a setting in which dividends and
consumption are cointegrated. The behavior of equity returns is almost identical in that setting to what we obtain with
the simple model Dt = Ct .
w
19
We have cov w (rt+1 ; mt+1 ) = ( 1 + (1
) bw ( )) 1 aaw (( )) 2w . The volatility of the SDF is determined by
( 1 + (1

) bw ( ))

w,

while the volatility of the return is

17

1

aw ( )
aw ( )

w.

are positive. The former term implies that average equity returns are higher when investors are
more pessimistic about mean consumption (and hence dividend) growth, w .
The term involving ( ct
) is zero on average, but it induces predictability in returns. When
the worst case implies higher future consumption growth, investors pay relatively more for equity compared to riskless assets, thus lowering expected excess returns. This channel leads to
procyclical asset prices and countercyclical expected returns when aw (L) implies more persistent
dynamics than a (L), similarly to Fuster, Hebert, and Laibson (2011).
The procyclicality of asset prices also depends on leverage, . Expected returns for assets more
risky than consumption ( > 1) are countercyclical, and we model equity as having > 1. But
for assets safer than consumption, prices will in fact be countercyclical, because variation in the
risk-free rate dominates variation in the risk premium for them.
We also note that since risk aversion and conditional variances are constant, the excess return
on a levered consumption claim has a constant conditional expectation from the perspective of investors. That is, while returns are predictable from the perspective of an econometrician, investors
believe that they are unpredictable. So if investors in this model are surveyed about their expectations of excess returns, their expectations will not vary, even if econometric evidence implies
that returns are predictable. This model therefore generates extrapolative expectations of the type
discussed by Greenwood and Shleifer (2014) (and many references therein), but that extrapolation
applies to consumption growth rather than equity returns.
Finally, it is worth noting that since asset prices depend purely on the history of consumption
growth, there is no way to improve to the investor’s estimates of consumption dynamics by including information on asset prices. The price of a consumption claim is completely redundant given
data on the history of consumption.

5.2

Interest rates

The risk-free rate follows
rf;t+1 =

log

+

w

+ aw (L) ( ct

w

)

1
2

2
w

+ (1

) bw ( )

2
w

(29)

With a unit EIS, interest rates move one for one with expected consumption growth. In the present
w
model, the relevant measure of expected consumption growth is w + aw (L) ( ct
), which is
the expectation under the worst-case model.
The online appendix derives analytic expressions for the prices of long-term zero-coupon
bonds, which we discuss in our calibration below.

18

6

Calibration

We now parameterize the model to analyze its quantitative implications. Most of our analysis is
under the assumption that the agent’s point estimate implies that consumption growth is white
noise and that the point estimate is also the true dynamic model. Despite this parsimony, we obtain
striking empirical success in terms of matching important asset pricing moments.
Many of the required parameters are standard. We use a quarterly calibration of = 0:991=4 ,
implying a pure rate of time preference of 1 percent per year. Setting smaller would imply that
the worst-case model is less persistent, but it also implies that interest rates are higher. We will see
below that the equilibrium real interest rate in the model ends up being still too large relative to the
data, which implies that should not be lower.
The steady-state dividend/price ratio used in the Campbell–Shiller approximation is 5 percent
per year, as in Campbell and Vuolteenaho (2004), so = 0:951=4 .20 Other parameters are calibrated to match moments reported in Bansal and Yaron (2004). The agent’s point estimate is that
consumption growth is i.i.d. with a quarterly standard deviation of 1.47 percent, which we also
assume is the true data-generating process. Finally, the leverage parameter for the consumption
claim, ; is set to 4.806 to generate mean annualized equity returns of 6.33 percent.
We calibrate to equal 106.8 to match the observed Sharpe ratio on equities. The calibration
of is slightly more difficult. Its effects on asset prices are highly similar to those of , so it is not
straightforward to calibrate both and from aggregate data.
It is possible to draw a link between and if we interpret as in the literature on multiplier
preferences. Hansen and Sargent (2005) and Barillas, Hansen, and Sargent (2009) show that the
coefficient of relative risk aversion in Epstein–Zin preferences – our – can be interpreted as a
measure of the agent’s willingness to consider alternative distributions of the innovations ", where
the measure of distance between distributions is again the KL divergence. That is, our combination of model uncertainty over with Epstein–Zin preferences can alternatively be viewed as
a combination of two layers of model uncertainty – over and the distribution of " separately.
The appendix formalizes this argument and shows that if agents use the same penalty on the KL
divergence over the distribution of " and also the model , then is linked to through the formula
=1+

1
(1

)

(30)

yielding = 4:73.
The link between and in equation (30) is useful because it eliminates a degree of freedom
from our calibration. In order to justify it, though, one must accept the assumptions of multiplier
20
Setting this parameter to a higher value (e.g. 0.9751=4 as in Bansal and Yaron (2004)) increases the mean and
standard deviation of equity returns but has no effect on the Sharpe ratio.

19

preferences, which include that agents have unit risk aversion over unambiguous risks. So while
(30) is helpful, it is also only a very special case. Alternatively, can simply be taken as another
free preference parameter, rather than as a measure of robustness (in the multiplier preference interpretation). So it is important that our calibration of be objectively reasonable as a measure of risk
aversion. Risk aversion of 4.73 is extraordinarily small in the context of the consumption-based
asset pricing literature with Gaussian innovations. It well within the upper bound of 10 proposed
by Mehra and Prescott (1985), and almost precisely equal to average risk aversion measured by
Barsky et al. (1997).21 therefore takes on a plausible value in its own right, separate from any
connection it may have to .
To further investigate how reasonable is, in the next section we show that it implies a worstcase model that is rarely rejected by statistical tests on data generated by the true model. An
investor with the true model as her point estimate might reasonably believe the worst case could
have actually generated the data that led to that point estimate.

7

Quantitative implications

7.1

The white noise case

We report the values of the parameters in the worst-case consumption process in table 1. As noted
above, the autocorrelation of the predictable part of consumption growth under the worst-case
model is , implying that trend shocks have a half-life of 70 years, as opposed to the three-year
half-life in the original calibration in Bansal and Yaron (2004).22 bw ( ) w , the relevant measure
of the total risk in the economy, is 0.036 at the quarterly frequency in our model, compared to
0.031 in theirs.23 Our model thus has more persistence and more total risk.
Note also that w is only 0.4 percent larger than . So the conditional variance of consumption
growth under the worst-case model is essentially identical to that under the benchmark. However,
because the worst-case model is so persistent, bw ( ) is 2.45 times higher than b ( ), thus implying
that the worst-case economy is far riskier than the point estimate.
21

Average here is the haromic mean – the inverse of the arithmetic mean of risk tolerance. In an update and extension
of Barsky et al.’s (1997) analysis, Kimball, Sahm, and Shapiro (2008) estimate similar values.
Our value for is less than half that used by Bansal and Yaron (2004), who themselves are notable for using a low
value, and it is similar to that of Barro (2006), who uses values of 2 to 4 in a model with rare disasters.
22
The 70-year half-life is sensitive to the choice of . It is equal to log (1=2) = log ( ). If = 0:9751=4 , the half-life
falls to 27 years. Nevertheless, the worst-case is robustly far more persistent than Bansal and Yaron’s (2004) trend –
to obtain a half-life of 3 years, we would need an annual rate of time preference of 21 percent. Online appendix table
A3 reports results under a calibration with = 0:951=4 .
23
Using the notation for the long-run risk model from above, and denoting theqAR(1) coefficient for x as , we

var( )
+ var ( ) which yields
calculate the equivalent of b ( ) in Bansal and Yaron’s (2004) model as b ( ) =
(1
)2
b ( ) = 0:029. Dew-Becker (2015) discusses this calculation and its relation with risk premia in more detail.

20

Table 1 about here.
7.1.1

Unconditional moments

Table 1 reports key asset pricing moments. The first column shows that the model can generate a
high standard deviation for the pricing kernel (and hence a high maximal Sharpe ratio), high and
volatile equity returns, and low and stable real interest rates, as in the data. The equity premium
and its volatility are 6.33 and 19.42 percent respectively, identical to the data. The real risk-free
rate has a mean of 1.89 percent and a standard deviation of 0.31 percent.
The second column in the bottom section of table 1 shows what would happen if we set = 1
but held fixed at 4.73, so that we would be back in the standard Epstein–Zin setting where there
is no uncertainty about dynamics. The equity premium then falls from 6.33 to 1.95 percent, since
the agent exhibits no concern for long-run risk. Furthermore, because the agent no longer behaves
as if consumption growth is persistent, a shock to consumption has far smaller effects on asset
prices. The standard deviation of returns falls from 19.4 to 14.1 percent and the standard deviation
of the price/dividend ratio falls from 19 percent to exactly zero. The agent’s fear of a model with
long-run risk thus raises the mean of returns by a factor of more than 3 and the volatility by a factor
of 1.4.
Recall from equation (28) that the mean equity premium is due both to the investor’s belief
about the covariance of returns with the pricing kernel – which depends on the worst-case dynamics, bw – and also the difference between the true and worst-case mean levels of consumption
w
growth,
. Of the 633 basis point equity premium, 54 basis points come from the mean effect,
while 579 come from the dynamics. So uncertainty about mean consumption growth is relatively
unimportant in driving the equity premium. That result is driven by the fact that the worst-case dynamics have two compounding effects on the equity premium – they make both the pricing kernel
and equity returns more volatile, interacting to generate a large increase in the equity premium.24
Going back to the first column, we see that there are large and persistent movements in the
price/dividend ratio in our model. The one-year autocorrelation of the price/dividend ratio at 0.96
is somewhat higher than the empirical autocorrelation, while the standard deviation is 0.19, similar
to the empirical value of 0.29. These results are particularly notable given that there is no free
parameter that allows us to directly match the behavior of prices.
Volatility in the price/dividend ratio has the same source as the predictability in equity returns
discussed above: the agent prices assets under a model where consumption growth has a persistent
component. So following positive shocks, she is willing to pay relatively more, projecting that
dividends will continue to grow in the future. From the perspective of an econometrician, these
24

Collin-Dufresne, Johannes, and Lochstoer (2015) also find that uncertainty about dynamics is relatively more
important for the equity premium than uncertainty about mean consumption growth.

21

Figure 2 about here.
movements seem to be entirely due to discount-rate effects: dividend growth is entirely unpredictable, since dividends are a multiple of consumption, and consumption follows a random walk.
On the other hand, from the perspective of the investor (or her worst-case model), there is almost
no discount-rate news. Rather, she prices the equity claim differently over time due to variation in
forecasts of future cash flows.
The bottom row of table 1 reports average gap between the yields on real 1- and 10-year zerocoupon bonds. The term structure is very slightly downward-sloping in the model, a feature it
shares with Bansal and Yaron’s (2004) results. The downward slope is consistent with the long
sample of inflation-indexed bonds from the UK reported in Evans (1998). A thorough analysis of
the implications of our model for the term structure of interest rates is beyond the scope of this
paper, but we simply note that the implications of the model for average yields are not wildly at
odds with the data and are consistent with past work.
A final feature of the data that papers often try to match is the finding that interest rates and
consumption growth seem to be only weakly correlated, suggesting that the EIS is very small.
Since consumption growth in this model is unpredictable by construction, standard regressions of
consumption growth on lagged interest rates that are meant to estimate the EIS, such as those in
Hall (1988) and Campbell and Mankiw (1989), will generate EIS estimates of zero on average.
7.1.2

Return predictability

To quantify the degree of predictability in returns, figure 2 plots percentiles of sample R2 s from
regressions of returns on price/dividend ratios in 360-quarter samples (the length of the empirical
sample). The gray line is the set of corresponding values from the data between 1926 and 2015.
We report R2 s for horizons of 1 quarter to 10 years. At both short and long horizons the model
matches the data well. The median R2 from the predictive regressions at the 10-year horizon
is 31 percent, while in the data it is 25 percent. This is in contrast with the complete lack of
predictability (reflecting the i.i.d. nature of the true data generating process) of consumption and
dividend growth.
Beeler and Campbell (2012) argue that cash flows are excessively predictable in Bansal and
Yaron’s (2004) calibration. In our setting, there is precisely zero cash-flow predictability by construction. Our results are thus consistent with evidence from various sources on the excess volatility
in aggregate asset prices (Leroy and Porter (1981), Shiller (1981), Cochrane (2008)).
In fact, our results are consistent with all three of Beeler and Campbell’s (2012) criticisms of the
long-run risk model: consumption and dividends are not predicted with asset prices, asset return
volatility is not predicted by asset prices, and consumption growth is not predicted by interest
22

rates. In other words, we obtain the major intuition and results of the long-run risk model –
that investors fear low-frequency fluctuations in consumption growth and thus demand large risk
premia on stocks – but without relying on significant predictability in consumption, dividends, or
volatility.
Of course, we obtain these results by exploiting the wedge between the true data generating
process and the one that informs investors’ evaluation of risky payoffs. But, as we show below, this
wedge is empirically reasonable, in the sense that the worst-case model is a perfectly reasonable
process for a person to believe generated the data, given, say, a century of data.
7.1.3

Probability of rejecting the worst-case dynamics

For our calibration of to be intuitively reasonable, the worst-case model should be thought plausible by the agent. One way of interpreting this statement is that the worst-case model should fit a
sample of data generated by the true model nearly as well as the true model itself.
We consider two tests of the fit of the worst-case model to the true white-noise consumption
process: Ljung and Box’s (1978) portmanteau test and the likelihood-based test of an ARMA(1,1)
suggested by Andrews and Ploberger (1996).25 The likelihood-based test is in fact a correctly
specified likelihood-ratio test and thus should be asymptotically most powerful. To test that the
worst-case model is the correct specification, we take a simulated sample of consumption growth,
ct , and construct artificial residuals,
"t

w

( ct

w

aw (L) ( ct

1

w )) ( w )

1

(31)

w

Under the null that the worst-case model is the correct specification, "t should be white noise.
The Ljung–Box and Andrews–Ploberger tests both ask whether that null can be rejected. Since
w
consumption growth is generated as white noise, "t is in fact not i.i.d.. In a sufficiently large
sample, an investor will be able to reject the hypothesis that consumption was driven by the worstw
case model by observing that "t is serially correlated. We obtain small-sample critical values for
the two test statistics by simulating their distributions under the null.
The top section of table 2 reports the probability that the agent would reject the hypothesis that
consumption growth was driven by the worst-case model after observing a sample of white-noise
consumption growth. We simulate the tests in both 50- and 100-year samples. In all four cases, the
rejection probabilities are only marginally higher than they would be if the null hypothesis were
actually true. The ARMA(1,1) test is the stronger of the two, with rejection rates of 5.5 and 6.3
25

The intuition behind this approach is similar to that underpinning the detection error probability (DEP) calculations of Barillas, Hansen, and Sargent (2009), which are widely used to calibrate robustness models. Although we
do not report them here, the DEPs in our case also indicate that the worst-case and benchmark models are difficult to
distinguish.

23

Table 2 about here.
percent in the 50- and 100-year samples, respectively, while the Ljung–Box test performs slightly
worse, with rates of 4.6 and 5.1 percent. The online appendix reports further results using other
statistical tests and longer samples.26
Table 2 thus shows that the worst-case model, while having economically large differences
from the point estimate in terms of its asset pricing implications, can barely be distinguished from
the point estimate in long samples of consumption growth. From a statistical perspective, it is
entirely plausible that an investor would be concerned that the worst-case model could be what
drives the data. Thus both and (which were calibrated jointly with only a single degree of
freedom) take on independently reasonable values.
Furthermore, the extreme difficulty of distinguishing the benchmark and worst-case models
with long samples of data also suggests that our decision not to model learning explicitly is likely
relatively innocuous. Although allowing for learning is conceptually desirable, any learning about
low frequency properties of the model would still apply to frequencies far shorter than those that
our investor emphasizes under the worst case, as determined by Z (!).
Since risk aversion and the conditional volatility of consumption growth are constant, stock
returns should not be predictable if the worst-case model is true. So another potential way to test
the worst-case model would be to test for return predictability. Since returns are a linear function
of current and past consumption growth, though, they contain no information not contained in
the history of consumption growth. We focus on formal statistical tests, like the likelihood ratio,
because they should use that information efficiently.
Stock prices and returns depend on the low-frequency characteristics of consumption growth,
though – exactly where the worst-case deviates from the benchmark. So we might expect them
to provide a stronger test. As an example, consider simulations of the R2 s for forecasts of fiveyear excess equity returns in 100-year samples. The average R2 when the data is generated by
the white-noise model but agents price under the worst-case is 0.19. The average R2 when the
data is generated by the worst-case model – in which case the equity premium is constant – is
0.13. So our ambiguity aversion substantially increases predictability (to the point that the model
matches the data well). But the difference is small relative to the dispersion in R2 s: in only 14.7
percent of samples does the R2 allow the agent to reject the worst-case at the 95-percent level.27
So even with a test that focuses on the frequencies where the worst-case model deviates from the
benchmark most, with a century of data, the agent would still statistically reject the worst-case
w

26

The under-rejection in small samples comes from the fact that "t is negatively autocorrelated when the data
is driven by the white-noise benchmark. The online appendix explores rejection probabilities for a larger set of test
statistics and with samples up to 1000 years long.
27
That is, the R2 for data generated under the benchmark is above the 95th percentile of the R2 for data generated
by the worst-case in 14.7 percent of the samples.

24

very infrequently.
7.1.4

Alternative calibrations of the pricing model

We derive the worst-case model endogenously, but similar models have also been assumed for investor expectations. Bansal and Yaron (2004) argue that a model with trend shocks with a quarterly
persistence 0.94 fits the data well. Hansen and Sargent (2010) consider a setting where investors
focus on two models for consumption growth, the more persistent of which has a trend component with an autocorrelation of 0.99. Due to ambiguity aversion in their model, asset prices are
primarily driven by the more persistent model.
The bottom section of table 2 examines how rejection probabilities change if we modify the
pricing model to use a less persistent trend. In all rows we hold bw ( ) fixed and simply modify
the persistence of consumption growth under the pricing (null) model. In other words, we ask how
easy different models are to distinguish from white noise, holding constant the relevant measure of
risk and varying the persistence of shocks.
The top row is the calibration from the main analysis, where persistence is equal to the time
discount factor. As the degree of persistence falls, the investor’s ability to reject the pricing model
in a century-long sample rapidly improves. When the persistence is 0.99, as in Hansen and Sargent
(2010), the pricing model is rejected 13.9 percent of the time – twice as often as our endogenous
worst-case model. When the persistence falls to 0.94 as in Bansal and Yaron (2004), the pricing
model is rejected 82.4 percent of the time. The result that the persistence of the worst-case model
should be equal to is clearly key to ensuring that the model is difficult to reject in simulated data.

7.2

Estimated consumption dynamics

We now examine the worst-case scenario associated with an estimated small-scale model of consumption growth. The above analysis assumes that the point estimate for consumption dynamics is
that consumption growth is i.i.d.. We now relax that assumption and examine a more sophisticated
estimation framework.
We study quarterly data on per capita non-durables and services consumption in the United
States. The Bayesian information criterion leads us to the choice of an ARMA(1,1) from a range of
small-scale ARMA models (up to an ARMA(5,5)). A key feature of the estimate is that the spectral
density f (!) varies across frequencies, which allows us to ask whether variation in estimation
uncertainty across frequencies is qualitatively or quantitatively relevant in determining the worstcase model.
We concentrate on the worst-case choice of dynamics and abstract from distortions to the innovation mean and variance. We first consider what worst-case model the agent would derive if she
25

Figure 3 about here.
were constrained to minimize utility with respect to a transfer function implied by an ARMA(1,1).
That is, utility is minimized by choosing a worst-case f ; g in the model
ct =

+ ( ct

1

) + "t

"t

1

(32)

in order to minimize lifetime utility subject to the penalty g b; b . We denote this “parametric”
worst case as bp . The restriction that the worst case only allows changes in the parameters and
is typical in the literature; it assumes that investors know the model driving consumption growth
and they need only estimate its parameters.28 We compare bp (which depends only on f p ; p g) to
the bw obtained following the main analysis above (i.e. allowing an arbitrary bw , instead of just an
ARMA(1,1)).
In figure 3 we plot the real part of the transfer functions implied under the benchmark and the
two worst cases. Except at low frequencies, the parametric worst case is essentially indistinguishable from the point estimate, while the nonparametric worst case differs much more dramatically.
Intuitively, since there are only two free parameters in the restricted parametric problem, it is impossible to generate the deviations very close to frequency zero that have both high utility cost and
low detectability. So, instead of large deviations on a few frequencies, as in the nonparametric
case, the parametric worst case puts very small deviations on a wide range of frequencies.
The specific parameters in the benchmark and parametric worst-case models are ;
=
p
f0:789; 0:587g and f p ; g = f0:803; 0:600g respectively, implying b ( ) = 1:94 and bp ( ) =
2:02. The parametric worst-case model is thus nearly identical to the benchmark model. The
relevant measure of risk in the economy is thus essentially identical under the two models, meaning
that the equity premium is almost completely unaffected by parameter uncertainty of this sort. In
contrast, under the unconstrained nonparametric worst case bw ( ) = 4:40, more than two times
higher than b ( ).
Thus, when we allow the agent to choose an unrestricted worst-case model, the outcome is
very similar to what we obtained for the white-noise benchmark. The worst-case deviates from
the benchmark at very low frequencies. This is true even though in this setting the estimation
uncertainty (through f (!)) varies across frequencies.
28

See Collin-Dufresne, Johannes, and Lochstoer (2013) for a deep analysis of the case of parameter uncertainty
with Epstein–Zin preferences. Andrei, Carlin, and Hasler (2013) study a model in which investors disagree about the
persistence of trend growth rates.

26

8

Conclusion

This paper studies asset pricing when agents are unsure about the endowment process. The fundamental insight is that the long-run risk model, precisely because it is difficult to test for empirically
and yet has important welfare implications, represents a natural model for investors who are unsure of the true data-generating process to use for pricing assets. More technically, for an agent
with Epstein–Zin preferences who estimates consumption dynamics nonparametrically, the model
that leads to the lowest lifetime utility for a given level of plausibility displays large amounts of
long-run risk in consumption growth. In fact, when the agent’s point estimate is that consumption
growth is i.i.d., the worst-case model is literally the homoskedastic long-run risk model of Bansal
and Yaron (2004). Furthermore, the nonparametric worst-case model can differ substantially from
a parametric worst case that only features parameter uncertainty, instead of uncertainty about the
actual model driving consumption growth.
We are able to obtain solutions in a setting that previously resisted both analytic and numerical
analysis. The results show exactly what types of models agents fear when they contemplate unrestricted dynamics: they fear fluctuations at the very lowest frequencies. Not only do these fears
raise risk premia on average, but they also induce countercyclical risk premia, raising the volatility
of asset prices and helping to match the large movements in aggregate price/dividend ratios.
In a calibration of our model where the true process driving consumption growth is white noise,
we generate a realistic equity premium, a volatile and realistically persistent price/dividend ratio,
returns with similar predictability to the data at both short and long horizons, and estimates of
the EIS from aggregate regressions of zero. None of these results require us to posit that there is
long-run risk in the economy. They are all driven by the agent’s worst-case model. And we show
that the worst-case model is not at all implausible: it is rejected at the 5 percent level in less than
10 percent of simulated 100-year samples.
Economists have spent years arguing over what the consumption process is. We argue that a
reasonable strategy, and one that is tractable to solve, for an investor facing that type of uncertainty,
would be to make plans for a worst-case scenario. The message of this paper is that worst-case
planning is able to explain a host of features of the data that were heretofore viewed as puzzling
and difficult to explain in a setting that was even remotely rational.

References
Abel, Andrew B. 1999. “Risk Premia and Term Premia in General Equilibrium.” Journal of Monetary
Economics, 43: 3–33.

27

Abel, Andrew B. 2002. “An exploration of the effects of pessimism and doubt on asset returns.” Journal
of Economic Dynamics and Control, 26(7): 1075–1092.
Andrei, Daniel, Bruce Carlin, and Michael Hasler. 2013. “Model Disagreement, Volatility, and Trading Volume.” Working paper.
Andrews, Donald W.K., and Werner Ploberger. 1996. “Testing for Serial Correlation Against an
ARMA(1,1) Process.” Journal of the American Statistical Association, 91(435): 1331–1342.
Bansal, Ravi, and Amir Yaron. 2004. “Risks for the Long-Run: A Potential Resolution of Asset Pricing
Puzzles.” Journal of Finance, 59(4): 1481–1509.
Bansal, Ravi, Dana Kiku, and Amir Yaron. 2012. “An Empirical Evaluation of the Long-Run Risks
Model for Asset Prices.” Critical Finance Review, 1(1): 183–221.
Barillas, Francisco, Lars P. Hansen, and Thomas J. Sargent. 2009. “Doubts or Variability?” Journal
of Economic Theory, 144(6): 2388–2418.
Barro, Robert J. 2006. “Rare Disasters and Asset Markets in the Twentieth Century.” Quarterly Journal
of Economics, 121(3): 823–866.
Barsky, Robert B., and Bradford De Long. 1993. “Why Does the Stock Market Fluctuate?” Quarterly
Journal of Economics, 108(2): 291–311.
Barsky, Robert B., F. Thomas Juster, Miles S. Kimball, and Matthew D. Shapiro. 1997. “Preference Parameters and Behavioral Heterogeneity: An Experimental Approach in the Health and
Retirement Study.” The Quarterly Journal of Economics, 112(2): 537–579.
Beeler, Jason, and John Y. Campbell. 2012. “The Long-Run Risks Model and Aggregate Asset Prices:
An Empirical Assessment.” Critical Finance Review, 1(1): 141–182.
Berk, Kenneth N. 1974. “Consistent Autoregressive Spectral Estimates.” The Annals of Statistics,
2(3): 489–502.
Bidder, Rhys, and Matthew E. Smith. 2015. “Doubts and Variability: A Robust Perspective on Exotic
Consumption Series.” Working paper.
Brandt, Michael W, Qi Zeng, and Lu Zhang. 2004. “Equilibrium stock return dynamics under alternative rules of learning about hidden states.” Journal of Economic Dynamics and Control,
28(10): 1925–1954.
Brennan, Michael J., and Yihong Xia. 2001. “Stock Price Volatility and the Equity Premium.” Journal
of Monetary Economics, 47: 249–283.
28

Brockwell, P.J., and R.A. Davis. 1988a. “Applications of Innovation Representations in Time Series
Analysis.” In Probability and Statistics: Essays in Honor of Franklin A. Graybill. , ed. J.N.
Srivastava, 61–84. Elsevier.
Brockwell, P.J., and R.A. Davis. 1988b. “Simple consistent estimation of the coefficients of a linear
filter.” Stochastic Processes and their Applications, 28(1): 47–59.
Campbell, John Y., and N. Gregory Mankiw. 1989. “Consumption, Income and Interest Rates: Reinterpreting the Time Series Evidence.” In NBER Macroeconomics Annual. , ed. Olivier Jean Blanchard and Stanley Fischer.
Campbell, John Y., and Tuomo Vuolteenaho. 2004. “Bad Beta, Good Beta.” American Economic
Review, 94(5): 1249–1275.
Cecchetti, Stephen G, Pok-Sang Lam, and Nelson C Mark. 2000. “Asset Pricing with Distorted Beliefs: Are Equity Returns Too Good to Be True?” American Economic Review, 787–805.
Chen, Long, Zhi Da, and Richard Priestley. 2012. “Dividend Smoothing and Predictability.” Management Science, 58(10): 1834–1853.
Chen, Xiaohong. 2007. “Large sample sieve estimation of semi-nonparametric models.” Handbook of
econometrics, 6: 5549–5632.
Chow, Yun-Shyong, and Ulf Grenander. 1985. “A Sieve Method for the Spectral Density.” The Annals
of Statistics, 13(3): 998–1010.
Cochrane, John H. 2008. “The Dog That Did Not Bark: A Defense of Return Predictability.” Review of
Financial Studies, 21(4): 1533–1575.
Cogley, Timothy, and Thomas J. Sargent. 2008. “The market price of risk and the equity premium: A
legacy of the Great Depression?” Journal of Monetary Economics, 55(3): 454–476.
Collin-Dufrese, Pierre, Michael Johannes, and Lars A. Lochstoer. 2013. “Parameter Learning in
General Equilibrium: The Asset Pricing Implications.” Working paper.
Collin-Dufresne, Pierre, Michael Johannes, and Lars Lochstoer. Forthcoming. “Parameter Learning
in General Equilibrium: The Asset Pricing Implications.” American Economic Review.
Dahlhaus, R. 1996. “On the Kullback-Leibler information divergence of locally stationary processes.”
Stochastic Processes and their Applications, 62(1): 139–168.
Dahlhaus, Rainer. 2000. “A Likelihood Approximation for Locally Stationary Processes.” The Annals
of Statistics, 28(6): 1762–1794.
29

Dew-Becker, Ian. 2015. “How risky is consumption in the long-run? Benchmark estimates from a novel
unbiased and efficient estimator.” Working paper.
Diaconis, Persi, and David Freedman. 1986. “On the consistency of Bayes estimates.” The Annals of
Statistics, 14(1): 1–26.
Doob, J.L. 1948. “Application of the Theory of Martingales.” Vol. 13 of Colloques Internationaux du
Centre National de la Recherche Scientifique, 23–27.
Drechsler, Itamar. 2013. “Uncertainty, Time-Varying Fear, and Asset Prices.” The Journal of Finance,
68(5): 1843–1889.
Drechsler, Itamar, and Amir Yaron. 2011. “What’s Vol Got to Do with it?” The Review of Financial
Studies, 24(1): 1–45.
Ellsberg, Daniel. 1961. “Risk, ambiguity, and the Savage axioms.” The Quarterly Journal of Economics,
75(4): 643–669.
Epstein, Larry, and Stan Zin. 1991. “Substitution, Risk Aversion, and the Temporal Behavior of
Consumption and Asset Returns: An Empirical Investigation.” Journal of Political Economy,
99: 555–576.
Epstein, Larry G., and Martin Schneider. 2003. “Recursive Multiple Priors.” Journal of Economic
Theory, 113(1): 1–31.
Epstein, Larry G., and Martin Schneider. 2007. “Learning Under Ambiguity.” Review of Economic
Studies, 74(4): 1275–1303.
Evans, Martin. 1998. “Real Rates, Expected Inflation and Inflation Risk Premia.” Journal of Finance,
53(1): 187–218.
Faust, Jon. 1999. “Conventional Confidence Intervals for Points on Spectrum Have Confidence Level
Zero.” Econometrica, 67(3): 629–637.
Fuster, Andreas, Benjamin Hebert, and David Laibson. 2011. “Natural Expectations, Macroeconomic
Dynamics, and Asset Pricing.” NBER Macroeconomics Annual, 26(1): 1–48.
Gilboa, Itzhak, and David Schmeidler. 1989. “Maxmin expected utility with non-unique prior.” Journal
of Mathematical Economics, 18(2): 141–153.
Gray, Robert M. 2000. “Toeplitz and Circulant Matrices: A Review.” Foundations and Trends in Communications and Information Theory, 2(3): 155–239.
30

Greenwood, Robin, and Andrei Shleifer. 2014. “Expectations of Returns and Expected Returns.” Review of Financial Studies, 27(3): 714–746.
Grenader, Ulf, and Gabor Szego. 1958. Toeplitz Forms and their Applications. University of California
Press.
Hall, Robert E. 1988. “Intertemporal Substitution in Consumption.” Journal of Political Economy,
96(2): 339–357.
Hansen, Lars P., and Thomas J. Sargent. 2001. “Robust Control and Model Uncertainty.” American
Economic Review, 91(2): 60–66.
Hansen, Lars P., and Thomas J. Sargent. 2005. “Robust estimation and control under commitment.”
Journal of Economic Theory, 124(2): 258–301.
Hansen, Lars P., and Thomas J. Sargent. 2007. Robustness. Princeton University Press.
Hansen, Lars P., and Thomas J. Sargent. 2010a. “Fragile Beliefs and the Price of Uncertainty.” Quantitative Economics, 1(1): 129–162.
Hansen, Lars Peter, and Thomas J. Sargent. 2010b. “Fragile beliefs and the price of uncertainty.”
Quantitative Economics, 1(1): 129–162.
Hirshleifer, David, and Jianfeng Yu. 2012. “Asset Pricing in Production Economies with Extrapolative
Expectations.” Working Paper.
Ju, Nengjiu, and Jianjun Miao. 2012. “Ambiguity, Learning, and Asset Returns.” Econometrica,
80(2): 559–591.
Kimball, Miles S., Claudia R. Sahm, and Matthew D. Shapiro. 2008. “Imputing risk tolerance from
survey responses.” Journal of the American Statistical Association, 103(483): 1028–1038.
Klibanoff, Peter, Massimo Marinacci, and Sujoy Mukerji. 2005. “A Smooth Model of Decision Making under Ambiguity.” Econometrica, 73(6): 1849–1892.
Knight, Frank H. 1921. Risk, Uncertainty, and Profit. Houghton Mifflin.
Kreps, David M., and Evan L. Porteus. 1978. “Temporal Resolution of Uncertainty and Dynamic
Choice Theory.” Econometrica, 46(1): 185–200.
Leippold, Markus, Fabio Trojani, and Paolo Vanini. 2008. “Learning and Asset Prices Under Ambiguous Information.” Review of Financial Studies, 21(6): 2656–2597.

31

LeRoy, Stephen F., and Richard D. Porter. 1981. “The Present-Value Relation: Tests Based on Implied
Variance Bounds.” Econometrica, 49(3): 555–574.
Liu, Jun, Jun Pan, and Tan Wang. 2004. “An Equilibrium Model of Rare-Event Premia and Its Implication for Option Smirks.” Review of Financial Studies, 18(1): 131–164.
Ljung, G. M., and G. E. P. Box. 1978. “On a Measure of Lack of Fit in Time Series Models.” Biometrika,
65(2): pp. 297–303.
Maccheroni, Fabio, Massimo Marinacci, and Aldo Rustichini. 2006. “Ambiguity Aversion, Robustness, and the Variational Representation of Preferences.” Econometrica, 74(6): 1447–1498.
Machina, Mark J., and Marciano Siniscalchi. 2014. “Ambiguity and Ambiguity Aversion.” In Handbook of the Economics of Risk and Uncertainty. 729–807. Elsevier.
Maenhout, Pascal J. 2004. “Robust Portfolio Rules and Asset Pricing.” Review of Financial Studies,
17(4): 951–983.
Marakani, Srikant. 2009. “Long run consumption risks: Are they there?” Working paper.
Marsh, Terry A., and Robert C. Merton. 1987. “Dividend Behavior for the Aggregate Stock Market.”
The Journal of Business, 60(1): 1–40.
Mehra, Rajnish, and Edward C. Prescott. 1985. “The Equity Premium: A puzzle.” Journal of Monetary Economics, 15(2): 145–161.
Monti, Anna Clara. 1997. “Empirical Likelihood Confidence Regions in Time Series Models.” Biometrika, 84(2): 395–405.
Müller, Ulrich K., and Mark W. Watson. 2013. “Measuring Uncertainty about Long-Run Predictions.”
Working paper.
Routledge, Bryan R., and Stanley E. Zin. 2009. “Model Uncertainty and Liquidity.” Review of Economic Dynamics, 12(4): 543–566.
Sbuelz, Alessandro, and Fabio Trojani. 2008. “Asset prices with locally constrained-entropy recursive
multiple-priors utility.” Journal of Economic Dynamics & Control, 32: 3695–3717.
Shiller, Robert J. 1981. “Do Stock Prices Move Too Much to be Justified by Subsequent Changes in
Dividends?” American Economic Review, 71(3): 421–436.
Shimotsu, Katsumi, and Peter C. B. Phillips. 2005. “Exact local Whittle estimation of fractional integration.” Annals of Statistics, 33(4): 1890–1933.
32

Sims, Christopher A. 1971. “Distributed lag estimation when the parameter space is explicitly infinitedimensional.” The Annals of Mathematical Statistics, 42(5): 1622–1636.
Sims, Christopher A. 1972. “The Role of Approximate Prior Restrictions in Distributed Lag Estimation.” Journal of the American Statistical Association, 67(337): 169–175.
Skiadas, Costis. 2013. “Smooth Ambiguity Aversion Toward Small Risks and Continuous-Time Recursive Utility.” Journal of Political Economy, 121(4): 775–792.
Strzalecki, Tomasz. 2011. “Axiomatic Foundations of Multiplier Preferences.” Econometrica,
79(1): 47–73.
Uppal, Raman, and Tan Wang. 2003. “Model Misspecification and Underdiversification.” Journal of
Finance, 58(6): 2465–2486.
Veronesi, Pietro. 2000. “How Does Information Quality Affect Stock Returns?” Journal of Finance,
55(2): 807–837.
Weil, Philippe. 1989. “The Equity Premium Puzzle and the Risk-Free Rate Puzzle.” Journal of Monetary
Economics, 24(3): 401–421.
Whittle, Peter. 1953. “Estimation and Information in Stationary Time Series.” Arkiv for Matematik,
2: 423–434.

33

Figure 1a. Weighting function |Z|2
10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
0
153 78

52

39

31

26 22 20 17 16
Cycle length (years)

14

13

12

11

10

11

10

Figure 1b. Spectrum under benchmark and worst case for
white-noise benchmark
0.0035
0.003
0.0025
0.002
0.0015

Worst-case spectrum

0.001
0.0005
0

Benchmark spectrum

153 78

52

39

31

26 22 20 17 16
Cycle length (years)

14

13

12

Figure 2. Empirical and model-implied R2's from return forecasting regressions
0.45

0.4

0.35

0.3

Simulated 75th percentile
Simulated median

R2

0.25

0.2

0.15

0.1

Data
Simulated 25th percentile

0.05

0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
Horizon (quarters)
Notes: Black lines give results from simulated regressions on 90-year samples. The grey line plots R2s from regressions of aggregate equity returns
on the price/dividend ratio between 1926 and 2015.

Figure 3. Benchmark and worst-case transfer function
6

5

4

Non-parametric worst case

3

Parametric worst case
2

Benchmark

1

0
0

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Frequency (radians/year)
Notes: benchmark, parametric, and non-parametric transfer functions for the ARMA(1,1) specification estimated on post-war US consumption growth. The
parametric worst case is the ARMA(1,1) that minimizes lifetime utility subject to the same KL divergence as is used for the nonparametric worst case.

Table 1: Asset pricing moments for the white-noise benchmark
Consumption dynamics
Benchmark
Worst-case
Consumption conditional volatility
1.47%
1.47%
σ
b(β) Long-run volatility
1
2.449
μ
Mean consumption growth
0.45%
0.37%
Asset pricing moments (annualized)
Standard deviation of pricing kernel
Mean excess equity return
Standard deviation of equity return
Mean risk-free rate
Standard deviation of risk-free rate
1-year autocorrelation of P/D
Standard deviation of P/D
Mean 10-year/1-quarter term spread
Implied estimate of EIS

Model
0.30
6.34
19.44
1.89
0.31
0.96
0.19
-13.3bp
0

Standard EZ
0.14
1.95
14.09
2.44
0
N/A
0
0
N/A

Data
N/A
6.33
19.42
0.86
0.97
0.81
0.29
N/A
0.14

Notes: Moments from the model with a white-noise benchmark process for consumption growth. The "standard
Epstein–Zin" results are for where the agent is sure of the consumption process. P/D is the price/dividend ratio for the
levered consumption claim. The values in the data treat the aggregate equity market as analogous to the levered consumption
claim. The EIS estimate is based on a regression of consumption growth on interest rates. In the second column interest
rates are constant, so the regression is degenerate.

Table 2. Probability of rejecting the pricing model
Rejection probs. (5% critical value, H0=worst-case model)
50-year sample
100-year sample
Ljung–Box
5.1%
5.1%
ARMA(1,1)
5.6%
6.0%
ARMA(1,1) rejection probabilities for alternative persistence in pricing model
Persistence
100-year sample
0.9975
6.0%
(Our worst case)
0.995
8.7%
0.99
13.5%
(Hansen and Sargent (2010))
0.98
28.3%
0.94
82.7%
(Bansal and Yaron (2004))
Notes: Rejection probabilities are obtained by simulating the distributions of the test statistics in 50- and 100-year
simulations of the cases where consumption growth is generated by the worst-case and white-noise models and
asking how often the statistics in the latter simulation are outside the 95% range in the former simulation. In the
bottom section, persistence is reduced but the price of risk in the pricing model is held constant.

Long-Run Risk is the Worst-Case Scenario
Appendix for online publication
Rhys Bidder and Ian Dew-Becker
May 4, 2016

Contents
A Full derivation of proposition 1 and corollary 2

2

A.1 Deriving the Kullback–Leibler divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

A.2 Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

A.3 The white-noise benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

B Testing the worst-case model

7

B.1 Test statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

B.2 Extended results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

C Interpretation of the distance measure as a Wald test

10

D Lifetime utility (assumption 3)

12

E Multiplier preference interpretation

13

F Asset prices and expected returns

14

F.1 Pricing a levered consumption claim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

F.2 The risk-free rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

F.3 Expected excess returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

F.4 The behavior of interest rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

F.5 Results used in table 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

F.6 Returns in the absence of model uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

G Dividends cointegrated with consumption

19

G.1 Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

G.2 Expected excess returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

G.3 Price/dividend ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

G.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

1

A

Full derivation of proposition 1 and corollary 2

A.1

Deriving the Kullback–Leibler divergence

We model the agent as comparing models based on the expected value of a squared distance. In the case
of a Gaussian model, the distance is exactly the expected likelihood ratio. When the time series are nonGaussian, it becomes a quadratic distance that has been widely studied in the time series econometrics
literature.
Models are indexed by the parameter set
consumption growth dynamics,
model

as

. The investor has a benchmark model for

. Denote the covariance matrix of consumption growth implied by a

. The log likelihood for a sample of consumption growth under the model
1
log j
2

where

2

b; ;

j

1
( c1;:::;T
2

)0

1

( c1;:::;T

)

is
(A.1)

c1;:::;T denotes a column vector containing the sample of observed consumption growth between

dates 1 and T . Now suppose consumption growth is generated by the model
T ! 1, the expected log likelihood for the model
1

lim T

T !1

=

1 1
22

1
log j
2

E

Z

j

log f (!) d!

= b; ;

2

. One may show that as

is equal to

1
1
( c1;:::;T
)0
( c1;:::;T
)
2
Z
f (!)
1 1
1(
)2
1d!
22
f (!)
2 f (0)

where E denotes an expectation when the data is generated by the model

(A.2)

. (A.2) is simply the expected

value of Whittle’s (1953) expression for the log likelihood. Formally, the limit is an application of a well
known result from Grenander and Szego (1958) that Toeplitz matrices converge asymptotically to circulant
matrices. See Gray (2006) for a recent textbook review of such results. Examples of recent work using and
extending the Whittle likelihood include Monti (1997), Dahlhaus (2000) and Shimotsu and Phillips (2005).
Now note that
1
2

Z

f (!)
1
d! =
f (!)
2

2
2

Z

jB (!)j2
B (!)

2 d!

Also, as long as the roots of B and B are inside the unit circle, we have

(A.3)
1
2

R

B(!)
d!
B(!)

= 1.1 We can

Q
To con…rm this, write b (L) as b (L) = j (1 aj L) for jaj j < 1. Using the same form for b, note that each of the factors
P
k k
of 1=b (L) has a convergent Taylor series, 1 1aj L = 1
k=0 aj L . Then the ratio B (!) =B (!) may be written as
1

B (!) =B (!) =

Y

1

aj e

j

i!

1
X

k=0

akj ei!k

!

(A.4)

This function only has Fourier coe¢ cients on the positive side of the origin,
and the coe¢ cient on the constant is a0j = 1.
R
That is, all the terms multiplying ei!k for k > 0 integrate to zero, so 21
B (!) =B (!) d! = 1.

2

therefore write
1
2

Z

jB (!)j2
B (!)

2 d! =

1
2

=

1
2

Z

B (!)
B (!)

Z

2

B (!)
B (!)

B (!)
+ 2d!
B (!)

(A.5)

2

B (!) B (!)
B (!)

d! + 1

(A.6)

Which implies that
1
2

Z

2
2

jB (!)j2
B (!)

2

1d! =

1
2

=

1
2

2
2
2
2

Note also that Kolmogorov’s formula implies that

Z
Z

R

1
2

B (!) B (!)
B (!)

2

B (!) B (!)
B (!)

2

2

d! +

2

d! +

log f (!) d! = log

The investor measures the distance between the benchmark model

1

2

(A.7)

2

(A.8)

2
2.

and an alternative

as the

di¤erence in the asymptotic expected log likelihoods of the two models when the data is generated by

,

which is the KL divergence,
1

lim T

T !1

E

LL (T; )

LL T;

1 1
22

=

1
2

A.2

2
2

Z

B (!)

B (!)
2

log

B (!)

2

2

2

2

d!

2
2

+

1(
^ )2
2 f (0)

(A.9)

Minimization

The investor’s optimization problem to …nd the worst-case model is
min

b; ;

2

1
1

2

2

b( )

2

+

+

1

where the integral sign without limits denotes

1
2

2

R

"Z

f (!)
f (!)

f (!)
(
)2
log
d! +
f (!)
f (0)

#

(A.10)

.

The spectral density f (!) can be expressed as

0

f (!) = exp @2

1
X
j=0

1

cj cos (!j)A

(A.11)

for a set of real coe¢ cients cj (Priestley (1981)). The coe¢ cients cj are simply the Fourier coe¢ cients of
the log of the spectrum; we only include coe¢ cients for non-negative j because the spectrum is a real and

3

even function. Furthermore, setting

= exp (c0 ), we have

0
1
1
X
B (!) = exp @
cj ei!j A

(A.12)

j=0

0
1
1
X
e i!m exp @
cj ei!j A d!

Z

bm =

(A.13)

j=1

where the bj are the coe¢ cients from the Wold representation for the spectrum jB (!)j2 (Priestley (1981)).
Since

= exp (c0 ), b0 = 1. Furthermore, bj = 0 for all j < 0. (A.12) is known as the canonical factorization

of the spectrum. We solve the optimization problem by directly choosing the cj . Since the Fourier transform
is one-to-one, choosing the cj is equivalent to optimizing over the spectrum directly. Since B (!) is obtained
from the Wold representation, it is guaranteed to be causal, invertible, and minimum-phase. Last, the
innovation variance associated with the spectrum f (!) is

2

= exp (2c0 ).

We …rst calculate derivatives involved in the optimization
d
[ b ( )] =
dcj

1
d X
dcj

m

m=0

1
X

0

Z

exp @
0

Z

1
X
j=0

1
X

1

cj ei!j A e

i!m

d!

1

d
exp @
cj ei!j A e i!m d!
dcj
m=0
j=0
0
1
Z
1
1
X
X
m
=
exp @
cj ei!j A e i!(m j) d!
=

m

m=0

=
=

(A.14)

(A.15)

(A.16)

j=0

1
X

m=0
1
X

m

bm

m+j

(A.17)

j

bm = b ( )

j

(A.18)

m=0

where the derivative can be passed inside the integral because B (!) is continuous and di¤erentiable with
respect to the cj and the last line follows from the fact that bj = 0 for j < 0.
Next, the derivative of the ratio of the spectra is
d
dcj

Z

Z exp 2 P1 cj cos (!j)
j=0
d
d!
dcj
f (!)
Z
f (!)
= 2
cos (!j) d!
f (!)

f (!)
d! =
f (!)

And last,
d
dc0

Z

log

f (!)
d! = 2
f (!)

4

(A.19)
(A.20)

(A.21)

So we have
min

b; ;

2

1
1

2

2

2

b( ) +

+

1

2

"Z

f (!)
f (!)

+

Z

f (!)
(
)2
log
d! +
f (!)
f (0)

#

(A.22)

The …rst-order condition for each j > 0 is
0=2

1
1

( )2

2 w
wb

2

j

2

For j = 0,
0=2

1
1

2

2 w
wb

2

( ) +

2

Z

f w (!)
2 cos (!j) d!
f (!)
f w (!)
2d!
f (!)

(A.23)

(A.24)

Now multiply each of the …rst-order conditions by cos (j ) for some .
0 = 2
= 2
= 2

1
1

2
1

1

2
1

1

2

2 w
wb

( )2 cos (j )

j

2 w
wb

( )2 cos (j )

j

2 w
wb

( )2 cos (j )

j

Z

f w (!)
2 cos (j ) cos (!j) d!
2
f (!)
Z w
f (!)
+
(cos (j ( + !)) + cos (j (
2
f (!)
Z w
f (!)
+
2 cos (j ( + !)) d!
2
f (!)
+

(A.25)
!))) d! (A.26)
(A.27)

where the third line follows by
= 2
= 2
= 2

1
1

2
1

1

2
1

1

2

That is, since

f w (!)
f (!)

2 w
wb

( )2 cos (j )

j

2 w
wb

( )2 cos (j )

j

2 w
wb

( )2 cos (j )

j

Z w
f (!)
f w (!)
cos (j ( + !)) d! +
cos (j (
!)) d!
2
f (!)
f (!)
Z w
Z w
f (!)
f ( !)
+
cos (j ( + !)) d! +
cos (j ( + !)) d!
2
f (!)
f ( !)
Z w
Z w
f (!)
f (!)
+
cos (j ( + !)) d! +
cos (j ( + !)) d!
2
f (!)
f (!)
+

Z

is even, we can always reverse the sign of ! in the integration.

Now take the …rst-order condition (FOC) for j = 0, multiply it by 21 , and add to the sum of the FOCs
for j > 0 multiplied by cos ( j),
0 =

1
1
+

=

1

2

2

( ) +

Z

f w (!)
d!
f (!)

2
1
X
2
2 w
2 cos (j )
wb ( )

2

(A.28)
Z

1

f w (!) X
2 cos (j ( + !)) d!
(A.29)
2
2
f
(!)
j=1
j=1
0
1
0
1
Z
1
1
w
X
X
1
f (!) @
2@
2 w
1+
2 cos (j ) j A +
1+
2 cos (j ( + !))A d!
wb ( )
2
2
2
f
(!)
j=1
j=1
1

1

2 w
wb

j

+

5

We have
1+

1
X

2 cos (j ( + !)) = ( + !)

(A.30)

j=1

where ( ) is the Dirac delta function. Furthermore, note that Z (!) is the transfer function of an AR(1)
model with autocorrelation of . It then follows that
jZ ( )j2 = 1 +

2

1

1
X

j

2 cos (j )

(A.31)

j=1

The FOC then becomes
0 =
f w (!)
f (!)

1
1

2 w
wb

2
1

1 =

(

1

f w (!) = f ( ) +

( )2 1
2 w
wb

1)

1

jZ ( )j2 +

2

( )2 1

(1 + ) (

2

1) f ( )

2

f w (!)
f (!)

jZ ( )j2

2 w
wb

1

(A.32)
(A.33)

( )2 jZ ( )j2

(A.34)

This is the main result in the text.

A.3

The white-noise benchmark

In the white noise case, f ( ) =

2.

The mean immediately follows,
w

1

=

2

(A.35)

1

For the dynamics,
f w (!) =

2

1

+

(1 + ) (

w.
j

Denote the autocovariances under the worst-case model as

w
0

+2

1
X

w
j

cos (!j) =

j=1

0

2@
1

where '

2 2 w
wb

1)

1 + ' @1 + 2
(

(A.36)

Then

0

1

( )2 jZ (!)j2

1)

1
X

cos (!j)

j=1

2 w
wb

( )2

11

j AA

(A.37)
(A.38)

Matching coe¢ cients on each side yields
w
0

=

2

(1 + ')

w
j

=

2

'

6

j

for jjj > 0

(A.39)
(A.40)

These may be recognized as the autocovariances of an ARMA(1,1) process. Speci…cally, set
ct = x t + v t
xt =

Then one may con…rm that

xt

2
v

=

2

2

=

2

(A.41)

+

1

(A.42)

t

(A.43)
2

' 1

(A.44)

w.
j

ct has autocovariances

To …nd the equivalent univariate ARMA(1,1) representation, note that
ct

ct

1

= xt
=

t

xt

+ vt

1

+ vt

vt

vt

(A.45)

1

(A.46)

1

The second line is an MA(1), with
mt
var (mt ) =
cov (mt ; mt
We then …nd

and

2
w

1)

2
v

=
which immediately yields
We therefore solve for
and

+ vt

2

+ 1+
2
v

=

vt

(A.47)

1

2

2
v

2

= 1+

2
w

(A.48)

2
w

=

(A.49)

by solving that pair of equations. We have
2

then calculate

t

2.
w

and
2
w

+ 1+

2

1

r

2
2
v

2

+ 1+

2

2

4
(A.50)

2

Now
2
"

depends on

2,

which depends on '. But ' itself depends on b ( ).
1

iteratively. Speci…cally, begin by guessing that ' =

for that guess, and update ', with ' =

1
1

(

1)

2
w

1
1
1

(
2

2

1)

2.

We

and iterate to

convergence.

B

Testing the worst-case model

This section provides details and further results for the small-sample tests of the worst-case model.

B.1

Test statistics

We examine three tests: the ARMA(1,1) likelihood-ratio test suggested by Andrews and Ploberger (AP;
1996), the Ljung–Box (LB; 1978) test, and a test based on the Newey–West (1987) estimator of the long-run
variance.
For the AP and LB tests, as discussed in the text, we assume that the agent takes an observed

7

consumption history and creates a series of residuals,
"t

w

( ct

aw (L) ( ct

w

Under the null hypothesis that the worst-case model is true, "t
"t

w

under the benchmark model, note that we can write "t
"t

(where

1
1

=

L
L

w

1

w

w

1

(B.1)

is white noise. To see the dynamics of

as

( ct

w)

(B.2)

is de…ned for the worst-case model above). Under the benchmark,

write
"t
where "t

w

w

w ))

1

w

=

1
1

L
L

"t +
w

1
1

w

1

(

ct

N

w)

;

2

, so we can
(B.3)

N (0; 1).

When we simulate the distribution of the AP and LB test statistics conditional on the benchmark
model being true, we construct them on simulated samples of "t

w

using (B.3).

As discussed in the text, for the AP and LB tests, we …rst calculate critical values under the benchmark
model. That is, we simulate samples of the time series "t

N (0; 1) and then construct the AP and LB

test statistics for each sample. The critical values are the 95th percentiles of those simulated distributions.
The AP statistic is constructed exactly as in Andrews and Ploberger (1996). Speci…cally, for a sample
"t , t 2 f1; 2; :::; T g, de…ne
2

1

!
~ =T

T
X

"2t

(B.4)

t=1

!
~ 2 is the log likelihood (ignoring constants) under the null hypothesis that "t

N (0; 1)

Second, de…ne
"t

"t

T

1

T
X

"t

t=1

!
^2 ( )

T

1

T
X

("t )2

t=1

(B.5)
2

6
4T

1

PT

Pt 2
t=2 "t
i=0
PT
Pt 2
t=2

i=0

i
i

"t

"t

2
i 1
i 1

3
7
5

(B.6)

!
^ 2 ( ) is the log likelihood when the mean of "t is estimated freely and we also allow estimation of the
parameter .
The likelihood ratio statistic is then
LR
For each simulated sample, we optimize over

sup T log

!
~2
!
^2 ( )

(B.7)

numerically (…rst searching over a grid, then using the

simplex algorithm from the best grid point).

8

Note that the LR statistic here compares the likelihood of the data under assumptions both that "t is
serially uncorrelated and also that its mean is zero. !
^ 2 ( ) is the maximized likelihood under an alternative
model that allows both for serial correlation (of an ARMA(1,1) form) and also a non-zero mean. We also
consider a version of the AP test that ignores the deviation in the mean under the null. This constraint
may potentially improve the power of the test, because it means that we are only testing the dynamics of
consumption growth, not the level. Speci…cally, the AP statistic with a …xed mean is
LR

sup T log

(~
! )2

1

T

T
X

(~
! )2
!
^2 ( )

(B.8)

("t )2

(B.9)

t=1

LR di¤ers from LR only in that the numerator of the likelihood ratio now uses demeaned data. In other
words, the null allows for an estimated mean.
The LB statistic is calculated using the autocorrelations of the sample of "t , which we denote ^ j . The
statistic, for a maximum lag of j, is
LBj

T (T + 2)

j
X
k=1

PT

^ 2k
T

(B.10)

k

t=k+1 "t "t k
2
t=k+1 "t

^k

(B.11)

PT

Finally, we also examine here a test based on the Newey–West (1987) estimator for the long-run variance
of a time series. We ask whether, observing a sample of data generated by the benchmark model, a person
would reject the hypothesis that the long-run variance is as large as implied by the worst-case model.
Speci…cally, we calculate the Newey–West estimate of the long-run variance
LRVj

^j

= ^0 + 2

T

1

j
X

k=1
T
Xj
k=1

1

ck

k
j
T

^j
1

T
X
t=1

(B.12)

ct

!

ck+j

T

1

T
X
t=1

ct

!

(B.13)

We simulate the distribution of LRVj given data generated by the worst-case model and de…ne LRVj
to be the 5th percentile of that distribution. The agent then rejects the hypothesis that the data was
driven by the worst-case model after observing a sample drawn from the benchmark model if LRVj in that
particular sample is less than LRVj . That is, we ask how often the estimated long-run variance estimated
under the benchmark model is smaller than the 5th percentile of the long-run variance estimated under
the worst-case model.

9

B.2

Extended results

The main text discusses results for the LB and AP tests on samples of 50 and 100 years. Table A2 reports
results using the Newey–West based test and using longer samples up to 1000 years.
As one would expect, as the samples grow, the rejection rates across all four tests rise. For 1000-year
samples, all but the Ljung–Box test reject with probabilities greater than 85 percent, con…rming that they
eventually converge to the correct result asymptotically. However, one can see looking across the table
that all the tests converge rather slowly. With 250 years of data, the AP tests reject the worst case still
less than 10 percent of the time, while the NW test rejects approximately 25 percent of the time.
A natural question is why the rejections probabilities are so low, even for the Newey–West based test.
A simple way to see the intuition is to consider the periodogram. In a …nite sample, the lowest frequency at
which the periodogram is observed is 2 =T radians, which corresponds to a cycle with wavelength equal to
the sample. Asymptotically, the periodogram is distributed exponentially with mean equal to the spectral
density. What distinguishes the worst-case model from the benchmark is that its spectrum is much larger
at low frequencies.
Speci…cally, the spectrum under the worst case has a value at frequency zero of f w (0) = bw (1)2
0:00491;, whereas under the true model, f (0) = 0:000215. So

fw

2
w

=

(0) is 23 times larger than f (0). Given

that the standard deviation of the periodogram is equal to the level of the spectrum itself, f w (0) is 22
standard deviations higher than f (0) and should be easily distinguishable.
However, since we do not observe the periodogram at frequency zero, what really matters is the value of
the spectrum at ! = 2 =T . For T = 200, f w (2 =200) = 0:000244, which is only higher than f (2 =200) =
2

by a factor of 1.13. So in a sample with 200 observations, there simply is little information in the sample

that reveals the deviations between f w and f .
In a 100-year sample, rejection is obviously easier. The …rst periodogram ordinate has mean f w (2 =400) =
0:00312, which is now substantially larger than f . On the other hand, this is still only a single data point
for the estimators to use.

C

Interpretation of the distance measure as a Wald test

This section provides an alternative of the distance measure used in the main text as a Wald test on
R jB(!) B(!)j2
estimated MA coe¢ cients. Speci…cally, the part of the distance measure
d! represents the
f (!)

asymptotic expected value of a Wald statistic for a joint test of all the MA coe¢ cients in the lag polynomial
b (L).
Brockwell and Davis (1988b) show that for an MA model of order m, the coe¢ cients are asymptotically

10

normal with a covariance matrix denoted

As m ! 1,

m

T rue T rue0
! Jm
Jm
2 T rue T rue
b0
b1
6
bT0 rue
6 0
6 .
..
6 .
.
4 .

m

T rue
Jm

where

m.

0

converges to a product,2

..

7
7
7
7
5

..
.

.

bT0 rue

0

(C.1)

3

bTmrue
bTmrue1

(C.2)

A natural empirical counterpart to that variance is to replace J T rue with J, de…ned analogously using the
point estimate b. The Wald statistic for the MA coe¢ cients (ignoring scale factors) is then
m

1

b1:m

1

0
Jm Jm

b1:m

b1:m

b1:m

0

(C.3)

where b1:m is the row vector of the …rst m elements of the vector of coe¢ cients in the model b.
Jm is a Toeplitz matrix, and it is well known that Toeplitz matrices, their products, and their inverses,
asymptotically converge to circulant matrices (Grenander and Szeg½o (1958) and Gray (2006)). So
an approximate orthogonal decomposition, converging as m ! 1, such
1
m

m Fm

1

with element j; k equal to exp ( 2 i (j

1) (k

has

that3
(C.4)

m

where here represents transposition and complex conjugation,

1
m

m

is the discrete Fourier transform matrix

1) =m), Fm is diagonal with elements equal to the discrete

Fourier transform of the autocovariances. Now if we de…ne the vector B to be the Fourier transform of b,
B1:m

m

b1:m

1

b1:m

m,

we have

b1:m

1
m

b1:m

b1:m

0

m

1

Bm

= m

1

Bm

Bm

m

m

m Fm

Bm Fm 1 Bm

1

m

Bm

Bm0

0
m

Bm0

0 0
m (C.5)

(C.6)

which itself, by Szeg½o’s theorem, converges as m ! 1 to an integral,
1

m

So the integral

R jB(!)

Bm

B(!)j
f (!)

Bm Fm

1

Bm

Bm

!

Z

2

B (!) B (!)
d!
f (!)

(C.7)

2

d! may be interpreted as the limiting value of a Wald statistic for the lag

polynomial b taking b as the point estimate.
2

The distribution result used here is explicit in Brockwell and Davis (1988). It is implicit in Berk (1974) from a simple
Fourier inversion of his result on the distribution of the spectral density estimates. Note that Brockwell and Davis (1988)
impose the assumption that b0 = 1, which we do not include here.
3
0
0
0
Speci…cally, Jm
m Fm m ,
m Bm m =
m Bm m m Bm m =
m B m Bm
m =
m Bm m , and thus Jm Jm
where Bm is the diagonal matrix of the discrete Fourier transform of b0 ; b1 ; :::; bm . Again, the aproximations become exact
as m ! 1.

11

D

Lifetime utility (assumption 3)

As discussed in the text, the agent’s expectation of future consumption growth, Et [ ct+j j ] is equal to
expected consumption growth at date t + j given the past observed history of consumption growth and
the assumption that "t has mean zero. Given that the agent believes that the model

= b; ;

2

drives

consumption growth, we can write the innovations implied by that model as
"t = ( ct

a (L) ( ct

))

1

(D.1)

That is, "t is the innovation that the agent would believe occurred given the observed history of consumption growth and the model

. The agent’s subjective expectations for future consumption growth

are then
Et [ ct+j j ] =

+

1
X

bk+j "t

k

(D.2)

N (0; 1)

(D.3)

k=0

with subjective distribution
ct+1
We guess that v

ct ;

Et [ ct+1 j ]

takes the form
v

ct ;

= ct + k +

1
X

kj "t

(D.4)

j

j=0

Inserting into the recursion for lifetime utility yields
k+

1
X

kj "t

j

=

j=0

=

"

k + + (k0 + 1) "t+1
P
+ 1
j=1 (kj + bj ) "t j+1

log Et exp

1

k+

1
X

+

(kj+1 + bj+1 ) "t

j

+

1

j=0

2

!

!

(1

) j

(k0 + b0 )2

2

#

(D.5)

(D.6)

Matching the coe¢ cients on each side of the equality yields
kj =

v

ct ; b

= ct +

= ct +
= ct +

1
1

2
1

1

2
1

1

2

(kj+1 + bj+1 )

b ( )2

b( )

2

b ( )2

2

2

2

+

+

+

+

1

+

1
1
X
k=1

12

(D.7)

1
X
k=1
1
X
j=0

k

k

1
X

bj+k "t

j=0

1
X
k=1

Et [ ct+k j ]

k

bj+k

(D.8)

j

!

"t

j

(D.9)

(D.10)

E

Multiplier preference interpretation

In our main analysis, we model agents as having Epstein–Zin preferences. Such preferences are observationally equivalent (in the sense that they rank all consumption streams identically) to Hansen and Sargent’s
(2001) multiplier preferences. In that model, agents have log utility over consumption, but they form
expectations using a worst-case model over innovations to the consumption process. Speci…cally, their
preferences are obtained through
vt = min ct +
ht+1

(Et [ht+1 vt+1 ] + Et [ht+1 log ht+1 ])

(E.1)

where ht+1 is a change of measure with E [ht+1 ] = 1. ht+1 represents an alternative distribution of the
innovations to the state variables at date t + 1. In this model, agents select an alternative distribution for
innovations (instead of a full distribution over consumption growth) penalizing alternative distributions
based on their KL divergence (Et [ht+1 log ht+1 ]).
Inserting the value of ht+1 that solves the minimization problem yields
vt = ct

1

log Et exp

vt+1

(E.2)

That is, the Epstein–Zin preferences used in the main text can be interpreted as multiplier preferences
with

1

= (1

).

We can thus interpret the model described in the paper as involving two layers of robustness, or two
evil agents. First, there is an evil agent who, in a timeless manner, selects a full worst-case process for
consumption growth. Next, taking the preferences (E.2) a second evil agent causes further deviations in
the innovations to that process.
The second evil agent’s minimization problem is (E.1), and the minimized value function is then (E.2),
which is exactly the preference speci…cation that is minimized in the main text. In other words, both the
minimization problem over the full models for consumption growth that we study and also the minimization
over one-step deviations –which induces Epstein–Zin preferences –depend on a KL divergence penalty.
A natural benchmark is to equalize the penalty on the KL divergence that is involved in both minimization problems. Since the entropy penalty for the second agent is applied in every period, we naturally
scale it up by the discount rate. That is,
= = (1
Which immediately yields a connection between
=

)

(E.3)

and ,
1

1

1

1

(E.4)

1

= 1+

13

1

(E.5)

F

Asset prices and expected returns

F.1

Pricing a levered consumption claim

Using the Campbell–Shiller (1988) approximation, the return on a levered consumption claim can be
approximated as (with the approximation becoming more accurate as the length of a time period shrinks)
rt+1 =
where

0

+ pdt+1 +

ct+1

pdt

(F.1)

is a linearization parameter slightly less than 1.

We guess that
pdt = h +

1
X

h j ct

(F.2)

j

j=0

for a set of coe¢ cients h and hj .
The innovation to lifetime utility is
1
X

w

vt+1

Et [vt+1 jb ] =

k

k=0
w

w

Et+1 [ ct+k+1 j

]

(F.3)

w

= b ( ) "t+1

(F.4)

w

where the investor prices assets as though "t+1 is a standard normal.
The pricing kernel can therefore be written as
Mt+1 =

w

) bw ( ) "t+1

ct+1 + (1

exp

)2

(1
2

bw ( )2

2
w

!

(F.5)

The pricing equation for the levered consumption claim is

0 = log Et

"

= ( h0 +

0

exp

+(

1) h + ( h0 +

w

1) ((1

a (1))

w

w

+ a (L)

ct ) +

1
X

P1

j=0 ( hj+1
(1
)2 w
b ( )2 2w
2

ct+1 +

) bw ( ) "t+1

+ (1
w

1)

( hj+1

hj )

ct

hj )

ct

j

!

j

w

#

(F.6)

j

j=0

+

0

+

1
( h0 +
2

1)2 + ( h0 +

Matching coe¢ cients on
(

1) h + log

( hj+1

ct
+

0

j

1) (1

) bw ( )

2
w

+(

1) h + log

(F.7)

and on the constant yields two equations,
=

hj ) =

1
( h0 +
1)2 + ( h0 +
2
( h0 +
1) (1 aw (1)) w
( h0 +

1) aw
j

14

1) (1

) bw ( )

2
w

(F.8)
(F.9)

And thus
h0 =

1) aw ( )
aw ( )

(
1

and
h0 +

1=

(F.10)

1
( )

(F.11)

aw

1

Note then that
varw (rm;t+1 ) = ( h0 + )2

2
w

(F.12)
) bw ( ))

covw (rm;t+1 ; mt+1 ) = ( h0 + ) ( 1 + (1

F.2

2
w

(F.13)

The risk-free rate

For the risk-free rate, we have
rf;t+1 =

F.3

log Et

"

exp

=

log

+ (1

=

log

+

aw (1))

w

) bw ( ) "t+1

ct+1 + (1
w

+ aw (L)

+ aw (L) ( ct

w

)

1
2

ct
1
2

2
w

2
w

)2

(1

w

2

) bw ( )

+ (1

) bw ( )

+ (1

bw ( )2

2
w

!

j

w

#

(F.14)

2
w

(F.15)

2
w

(F.16)

Expected excess returns

The expected excess return on the levered consumption claim from the perspective of an econometrician
who believes that consumption dynamics are the point estimate
2

= Et 4

Et rt+1 j

=

=

0

0

0

1) h

rf;t+1 j

=

ct+1 +

1) aw (L)

( h0 +

( hj+1

hj )

ct

0

+(

jj

ct+1 j

3
5

(F.17)
(F.18)

ct

a (1))

(F.19)

1) h + ( ( h0 +

+ ( h0 + ) (1
+ log

ct + Et ( h0 + )

1) aw (L) + ( h0 + ) a (L))

1) h + ( ( h0 +

+ ( h0 + ) (1

Et rt+1

1) h + ( h0 + )

1
X
j=0

+(

+(

+(

is

(1

1) aw (L) + ( h0 + ) a (L))

ct

a (1))
aw (1))

15

w

aw (L)

ct +

1
2

2
w

(1

) bw ( )

2
w

(F.20)

Inserting the formula for (
(

1) h + log

1) h + log
+

Et rt+1

0

+

0

from above yields

1
( h0 +
1)2 + ( h0 +
2
( h0 +
1) (1 aw (1)) w

=

rf;t+1 j

1) (1

2
w

(F.21)

aw (L)) ( ct

= ( h0 + ) (a (L)

) bw ( )

)

w
+ ( h0 + ) (1 aw (1)) (
)
1
varw (rm;t+1 ) covw (rm;t+1 ; mt+1 )
2

(F.22)

where
varw (rm;t+1 ) = ( h0 + )2

2
w

(F.23)
) bw ( ))

covw (rm;t+1 ; mt+1 ) = ( h0 + ) ( 1 + (1
Substituting in
h0 +

=

1) aw ( )
+
aw ( )

(
1

=

2
w

(F.24)

aw ( )
aw ( )

1

(F.25)

yields the result from the text.
Et rt+1

F.4

rf;t+1 j

aw ( )
(a (L) aw (L)) ( ct
)
1
aw ( )
aw ( )
w
+
(1 aw (1)) (
)
1
aw ( )
1
varw (rm;t+1 ) covw (rm;t+1 ; mt+1 )
2

=

(F.26)

The behavior of interest rates

The mean of the risk-free rate is
log

+ (1

aw (1))

+ aw (1)

1
2

std (aw (L)

ct )

w

2
w

) bw ( )

+ (1

2
w

(F.27)

And its standard deviation is
(F.28)

When consumption growth is white noise, this is
std (aw (L)

0

ct ) = std @(

)p

= (

16

)

1
X
j=0

c

1

2

j

ct

1

jA

(F.29)
(F.30)

We denote the log price on date t of a claim to a unit of consumption paid on date t + j as pj;t , and we
guess that
(j)

pj;t =
for a lag polynomial

(j)

w

(L) ( ct

) + nj

(F.31)

and a constant nj that di¤er with maturity.

The pricing condition for a bond is
Mt+1 =

(j)

(L)

ct + nj

exp

ct+1 + (1

"

= log Et exp
= log

(1
2

(j 1)
0

+

) b ( ) "t+1

log
)2 w
b

1 (

w

2

2
w

+

(j 1)

w

b ( )

2

2
w

!

(F.32)

w

) bw ( ) "t+1

ct+1 + (1

( )2

)2

(1

w

w

w)

(L) ( ct+1

+ aw (L) ( ct

w

))

(j 1) w
0

+ nj

+

1
X

1

!

j

(j 1)
k+1 (

w

#
ct

(F.33)

k

w

)

k=0

)2

(1
2

bw ( )2

2
w

+ nj

1

+

1
(1
2

) bw ( )

1+

(j 1) 2
0

2
w

(F.34)

Matching coe¢ cients yields,
(j)

(j 1)
0

(L) =

1 aw (L) +

1
X

(j 1) k
k+1 L

(F.35)

k=0

nj = log

w

)2

(1
2

bw ( )2

2
w

+ nj

1

+

1
(1
2

) bw ( )

1+

(j 1) 2
0

2
w

(F.36)

We also have the boundary condition that the price of a unit of consumption today is 1, so that n0 = 0
and

(0)

(L) = 0. Note that the mean price of any of these claims is
E [pj;t ] =

F.5

(j)

w

(1) (

) + nj

(F.37)

Results used in table 1

Under the worst-case, consumption growth follows an ARMA(1,1). We have
ct =

ct

aw (L) = (

1

)

+ "t
1
X
j=0

17

"t
j

Lj

1

(F.38)
(F.39)

where

(1

')

and ' is obtained above. We then have
aw ( ) =
aw (1) =
j 1

bj =

(F.40)

1

(F.41)

1
(

)

(F.42)

For the coe¢ cients in the price/dividend ratio, we have
( hj+1

hj ) =
hj

( h0 +

= ( h0 +

1)

1) aw
j
1
X

(F.43)
k w
aj+k

(F.44)

k=0

= ( h0 +

1)

1
X

k

(

j+k

)

(F.45)

k=0

= ( h0 +
And thus
pdt = h +

( h0 +

j

1) (

)
1

)X

1) (
1

(F.46)

1

j

ct

(F.47)

j

j=0

The standard deviation of the price/dividend ratio under the true white-noise process for consumption
growth is then
std (pdt ) =

F.6

( h0 +

1) (

)

1

Returns in the absence of model uncertainty

p

1

(F.48)

2

When there is no model uncertainty, the SDF is the same as in our main case, but everything is calculated
using the benchmark model instead of the worst case. For interest rates, then
rf;t+1 =
=

"

exp

+

1
2

log Et
log

ct+1 + (1
2

E [rf;t+1 ] =

+ (1

log

)

+

std (rf ) = 0

) "t+1

)2

(1
2

2

1
2

2

!

j

#

(F.49)
(F.50)

2

+ (1

)

2

(F.51)
(F.52)

18

For the price/dividend ratio, we have hj = 0 for all j, which implies
var (rm;t+1 ) =

2 2

(F.53)
2

cov (rm;t+1 ; mt+1 ) =

(F.54)

The standard deviation of the log pricing kernel is
std (mt+1 ) =

G

(F.55)

Dividends cointegrated with consumption

Two drawbacks of our main speci…cation for dividends are that it implies that dividend and consumption
growth are perfectly correlated and that it implies dividends are slightly more volatile than observed
empirically. To generate more realistic behavior for dividends, we now consider a setting where dividends
and consumption are cointegrated. We want to exactly match three major features of the joint dynamics
of consumption and dividends: the standard deviations of the two series, the correlation between the two
series, and the fact that dividends appear to be smoothed over time (Marsh and Merton (1987); Chen, Da,
and Priestley (2012)).
We assume the following model holds
dt = gc (L) ct + g (L)
where

t

(G.1)

t

is a normally distributed innovation with unit variance and g (L) is a lag polynomial. We assume

that g (L)

t

is stationary with …nite variance (the case where g (L) has a unit root would correspond

to a situation where dividends and consumption are no longer cointegrated, but their growth rates are
correlated).
The function gc (L) is what models dividends as a smoothed form of consumption. We normalize the
lag polynomial so that gc (1) = 1. As a simple example, if gc (L) = 1 + L + L2 , then dividends are a
three-year moving average of consumption plus noise (g (L)

t ).

Allowing a lagged response of dividends

to fundamentals (consumption) allows us to model the dividend smoothing observed in Marsh and Merton
(1987) and Chen, Da, and Priestley (2012).
represents the cointegrating coe¢ cient between dividends and consumption –it determines how much
the long-run level of dividends responds to a unit shock to the long-run level of consumption.
In terms of growth rates we have
dt =
g~ (L)

gc (L)
g (L) (1

ct + g~ (L)
L)

t

(G.2)
(G.3)

We then recapitulate the analysis from above. Speci…cally, we add a superscript C to the coe¢ cients

19

in the price/dividend function to yield the guess
C
pdC
t =h +

1
X

hC
c;j ct

+ hC;j

j

(G.4)

t j

j=0

C
rt+1
=

+ pdC
t+1 + gc (L)

0

ct+1 + g~ (L)

pdC
t

t+1

(G.5)

The pricing equation for the dividend claim is
2

0

0

6
B
6
B
6
B
0 = log Et 6 exp B
6
B
4
@
hC
c;0 + gc;0

=

+

+(
P1

hC;0 + g~

1

hC
c;j

1

ct+1
ct

j

2

w

) bw ( ) "t+1 (1 2 ) bw ( )2 2w
P1
hC;j+1 hC;j + g~ ;j+1
t+1 +
j=0

;0

aw (1))

1 ((1

hC
c;j+1 + gc;j+1

j=0

+ (1
+

hC
c;0 + gc;0

1) h +

w

+ aw (L)

ct ) +

1
X

t j

hC
c;j+1 + gc;j+1

3

C
C
C
Cj
C
A

7
7
w7
7
7
5

hC
c;j

ct

(G.6)

j

j=0

+
+

0

+

1
X

1
2

hC
c;0 + gc;0
hC;j + g~

hC;j+1

1

2

;j+1

hC
c;0 + gc;0

+
t j

+

j=0

Matching coe¢ cients on
(

1) hC + log

+

ct

0

j,

t j,

1
2

=

1
2

) bw ( )

1 (1
2

hC;0 + g~

;0

2
w

1) hC + log

+(

2

(G.7)

and on the constant yields three equations,
hC
c;0 + gc;0

hC
c;0 + gc;0

hC
c;j+1

hC
c;j

=

hC;j+1

hC;j

=

1

1 (1

2

hC
c;0 + gc;0

+

aw (1))

hC
c;0 + gc;0
g~

w

1 aw
j

) bw ( )

1 (1

1
2

hC;0 + g~

gc;j+1

2
;0

2

2
w

(G.8)

(G.9)
(G.10)

;j+1

And thus
hC
c;j
hC
c;0

hC
1 aw
hC
c;j+1 +
c;0 + gc;0
j + gc;j+1
1
1
X
X
w j
1
=
hC
+
g
1
a
+
gc;j
c;0
c;0
j
j=0

hC
c;0

(G.11)

=

+ gc;0

1 =

hC
c;0 + gc;0

1 =

hC
0

j

(G.12)

j=1

+ gc;0

gc ( ) 1
1
aw ( )

20

1

w

a ( ) + gc ( )

1

(G.13)
(G.14)

gc ( )
aw ( )
w
1
a ( )

hC
c;0 + gc;0 =

(G.15)

Note that when gc (L) = 1, gc ( ) = 1, and gc = 1, so the above equation reduces to precisely what is
obtained above for hC
c;0 +

1. Furthermore, note that for

1, gc ( )

gc (1) = 1.

For the coe¢ cients on , we have
hC;0 + g~

;0

= g~ ( )

(G.16)

Note then that
hC
c;0 + gc;0

varw (rm;t+1 ) =

2

2
w

+ g~ ( )2

hC
c;0 + gc;0 ( 1 + (1

covw (rm;t+1 ; mt+1 ) =

(G.17)
) bw ( ))

2
w

(G.18)

So what we have is that the variance of the return is simply increased through the additional noise added
to dividends, g~ ( )2 , while the covariance is una¤ected. Furthermore, we note that g~ (1) = 0, so for
close to 1, we would expect the term g~ ( )2 to be small.

G.1

Calibration

We leave the calibration of

the same as in the main text. We also maintain the calibration that con-

sumption growth in the benchmark model is white noise. We then have
corr ( c; d) = gc;0

std ( c)
std ( d)

(G.19)

Following Bansal and Yaron (2004) (who use real dividend growth for the CRSP value-weighted index), we
set std ( d) = 0:057 and corr ( d; c) = 0:55, which then implies gc;0 = 0:44 (given the value of

from

table 1). For the sake of simplicity, we assume that gc is a simple MA(1), yielding gc;;1 = 0:56 and gc;j = 0
for j > 1.
Finally, we calibrate g~ to match the variance of dividend growth. We have
var ( d) =

2

2
2
gc;0
+ gc;1
var ( c) + var (~
g ( )

Again, for the same of simplicity, we assume that the error g (L) = g
g

;0

g

;0 L.

;0 ,

t)

(G.20)

which implies that g~ (L) =

Finally,
var ( d) =

2

2
2
gc;0
+ gc;1
var ( c) + 2g 2;0

(G.21)

(under the normalization that var ( t ) = 1). Inserting the calibrated values for the other parameters, we
obtain
1
var ( d)
2
= 0:019

g 2;0 =
g

;0

2

2
2
gc;0
+ gc;1
var ( c)

(G.22)
(G.23)

21

That is, the …nal model of dividends is
dt = 2:13ct + 2:67ct

(G.24)

t

N (0; 1)

t

G.2

+ 0:019

1

(G.25)

Expected excess returns

The expected excess return on the levered consumption claim from the perspective of an econometrician
who believes that consumption dynamics are the point estimate
2

6
= Et 6
4

Et rt+1 j

=

0

+
+

+(

=

rf;t+1 j

1) h +

hC
c;0 + gc;0 Et

0

+(

+(

rf;t+1 j

+ log

(1

1) h + log

+

0

+

hC
c;0 + gc;0 (1
1
2

2
w

1
2
1
2

rf;t+1 j

1 aw (L)

ct (G.27)

ct
(G.28)

aw (L)) ( ct

)

(1

) bw ( )

=

2
;0

(1

aw (L)) ( ct
aw (1)) (

hC
c;0 + gc;0
hC;0 + g~

2
w

) bw ( )

2
w

(G.29)

from above yields

hC
c;0 + gc;0 (a (L)

=
+

Et rt+1

aw (1))
1
aw (1)) w
t +
2

(G.26)

))

hC
c;0 + gc;0 (a (L)

1) h +

7
j 7
5

hC
c;0 + gc;0 (1

+

Et rt+1

1 aw (L)

hC
c;0 + gc;0 ( + a (L) ( ct

0

t j

hC
c;0 + gc;0

ct+1 j

hC
c;0 + gc;0

1) h

3

ct+1

hC
hC
ct j
c;j+1 + gc;j+1
c;j
P1
hC;j+1 hC;j + g~ ;j+1
t+1 +
j=0

;0

=

Inserting the formula for (

j=0

hC
c;0 + gc;0

1) h +

hC;0 + g~

+

Et rt+1

+(
P1
0

is

1

w

)
)

2
w
2

+

hC
c;0 + gc;0

1 (1

) bw ( )

2

gc ( )
aw ( )
(a (L) aw (L)) ( ct
1
aw ( )
gc ( )
aw ( )
w
+
(1 aw (1)) (
)
1
aw ( )
1
covw (rm;t+1 ; mt+1 )
varw (rm;t+1 )
2
22

2
w

(G.30)

)

(G.31)

where, from above,
gc ( )
aw ( ) 2 2
~ ( )2
w+g
1
aw ( )
gc ( )
aw ( )
( 1 + (1
) bw ( ))
1
aw ( )

varw (rm;t+1 ) =
covw (rm;t+1 ; mt+1 ) =

G.3

(G.32)
2
w

(G.33)

Price/dividend ratio
hC
c;j+1

hC
c;j

=

hC;j+1

hC;j

=

hC
c;0 + gc;0
g~

;j+1

h

;0

gc;j+1

(G.34)
(G.35)

=g

(G.36)

;1

hc;0 =

gc ( ) 1
(
1
aw ( )

)

hc;j

gc ( ) 1
(
1
aw ( )

)

=

1 aw
j

1

+ gc;1

1

(G.37)

j

(G.38)

1

So the standard deviation of the pricing kernel is now
pdC
t

=

1
X

hC
c;j ct

j

+g

(G.39)

;1 t

j=0

var pdC
t

=

gc ( ) 1 (
1
aw ( ) 1
4

corr (pdt ; pdt

G.4

4)

1

=

)

2

2
2

1

gc ( ) 1 (
aw ( ) 1

var

)

2

+ g 2;1

(G.40)

2

1

2

pdC
t

(G.41)

Results

Table A1 reports an alternative version of table 1 in which we use the more sophisticated model of dividends
that are cointegrated with consumption growth. Since the consumption process is unchanged, there is no
e¤ect on the worst-case model of consumption. The only di¤erence between table A1 and table 1 is that
they use di¤erent models of dividends and hence have di¤erent implications for equity returns.
The mean and standard deviation of returns are both slightly reduced – the mean is lower by 5 basis
points and the standard deviation by 14 basis points. The small reduction is due to the fact that gc ( ) =
0:993. The di¤erence between the returns under the two models of dividends depends purely on that term
being di¤erent from 1. The fact that it is not (which is a consequence of cointegration) is why the returns
are essentially unchanged. The autocorrelation and standard deviation of the price/dividend ratio are
also numerically nearly identical to what is obtained in table 1. Finally, the bottom two rows of table
A1 con…rm that the model is calibrated here so that the standard deviation of dividend growth and the
23

correlation between dividend growth and consumption growth is identical to the data (the data moments
are drawn from Bansal and Yaron (2004), as is the case with our other empirical targets).

24

Table A1: Asset pricing moments for the white-noise benchmark – extended dividend model
Fundamental parameters
Implied worst-case
σw
Cons. vol. point est.
σ
0.01470
0.01465
w
b(β) Long-run vol. point est.
b (β)
2.449
1
μ
0.0073
μ
Mean cons. growth
0.0045
w
β
Time discount
0.997
θ
0.99021
Standard Epstein–Zin / robust-control
λ
Ambiguity aversion
106.8
b
0.01465
α
RRA (implied by λ)
4.73
0
b(β)
γ
Leverage
4.806
1
Asset pricing moments (annualized)
Model
std(M)
0.30
E[R-Rf]
6.28
std(r)
19.27
AC1(PD)
0.95
std(P/D)
0.19
std(Δd)
11.49
corr(Δd,Δc)
0.55

Data
N/A
6.33
19.42
0.81
0.29
11.49
0.55

Notes: see table 1. Results for the case where dividends are cointegrated with consumption but have transitory error. The
standard deviation of dividends and correlation with consumption growth are taken from Bansal and Yaron (2004).

Table A2. Probability of rejecting the pricing model – extended results
Rejection probs. (5% critical value, H0=worst-case model)
50 years
100 years
250 years
500 years
Ljung–Box
5.1%
5.1%
5.3%
5.7%
ARMA(1,1)
3.7%
4.4%
9.0%
27.3%
ARMA(1,1), fixed mean
5.4%
6.8%
13.3%
35.0%
Newey–West(5)
6.0%
8.4%
24.9%
58.7%
Newey–West(10)
5.9%
8.6%
23.2%
48.3%
Newey–West(20)
5.8%
8.8%
23.5%
48.6%

1000 years
6.6%
77.9%
82.3%
89.7%
75.6%
73.4%

Notes: Rejection probabilities are obtained by simulating the distributions of the three statistics in 50- and 100-year simulations
of the cases where consumption growth is generated by the worst-case and white-noise models and asking how often the
statistics in the latter simulation are outside the 95% range in the former simulation. The numbers in parentheses in the
Newey–West rows are lag orders.

Table A3: Asset pricing moments for the white-noise benchmark – high discounting
Fundamental parameters
Implied worst-case
σw
Cons. vol. point est.
σ
0.01472
0.01465
w
b(β) Long-run vol. point est.
b (β)
1.353
1
μ
0.0033
μ
Mean cons. growth
0.0045
w
β
Time discount
0.987
θ
0.97821
Standard Epstein–Zin / robust-control
λ
Ambiguity aversion
13.88
b
0.01465
α
RRA (implied by λ)
6.65
0
b(β)
γ
Leverage
4.806
1
Asset pricing moments (annualized)
Model
std(M)
0.25
E[R-Rf]
6.33
std(r)
18.10
E[rf]
5.94
std(rf)
0.26
AC1(PD)
0.92
std(P/D)
0.10
E[y10-rf]
-7.8bp
EIS estimate
0

Standard EZ
0.19
2.75
14.08
6.40
0
N/A
0
0
N/A

Data
N/A
6.33
19.42
0.86
0.97
0.81
0.29
N/A
0.14

Notes: see table 1. This table uses a higher rate of time preference – 5 percent per year. λ is then reduced to a value low
enough to match the equity premium, which also implies a higher value of risk aversion.

