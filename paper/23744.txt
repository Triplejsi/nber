NBER WORKING PAPER SERIES

EDUCATION TECHNOLOGY:
AN EVIDENCE-BASED REVIEW
Maya Escueta
Vincent Quan
Andre Joshua Nickow
Philip Oreopoulos
Working Paper 23744
http://www.nber.org/papers/w23744

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
August 2017

We are extremely grateful to Caitlin Anzelone, Rekha Balu, Peter Bergman, Brad Bernatek, Ben
Castleman, Luke Crowley, Angela Duckworth, Jonathan Guryan, Alex Haslam, Andrew Ho, Ben
Jones, Matthew Kraft, Kory Kroft, David Laibson, Susanna Loeb, Andrew Magliozzi, Ignacio
Martinez, Susan Mayer, Steve Mintz, Piotr Mitros, Lindsay Page, Amanda Pallais, John Pane,
Justin Reich, Jonah Rockoff, Sylvi Rzepka, Kirby Smith, and Oscar Sweeten-Lopez for providing
helpful and detailed comments as we put together this review. We also thank Rachel Glennerster
for detailed support throughout the project, Jessica Mardo and Sophie Shank for edits, and to the
Spencer Foundation for financial support. Any errors or omissions are our own. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2017 by Maya Escueta, Vincent Quan, Andre Joshua Nickow, and Philip Oreopoulos. All
rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without
explicit permission provided that full credit, including © notice, is given to the source.

Education Technology: An Evidence-Based Review
Maya Escueta, Vincent Quan, Andre Joshua Nickow, and Philip Oreopoulos
NBER Working Paper No. 23744
August 2017
JEL No. I20,I29,J24
ABSTRACT
In recent years, there has been widespread excitement around the potential for technology to
transform learning. As investments in education technology continue to grow, students, parents,
and teachers face a seemingly endless array of education technologies from which to choose—
from digital personalized learning platforms to educational games to online courses. Amidst the
excitement, it is important to step back and understand how technology can help—or in some
cases hinder—how students learn. This review paper synthesizes and discusses experimental
evidence on the effectiveness of technology-based approaches in education and outlines areas for
future inquiry. In particular, we examine RCTs across the following categories of education
technology: (1) access to technology, (2) computer-assisted learning, (3) technology-enabled
behavioral interventions in education, and (4) online learning. While this review focuses on
literature from developed countries, it also draws upon extensive research from developing
countries. We hope this literature review will advance the knowledge base of how technology can
be used to support education, outline key areas for new experimental research, and help drive
improvements to the policies, programs, and structures that contribute to successful teaching and
learning.
Maya Escueta
Teachers College
Columbia University
525 W 120th St
New York, NY
10027
mme17@tc.columbia.edu
Vincent Quan
Abdul Latif Jameel Poverty Action Lab,
North America (J-PAL North America)
400 Main Street, E19-201
Cambridge, MA 02142
quanv@mit.edu

Andre Joshua Nickow
Northwestern University
Department of Sociology
1810 Chicago Ave.
Evanston, IL 60208
a-nickow@northwestern.edu
Philip Oreopoulos
Department of Economics
University of Toronto
150 St. George Street
Toronto, ON M5S 3G7
CANADA
and NBER
philip.oreopoulos@utoronto.ca

1. Introduction

Technological innovation over the past two decades has indelibly altered today’s
education landscape. Revolutionary advances in information and communications technology
(ICT)—particularly disciplines associated with computers, mobile phones, and the Internet—
have precipitated a renaissance in education technology (ed-tech), a term we use here to refer to
any ICT application that aims to improve education. In the United States, the market for PreK-12
software alone had exceeded $8 billion1, and a recent industry report projects an estimated value
of $252 billion for the global ed-tech industry by 2020.2 Governments, schools, and families
increasingly value technology as a central part of the education process, and invest accordingly.3
In the coming years, emerging fields like machine learning, big data, and artificial intelligence
will likely compound the influence of these technologies even further, expanding the already
dizzying range of available education products, and speeding up cycles of learning and
adjustment.
Collectively, these technologies offer the potential to open doors and build bridges by
expanding access to quality education, facilitating communication between educators, students,
and families, and alleviating frictions across a wide variety of educational contexts from early
childhood through adulthood. For example, educational software developers work to enable
educators to deliver the latest learning science advances to schools in inner cities and remote
rural areas alike. The proliferation of cell phones and growing ease in connecting them to

1

SIIA, 2015. http://www.siia.net/Press/SIIA‐Estimates‐838‐Billion‐Dollars‐US‐Market‐for‐PreK‐12‐Educational‐Software‐and‐
Digital‐Content.
2
Morrison, 2017. https://www.forbes.com/sites/nickmorrison/2017/05/09/google‐leapfrogs‐rivals‐to‐be‐classroom‐
king/#32966ae927a6.
3 Bulman and Fairlie, 2016.

2

Internet-based information systems has enabled the scaling of automated text messaging systems
that aim to inform, simplify, and encourage students and their parents as they traverse difficult
sticking points in education, like the transition to college. And online educational institutions
may bring opportunities to earn degrees to students who would otherwise be constrained by
work, families, disabilities, or other barriers to traditional higher education.
But the rapid proliferation of new technologies within education has proved to be a
double-edged sword. The speed at which new technologies and intervention models are reaching
the market has far outpaced the ability of policy researchers to keep up with evaluating them.
The situation is well-summarized by a recent headline: “Ed-Tech Surges Internationally—and
Choices for Schools Become More Confusing.”4 While most agree that ed-tech can be helpful
under some circumstances, researchers and educators are far from a consensus on what types of
ed-tech are most worth investing in and in which contexts.
Furthermore, the transformations associated with ed-tech are occurring in a context of
deep and persistent inequality. Despite expanding access to some technologies, the digital divide
remains very real and very big. While 98 percent of children in United States households with
incomes exceeding $100,000 per year have a computer at home, only 67 percent of children in
households with incomes lower than $25,000 have them.5 Even when disadvantaged students can
physically access technology, they may lack the guidance needed for productive utilization—a
“digital-use divide.”6 Depending on design and implementation, education technologies could
alleviate or aggravate existing inequalities. Equity considerations thus add another layer to the
need for caution when implementing technology-based education programs.

4

Molnar, 2017. https://marketbrief.edweek.org/marketplace‐k‐12/ed‐tech‐surges‐internationally‐choices‐schools‐become‐
confusing/.
5
Bulman and Fairlie, 2016.
6
Brotman, 2016. https://www.brookings.edu/blog/techtank/2016/01/28/the‐real‐digital‐divide‐in‐educational‐technology/.

3

Of course, not every intervention model can be evaluated, and the extent of success
inevitably varies across educational approaches and contexts even within well-established fields.
But the speed and scale with which many ed-tech interventions are being adopted, along with the
enormous impact they could have over the next generation, demand a closer look at what we
know. To confront this issue, the present review takes stock of rigorous quantitative studies on
technology-based education interventions that have been conducted so far, with the goal of
identifying policy-relevant insights and highlighting key areas for future inquiry. In particular,
for reasons explained in the following section, we assembled what we believe to be a
comprehensive list of all publicly available studies on technology-based education interventions
that report findings from studies following either of two research designs, randomized control
trials or regression discontinuity designs, and based our analyses primarily on these studies.
In the next section, we discuss our literature review methodology in greater depth.
Sections 3-6 constitute the core of the review—these sections respectively synthesize the
evidence on the four topic areas that encapsulate the overwhelming majority of studies that we
included: 1) access to technology, 2) computer-assisted learning, 3) online courses, and 4)
behavioral interventions. Section 7 offers concluding observations and considers several of the
priority areas for future research that we consider vital to ongoing efforts at more effectively and
equitably leveraging technology for learning.

4

2. Literature Review Methodology

Several recent reviews have synthesized empirical evidence relevant to aspects of ed-tech
policy.7 The present paper aims to contribute to these efforts in two main ways. First, while
existing reviews have covered subsets of ed-tech, no recent review has attempted to cover the
full range of ed-tech interventions. In particular, no previous review to our knowledge brings
together computer- and internet-based learning on one hand and technology-based behavioral
interventions on the other. Of course, expanding our scope must come with some sacrifice—it
would not be feasible to meaningfully integrate all studies relating to all areas of ed-tech into a
single paper. Instead, we focus on studies presenting evidence from randomized control trials
(RCT) and regression discontinuity designs (RDDs). Our core focus on RCT- and RDD-based
studies constitutes a second unique contribution of this review—we argue that, in addition to
helping us define sufficiently clear and narrow inclusion conditions, a focus on RCTs and RDDs
adds a productive voice to broader and more methodologically-diverse policy research dialogues
in an environment characterized by complex tangles of cause and effect.
Why focus on RCTs and RDDs? In the fields of program evaluation and applied
microeconomics, RCTs—when properly implemented—are generally considered the strongest
research design framework for quantitatively estimating average causal effects.8 RCTs are
randomized experiments, studies in which the researcher randomly allocates some participants
into one or more treatment group(s) subjected to an intervention, program, or policy of interest,
and other participants into a control group representing the counterfactual—what would have

7
8

Bulman and Fairlie, 2016; Lavecchia, Liu, and Philip Oreopoulos, 2014; Means et al., 2010.
Angrist and Pischke, 2008.

5

happened without the program.9 Randomization assures that neither observable nor unobservable
characteristics of participants predict assignment, “and hence that any difference between
treatment and control…reflects the impact of the treatment.”10 In other words, when done
correctly, randomization ensures that we are comparing apples to apples and allows us to be
confident that the impacts we observe are due to the treatment rather than some other factor. Yet
as a result of cost, ethics, and a variety of other barriers, RCTs are not always possible to
conduct.
Over the past several decades, methodologists have developed a toolkit of research
designs, known broadly as quasi-experiments, that aim to approximate experimental research to
the greatest extent possible using observational data. Commonly used examples include
instrumental variable, difference-in-difference, and propensity-score matching designs.
Regression discontinuity designs (RDDs) are quasi-experiments that identify a well-defined
cutoff threshold which defines a change in eligibility or program status for those above it—for
instance, the minimum test score required for a student to be eligible for financial aid. While
very high-scoring and very low-scoring students likely differ from one another in ways other
than their eligibility for financial aid, “it may be plausible to think that treatment status is ‘as
good as randomly assigned’ among the subsample of observations that fall just above and just
below the threshold.”11 So, when some basic assumptions are met, the jump in an outcome
between those just above and those just below the threshold can be interpreted as the causal
effect of the intervention in question for those near the threshold.12

9

Duflo, Glennerster, and Kremer 2008; Glennerster and Takavarasha, 2013.
Banerjee and Duflo, 2017.
11
Lee and Card, 2008.
12
Imbens and Lemieux, 2008; Thistlewaite and Campbell, 1960.
10

6

RDDs can only be used in situations with a well-defined threshold that determines
whether a study participant receives the intervention. We chose to include them but not other
quasi-experimental designs because they can be as convincing as RCTs in their identification of
average causal effects. With minimal sensitivity to underlying theoretical assumptions, RDDs
with large samples and a well-defined cut-off produce estimated program effects identical to
conducting RCTs for participants at the cut-off.13 Although RDDs are quasi-experiments, in the
remainder of this review we refer to the RCTs and RDDs included in this review as experimental
research for simplicity. We chose to focus on RCTs and RDDs not because we believe they are
inherently more valuable than studies following other research designs, but because we felt that
the policy literature on ed-tech is flooded with observational research and could benefit from a
synthesis of evidence from the designs most likely to produce unbiased estimates of causal
effects. Furthermore, we introduce, frame, and interpret the experimental results in the context of
broader observational literatures.
RCTs and RDDs estimate the impact of a program or policy on outcomes of interest. But
the estimates they come up with are sometimes difficult to compare with one another given that
studies test for impact on different outcomes using different measurement tools, in populations
that differ in their internal diversity. While these differences can never be completely eliminated
and effect sizes must always be considered in the contexts within which they were identified,
standard deviations offer a roughly comparable unit that can give us a broad sense of the general
magnitude of impact across program contexts. Standard deviations essentially represent the
effect size relative to variation in the outcome measurement. Economists studying education
generally follow the rule of thumb that less than 10 percent of a standard deviation is small, 10

13

Berk et al., 2010; Cook and Wong, 2008; Shadish et al., 2011.

7

percent to 25 percent is encouraging, 25 to 40 percent is large, and above 40 percent is very
large. We report effect sizes in standard deviations whenever the relevant data is available below
to facilitate comparison, while cautioning that these effect sizes must be considered in context to
be meaningful.
We also limited our core focus to studies conducted within developed countries, although
we touch on research conducted in developing countries where relevant to the discussion. After
considering both literatures, we determined that the circumstances surrounding the ed-tech
interventions that have so far been experimentally studied differed too greatly across developed
and developing country education systems to allow for integrating findings from both in a way
that would yield meaningful policy implications. Our decision to focus on the developed rather
than developing world in particular was driven by this review’s goal of analyzing experimental
research on the full range of ed-tech interventions. While experimental policy and evaluation
literature on certain classes of ed-tech literature like computer distribution and computer-assisted
learning have already begun to flourish in the developing world, experimental research on other
areas like technology-based behavioral interventions is less developed there so far.
Our first task in constructing this review was thus to collect all publicly available studies
using RCT or RDD designs within developed countries that estimate the effects of an ed-tech
intervention on any education-related outcome. To locate the studies, we assembled a list of
search terms, and used these to search a range of academic search engines, leading economics
and education journals, and evaluation databases. To ensure that no relevant studies had been
omitted, we followed backward and forward citations for all included articles and conducted
consultations with leading researchers, evaluators, and practitioners in the field. Given that much
of the relevant research is recent and has been conducted from both within and outside of

8

academia—as well as to avoid publication bias—we chose not to exclude any studies based on
their publication status. Our final list of included studies consists of published academic articles,
working papers, evaluation reports, and unpublished manuscripts. See our references section for
a complete list of studies we reviewed.
Once the articles had been assembled, we divided them into the four categories into
which we felt that they most naturally clustered: access to technology, computer-assisted
learning, technology-based behavioral interventions in education, and online courses. Although
not all studies fit neatly into these categories and there is some overlap, we felt that these four
best encapsulated the differences in the studies’ underlying themes, motivations, and theories of
change. The full list of studies is contained—separated by category—in Tables 1-4.
Within each category, we closely read all studies and organized them further according to
the approach of the intervention evaluated. We then considered each study’s findings in light of
the others’, taking into account to the greatest extent possible variations in both the nature of the
programs evaluated, the contexts in which they are implemented, and the specific research
designs with which they study. Where relevant, we also contrasted findings from these studies
with findings from observational research and from developing countries. In the remainder of the
review, we present the results of this analysis.

3. Access to Technology

3.1 Background and Context
A natural starting point when exploring the effects of ed-tech is to consider what happens
when students are provided with increased access to computers or the Internet. Since the
9

acceleration in technology’s incorporation into the classroom first took off during the 1990s,
governments and other stakeholders have invested substantial resources in an array of computer
and internet distribution and subsidy initiatives. We identified 11 RCT and 4 RDD papers14 on
such initiatives, presented in Table 1. Overall, the interventions were effective at increasing use
of computers and improving computer skills. These outcomes are noteworthy given the logistical
challenges of technology distribution—particularly within lower-capacity and otherwise
disadvantaged delivery contexts—and the potential reluctance of students and educators to
change their routines by incorporating the technologies. Results were more mixed for academic
achievement and other learning outcomes, but the research suggests areas of promise here as
well, particularly computer distribution at the postsecondary level and distribution at the K-12
level when combined with additional learning software. In the remainder of this section, we
provide a brief overview of the policy context of technology access initiatives before taking a
closer look at the empirical findings and discussing implications for future research priorities.
A large and growing share of students in developed countries can now access computers
with high-speed internet at home and at school. Today, nearly three-quarters of American adults
have broadband access at home—a remarkable increase from only 1 percent of adults in 2000.15
Among adults with children, the rate of at-home broadband access is even higher. A 2015 Pew
Research Center study found that 82.5 percent of American households with school-age children
have broadband access.16

14
The study of the effects of Internet speed in England by Faber et al. is an exception in that it does not directly evaluate a
technology access initiative, but instead leverages a coincidental divergence in internet speeds for an RDD examination of
effects on education. But the study nonetheless shares the rationale of the rest in attempting to identify the educational effects
of improved Internet access.
15
Pew Research Center, 2017. http://www.pewinternet.org/fact‐sheet/internet‐broadband/.
16
Horrigan, 2015. http://www.pewresearch.org/fact‐tank/2015/04/20/the‐numbers‐behind‐the‐broadband‐homework‐gap/.

10

But damaging holes in coverage remain. Approximately 5 million school-age children do
not have a broadband internet connection at home,17 potentially leading to a “homework gap”18
and other compounding layers of disadvantage. Students without computers or Internet are likely
to be the students who could most benefit from a boost in human capital, as they are much more
likely to come from lower-income households: “In the United States, 98 percent of the 12 million
schoolchildren living in households with $100,000 or more in income have access to a computer
at home, but only 67 percent of the 12 million schoolchildren living in households with less than
$25,000 in income have access.”19 And underrepresented minority students disproportionately
lack access: only 78 percent of African-American and Hispanic schoolchildren have computers
at home, in contrast to 92 percent of white schoolchildren.20 There is also a stark technology
access divide between rural and urban areas.21
Several program models have emerged to address these gaps in access to technology. One
model that has recently risen to prominence has been “one-to-one” technology, “in which all the
students in a class, grade level, school, or district are provided computers for use throughout the
school day and, in some cases, at home”.22 Several one-to-one initiatives have been implemented
at large scales. For instance, the state of Maine provides all of its middle and high school
students with laptops for use during the school year.23 More recently, some school districts
around the country have been pairing students up with tablets.24 One-to-one distribution has also

17

Anderson, 2017. http://www.pewresearch.org/fact‐tank/2017/03/22/digital‐divide‐persists‐even‐as‐lower‐income‐
americans‐make‐gains‐in‐tech‐adoption/.
18
Kang, 2016. https://www.nytimes.com/2016/02/23/technology/fcc‐internet‐access‐school.html.
19
Bulman and Fairlie, 2016.
20
Ibid, 263.
21
West and Karsten, 2016. https://www.brookings.edu/blog/techtank/2016/07/18/rural‐and‐urban‐america‐divided‐by‐
broadband‐access/.
22
Zheng et al., 2016.
23 Maine Learning Technology Initiative (MLTI), http://maine.gov/doe/mlti/about/history/index.html
24
McLester, 2012. https://www.districtadministration.com/article/one‐tablet‐child‐0.

11

caught on within developing countries, and governments as diverse as those of Peru, Kenya,
Turkey, and India have invested in variations of such programs.25 One particularly prominent
civic-led one-to-one initiative has been the One Laptop Per Child (OLPC) program, which aims
to “empower the children of developing countries to learn by providing one connected laptop to
every school-age child.”26 OLPC has distributed laptops to disadvantaged students in roughly a
dozen developing countries, along with two US cities. 27
Other initiatives have provided schools with subsidies to buy computers or software, or to
acquire or improve internet connections. In 1997, the United States federal government launched
its largest ever ed-tech program to connect U.S. schools and classrooms to the internet. Known
as E-Rate, the program has connected 97 percent of U.S. classrooms to the internet. In 2013,
President Barack Obama announced a new initiative known as ConnectED, which sought to
bring high-speed broadband to 99 percent of K-12 students by 2018.28 The initiative helped
provide an additional 20 million students29 with in-classroom access to broadband. Both the
private and public sectors have invested heavily to increase broadband access around the
country. Since 2009, more than 115,000 miles of network infrastructure have been built at a cost of more
than $260 billion.30

25

Trucano, 2013. http://blogs.worldbank.org/edutech/big‐educational‐laptop‐and‐tablet‐projects‐ten‐countries; BBC, 2013.
http://www.bbc.com/news/world‐asia‐india‐21738237; Simhan, 2011.
http://www.thehindubusinessline.com/economy/policy/distribution‐of‐free‐laptops‐to‐tn‐students‐from‐sept‐
15/article2123738.ece.
26
One Laptop per Child, http://laptop.org/en/vision/mission/.
27
Ibid. http://one.laptop.org/stories.
28
Benton Foundation, 2013. https://www.benton.org/initiatives/e‐rate?page=2%2C1%2C1.
29
Obama White House Archives, https://obamawhitehouse.archives.gov/issues/education/k‐12/connected.
30
Council of Economic Advisers, 2016.
https://obamawhitehouse.archives.gov/sites/default/files/page/files/20160308_broadband_cea_issue_brief.pdf.

12

3.2 Investing in Access
Given the wave of investments and policy interest in access to technology, what have
been the effects of access programs? With only a handful of RCT and RDD papers on the
subject, the experimental literature on its own cannot say much definitively. However, these
studies provide valuable suggestive insights, particularly when viewed within the context of the
broader quasi-experimental and observational literatures. In particular, seven articles report on
RCTs that were conducted by Robert Fairlie and collaborators: two31 on an RCT conducted in 15
California middle and high schools, and five32 on an RCT conducted in a community college33 in
California. Four papers are RDD studies on the educational effects of programs subsidizing
household computers for students in Romania,34 school computers in the Netherlands,35 and
internet access in California,36 plus a study of coincidental internet speed variation in England.37
Information on these studies is presented in Table 1.
Despite the differences in interventions and settings explored within the studies, the
papers consistently report success in programs’ intended proximate outcomes—distributing
computers, increasing time spent using computers, or decreasing time spent accessing computers
(e.g., less time waiting for computers in labs to become available). For example, among students
in California who were randomly assigned to receive free laptops, computer ownership
reportedly increased by 55 percentage points, computer usage reportedly increased by 2.5 more
hours per week, and the likelihood of at-home internet connection increased by 25 percentage

31

Fairlie and Kalil, 2017; Fairlie and Robinson, 2013.
Fairlie, 2012A; Fairlie, 2012B; Fairlie and Bahr, 2017; Fairlie and Grunberg, 2014; Fairlie and London, 2012.
33 The term “community college” in the US context generally refers to postsecondary institutions that provide only two‐year
degrees, traditionally catering in particular to disadvantaged or nontraditional students.
34
Malamud and Pop‐Eleches, 2011.
35
Leuven et al., 2007.
36
Goolsbee and Guryan, 2006.
37
Faber et al., 2015.
32

13

points relative to those who were not assigned to receive free laptops.38 Though they may seem
intuitive, these findings are noteworthy considering that the significant resources required to
expand computer and Internet access may be wasted because of the logistical difficulties of
distribution. And students and teachers facing constraints on time and cognitive capacity may be
reluctant to adopt technologies in the ways intended by providers.
Findings of effects on learning outcomes have been more mixed, although they do
suggest some promising possibilities—in particular for students at the post-secondary level. As
reported in four recent papers,39 an intervention that distributed laptops to low-income students at
a community college in Northern California saw a range of modest but positive effects, with an
overall impact on an academic performance index of 0.137 standard deviations40. The academic
performance index is a measure the authors constructed to aggregate four separate outcomes:
course success rate, the likelihood of taking a course for a grade, the likelihood of taking a
transfer course for a four-year college, and graduation rate. Further analysis suggested that the
benefits occurred not by increasing the time that students spend using computers, but by saving
them time costs involved with using computers in the college’s computer labs. Two separate
papers reporting on the same study also find that positive academic effects are significantly
stronger for minority than for non-minority students41, and that the program increased computer
skills most strongly for minorities, women, lower-income, and younger students.42 However, a
follow-up study showed no impact on earnings seven years after the program was
implemented.43

38

Fairlie and Robinson, 2013.
Fairlie, 2012A; Fairlie, 2012B; Fairlie and Grunberg, 2014; Fairlie and London, 2012.
40
Fairlie and London, 2011.
41
Fairlie, 2012A.
42
Fairlie, 2012B.
43
Fairlie and Bahr, 20167.
39

14

The few primary- and secondary-level computer distribution programs that have been
experimentally evaluated have yielded less evidence of positive impact. In the only such study
we are aware of that met our inclusion criteria, 6th to 10th grade students in 15 middle and high
schools across five California districts were randomly selected to receive free computers. Overall
findings suggest that “increasing access to home computers among students who do not already
have access is unlikely to greatly improve educational outcomes, but is also unlikely to
negatively affect outcomes.” In particular, no significant impact—positive or negative—was
found on homework time, grades, standardized test scores, attendance, or several other
outcomes.44 One intervention that subsidized computers for households in Romania and another
that subsidized schools in purchasing computers and software in the Netherlands both found
negative impacts on achievement outcomes, with the Romania study suggesting that this could
be in part a result of the students spending more time playing games.45 However, the negative
effects in the Netherlands study are weak and generally low, and in the Romania study negative
impacts on academic achievement are accompanied by positive impacts on computer skills and
cognitive test scores. And studies that respectively looked at internet subsidies in the US46 and
connection speed in England47 similarly found no evidence of substantial positive or negative
impact on academic achievement.
Where do these findings stand within the broader literature on interventions related to
technology access? Experimental research conducted in the developing world have, for the most
part, come up with similar results. Interventions giving computers to schools in Colombia,48 One

44

Fairlie and Robinson, 2013.
Leuven et al., 2007; Malamud and Pop‐Eleches, 2011.
46
Goolsbee and Guryan, 2006.
47
Faber et al., 2015.
48
Barrera‐Osorio and Linden, 2009; Rodriguez et al., 2015 find a positive impact from the same program after more time had
elapsed, but the latter study is primarily non‐experimental.
45

15

Laptop Per Child efforts in Peru,49 and tablets distributed to students in Kenya50 showed no
impact on learning outcomes in the experimental studies, while one of the interventions in Peru
yielded positive effects on cognitive outcomes and an intervention in China51 significantly
improved math scores. Perhaps instructively, the intervention in China was the only one of the
computer distribution initiatives in which computers were reliably equipped with educational
software that was actually used by the students.
Observational and quasi-experimental studies in both developed and developing countries
have, on the other hand, tended to find more positive results. One recent review of observational
and experimental studies on one-to-one programs implemented between 2001 and 2015 finds
that an expansive range of positive impacts have been documented, including “…increased
academic achievement in science, writing, math, and English; increased technology use for
varied learning purposes; more student-centered, individualized, and project-based instruction;
enhanced engagement and enthusiasm among students; and improved teacher-student and home–
-school relationships,”52 although many of the studies reviewed are not equipped for rigorous
causal inference.

3.3 Looking Forward
What insights does the experimental literature bring to current policy debates and
considerations for future research? First, more research is needed on efforts to improve access to
technology at the post-secondary level. As helpful as computers and the Internet may be for

49

Beuermann et al., 2015; Cristia et al., 2012.
Piper et al., 2016.
51
Mo et al., 2015.
52
Zheng et al., 2016.
50

16

primary and secondary students, college demands a variety of more complex tasks that, in many
cases, truly necessitate the need for a computer. Although students enrolled in colleges are more
likely to have computer access,53 computer ownership and Internet access are far from universal
among lower-income and otherwise disadvantaged students, and accessing computers at labs
may waste scarce time. Notwithstanding the lack of impact found on earnings, Fairlie’s research
has shown promising results in this area, but a single study at a single college is far from
sufficient for making policy claims.
Second, while the few technology access programs that have been experimentally
evaluated at the primary and secondary levels show few positive effects on academic
achievement, improving access in combination with other activities may yield better results. For
instance, the survey conducted for the Romania study discussed above found some suggestive
evidence that the negative effects of home computers on grades was attenuated with certain
parental rules—approaches to regulating children’s computer use or providing more structure
and guidance for how the computer should be used may be worth studying. And, although
increasing access to computers and Internet may not on their own measurably improve academic
achievement, they have been successful in increasing the time and/or ease of use. This
observation, in combination with the positive results found for educational software discussed in
the following section, suggests that the most promising policy models may be those that integrate
hardware distribution with more specific learning programs. We turn to discussing such
programs in the following section.

53

Anderson, 2015. http://www.pewinternet.org/2015/10/29/the‐demographics‐of‐device‐ownership/; School Guides, 2014.
http://www.schoolguides.com/College_News/Survey_reveals_how_much_college_students_rely_on_technology_643742.html
; MarketWatch, 2014. http://www.marketwatch.com/story/laptops‐move‐to‐the‐head‐of‐the‐class‐among‐college‐students‐
according‐to‐amd‐back‐to‐school‐survey‐2014‐07‐10.

17

Table 1

Author

Carter,
Greenberg,
Walker
(2016)

Faber,
SanchisGuarner,
and
Weinhardt
(2015)

Fairlie
(2012A)

Fairlie
(2012B)

Fairlie
(2014)

Data Source

Sample

Findings

Education
Setting

Prohibiting use
of computers
during a college
economics class

West Point student
outcomes data

50 classrooms
and 726
students in
West Point,
New York

Average final exam scores
among students assigned to
classrooms that allowed
computers were 0.18 SDs
lower than exam scores of
students in classrooms that
prohibited computers.

Postsecondary

Differences in
broadband
connection
speeds

Administrative test
score records,
telecommunication
network data, survey
microdata on student
time use and internet
use in England

580,000
residential
postcodes in
England

Null results, “precisely
estimated zero effect”

Primary &
secondary

One-to-one
laptop
distribution

Administrative data
from the original
application to the
college and baseline
survey of treatment
and control

286 students
community
college students
receiving
financial aid in
California

(1) 0.15 GPA difference (2)
6.5 percentage point
difference in course
completion rates (3) 8.6
percentage point difference
for course success rate (4)
No impact on graduation
rate

Postsecondary

One-to-one
laptop
distribution

Administrative data
from original
application to college
and administrative
data from Butte
College

286 students
community
college students
receiving
financial aid in
California

(1) ITT increase in highlevel computer skills of 17
percentage points (2)
Benefits appear to be the
strongest among young,
minority, low-income, and
female students

Postsecondary

One-to-one
laptop
distribution

Data from Current
Popular Survey
Computer and
Internet Use
Supplements by the
U.S. Bureau of Labor
Statistics and Census
Bureau and survey
data on time use

1123 children
enrolled in
grades 6-10 in
15 different
middle and high
schools in 5
school districts
in California

No evidence is found
indicating that personal
computers crowd out
homework time and effort
for disadvantaged boys
relative to girls. Home
computers also do not have
negative effects on
educational outcomes such
as grades, test scores,
courses completed, and

Middle &
High School

Intervention

18

tardies for disadvantaged
boys relative to girls.

One-to-one
laptop
distribution

Administrative
earnings data
collected by the
California State
Employment
Development
Department UI
system,
administrative
database of the
California
Community College
(CCC)
system and National
Student
Clearinghouse (NSC)

286 students
community
college students
receiving
financial aid in
California

The experiment does not
provide any evidence that
computer skills have shortor-medium run effects on
earnings.

Postsecondary

One-to-one
laptop
distribution

Administrative data
from the original
application to the
college, including
career goals, baseline
survey, and
administrative data
on all courses taken
by study participants

286 students
community
college students
receiving
financial aid in
California

4.5 percentage point
increase in transferable
courses enrollment than the
control group of students
not receiving free
computers

Postsecondary

Fairlie and
Kalil (2017)

Free home
computers for
students in
grades 6-10

Baseline survey,
administrative data
on school
participation for all
children covering the
entire academic year,
and follow-up survey

(1) No negative effects on
social development found
(2) increase in online social
networking, but also more
in-person friend interaction

Middle &
High School

Fairlie and
Kalil54
(2016)

Free home
computers for
students in
grades 6-10

School-provided
administrative data,
baseline and follow
up survey

No find evidence that home
computers increase
cyberbullying.

Middle &
High School

Fairlie and
Bahr (2017)

Fairlie and
Grunberg
(2014)

54

This is related working paper to Fairlie and Kalil, 2017.

19

1123 children
enrolled in
grades 6-10 in
15 different
middle and high
schools in 5
school districts
in California.
1123 children
enrolled in
grades 6-10 in
15 different
middle and high
schools in 5
school districts
in California.

Fairlie and
London
(2012)

Fairlie and
Robinson
(2013)

Goolsbee
and Guryan
(2006)

Kirabo,
Jackson,
and
Makarin
(2016)

Leuven et
al. (2007)

Administrative data
provided by the
college, baseline
survey, and follow up
survey

286 students
community
college students
receiving
financial aid in
California

(1) 0.14 SDs improvement
"summary index of
educational outcomes" that
includes variables like
grades and degree
completion (2) benefits
strongest for students who
live farther from campus or
have a job

One-to-one
laptop
distribution

Administrative data
from schools, follow
up survey,
standardized test
scores, pretreatment
administrative data
and baseline survey

6-10th graders
in 15 middle
and high
schools in 5
districts in
California; vast
majority of
sample is
middle school
students

Null results

Middle &
High School

E-Rate, subsidy
for internet in
schools

Administrative data
on E-rate funding
applications. Stanford
Achievement Test
data

Every
California
public school

Null results on academic
outcomes. By the final year
of the sample, there were
approximately 68 percent
more Internet-connected
classrooms per teacher.

Primary,
Middle &
High School

Only providing teachers
with online access to
the lessons increased
students’ math achievement
by 0.06 of a SD, but
providing
teachers with online access
to the lessons along with
supports to promote their
use increased
students’ math achievement
by 0.09 of a SD.

Middle
School

(1) Null and mildly
negative results (2) Seems
especially detrimental for
girls' achievement

Primary

One-to-one
laptop
distribution

Middle school
math teachers
given access to
websites that
warehouse "off
the self"
instructional
materials

Administrative
records for teachers
and their students,
student results on the
math portion of the
Virginia Standards of
Learning (SoL)
assessment, teacher
survey data, and
student surveys

Subsidies for
computers and
software in
under-resourced
schools

Administrative data
on the numbers of
pupils of different
social backgrounds,
pupils’ results in
nationwide tests, and
school-level data of
the
share of female
teachers and teachers’
average age

20

Across all grade
levels, 59,186
Virginia
students were
enrolled in 62
Chesterfield
public schools;
In total, 50,569
students were
enrolled in 82
Henrico public
schools; and
18,264 students
were enrolled in
26 Hanover
public schools
267 schools in
the Netherlands
that had at least
70 percent of
pupils
belonging to the
disadvantaged
minority group
in 1998 and 551
schools that had
at least 70
percent of
pupils
belonging to the

Postsecondary

disadvantaged
minority group
in 1999

Malamud
and PopEleches
(2011)

Euro 200
program,
subsidy for
low-income
families with
schoolchildren
to buy
computers

Household survey,
child survey, untimed cognitive test,
and computer
test and self-reported
computer fluency

Over 3,000
households
from several
regions of
Romania

(1) Both positive and
negative effects (2) .25 -.33
SD reduction in
Math/English/Romanian
(3) .25 SD improvement in
computer skills (4) Some
evidence of improvement
in cognitive skills

Primary &
Secondary

4. Computer-Assisted Learning
4.1 Background and Overview
Computer and learning scientists have been working for decades to develop software to
deliver educational content, and the popularity of these programs has exploded in the wake of the
1990s’ ICT revolution. For the purposes of this review, we refer to initiatives relating to
educational software as computer-assisted learning (CAL) programs.55 CAL programs differ
from the technology distribution programs of the previous section in that they do not involve the
provision of hardware for general use, but instead center on “well-defined”56 use of specific
software packages. And they differ from the online courses discussed in the following section in
that they are software packages designed to develop particular skills, e.g., improving math

55
The programs discussed in this section are also frequently discussed under the rubric of “personalized learning”. While this
latter term is sometimes used as a synonym for CAL, we chose to use the term CAL in this paper since definitions of
personalized learning sometimes lack a technology component and because, while personalization is often a key goal of CAL,
CAL programs may vary in the extent to which they focus on it.
56
Rouse and Krueger, 2004.

21

computation or improving reading comprehension, rather than platforms through which to
administer courses. Hundreds of companies have entered the market to meet spiking demand
from educators and policymakers for CAL, resulting in the advent of a plethora of products being
used daily by millions of students worldwide. Yet, to date, decisions on whether to pursue CAL
and which CAL programs to use seem to have been based more on intuition than on hard
evidence. To what extent and under what circumstances are CAL programs effective? In this
section, we review the experimental literature on this question.
We identified 29 experimental studies of CAL programs in developed countries, all based
on RCTs. While CAL can conceivably include a wide range of program types from games to
research and networking tools, the CAL programs that have been evaluated experimentally
generally fall within the broad category of “intelligent tutoring systems,” i.e., software systems
that aim to help students practice particular skills.57 Taken together, the findings from these
studies suggest that CAL programs of the types evaluated in these studies show enormous
promise in improving learning outcomes, particularly when it comes to mathematics. Of the 29
studies included, only eight58 report no effect59 and one60 turned up negative effects. While these
eight studies evaluated programs attempting to improve a mix of language, math and other
outcomes, the majority of the studies finding positive effects (15 of 20) were focused on
improving math outcomes.61 Information on these studies is presented in Table 2.

57

Kulik and Fletcher, 2015.
Borman et al., 2009; Cabalo et al., 2007; Campuzano et al., 2009; Cavalluzo et al., 2012; Dynarski et al., 2007; Rouse and
Krueger, 2004; Rutherford et al., 2014; Van Kalveren et al., 2017. Of these eight, one (Rockoff 2015) specifically mentions that
the study was underpowered.
59
Campuzano et al., 2009 and Dynarski et al., 2007 represent notable exceptions to the overall pattern of findings. These
Department of Education studies evaluated roughly a dozen CAL programs and over two years and found a general pattern of
no effects. However, multiple programs are aggregated together in some of the analyses, and the multi‐program design
generally makes it difficult to interpret these results in the contexts of the other studies discussed here.
60
Pane et al., 2010.
61 Barrow et al., 2009; Beal et al., 2013; Hegedus et al., 2015; Karam et al., 2017; Kelly et al., 2013; Morgan and Ritter, 2002;
Pane et al., 2014; Ragosta, 1983; Ritter et al., 2007; Roschelle et al., 2010; Roschelle et al., 2016; Singh et al., 2011; Snipes et al.,
58

22

Of those evaluated, several interventions show especially strong promise, e.g., an
evaluation of a math homework program in Maine showed an effect size of 0.18 standard
deviations despite involving less than 30-40 minutes per week62, while a more intensive
software-based math curriculum intervention in Texas improved seventh and eighth grade math
scores by 0.63 and 0.56 standard deviations, respectively.63 Many of the CAL interventions
compare favorably with interventions like reduced class sizes, longer school days, and intensive
face-to-face tutoring. In the remainder of this section, we first discuss the way CAL programs are
hypothesized to improve learning by leveraging software to enable increased personalization of
learning. We then review findings from studies on CAL programs in math, considering models
from light-touch homework supplements to class curriculum changes to school-wide
personalized learning models, before turning to the few experimental studies on CAL reading
programs. Finally, we consider findings from the studies we included within the broader research
context, and highlight potentially promising directions moving forward.

4.2 Educational Software in and out of the Classroom
The most prominently discussed channel through which CAL is expected to improve
learning has been its potential to “personalize” education, i.e., to provide content that is better
suited to the learning needs of the student in question.64 Designers and evaluators of CAL
programs tend to focus on several particular ways in which the interventions can facilitate
increased personalization in learning. Perhaps most central here has been adaptivity—the

2015; Tatar et al., 2008; Wang and Woodworth, 2011. Pane 2014 only finds positive impacts on math outcomes in the second
year.
62
Roschelle et al., 2016.
63 Roschelle et al., 2010.
64
West, 2011.

23

increasingly sophisticated ability of CAL programs to harness emerging artificial intelligence
and machine learning techniques to model the cognitive processes of students and offer content
accordingly. When teaching a full classroom of students at different levels, a teacher can only
adapt so much—this has been a longstanding issue that education researchers have attempted to
overcome for decades. A variety of interventions not relying on technology have been evaluated
that enable students to spend dedicated time each day learning “at the right level,” and these
show a great deal of promise (e.g., Banerjee et al. 2007; 2015). Such efforts can better allow
students to master more basic concepts before moving on to more advanced concepts and to
practice more in areas where they are struggling and less in areas that they have picked up.
Aside from directly tailoring content toward students, CAL programs can help to
personalize learning by providing students with immediate or timely feedback. And they can
provide teachers with rapid and regular data that can be used to calibrate focus with individual
students, among other potential mechanisms of personalization. The program theories that guide
the interventions evaluated in the studies that we review typically include multiple of the above
dynamics in their respective visions.
While many CAL programs attempt to improve education by facilitating the increased
personalization of learning, these programs vary widely in how they do so. CAL programs can
range from light-touch interventions that provide practice opportunities outside of class, to more
intensive interventions that provide courses with entirely new curricula, to (in a few cases)
initiatives in which schools are organized entirely around CAL or CAL-like programs.
Beginning on the light-touch end, ASSISTments represents an especially promising
example. ASSISTments is a math homework platform released by the Worcester Polytechnic
Institute that does not require that schools adjust their curriculum or textbooks, and is available

24

free of charge.65 The program is designed to carry out “formative assessments,” i.e., to use “data
from students’ independent work to give them helpful feedback and guidance while enabling the
teacher to use the data to adjust instruction to meet students’ learning needs.”66 As students work
through individual problems, the computer informs them about whether their answer is correct
and offers guidance if necessary. Students are expected to benefit from the customized practice,
as well as from the rapid feedback of responses, and data supplied to teachers (in addition to, in
some cases, supplementary professional development to train the teachers on optimizing use of
ASSISTments). Two small-scale proof-of-concept studies67 found promising effects, but these
studies had samples numbering only in the dozens of students and implementation time
numbering only in the days.
More recently, however, a full-scale impact evaluation of an ASSISTments intervention
was conducted with a sample of 2,850 seventh-graders across 43 schools in Maine.68 The authors
found that the program improved math scores for treatment students by 0.18 standard deviations.
This impact is particularly noteworthy given that treatment students used the program on average
for less than ten minutes per night, three to four nights per week.69 It is worth noting that the
program depends on students’ ability to access a laptop or tablet. This is part of the reason that
this evaluation was conducted in Maine, given its state policy of lending laptops to students.
While this hurdle may raise some external validity concerns with regard to this particular study, a
variety of possibilities exist for enabling access in other states, especially given that software and
licensing are free so costs are otherwise low. Also noteworthy is that impact was significantly

65

Worcester Polytechnic Institute, 2016. https://www.assistments.org/
Roschelle et al., 2016.
67
Kelly et al., 2013; Singh et al., 2011.
68
Roschelle et al., 2016.
69
Ibid, 6.
66

25

stronger for students at or below median than for those above, with an effect size of 0.29
standard deviations.70
Second, some programs move beyond homework supplements and instead offer full
curricula. A prime example—perhaps the most prominent of all of the CAL products discussed
in this review—is the set of Cognitive Tutor products published by Carnegie Learning. The
company recommends 40 percent computer time and 60 percent class time.71 Unlike
ASSISTments, the Cognitive Tutor programs generally provided curricula for entire mathematics
courses, including lesson plans, textbooks, training for teachers, and detailed guidelines.
Through the tutor, students receive individualized instruction in the form of challenging
problems that reflect real-world situations, enabling students to move from concrete to abstract
thinking.72 We identified nine papers reporting on experimental studies on Cognitive Tutor
programs in a variety of locations, including California, Hawaii, Maryland, and Oklahoma.73
While earlier papers were narrow in scope, a recent experiment in eight states has sought to
increase the external validity of the Cognitive Tutor literature by seeking to replicate realistic
scale-up conditions in a wide variety of locations.74 They found no effect the first year, but a 0.20
standard deviation impact in the second. Interestingly, the improvement in the second year was
not associated with increased fidelity of implementation, but instead with teachers reducing
(although not completely eliminating) their use of the activities called for by Cognitive Tutor
guidelines for non-computer class time.75

70

Ibid, 8.
Cabalo et al., 2007; Pane et al., 2010.
72
Pane et al., 2014.
73
Cabalo et al., 2007; Campuzano et al., 2009; Dynarski et al., 2007; Karam et al., 2017; Morgan and Ritter, 2002; Pane et al.,
2010; Pane et al., 2014; Ragosta, 1983; Ritter et al., 2007
74 Pane et al., 2014
75 Ibid.
71

26

Another medium-touch intervention that has recently risen to prominence is SimCalc.
Although SimCalc has not been used or tested as extensively as Cognitive Tutor programs, those
studies that have been conducted demonstrate strong potential. The mission of the SimCalc
project is “to democratize access to the mathematics of change and variation” (i.e. mathematics
relating to algebra and leading to calculus.)76 Based on using methods of “representational
infrastructure,” the program enables students to control the motions of animated characters by
building or editing mathematical functions. After editing the functions, students can press a
“play” button to see the corresponding animation.77 A study on a SimCalc intervention in Texas
turned up one of the largest effect sizes of any large-sample study covered in this review, with
0.63 and 0.56 standard deviation improvements in math scores for seventh and eighth graders,
respectively.78
We identified only four studies79 within the developed world exclusively examining
reading programs. Of these, two evaluated the Fast ForWord program, a program initially
designed for students with particular learning disabilities,80 but that has been in some cases
marketed and used to cope with broader reading challenges. The program works by providing
students with individualized exercises in a game-like computerized environment, where students
receive on screen rewards for correct answers and attentiveness to instruction. These studies—
the only ones, to our knowledge, that have evaluated Fast ForWord within a broader education
setting, found mostly weak and insignificant results. While Fast ForWord seems to have had
more success in addressing the impairments it was designed for, experimental evidence suggests

76

Roschelle et al., 2010.
Kaput and Rochelle, 2013.
78
Roschelle et al., 2010.
79
Borman et al., 2009; Deault et al., 2009; Rouse and Krueger, 2004; Wijekumar et al., 2014.
80
Rouse and Krueger, 2004.
77

27

that further adjustments or at least more testing may be needed before scale-up can be
recommended.
In contrast, two recent studies81 that evaluated a reading comprehension program called
Intelligent Tutoring for the Structure Strategy (ITSS)—that teaches students a particular
technique for breaking down texts—show significant positive results. It differs from Fast
ForWord in that it is geared toward middle school students and aims to improve reading
comprehension rather than basic literacy. ITSS is a web-based intelligent tutor that utilizes a
“structure strategy” to teaching literacy that begins a lesson by describing what the student is
going to learn, models the strategy, and asks the student to practice. The tutor then provides
feedback to the student based on his/ her answers, and gives the student the chance to correct the
answer if needed. Effect sizes on a series of reading comprehension measures ranged from 0.2 to
0.53 standard deviations.
CAL is becoming increasingly popular within the developing world as well, and an
experimental literature on these interventions is growing rapidly in China82 and India.83 On one
hand, CAL programs may prove to be more effective in developing countries given the often
tight capacity constraints faced. On the other hand, infrastructure limitations and other challenges
could impede CAL implementation. Findings so far have been overwhelmingly positive. One
recent study conducted in Delhi84 finds especially large effects that seem to occur through
mechanisms of personalization akin to those described above. The program, called Mindspark,
administers its self-developed educational software at study centers for a small fee. After a
treatment period of under five months, the authors find an effect of 0.36 standard deviations on

81

Wijekumar et al., 2012; Wijekumar et al., 2014.
Bai et al., 2016; Feng et al., 2014; Lai et al., 2012; 2015; 2016; Mo et al. 2014A; 2014B; 2015.
83
Banerjee et al., 2007; He et al., 2007; Linden et al., 2008; Muralidharan et al., 2016; Naik et al., 2016.
84
Muralidharan et al., 2016.
82

28

math scores and 0.22 standard deviations on Hindi language scores, the two subject areas for
which the program was tested. Although there is no treatment arm that offers the same content
without the adaptivity component, they present strong suggestive evidence that adaptivity played
a key role in accounting for the impact. There is an expansive range of levels between students
within each grade, and the Mindspark program records that report the questions generated show
that they matched this wide range. Given that no teacher could possibly have covered such a
huge spread of levels, the authors argue that the adaptation element of the program must have
played a central role in enabling its positive impact and could therefore be an integral part of a
solution to the unevenness of levels that challenge many schools in India and elsewhere.

4.3 Looking Forward
As the above discussion demonstrates, CAL technologies may be able to significantly
improve learning outcomes, with the evidence particularly strong for math. Supplementary
programs like ASSISTments demonstrate that even programs that require only minutes each day
can generate significant effects on learning outcomes. And more intensive interventions like
SimCalc show that heavier-touch CAL interventions can generate transformative results.
Although experimental evidence on CAL for subjects other than math remains scarce, the ITSS
program has shown that positive impact in other areas is possible.
Numerous important tasks remain, however, for future researchers to complete if CAL’s
potential is to be efficiently leveraged. One vital area is test the extent to which learning from
CAL lasts in the longer term. To what extent do effects compound or diminish in subsequent
years? Another important task will be to further explore whether and when CAL can work
effectively for subjects other than math. Do the cognitive processes that underpin mathematical

29

reasoning inherently lend themselves better to software algorithms? More broadly, which areas
of education could CAL add most value to? And when are light- vs. heavy-touch interventions
most appropriate and cost-effective? An important crosscurrent that undercuts many of these
other concerns is the issue of implementation. One way to gain greater leverage on this issue
could be to test a particular CAL program in a particular population while varying elements of
the implementation plan. Finally, we still know little about how CAL programs interact with
teachers’ efforts. Unpacking interconnections could highlight opportunities for complementarity
and synergy.
Table 2
Author

Barrow,
Markman, and
Rouse (2009)

Beal et al.
(2013)

Intervention

I Can Learn© aka
“Interactive
Computer Aided
Natural Learning”

AnimalWatch webbased math
tutoring program

Data Source

Sample

Data from
customized prealgebra and
algebra tests
administered in
the study sites

Eight high
schools and two
middle schools
in three large
urban school
districts in the
Northeast,
Midwest and
South) with a
high proportion
of minority
students

Student test scores
on standardized
tests and projectbased quizzes

58 teachers’
classes (35
Treatment, 23
Control) for 6th
grade. Data
from
over 1200
students are

30

Findings
(1) Students
randomly assigned
to the computeraided instruction
scored 0.17 SD's
higher on a prealgebra and
algebra test than
students assigned
to traditional
instruction (2) The
strongest effects
were for larger
classes (especially
with more
heterogeneity in
student levels) and
classes with more
absences, possibly
indicating that
impact occurs
through "increased
individualized
instruction"
The AnimalWatch
program had
significant effects
on student scores
on a state
achievement test
and project-based
quizzes (Positive

Subject

Math

Math

included in the
analysis (795
Treatment, 496
Control).

Borman,
Benson, and
Overman
(2009)

Cabalo et al.
(2007)

Campuzano et
al. (2009)

impact of around
0.3 SDs on scores)

Fast ForWord
computer-based
language training
program

Primary data from
a school
administered
language and
reading
comprehension
test.

Second and
seventh grade
students in
Baltimore who
were more
generally at risk
for poor
reading and
language
outcomes in
eight
elementary and
middle schools

Null results
(attributed to
implementation
shortcomings): (1)
No statistically
significant effect
for second graders
in reading
comprehension or
language; (2)
There were
statistically
significant main
effects for reading
comprehension for
seventh graders
(effect size 0.21),
but not language

Reading

Cognitive Tutor's
Bridge to Algebra
program

Primary data from
a standardized
math assessment
(Northwest
Evaluation
Association
(NWEA))

32 pre-algebra
classes in 5
schools in the
Mau, Hawaii
school district

Null results

Math

Thirty-three US
school districts,
132 schools,
428 teachers. It
focused on
school districts
that had low
student
achievement
and large
proportions of
students in
poverty.

(1) For reading,
there were no
statistically
significant
differences
between the
effects the
products had on
standardized
student test scores
in the first and
second year (see
Dynarski et a.,
2007); (2) For
sixth grade math,
product effects on
student test scores
were statistically
significantly lower
(more negative) in
the second year
than in the first
year. (3) For
algebra I, effects
on student test

Math and
Reading

16 types of
software products

Primary data
collection of
student
achievement tests,
including the
Stanford
Achievement Test,
the Iowa Tests of
Basic Skills
(ITBS), the
California
Achievement Test,
the New Mexico
Standards Based
Assessment, the
ETS End-ofCourse Algebra
Test, and school
administrative
data

31

scores were
statistically
significantly
higher (.15 SDs) in
the second year
than in the first
year.

Cavalluzzo et
al. (2012)85

Dynarksi et al.
(2007)

Kentucky Virtual
Schools hybrid
program for
Algebra 1

16 types of
software products

Administrative
data from
standardized
assessments
(American
College Testing
PLAN) and 10th
grade math course
enrollment
Primary data
collection of
student
achievement tests,
including the
Stanford
Achievement Test,
the Iowa Tests of
Basic Skills
(ITBS), the
California
Achievement Test,
the New Mexico
Standards Based
Assessment, the
ETS End-ofCourse Algebra
Test, and school
administrative
data

47 Kentucky
schools (30 of
which were in
rural areas)
with Grade 9
Algebra classes

The treatment has
no statistically
significant effect
for either outcome.

Math

Thirty-three US
districts, 132
schools, and
439 teachers
participated in
the study. It
focused on
school districts
that had low
student
achievement
and large
proportions of
students in
poverty

Test scores were
not significantly
higher in
classrooms using
selected reading
and mathematics
software products.
(First student
cohort –second
cohort results
reported in
Campuzano et al.
2009)

Math and
Reading

85 This could also be considered a blended online learning and face‐to‐face intervention. In Kentucky Virtual Schools, instruction
time is 60 percent face‐to‐face instruction and 40 percent is using online resources. The findings from this paper are consistent
with the outcomes we observe in other blended classroom interventions.

32

Deault,
Savage, and
Abrami (2009)

Hegedus,
Dalton, and
Tapper (2015)

Karam et al.
(2017)

ABRACADABRA
web-based literacy
program

Primary data
collection of
scores on tests of a
range of literacy
and skills attention
measures.

Grade 1
students from
schools in
Montreal,
Canada for a
total of 144
students from
13 different
classrooms.

SimCalc interactive
math software

Primary data
collection using
instruments to
measure student
learning and
related factors.

7 high schools
in Southeast
Massachusetts
of varying
achievement
levels

Cognitive Tutor
Algebra I

Primary data
collection on
survey data of
dosage and
frequency of
implementation
and administrative
data on student
grades and
Algebra 1 scores

74 middle and
73 high schools
in 51 school
districts
representing
seven states in
the U.S. that
varied in
contexts.

33

(1) Overall,
significant effects
of the intervention
were evident for
about half of the
reading and related
measures. (2) The
intervention
reduced preintervention
negative
correlations
between attention
and learning
outcomes,
indicating that it
may help kids with
attention problems
Significant impact
on student learning
of core algebra
concepts including
both procedural
and conceptual
problems
(1) Use of
traditional student
activities in
classrooms was
significantly
negatively
associated with
student outcomes
on Algebra 1 for
middle school in
years 1 and 2 (i.e
use of CTAI had a
positive effect on
Algebra 1
outcomes). (2)
Although not
statistically
significant, we see
similar negative
associations
between teachers’
use of traditional
instructional
methods and
student outcomes
for middle schools
in both study years
and in high school
in year 2

Reading

Math

Math

Kelly et al.
(2013)

ASSISTments
online homework
support

Student learning
data collected
from the
ASSISTments
system.

63 thirteen and
fourteen year
olds who were
currently
enrolled in an
eighth grade
math class, in a
suburban
middle school
in
Massachusetts.

Mitchell and
Fox (2001)

DaisyQuest and
Daisy's Castle
reading game

Student learning
data on various
literacy outcome
measures

36 US
kindergarten
and 36 first
grade students

Cognitive Tutor
Algebra I

Survey data on
student attitudes
toward
mathematics and
ETS Algebra I
End-of-Course
test

Ninth graders
in 5 junior high
schools in
Moore
Independent
School District,
Oklahoma

Morgan and
Ritter (2002)

34

(1) Students
receiving the
intervention
learned reliably
more with an
effect size of 0.56
SDs (2)
Additionally,
teacher use of the
homework data
lead to a more
robust and
systematic review
of the homework.
(1) Students
receiving
computer
administered
phonological
awareness
instruction and
teacher delivered
phonological
awareness
instruction showed
a significant
increase over the
instructional
technology
(drawing and math
software) control
group. (2) The
teacher-delivered
group
outperformed the
computer
administered
group on several
literacy measures.
(1) Positive effects
(0.29 SDs) in math
outcomes on the
ETS test [0.23 SDs
according to Pane
et al. p. 130,
reporting the
WWC adjusted
estimates] (2)
Students receiving
the CTAI program
were significantly
more confident
and more likely to
rate mathematics
as useful than

Math

Reading

Math

students in the
traditional class.

Pane et al.
(2010)

Pane et al.
(2014)

Ragosta
(1982)

Ritter et al.
(2007)

Rockoff
(2015)

(1) The CTAI
program has
negative effects on
math outcomes (0.19 SDs) (2) No
effect on student
attitudes toward
mathematics and
technology.

Math

Cognitive Tutor
Geometry

Student
achievement data
in geometry

8 high schools
in Baltimore
Country Public
School District
(BCPS); after
attrition, final
sample size of
699

Cognitive Tutor
Algebra I

Primary data
collection of
student
achievement data
on an Algebra I
Proficiency Exam
and school
administrative
data on other
student
characteristics and
learning outcomes

74 public
middle schools
and 73 public
high schools
across seven
US states and
51 school
districts in
urban,
suburban, and
rural areas

(1) No effect of
CTAI in the first
year (2) 0.20 SD
positive impact in
second year for
high schools, but
no significant
effect for middle
schools.

Math

Cognitive Tutor

Student learning
data in
mathematics
collected from the
CAI system and
on standardized
tests

Four
elementary
schools in Los
Angeles

The curriculum
was effective in
raising student
scores on tests
derived by the
CAI curriculum
and on
standardized tests

Math

Cognitive Tutor

Administrative
data on student
grades, student
performance on
the ETS Algebra I
End-of-Course
assessment and
survey data on
student attitudes
towards math

Ninth graders
in 5 junior high
schools in
Moore
Independent
School District,
Oklahoma

Statistically
significant effects
of CTAI on
student grades and
Algebra I scores

Math

School of One

Student
achievement data
in math and
survey data on
student and
teacher attitudes

Eight New
York City
public schools

No effects on
student math
outcomes, but
study was not
powered to detect
small to moderate
effects

Math

35

Roschelle et
al. (2010)

Roschelle et
al. (2016)

SimCalc interactive
math software

Student math
scores

Seventh and
eighth grade
classrooms in
Texas public
schools

ASSISTments
online homework
support

Student data
collected from the
ASSISTments
system and
student outcomes
on an end of year
standardized math
assessment

2,850 seventh
graders in 43
schools in
Maine

Rouse and
Krueger
(2004)

Fast ForWord
computer-based
language training
program

Student outcomes
on measures of
language and
reading ability

Rutherford et
al. (2014)

Spatial-Temporal
(ST) Math

Student outcomes
on a standardized
test series in math

ASSISTments
online homework
support

Student data
collected from the
ASSISTments
system and
student outcomes
on a post-test
math assessment

Singh et al.
(2011)

Snipes et al
(2015)

Elevate summer
math program

Student data on
tests of Algebra
readiness

36

Significant effects
of SimCalc on
student learning
(0.63 and 0.50
SDs)

Math

Positive effects of
ASSISTments on
student math
outcomes (0.18
SDs)

Math

Null results

Reading

Null (although
positive, but small
effects (0.07 SDs)
at p = .089)

Math

Eight classes of
eighth grade
students in
Maine

Positive effects
(although sample
size is small)

Math

8th grade
students from
eight schools in
six districts in
California’s
Silicon Valley

(1) The Elevate
Math summer
program, which
included daily use
of Khan Academy,
significantly
improved math
achievement and
algebra readiness
(0.7 SDs on a test
of algebra
readiness) (2)
Despite significant
positive effects
from the program,
most students were
still not ready for
Algebra I content.

Math

4 schools in an
urban school
district in the
Northeast;
around 40
percent African
American and
50 percent
Hispanic
13,000
students, 52
elementary
schools in
Southern
California

Tatar et al.
(2008)

Van Klaveren
et al. (2017)

Wang and
Woodworth
(2011)

Wijekumar et
al. (2012)

Wijekumar et
al. (2014)

SimCalc interactive
math software

Student and
teacher
performance on an
researcher created
instrument based
on the Texas state
assessment;
survey data on
teacher
characteristics,
school context and
teacher attitudes

21 seventh
grade
mathematics
teachers in
Texas

Positive effects on
student and
teacher
mathematics
knowledge

No statistically
significant
improvement from
the adaptive CAL
program relative to
non-adaptive CAL
program (however,
there is no nonCAL control
group)
(1) Dreambox
treatment group
scored 2.3 points
higher on the
NWEA math test
(effect size of 0.14
SDs), and 2.9
points higher on
the geometry
subtest (effect size
of 0.16 SDs). (2)
No significant
impact of
Reasoning Mind
on the NWEA or
other tests

Math

Student
performance data
on standardized
test scores

Dutch
secondary
schools

DreamBox
Program and
Reasoning Mind
(math programs)

Student
performance on
the NWEA math
test and other
math tests

Kindergarten
through 5th
grade students
in 3 schools in
an elementary
charter school
network in San
Francisco

ITSS (Intelligent
Tutoring for
Structure Strategy)

Student
performance on
Gray Silent
Reading Test
(GSRT) and
researcherdesigned measures

60 rural and 71
suburban 4th
grade
classrooms

Positive effects on
language; (.1 SDs)
on GSRT, (.49
SDs) on main idea
quality.

Language

ITSS (Intelligent
Tutoring for
Structure Strategy)

Student
performance on
standardized tests
and researcher
designed
assessments
measuring reading
comprehension

128 fifth-grade
classrooms in
45 schools
within 12
school
districts in rural
and suburban
settings in
Pennsylvania

Positive effects on
literacy (0.2 SDs)
and signaling
(0.42 SDs) tests

Language

Adaptive CAL
program compared
against a static one

37

Multiple

Math

5. Behavioral Interventions

5.1 Background and Context
Next, we shift focus to education technologies that draw on the theory and practice of
behavioral economics to guide students (and, in some cases, their parents) toward behaviors that
are expected to facilitate greater academic achievement. The idea behind this approach is that
people are subject to systematic biases in decision-making that lead to sub-optimal outcomes,86
like ending up in a job one does not like because of not having studied hard enough in school.
The behavioral insights literature was relatively slow to come to the education sector, but has
taken off over the past several years.87 Behavioral issues are especially important to think about
in the context of education, since important long-run decisions are being made during a time
when the brain’s ability to think of the future is not fully developed. So, while we all face
challenges in making decisions involving long run uncertain benefits and immediate costs,
children and youth particularly struggle.88 On the plus side, with this knowledge of behavioral
barriers getting in the way of realizing better long-run outcomes, technology may be used to
develop simple and inexpensive solutions to give individuals more support for making better
choices.
We identified 47 experimental papers studying behavioral ed-tech programs. These
studies evaluated programs aimed at solving a wide variety of problems and drawing on a variety

86

Thaler and Sunstein, 2008.
Koch et al., 2015; Lavecchia et al., 2014; Levitt et al., 2012.
88
Lavecchia et al., 2014.
87

38

of techniques implemented at different points across the life course, from giving parents ideas of
how to practice reading skills with their kids to reminding college students to submit the FAFSA.
In particular, we identified studies of interventions across four clusters: seven on encouraging
parental engagement in learning activities, 10 on attempting to improve school-parent
information flows, 17 on encouraging success in transitioning to and through college, and 13 on
mindset interventions. Information on the studies is presented in Table 3. The studies show
strong promise in each of these areas, with only a few of the interventions reviewed showing no
impact. In the remainder of this section, we review the evidence on each of the four clusters in
turn.

5. 1 Encouraging Parental Learning Engagement During Early Childhood
Research suggests that one of the most effective means of improving educational
outcomes is for parents to engage in learning activities with their children.89 But parents report
spending less time on these activities than might be expected in light of the possible benefits.
The problem of low engagement is particularly acute among disadvantaged households, a pattern
that may reinforce broader disparities in educational outcomes.90 Policymakers have found costeffective responses elusive, with even expensive and resource-intensive programs turning up
modest results.91 Yet because young children spend a great deal of time at home, school-based
programs cannot substantially substitute for engagement “unless they are very intensive,
extensive and expensive.”92 This dilemma has inspired a growing literature that explores whether

89

Levine et al., 2010; Price, 2010; Sénéchal and LeFevre, 2002.
Guryan et al., 2008; Kalil et., 2015; Lee and Bowen, 2006.
91
York and Loeb, 2014.
92
Mayer et al., 2015.
90

39

and how behavioral interventions might contribute toward reducing disparities in engagement.
We identified six experimental evaluations of technology-based interventions aiming to increase
the quantity and quality of time spent by parents practicing skills with their preschoolers,93
kindergarteners,94 or 1st-4th graders.95 All of the programs studied relied centrally on sending
text message reminders to parents, and all found positive results.
Why might nudges be expected to increase parental learning engagement within
disadvantaged households? After all, behavioral interventions are unlikely to substantially
address resource constraints like the time scarcity faced by low-income parents. However, the
behavioral economics literature suggests that cognitive constraints as well as resource limitations
lead to underinvestment. Even when cognitive burdens themselves are aggravated by resource
constraints, small adjustments in the decision structures that people face can help to correct these
biases and move them toward more optimal behavior.96 So, in the present context, a behavioral
economics perspective would indicate potential benefits from reminders and instructions
inspiring and guiding parents toward more productive engagement.
READY4K!—a preschool literacy program implemented in San Francisco—was the
earliest experimentally evaluated, technology-based intervention we identified that attempted to
leverage this rationale to improve parental learning engagement. The program sent parents three
text messages per week with tips and encouragement to engage in literacy activities.97 The
behavioral logic that guides READY4K! suggests that “the complexity of parenting may
overwhelm some parents, leading them to underinvest in their children”.98 Furthermore, literacy

93

Hurwitz et al., 2015; Mayer et al., 2015; Meuwissen et al., 2017; York and Loeb, 2014.
Doss et al., 2017.
95
Kraft and Monti‐Nussbaum, 2017.
96
Thaler and Sunstein, 2008.
97
York and Loeb, 2014.
98
Ibid.
94

40

activities constitute a case of “delayed gratification,” necessitate “interrupting the status quo”
and are often overcome by “limited attention.”99 So the program sends suggestions of small, easy
tasks that parents can do without feeling overwhelmed; provides encouragement to sustain
parents’ investment in longer term gratification; provide tips for integrating the activities into
daily life so that the status quo barrier can be overcome; and address attention constraints by
regularly reminding parents.
The study found an impact of 0.29 standard deviations of the program on a composite
score for “global early literacy parenting” measuring activities like reading to a child, pointing
out words that rhyme, and taking the child to a library or museum.100 The study also found effect
sizes ranging from 0.21 to 0.34 standard deviations on PALS literacy tests.101 The fact that the
program led to an increase in specific literacy tasks but not general ones suggests that the impact
was likely generated by the program’s provision of specific, manageable tasks, rather than
reminding parents to engage in activities they might have engaged in anyway. The effect sizes
detected are impressive given the exceedingly low costs of the intervention, at less than a dollar
per family.102 A great deal could be gained from follow-up research that unpacks these findings
and tests similar initiatives in new contexts.
Ensuing research has attempted to work toward untangling the specific mechanisms
underpinning the effectiveness of this type of intervention, as well as better understanding
potentially differing effects across subgroups. The two remaining interventions of this type for
preschoolers that have been experimentally evaluated took place within midwestern Head Start

99

Ibid.
Ibid.
101
Ibid.
102 Ibid.
100

41

and Early Head Start centers.103 One of these interventions provided households with tablets
containing numerous children’s books.104 The treatment group additionally received three
nudges—daily text message reminders to read to the kids, a goal-setting tool that asked the
parents to set reading goals and reported back on whether these goals were met, and social
rewards, specifically congratulatory texts or cartoons when goals were reached.105 Following the
six week study period, the group receiving the behavioral interventions used the tablet a full
standard deviation more than parents who did not. They read more than twice as many books to
their children, with control group families reading an average 14.8 books during the six-week
intervention period while treatment group families read an average of 31.4 books.106 The second
Head Start intervention sent daily text messages to parents encouraging them to engage in any of
a variety of learning activities covering reading, science, and math, and found that the treatment
increased the range of learning activities that parents engaged in.107
As children progress from preschool to Kindergarten and then first grade, they tend to
spend larger shares of their time at school. To what extent might programs like the ones
described above prove effective beyond preschool? Two interventions were recently
experimentally evaluated that adopt a similar model, but for kindergarteners108 and 1st-4th
graders109 instead of preschoolers. The kindergarten intervention was an extension of
READY4K!, with the evaluation including the same preschool sample as the children entered
kindergarten along with additional San Francisco kindergarteners.110 In addition to replicating

103

Mayer et al., 2015; Hurwitz et al., 2015.
Mayer et al., 2015.
105 Ibid.
106 Ibid.
107 Hurwitz et al. 2015.
108 Doss et al., 2017.
109 Kraft and Monti‐Nussbaum, 2017.
110 Doss et al., 2017.
104

42

the same intervention within a kindergarten context, a second treatment arm was added that sent
parents “personalized” and “differentiated” texts. Texts to parents in this second treatment arm
contained child-specific information and sent recommendations for tasks matching the child’s
level. Interestingly, the researchers found the original treatment that had been effective in
preschool showed no significant effects in kindergarten. However, the personalized and
differentiated text messages did show substantial benefits, with children whose parents received
the treatment “50 percent more likely to read at a higher level.”111
Finally, the most recent intervention falling into this category to undergo experimental
evaluation extended the idea of texting parents to encourage engagement in literacy activities to
the 1st-4th grade. Recognizing that elementary students spend more time engaged in school
throughout the year, this intervention targeted a specific friction point within the elementary
education process—“summer reading loss”—the tendency of elementary students to fall behind
in their reading skills because of the gap in practice they experience during the summer. This
study finds that the texting intervention improves reading comprehension scores for students in
the treatment group by 0.21-0.29 standard deviations.112

5.2 Improving School-Parent Information Flows
As children get older, the role of parents shifts away from practicing skills with their kids
directly and towards encouraging the kids to put more effort into school. So, behavioral
interventions for middle and high schoolers tend to focus on sending parents information on their
kids’ performance—for example updates on grades, attendance, and behavior—to prompt the

111

112

Ibid.
Kraft and Monti‐Nussbaum, 2017.

43

parents toward providing this encouragement. If parents are constrained by a gap in information
on how hard their children are working or how well they are performing, and if children are not
already expending maximum effort, then closing these gaps may provide parents the opportunity
to apply that alchemical combination of guidance, pressure, and support that constitutes
parenting. This issue may be especially important for low-performing schools, which already
exhibit lower rates of communication satisfaction from parents113 and where parents may be
relatively more constrained in their ability to absorb monitoring costs.114 We identified 10 RCTbased studies evaluating programs that sought to leverage technology to improve the flow of
information from school to parents in this way.115 These programs followed two main
approaches: first, sending information to parents that was generated anyway as part of regular
school activities (like grades and attendance), and, second, having teachers send personalized
messages to parents. Overall, these studies have found positive results, indicating a potentially
fruitful set of opportunities.
The majority of the school-parent information flow interventions that have been
experimentally evaluated fall into the first of the two categories listed above. The first
intervention in this category to be experimentally evaluated was a program aimed at middle and
high school students at a single public school in a low-income neighborhood of Los Angeles.116
Parents whose children were in the treatment groups were notified when their children missed
attending class or missed an assignment through text messages, phone calls, and e-mail.
Following the semester-long intervention, students in the treatment group had earned GPAs and

113

Bergman, 2015.
Ibid.
115 Balu et al., 2016; Bergman, 2015; Bergman, 2016; Bergman, Edmond‐Verley et al., 2016; Bergman and Chan 2017; Bergman
and Rogers 2016; Kraft and Dougherty, 2013; Kraft and Rogers, 2015; Kraft and Monti‐Nussbaum., 2017; Rogers and Feller,
2016.
116
Bergman, 2015.
114

44

standardized math test scores that were about 0.20 standard deviations over the control group.117
An evaluation of a similar intervention—Papás al Día (“Parents up to Date”), carried out in two
low-income municipalities of Santiago, Chile—also finds positive results, including a 0.09
standard deviation improvement in math grades, a reduction in bad behavior, and positive
spillover effects within classes.118
While these two interventions sought to channel existing information on students’
performance to parents rather than generating new information, both were somewhat labor
intensive, requiring substantial manual data entry. More recent interventions have tended to
automate the process to the greatest extent possible to cut down on costs. One recent
experimental study evaluated the effects of a more automated school-parent information program
on a sample of 22 middle and high schools in a district of West Virginia.119 This program
automatically pulled information from the school’s student information system and texted it
directly to parents. Parents received weekly texts stating the number of classes and/or
assignments that students had missed, as well as monthly texts if their child was averaging below
70 percent on any class.120 Because of the automation, the intervention was extremely cheap,
with 32,000 text messages totaling to only $63 and training coming down to $7 per student.121
The study showed impacts that were very impressive given the low costs of the intervention: the
treatment group saw a 39 percent reduction in failed courses, an 18 percent increase in class
attendance, meaning that the treatment group attended 50 more classes on average122 and a 0.10

117

Ibid.
Berlinski et al., 2016.
119
Bergman and Chan, 2017.
120
Ibid.
121
Ibid.
122
Ibid.
118

45

standard deviation improvement in GPA.123 Interestingly, the data suggests that parents already
had a good idea of their children’s final grades, but the program reduced parents’
underestimation of the number of assignments their kids were missing, which likely helped to
better target the pressure they placed on their kids to increase effort.124 The strongest benefits
went to those with below-average GPAs, who saw a reduction in class failures of 0.9 classes, an
increase in attendance of 64 classes, and a GPA increase of 0.26 points.125
In contrast, another fully automated intervention that focused on exclusively on
attendance showed no evidence of improving attendance rates.126 Here, parents of New York
City Public School received automated text messages on each day their student did not show up
for school, in addition to weekly attendance reports. Further research will be needed to explore
the extent to which this lack of impact was most likely a result of the intervention’s exclusive
focus on attendance, its location in New York City (which may be more saturated with
automated information flows than most other environments), or something more contingent and
specific to the intervention in question.
Two recent studies have highlighted an important qualification to the line of research just
described.127 While technologies that improve school-parent information flows may be effective
in improving education, these effects will be heavily mediated by the extent to which the
technologies are actually used. For instance, one recent study showed a letter and phone call
prompting students to access an online system containing attendance and grades significantly
increased rates of access and ultimately resulted in a GPA increase 0.10 points.128 Another

123

Ibid.
Ibid.
125
Ibid.
126
Balu et al., 2016.
127
Bergman, 2016.
128
Ibid.
124

46

program—this one conducted in a dozen Washington, DC middle and high schools offered text
message updates of the kind mentioned above, but varied in how the program was
implemented.129 Three treatment groups—one that received a text instructing them on how to
sign up online for the service, one that received a text inviting sign-up through a text message
response, and one that automatically enrolled parents in the texting program but gave them the
opportunity to opt out—were contrasted with a control group that did not receive any prompt to
sign up for the texting service. Only 1 percent of participants in the first group and 8 percent in
the second group signed up, while only 4 percent in the automatic enrollment group chose to opt
out. This massive difference in adoption shaped the effectiveness of the texting program in
generating academic performance outcomes: while no significant effects on performance
outcomes emerged from the first two treatment groups, the automatic enrollment group saw
improvements in GPA by roughly a quarter to a third of a letter grade, and reduced class failure
by roughly a fifth to a quarter.130 These lessons on the importance of encouragement and
especially opt-in systems to promote technology adoption are relevant to a broad range of edtech applications, but are mentioned here since they were evaluated in reference to school-parent
communication intervention.
The interventions discussed so far in this sub-section attempt to transfer already-existing
information to parents. Another approach that has been experimentally evaluated in the context
of two separate interventions has teachers communicate personalized messages to parents. The
first experimentally evaluated intervention falling into this category took place during a required
summer program in a Boston charter school.131 Parents received two communications per day for

129

Bergman and Rogers, 2017.
Ibid.
131
Kraft and Dougherty, 2013.
130

47

five consecutive school days—a phone call from an English teacher and a text message from a
math teacher. The intervention improved engagement as measured by three variables:
homework completion, participation, and number of instances in which teachers had to direct
students’ attention back to the topic at hand.132 Qualitative evidence suggests that this effect
occurred through three mechanisms: improving relationships between students and teachers,
expanding parental involvement, and increasing students’ motivation.133
The second intervention in this category took place “during a traditional summer school
program offered by a large urban school district in the Northeastern United States.”134 Here,
teachers themselves wrote out one-sentence messages which were then sent to parents weekly by
research assistants through text message, phone, or email.135 Two separate program variations
were given: one consisting of “positive” messages about what the student was already doing
well, and the other consisting of “improvement” messages about areas that the student could use
work on. Averaging across the two treatment arms, inclusion in the program led to an increase in
the success rate of students passing the class and obtaining the credit, up 6.5 percentage points
from an 84.2 percent passing rate in the control group. Interestingly, the impact estimate is
substantially higher for the improvement treatment arm, although the experiment lacks the power
to detect significance in this difference.136 The program seems to work not by increasing the
amount of time parents spend talking with their kids about school, but rather by directing the
content of these conversations. The program also seems to have led to the unintended
consequences of lower student perceptions of their own performance, and weaker student-teacher

132

Ibid.
Ibid.
134
Kraft and Rogers, 2015.
135
Ibid.
136
Ibid.
133

48

relationships as reported by teachers also.137 Perhaps the best of both variations could be
captured by sending messages that include actionable steps as in the “improvement” version, but
are more positive in tone.
Overall, other than the lack of impact generated reported by the New York City
attendance program, interventions that seek to improve school-parent information flows seem
highly promising. Two of the studies discussed above138 came upon unintended anecdotal
evidence to this effect when the schools they worked with decided to provide comparable
interventions to sections of the control group.

5.3 Transitioning to and Succeeding in College
Another area of focus for technology-based nudge interventions in the education sector
has been the challenge of transitioning to and making it through college. The behavioral
economics literature suggests that people—and especially children, adolescents, and young
adults—tend to rely heavily on routines, and the transition to college requires students to break
from routine.139 The behavioral literature has also documented the paralyzing effect of too much
information and too many choices, and the transition to college is fraught with these as well.140
Experimental evaluations have been conducted on four main types of college-related behavioral
interventions: information campaigns, nudges to complete important tasks, intensive application
assistance, and college advising.

137

Ibid.
Bergman 2015; Kraft and Rogers, 2015.
139
Lavecchia et al., 2014.
140
Ibid.
138

49

First, several interventions have sought to leverage information technology to
inexpensively provide students with more college-related information. On one hand, two
relatively minimalistic interventions in the U.S. generated no impact. One of these—tested in a
field experiment with a sample of over a million prospective and enrolled college students in
Texas—sent one e-mail and one letter containing information about higher education tax credits,
but those who received these showed no more likelihood of applying to or enrolling in college
than those who did not.141 Another intervention conducted in a single public university emailed
letters to students explaining their current financial aid package and associated plans, but this
information too had negligible effects.142
On the other hand, two information interventions implemented respectively in Canada
and Chile found positive effects. The first of these interventions showed videos to students in
disadvantaged Toronto high schools on the benefits of higher education, and allowed the students
an opportunity to try out a financial aid calculator. Students who participated in the program
reported more favorable views of higher education.143 The other program sent eighth graders in
metropolitan Santiago, Chile, DVDs containing practical information on higher education
financing. Participants not only showed greater knowledge of financial aid, but also were more
likely to enroll in college preparatory high schools, and also exhibited attendance rates that were
8.8 percent higher.144 This latter intervention is also unique among programs that have been
experimentally evaluated in that it targets higher education at the eighth-grade level, which could
allow more time for participants to plan for college.

141

Bergman et al., 2016.
Darolia, 2016.
143
Oreopoulos and Dunn, 2013.
144
Dinkelman and Martínez, 2014.
142

50

Another approach to supporting the transition to college has been through nudge
campaigns. Although the term “nudge” as commonly used in the behavioral economics literature
can be applied to many of the interventions described throughout this section, here we use the
term “nudge campaigns” to refer to interventions providing sustained efforts to guide, encourage,
and/or remind program participants about one or more aspects of college success. Five recent
studies suggest that nudge campaigns can be effective in improving decisions and task
fulfillment surrounding financial aid and college matriculation and enrollment.
Of these, three interventions attempted to encourage better-informed financial aid
decisions. One program sent students at a large community college in Baltimore County eight
text messages over a period of several weeks prompting them to make more “active” financial
aid decisions. The intervention resulted in a 3.1 percentage point reduction among students who
received the text messages in accepting unsubsidized Stafford loans, and those who still did
accept the loans borrowed less. Results were strongest among students showing less financial
literacy and with more debt. The study also produced some evidence that the texts led students
who had attained marginal academic success to leave school earlier.145 Another program sent text
messages to college freshmen who, as high school students, had worked with a Massachusettsbased education nonprofit called uAspire. The messages encouraged students to refile the
FAFSA for their sophomore year and found an increase of nearly 14 percentage points on
continuous enrollment through sophomore year among students attending community colleges
(those attending four year universities already had high rates of continuous enrollment).146 Most
recently, the largest experimentally evaluated FAFSA nudge-campaign to date sent three
versions of a message to low-income and first-generation students filling out the Common

145
146

Barr et al., 2016.
Castleman and Page, 2016.

51

Application encouraging them to apply early for the FAFSA. One version provided specific
planning structure, one gave information on the human capital returns to college, and one
attempted to advocate productive identities. No effects were found for the latter two frames, but
the planning message led to a 1.1 percentage point increase in college enrollment among all
recipients and 1.7 percentage points for first generation college students.147 In addition to
supporting task completion related to financial aid, one nudge campaign has been experimentally
shown to reduce “summer melt,” the phenomenon whereby students who are admitted to and
indicate a decision to attend a particular college do not actually complete the matriculation
process or do not actually show up for classes.148
A nudge campaign may be sufficient to induce students to think through financial aid
decisions and remind them to do the right paperwork on time to enroll in and get through school.
However, it is perhaps less likely that nudges would be effective at getting a student to fill out an
admissions or financial aid application in the first place—this is a much more daunting task. We
identified evaluations of two programs that leveraged technology for more intensive application
assistance and support.149 In the first instance of these programs, families with a college-age
child who were filing their taxes at H&R block were given the opportunity to quickly file their
FAFSA at the same time. This was possible as a result of a software program designed to
automatically feed data from the tax entry system into the FAFSA, collecting additional FAFSA
questions not covered during the course of the regular tax filing in ten or so minutes following
the tax filing. College enrollment of high school seniors with parents receiving the treatment
increased by 8 percentage points.150 The program LifeAfterHighSchool, on the other hand,

147

Bird et al., 2017.
Castleman and Page, 2015.
149
Bettinger et al., 2012; Oreopoulos and Ford, 2016.
150
Bettinger et al., 2012.
148

52

focused on providing support for the admissions process directly to students by incorporating
relevant activities into the high school curriculum.151 The program aimed to ensure that every
senior in high schools given the program graduate from high school with a college program offer
of acceptance and a financial aid package. The program consisted of workshops involving
interactive activities, for instance having students enter their grades into a computer program,
which would then generate a list of local programs in their area for which they would likely be
accepted if they applied. In addition to large gains in application rates, college enrollment
increased by about nine percentage points among the seniors who had not been taking any
university-track courses.152
Finally, two recent studies have examined the extent to which technology can be
leveraged to increase access to college advising. One experiment conducted at a large Canadian
university tested three treatment arms: one-on-one coaching, an online exercise, and a text
messaging support program. Only the one-on-one coaching arm showed significant results,
potentially indicating limits in using electronic communication in helping foster longer-term
academic performance.153 The other study evaluated a program at Georgia State University that
leveraged AI technology in developing a texting program with AdmitHub that sent customized
messages to students guiding them through many aspects of the college enrollment process.154
The “augmented intelligence technology” upon which the program was based made it possible
for the computer to respond to a large majority of incoming questions, saving scarce time for

151

Oreopoulos and Ford, 2016
Ibid.
153
Oreopoulos and Petronijevic, 2017.
154
Page and Gehlbach, 2017
152

53

college advisers and administrators. For the sample of students that had committed to attending
Georgia State, the texting program increased enrollment there by 3.3 percentage points.155

5.4 Mindset Interventions
Finally, several recent programs have been experimentally evaluated that use technology
in implementing “mindset interventions”—programs that attempt to improve education outcomes
by cultivating “attitudes, beliefs, and dispositions about school and learning that are associated
with positive academic outcomes and school success,”156 often through brief reading and writing
exercises. During these interventions, students are typically encouraged to think about setbacks,
or feeling out of place, or lack of motivation from a different perspective. The idea is that if
students recognize setbacks and mistakes as an important part of the learning process, they can
keep trying and have a greater chance of success. Similarly, if students recognize the feeling of
not fitting is as a normal part of the transition process, they may be more likely to keep making
efforts and eventually feel socially integrated, which in turn will raise the chances that they will
complete college.
Previous experiments have shown promise for these interventions in in-person settings,
but these next experiments extend them using technology, which allows for the provision of
these exercises to students online, at virtually no cost other than a small amount of participating
students’ time. Of the 12 online mindset intervention studies we identified, a majority showed
positive results.157 In one of the largest-scale studies to date, a sample of nearly 10,000 students

155

Ibid.
Snipes et al., 2012; Dweck, 2006.
157
Good et al., 2003; Morisano et al., 2010; Paunesku et al., 2015; Unkovic et al. 2016, Yeager et al., 2013, Yeager et al., 2014,
Yeager et al., 2014; Yeager et al., 2016; Yeager et al., 2017; and Yeager et al., 2017 found positive results. Bursztyn and Jensen,
2015; Forsyth et al., 2007 found negative results.
156

54

transitioning from high school to college across diverse contexts were given multiple variations
of internet-based “lay theory” interventions that aimed to prepare the students to encounter
adversity and help them to understand that this is a natural part of the college transition process.
These interventions showed positive impacts on a variety of outcomes relating to persistence,
with authors estimating that the gains could mean a 31-40 percent reduction in the gap between
“advantaged” and “disadvantaged” students.158 Mindset interventions have been shown to
improve high school performance as well: another program delivered exercises similar to the lay
theory units described above to a treatment group within a sample of 1,594 students from 13
public, private, and charter high schools across the U.S. This study found GPA improvements
and a positive impact of 6.4 percentage points on achieving satisfactory grades in core classes
among the third of students classified as “at risk” for dropping out of high school.159

5.5 Looking Forward
As has been shown to be the case within a variety of policy sectors, evidence consistently
shows that technology-enabled behavioral interventions can have meaningful, if modest, impacts
on a variety of education-related outcomes, often at extremely low costs. Moving forward,
several tasks will be important to advance the policy-relevant research. For one, many of the
interventions discussed in this section have relied on text messages, and the effectiveness of text
messages may in part rest on the fact that they are still somewhat novel. It may be that people
become less responsive to text messages as they grow increasingly inundated with messages and
pay less attention to them. It is thus essential for policy researchers to explore more specific

158
159

Yeager et al., 2016.
Paunesku et al., 2015.

55

lessons about why particular types of text campaigns work well, so as to facilitate lessons that
may be explored across different communications platforms.
For the time being however text message-based programs may exert significant impact at low
cost, and explorations should continue as to which points in the education life cycle are most
responsive to text-based nudges and information channels (e.g., as in the school-to-parents text
notifications discussed above). Evidence discussed in this section also highlights the importance
of personalization and customization of messaging, but such customization can be costly.
Research should thus also continue to explore the most effective ways to integrate artificial
intelligence and machine learning into these interventions, as in the AdmitHub example
discussed above. Finally, the research on large-scale internet-based mindset interventions
remains in its infancy, but given the substantial results that have been found at scale thus far,
learning more about which approaches to mindset changes are most effective, and in which
contexts.

Table 3
Author

Intervention

Data Source

Sample

Balu,
Porter, and
Gunton
(2016)

Automated text
messages to parents
of high school
students informing
about absence

School
administrative
data on student
absences

3,957 New
York City
high school
students

No effect found

High school

Barr, Bird,
and
Castleman
(2016)

Text messaging
campaign prompting
loan applicants at a
large community
college to
make informed and

School
administrative
data on student
demographics,
socioeconomic
status, academic
information, and

2,807
community
college
loan
applicants
in

(1) Students reduced their
unsubsidized loan borrowing,
a result driven by those with
low financial literacy levels
and high debt. (2) Short-term
academic effects suggest that
the intervention may also

Postsecondary

56

Findings

Education
Setting

active borrowing
decisions

financial aid
disbursement

Baltimore
County

Bergman
(2015)

Automated texts to
parents about
performance

462
students in
grades 6-11
in Los
Angeles

Bergman
(2016)

Learning
Management System
(parents have access
to an online portal
with child's classes,
grades, assignments,
etc)

School
administrative
data on
assignment
completion, work
habits,
cooperation,
attendance and
test scores; parent
and study surveys
Deidentified data
from a Learning
Management
System (LMS)
company, NCES
Common Core
Data, decile
performance
ratings
constructed by
GreatSchools

Bergman
and Chan
(2017)

Automated texts to
parents about
performance

Administrative
data, gradebook
data, survey data,
and texting data

Bergman,
Denning,
and Manoli
(2016)

E-mails and letters
to potential/
prospective/current
college students on
financial
aid/incentives

ApplyTexas basic
demographic data,
THECB
administrative
data on all
students in public
universities and
community
colleges in the
state of Texas,
data on who

57

15 US
school
districts
operating
learning
manageme
nt
company;
two-stage
experiment
providing
families
their
account
information
in 59
schools
across three
districts.
22 middle
and high
schools in
Kanawha
County
Schools in
West
Virginia

1,042, 303
students
who had
applied to
any public
Texas
college or
university
using the
ApplyTexa

have led marginal students to
withdraw one semester
earlier than they otherwise
would have.
(1) Positive effects .19 SD
high school GPA increase (2)
7.5 percentage point decrease
of missing final exam project
(3) .21 SD increase for math
standardized exam scores (4)
Null for English

Middle &
High School

(1) A quarter of parents ever
use it (2) Adoption follows
an S-shape (3) Significant
spillovers occur along
intensive but not extensive
margins (4) There is evidence
student grades improve as a
result.

Middle &
High School

(1) Reduces course failure by
nearly 40 percent. (2) GPA
increases by about .10 of a
point for middle school
students and .25 of a point
for high school students. (3)
Treatment group students
attend 17 percent more
classes. (4) No improvements
in state math and reading
scores. (5) .10 SD increase on
in-class exam scores.
No effects found

Middle &
High School

Postsecondary

opened the emails
researchers sent

s.org
portal.

Bergman,
EdmondVerley,
and
NotarioRisk
(2016)

Community-based
organizations
provided regular
information to
families about their
child’s academic
progress in one arm
and supplemented
this with home visits
on skills-based
information in a
separate arm

District
administrative
data on math and
reading test
scores, GPA, and
attendance,
program data
from
implementing
partners

1,120
families
from 3
participatin
g schools
in an urban,
Midwester
n school
district

Bergman
and Hill

Publishing teacher
ratings online

LAUSD data on
identifiable
teacher names
linked to deidentified
student test
scores, LA Times
value added
scores

3,089
teachers in
Los
Angeles

Bergman
and Rogers
(2016)

Text message to
parents regarding
their child’s
academic
performance,
including grades,
upcoming tests and
missing assignments

District
administrative
records and
collected data on
parents and
students
daily activity in
the “parent portal”

6,976
students in
12 US
schools

Bettinger
et al.
(2012)

H&R Block study-help with FAFSA
during tax filing

Researchers
linked their final
sample to data
from three
sources: the DOE,
the
Ohio Board of
Regents (OBR),
and the National
Student

4,187
individuals
from the
dependent
sample,
868 seniors
in high
school
(main
dependent
sample);

58

(1) Math and English test
scores improved for the
treatment arm with home
visits (2) There are large
eﬀects on retention for both
groups during the year,
though learning gains tend to
accrue for students with
average-and-above baseline
performance and students at
the lower-end of the
distribution appear
marginally retained.
(1) High-performing students
sort into classrooms with
highly-rated teachers (2)
Conditional on publication,
ratings labels induce sorting
as well as teacher attrition:
low-rated teachers teach
lower-performing students
and are more likely to leave
the district in subsequent
years relative to higher-rated
teachers (3) There is no effect
of publication on test scores
(1) ITT estimates indicate
that being assigned to the Opt
Out group increased grades
by 0.06 SDs for Term 3 and
in 0.04 SDs for Term 4 (2)
Overall, grades increased by
0.05 SDs in Terms 3 and 4,
with a 5 percent significance
level.
(1) The combined assistance
and information treatment
substantially increased
FAFSA submissions and
ultimately the likelihood of
college attendance,
persistence, and aid receipt.
(2) High school seniors
whose parents received the
treatment were 8 percentage
points more likely to have

Middle &
High School

Grade 3-5
teachers

Middle &
High School

Postsecondary

Clearinghouse
(NSC)

Bird et al.
(2017)

Nudges for early
FAFSA filing
through Common
App

Bursztyn
and Jensen
(2015)

Two interventions:
1. performance
leaderboard into
computer-based high
school courses 2.
Complimentary
access to an online
SAT preparatory
course. Sign-up
forms differed
randomly across
students only in
whether they said
the decision
would be kept
private from
classmates.

Student-level
college
application data
provided by
Common
Application and
college enrollment
data provided by
the National
Student
Clearinghouse
Study 1: Data for
the universe of
questions
answered, with
each student
uniquely
identified by an
ID code Study 2:
student survey;
data on whether
students actually
logged into the
system
later to activate
their accounts

59

independen
t sample of
15,874
individuals,
further
separated
into those
without
prior
college
experience
(9,228) and
those with
prior
college
experience
(6,646) in
Ohio and
Charlotte,
North
Carolina
454,243
US high
school
seniors
who had
registered
with the
Common
Application
Study 1:
5,000
students
across
more than
100 schools
in Los
Angeles.
Study 2: 26
classrooms
across the
four
schools in
Los
Angeles,
with a total
of 825
students

completed two years of
college, going from 28 to 36
percent, during the
first three years following the
experiment.

Positive effect for treatment
arm that involves concrete
planning prompts

Postsecondary

(1) 24 percent performance
decline. The decline appears
to be driven by a desire to
avoid the leaderboard. (2) In
nonhonors classes, sign-up
was 11 percentage points
lower when decisions were
public rather than private.
Honors class sign-up was
unaffected.

High School

Castleman
et al.
(2012)

Providing college
counseling to low
income students
during the summer

School
administrative
data, college/
transition
counselor
interaction logs,
National Student
Clearinghouse
data

162 senior
students
across 7
high
schools in
Providence,
Rhode
Island

Castleman
and Meyer
(2016)

A text messaging
campaign to provide
lower-income
college students with
simplified
information,
encouragement, and
access to one-on-one
advising

1,198
students in
West
Virginia

Castleman
and Page
(2015)

Text messages to
reduce summer melt

Data from Signal
Vine, the texting
platform with
whom WVHEPC
contracted to send
the messages,
dataset provided
by WVHEPC,
which listed all
students who
matriculated into
a state public
university or
community
college
College
enrollment data
from the National
Student
Clearinghouse

Castleman
and Page
(2016)

Text message to
improve FAFSA refiling for sophomore
year

Castleman
and Page
(2016)

Text messages to
improve enrollment
tasks

uAspire
administrative
data, data from
the text messaging
platform utilized,
uAspire student
interaction logs,
National Student
Clearinghouse
data
uAspire
administrative
data, data from
the text messaging
platform utilized,
National Student
Clearinghouse
data

60

(1) Substantial improvements
in both the rate and quality of
college enrollment (2)
Students in the treatment
group were 14 percentage
points more likely to enroll
immediately in college and
19 percentage points more
likely to keep the
postsecondary plans they
developed during senior year.
Students participating in the
texting campaign tend to
complete more freshman year
credits

High School

12,676
recent high
school
graduate in
Dallas,
Boston,
and
Philadelphi
a
808 firsttime
college
freshmen in
Massachus
etts

Increased enrollment among
students with less access to
college-planning supports
and who were not as far
along with their college
planning at the completion of
high school.

Postsecondary

Positive effects

Postsecondary

3,906 high
school
graduate in
Boston,
Lawrence,
and
Springfield,
Massachus
etts

Positive effects (although no
additional benefit from
including parents on nudges)

Postsecondary

Postsecondary

Chande et
al. (2015)

Texting motivational
messages and
organizational
reminders to
students, with
messages drawing
on insights from
behavioral
economics
Letter e-mailed to
students regarding
financial aid

College
administrative
data on student
attendance

1,179
students in
England

Simple text messages reduce
the proportion of
students that stop attending
by 36 percent and lead to a 7
percent increase in average
attendance relative to the
control group.

Adult
learners

Administrative
data on loan
disbursement

READY4K!
Continuation into
kindergarten with
additional
differentiated/person
alized treatment arm

Teacher survey,
parent survey, and
scores from
Fountas and
Pinnell Benmark
Assessment
System

Forsyth et
al. (2007)

Self-esteem
bolstering
intervention

Primarily score on
a final
examination

90 US
college
students

No effects found overall.
However, some key student
subgroups changed their
borrowing in response to the
letter, particularly those with
low GPAs.
(1) Children in the
differentiated and
personalized program were
50 percent more likely to
read at a higher level
(p<0.01) compared to the
general group (2) Parents
reported engaging more in
literacy activities by 0.31
SDs (p<0.01) compared to
the control group (3) No
effects detected for other
treatment arm
The D and F students got
worse as a result of self–
esteem bolstering and
students in the other
conditions did not change.

Postsecondary

Doss et al.
(2017)

Approxima
tely 10,000
college
students in
the
Midwest
794
kindgergart
en students
and
families in
California

Fryer
(2016)

Students were
provided with free
cellular phones and
daily information
about the link
between human
capital and future
outcomes via text
message in one
treatment and
minutes to talk and
text as an incentive
in a second
treatment

Administrative
data from all
schools in
OKCPS; post
treatment student
survey

1907
students in
sixth and
seventh
grades in
Oklahoma

(1) Students’ reported beliefs
about the relationship
between
education and outcomes were
influenced by the information
treatment (2) There were no
measurable changes in
student effort, attendance,
suspensions, or state test
scores, though there is
evidence that scores on
college entrance exams four
years later increased.

Middle
School

Darolia
(2016)

61

Early
childhood

Postsecondary

Seventh-grade
students in the
experimental
conditions were
mentored by college
students through an
email platform who
encouraged them
either to view
intelligence as
malleable or to
attribute academic
difficulties in the
seventh grade to the
novelty of the
educational setting.
The three-part
intervention
consisted of two
brochures mailed to
parents and a Web
site, all highlighting
the usefulness of
STEM courses
Texting program to
promote learning
engagement of Head
Start parents

Math and reading
test scores

138
seventh
grade
students in
Texas

(1) Females in both
experimental conditions
earned significantly higher
math standardized test scores
(2) The students—who were
largely minority and lowincome adolescents—in the
experimental conditions
earned significantly higher
reading standardized test
scores.

Middle
School

Primarily student
surveys which
measured success
expectancies and
initial and post
treatment interest
in science

188 high
school
students in
Wisconsin

High School

Parent survey

253
Midwester
n parents

Kraft and
Dougherty
(2013)

Parents texted on
student
behavior/performanc
e

Teacher surveys;
teacher
communication
logs; student
interviews;
student
demographic data

140 rising
sixth and
ninth grade
students in
Boston,
Massachus
etts

Kraft and
MontiNussbaum
(2017)

Parents texted to
encourage to engage
in activities to
counteract summer
learning loss

Scores from
Standardized Test
for
the Assessment of
Reading (STAR)
and the Strategic
Teaching and
Evaluation of
Progress (STEP),
measures of
parent
engagement,
parent survey

183 US
families

Students whose parents were
in the experimental group to
take, on average, nearly one
semester more of science and
mathematics in the last 2
years of high school,
compared with the control
group
Parents who received the
service engaged in more
learning activities; this was
particularly true of fathers
and
parents of boys.
(1) On average, teacher–
family communication
increased the odds that
students completed their
homework by 40 percent,
decreased instances in which
teachers had to redirect
students’ attention to the task
at hand by 25 percent and
increased class participation
rates by 15 percent
Effects on reading
comprehension are
concentrated among 3rd and
4th graders with effect sizes
of .21 to .29 SDs, more than
compensating for summer
learning loss

Good et al.
(2003)

Harackiewi
cz et al.
(2012)

Hurwitz et
al. (2015)

62

Early
Childhood

Middle &
High School

Elementary

Kraft and
Rogers
(2015)

Parents texted on
student
behavior/performanc
e

School
administrative
records, teacher
surveys, student
surveys

435 high
school
students in
Northeaster
n United
States

Ksoll et al.
(2014)

Innovative mobile
phone-based adult
education program
(Cell-Ed)

70 adult
learners in
Los
Angeles

Mayer et
al. (2015)

Texting program to
promote learning
engagement of Head
Start parents

Reading
assessment scores,
student household
characteristics
survey, interviews
with students,
Cell-Ed real time
usage data
Time stamped
data from the
reading app,
parent surveys

McGuigan,
McNally,
and
Wyness
(2012)

Information
campaign about the
costs and benefits of
pursuing post
compulsory
education

Student surveys

6,614 Year
10 students
in England

Meuwissen
et al.

Text2Learn, a
mobile phone
texting program for
low income parents
of preschoolers.

Parent survey

110 parents
in
Minnesota

63

169 parents
in Chicago

(1) Messages decreased the
percentage of students who
failed to earn course credit
from 15.8 percent to 9.3
percent—a 41 percent
reduction (2) This reduction
resulted primarily from
preventing drop-outs, rather
than from reducing failure or
dismissal rates.
(1) Significantly increased
students’ basic and broad
reading scores, equivalent to
a 2-4 year increase in reading
levels over a four-month
period (2) The program also
increased participants’ selfesteem by 7 percent.
(1) Increased usage of the
reading application by one
SD after the six-week
intervention. (2) Evidence
suggests that the large effect
size is not accounted for by
the information
component of the
intervention and that the
treatment impact was much
greater for parents who are
more present-oriented than
for parents who are less
present-oriented.
Students with higher
expected net benefits from
accessing information are
more likely to avail
themselves of the opportunity
presented by our experiment

High School

(1) Parents reported engaging
in more literacy activities
with their children after
receiving the
texts, and appreciated getting
reminders about activities (2)
They did not report increased
use of community resources,
such as libraries, or changes
in attitudes about literacy.

Early
Childhood

Adult
learners

Early
Childhood

High School

Morisano
et al.
(2010)

Goal-setting
program

Student surveys,
university
transcripts

85 college
students in
Canada

Oreopoulo
s and Dunn
(2013)

3-minute video and
opportunity to use
financial aid
calculator
Text-based advising

Student surveys

1,616 high
school
students in
Canada
4,900 first
year
college
students in
Canada

Oreopoulo
s and
Petronijevi
c (2017)
Oreopoulo
s and Ford
(2016)

Application
assistance is
incorporated into the
high school
curriculum for all
graduating seniors at
low-transition
schools

Page,
Castleman,
and Meyer
(2016)

FAFSA texting
program

Interaction log,
student survey,
college
administrative
data on course
grades and GPA
Ontario Ministry
of Education
administrative
data which
included
demographic data,
high school
performance data,
and postsecondary
enrollment data

Administrative
data on the status
of students’
FAFSA
submissions,
district
administrative
data in Texas,
administrative
data from
ApplyTexas
portal, National
Center for
Education
Statistics
Common Core of
Data (CCD) for
Delaware; text
messaging records

64

86 schools
in Canada

Texas: 66
high
schools
serving
over
17,000
high school
seniors;
Delaware:
4,095 high
school
seniors

After a 4-month period,
students who completed the
goal-setting intervention
displayed significant
improvements in academic
performance compared with
the control group.
Positive effects on PSErelated benefit-cost

Postsecondary

High School

No effects found

Postsecondary

(1) Increased application
rates from 64 to 78 percent,
college enrollment increased
the following school year by
5.2 percentage points with
virtually all of this increase in
two-year community college
programs (2) The greatest
impact was for students who
were not taking any
university-track courses in
high school: the application
rate for these students
increased by 24 percentage
points with a nine percent
increase in two-year college
enrollment
(1) The intervention
substantially increased
enrollment among students
with less access to collegeplanning supports and who
were not as far along with
their college planning at the
completion of high school.

High School

High School

Paunesku
et al.
(2015)

Growth-mindset and
sense-of-purpose
interventions

Student
transcripts and
psychological
measures

1,594 US
students in
13
geographic
ally diverse
high
schools

Rogers and
Feller
(2016)

Parents of high-risk,
K-12 students
received one of three
personalized
information
treatments
throughout the
school year

Daily attendance
data

28,080
households
across 203
US schools

Unkovic et
al. (2016)

Personalized emails
encouraging
graduate students to
apply for a
conference

Conference
registration
information,
student survey

3,945 US
graduate
students

Yeager et
al. (2013)

6-session
intervention that
taught an
incremental theory
(a belief in the
potential for
personal change).

Student survey,
school
administrative
data which
included
demographic and
academic
information

230 ninth
and tenth
grade
students in
California

Yeager et
al. (2014)

A malleable
(incremental) theory
of
personality—the
belief that people
can change.

Student surveys,
scores from the
Cyberball
procedure on
social exclusion,
scores from 10item Perceived
Stress Scale,
physical health

158 ninth
grade
students in
Californian

65

Among students at risk of
dropping out of high school
(one third of the sample),
each intervention raised
students’ semester grade
point averages in core
academic courses and
increased the rate at which
students performed
satisfactorily in core courses
by 6.4 percentage points
(1) The most effective
versions reduced chronic
absenteeism by 10 percent,
partly by correcting parents'
misbeliefs about their
students’ total absences (2)
The intervention reduced
student absences comparably
across all grade levels, and
reduced absences among
untreated cohabiting students
in treated households.
Robust, positive effect
associated with this simple
intervention and suggestive
evidence that women
responded more strongly than
men. However, women’s
conference acceptance rates
are higher within the control
group than in the treated
group. This is not the case for
men, female applicants in the
treated group solicited
supporting letters at lower
rates.
Compared to no-treatment
and coping skills control
groups, the incremental
theory group behaved
significantly less
aggressively and more
prosocially 1 month post
intervention and exhibited
fewer conduct problems 3
months post intervention.
The incremental theory group
showed less negative
reactions to an immediate
experience of social adversity
and, 8 months later, reported
lower overall stress and
physical illness. They also
achieved better academic
performance over the year.

High School

K-12

Postsecondary

High-School

High School

measures, and end
of term core
course grades

Yeager et
al. (2014)

Promoting a
prosocial, selftranscendent purpose

Study 1: Primarily
student surveys on
behavior; National
Student
Clearinghouse
data; Study 2:
STEM GPA;
Study 3: student
exam answers;
Study 4: Primarily
student surveys on
behavior

Yeager et
al. (2016)
Design

Working to scale
previous
interventions:
Qualitative inquiry
and rapid, iterative,
randomized “A/B”
experiments were
conducted with
3,000 participants to
inform intervention
revisions for this
population.
"Lay theory"
intervention

Study 1: Primarily
student surveys
and behavioral
measures
Study 2: Student
GPA and
behavioral
measures

A program teaching
a growth mindset of
intelligence

Behavioral
assessments and
mindset
assessments

Yeager et
al. (2016)

Yeager et
al. (2017)

Study 1: Primarily
student surveys
and National
Student
Clearinghouse
enrollment data;
Study 2: Primarily
student surveys;
Study 3:Primarily
student surveys

66

Study 1:
1,364 US
high school
seniors;
Study 2:
338 US
ninth grade
students;
Study 3: 89
college
students;
Study 4:
429 US
college
students
Study 1:
7,501 ninth
grade US
students;
Study 2:
3,676 ninth
grade US
students

Those with more of a purpose
for learning also persisted
longer on a boring task rather
than giving in to a tempting
alternative and, many months
later, were less likely to drop
out of college. A brief, onetime psychological
intervention promoting a selftranscendent purpose for
learning could improve high
school science and math
grade point average (GPA)
over several months.

High School

The intervention was an
improvement over previous
versions in terms of shortterm proxy outcomes
and it improved 9th grade
core-course GPA and
reduced D/F GPAs for lower
achieving students when
delivered via the Internet

High School

Study 1:
584 US
high school
seniors;
Study 2:
7,335 US
first year
college
students;
Study 3:
1,592 US
college
students
14,866 US
ninth grade
students

Increased full-time
enrollment rates, improved
grade point averages, and
reduced the
overrepresentation of socially
disadvantaged students
among the bottom 20 percent
of class rank. The
interventions helped
disadvantaged students
become more socially and
academically integrated in
college.
(1) Although program effects
were positive across schools,
there was (modest)
heterogeneity, suggesting
that sampling from different
subsets of schools would
have yielded different
conclusions. (2) Overall,
results suggest growth

High
School/PostSecondary

High School

mindset approaches may be
useful in preparing
learners for the future
economy.

York and
Loeb
(2014)

Text messaging
program to nudge
preschool parents to
engage in literacy
activities with
children

The READY4K!
enrollment form,
an end of-year
survey of parents,
an end-of-year
survey of
teachers,
SFUSD’s
administrative
records, student
scores on the
district’s early
literacy
assessment

440
families in
California

Increases engagement in
literacy activities 0.22-0.34
SDs and parental
involvement at school by
0.13-0.19 SDs; learning gains
of 0.21 to 0.34 SDs

Early
Childhood

6. Online Courses

Since their emergence during the 1990s, online courses have come to constitute a sizeable
presence within the education field. By 2013, over a third of U.S. college students had taken an
online course at some point during their college career160 and more than 11 percent were enrolled
in entirely online programs.161 The rise of online learning bears heavily on policy issues relating
to educational equity, since two key justifications for the proliferation of online education have
been its promise of improving access and reducing costs. Moreover, at least at the postsecondary level, students in online programs tend to face disproportionate educational
disadvantages. For instance, data from the National Postsecondary Student Aid Study’s
2010/2011 representative survey indicates that “online students are older, have lower levels of

160
161

Bettinger et al., 2014, citing Allen and Seaman, 2013.
Deming et al., 2015.

67

parental education, are more likely to be single parents themselves, and are more likely to be
working full-time while enrolled in school than are other college students.”162 So how does
online education perform in terms of access, learning, and other important outcomes?
Online courses have, over the past several years, coalesced into two broad categories.
First, what we refer to as conventional online courses represent an online extension of the
“distance learning” or “correspondence course” format, an approach which has a long history in
higher education.163 These courses are typically offered as part of a degree program that consists
entirely of online courses, or that includes online, face-to-face, or blended164 courses. Second are
Massive Open Online Courses (MOOCs). Unlike conventional online courses, MOOCs are
typically offered free of charge and are not part of official degree programs. They broadly consist
of “structured and sequenced teacher-led activities (e.g., videos, readings, problem-sets) coupled
with online assessments and usually some venue for student interaction such as a discussion
forum.”165 Between 2012 and 2015, MOOCs saw enrollment rates exceeding 25 million.166
While conventional online courses and MOOCs developed to serve largely separate purposes,
the lines between them are becoming blurred. For instance, MOOC companies have increasingly
offered certification programs for a fee such as MicroMasters programs,167 and MIT has even
launched a MOOCs program that will lead to a traditional master’s degree.168
Nonetheless, within the present environment, conventional online courses to date have
followed mostly distinct pathways, and the research has clustered accordingly. Experimental

162

Ibid.
Means et al., 2009.
164
The term blended takes on different meanings in different contexts within the ed‐tech literature—in this case, we use the
term to refer to a single course that has both online and face‐to‐face components.
165
Hodges et al., 2016.
166
Kizilcec et al., 2017.
167 MicroMasters, https://www.edx.org/micromasters.
168 MIT announces MITx Micromasters program in development economics, with path to full master’s degree,
http://news.mit.edu/2016/mitx‐micromasters‐program‐development‐economics‐masters‐degree‐1205.
163

68

research on conventional online courses has compared online against face-to-face courses to
judge the extent to which the former improves access and can act as a viable substitute for faceto-face education. While researchers are also interested in the effects of MOOCs on education, it
is less clear what to compare them to since they generally do not substitute for face-to-face
courses that students would otherwise take. Experimental research on MOOCs up to this point
has thus focused primarily on whether and how a range of behavioral interventions can improve
MOOC completion rates and extend coverage to disadvantaged groups. In the remainder of this
section, we first discuss the experimental evidence on conventional online courses, and then turn
to a discussion of studies on MOOCs.

6.1 Conventional Online Courses
Online courses build on a tradition of correspondence courses that has existed for over a
century within the higher education field.169 As early as the latter 1800s, institutions like the
University of Chicago and the University of Wisconsin were teaching faraway students via the
postal service.170 Educators and entrepreneurs brought online college courses and degree
programs to market beginning in the 1990s, but proliferation expanded rapidly after a 2006
decision to end a regulation that had limited federal aid money for institutions conducting more
than half of their coursework via correspondence.171 Some institutions offer both online and faceto-face instruction, while others offer online courses exclusively. While a growing mass of

169

Means et al., 2009.
Deming et al., 2012 citing Watkins, 1991.
171
Deming et al., 2015.
170

69

selective universities offers online programs, online education remains heavily dominated by
large, for-profit colleges172 like University of Phoenix and Strayer University.173
How might online courses add value to education? One justification for online courses is
that online courses in many contexts may be much less expensive to implement than face-to-face
courses, so that if “Internet-based classes are at least reasonable substitutes for live lecture
classes, then the use of Internet-based classes could be a cost-effective method of combating
increased fiscal constraints.”174 A second is that they can expand access by allowing people to
take courses that would not otherwise be possible or worthwhile for them to take, for instance
because of geographic location, work or family obligations during class hours, or disabilities.175
And online courses may allow students more flexibility in accessing course materials at the most
convenient times, and in spending more time on content that they are struggling with and less on
content that they have mastered.176
Educators and researchers have also pointed out potential drawbacks of online courses.
The flipside of online courses’ flexibility is that students who do better with externally-induced
structure may be more likely to face time management issues than they would for a face-to-face
class, and may thus fall behind. 177 It is also possible that too large a shift toward online courses
could take away opportunities for networking and interaction that arise more naturally in face-toface environments.178 More generally, some educators and researchers believe that a valuable

172

Deming et al., 2012.
Burnsed, 2010. https://www.usnews.com/education/online‐education/slideshows/10‐largest‐online‐schools
174
Figlio et al. 2013; see also Cowen and Tabarrok, 2014; Means et al., 2009.
175
Goodman et al., 2016; Means et al., 2005; Poirier and Feldman, 2004.
176
Figlio et al., 2013.
177
Ibid, 764.
178
Sleeter, 2014. https://www.insidehighered.com/blogs/higher‐ed‐beta/meaningful‐interaction‐online‐courses
173

70

element of the teaching process is lost when the face-to-face dimension is reduced or
eliminated.179
We identified nine experimental studies examining the effects of conventional online
courses. Of these, seven RCTs180 compared online and face-to-face delivery (or various
gradations in between) of particular courses, one RDD181 tested the extent to which offering an
online degree option increased enrollment, and one audit RCT tested whether employers
distinguished between online and face-to-face degree when selecting resumes to follow up on.182
First, to what extent does the evidence suggest that Internet-based classes can match or
exceed learning outcomes from face-to-face classes? While a great deal more exploration and
replication would be needed to draw robust conclusions, the studies reviewed here are consistent
with the hypothesis that, without some degree of face-to-face teaching, learning outcomes may
suffer, leading to (albeit small) sacrifices in test scores for fully online courses relative to face-toface courses. In contrast, blended learning environments—meaning, in this case, courses that
have both a face-to-face component and an online component—have not yet been found to
significantly underperform purely face-to-face courses in studies meeting our methodological
criteria. So, while evidence at this point would not back substantial shifts toward fully online
courses, it does indicate that switching courses from fully in-person to blended could decrease
costs without negatively affecting quality.

179

Ibid.
Alpert et al., 2016; Bowen et al., 2014; Figlio et al., 2013; Heppen et al., 2012; Joyce et al., 2015; Keefe, 2003; Poirier and
Freeman, 2004. Zhang, 2005. Another experiment, reported by Snipes et al., 2015 and included in Table 2, experimentally
evaluates a middle school summer math program that includes an hour daily use of Khan Academy, but since the study
compares the program as a complete package against a control group that does not attend any program, the study cannot
identify independent effects of the online component.
181
Goodman et al., 2016.
182
Deming et al., 2016
180

71

The first full-scale field experiment to compare face-to-face with online courses took
place in an introductory economics course at a major research university, with a sample of over
300 students.183 The course was identical for all students, but some students were provided
access to online video lectures, while others attended these lectures in person. The study finds
that students in the in-person group show higher outcomes, but that the differences are relatively
small—around 3 percentage points on the midterm and about 2.5 percentage points on the final.
In actual university settings however, the choice will not necessarily be between courses that are
entirely face-to-face or entirely online—instead, the two are often mixed into blended courses.
Two subsequent experiments studied blending learning environments of this sort. One compared
outcomes for a statistics course in which one group received three hours per week of face-to-face
instruction time, while another group received only one hour of instruction time but additional
internet-based exercises. The second experiment tested the effects of reducing face-to-face in an
economics course where all students also had access to online resources. Neither experiment
found significantly better outcomes to be associated with more in-person class time in a blended
learning context.
Finally, the most comprehensive study in this strand of the literature—the only one to test
fully online, blended, and fully face-to-face courses within the same experiment—found results
consistent with each of the above.184 Here, the authors test the impact in an economics course of
two treatments arms—one purely online and one blended—along with a fully face-to-face

183

Keefe, 2003 conducted a related study in an undergraduate business course and comes up with results that are in the same
direction as Figlio et al., 2013 but this study had a sample of only 35 students (with students in face‐to‐face classes performing
better). Another study conducted in a university psychology class with a sample of only 23 students found opposite results, with
students in the online version performing marginally better than those in a face‐to‐face group. Zhang, 2005 and Zhang et al.,
2006 run experiments on 155 and 138 undergraduates respectively and find that interactive online modules outperform non‐
interactive online modules and face‐to‐face sessions, but the context is single‐session lab experiments rather than a field
experiment with actual classes.
184
Alpert et al., 2016.

72

control group in a single experimental context. This study finds that students in the purely online
version of the course do not perform as well as those in the purely face-to-face group, while
outcomes for the blended treatment group are not statistically different from the control.185
The majority of research on online courses has been conducted in post-secondary
settings, but educators have increasingly attempted to leverage online learning in middle and
high school environments as well. We identified one experimental study that tested the
effectiveness of online summer credit recovery courses relative to face-to-face courses for
students who had failed freshmen algebra.186 The study was conducted in 15 high schools in the
Chicago Public Schools system with the lowest rates of students passing freshmen algebra, with
a sample of nearly 1400 students across two cohorts. The hope was that the online course would
provide “a more individualized, interactive experience” prompting students to “be more engaged
and more likely to persist in the course.” However, students in the face-to-face course
outperformed those in the online course. Suggestive evidence from the study indicates that one
significant reason was that teachers in the face-to-face course were better able to flexibly
incorporate a range of topics, and thus were better able to accommodate and engage the
students.187
To what extent do online courses increase access to education for those for whom it may
not be feasible to pursue a face-to-face degree? One of the main justifications for the potential
usefulness of online courses is that they can improve access to degree programs for populations
who otherwise might have trouble accessing them. We identified only a single study fitting our
criteria that addressed this question. Specifically, the researchers relied on an RDD design to

185

Ibid.

186

Heppen et al., 2012.
Ibid.

187

73

reveal that prospective students applying to Georgia Tech’s online master’s program in computer
science who were just above an admissions cutoff (which was not known to the applicants) for
the online version of the program were 20 percentage points more likely to end up in any
postsecondary program than those just below the cut-off.188 The strongest effects were observed
among mid-career prospective students, who otherwise may have chosen not to complete a
degree at all had the online program not been offered to them.189 Another recent experiment,
however, finds that “a business bachelor’s degree from a for-profit online institution is 22
percent less likely to receive a callback than one from a nonselective public institution.”190 But
the design does not allow for untangling the effect of the education medium (online vs. face-toface) from the institution’s for-profit/not-for-profit status. And even if employers do place a
penalty on online degrees, this may change in the coming years given the ongoing expansion of
the online education sector.

6.2 Massive Open Online Courses (MOOCs)
The term MOOC was first used in 2008 by media theorists George Siemens and Stephen
Downes for a course they taught at the University of Manitoba entitled “Connectivism and
Connected Knowledge,” with 25 students participating in face-to-face sessions at the university,
and content broadcasted to 2,300 additional students via the Internet.191 In the subsequent
decade, MOOCs have proliferated rapidly, with hundreds of courses offered and hundreds of
thousands of students enrolled worldwide.192 Like online courses, educators and education

188

Goodman et al., 2016.
Ibid.
190 Deming et al., 2016.
191 Greene et al., 2015; see also Cormier and Siemens, 2010.
192 Ibid.
189

74

policymakers saw in MOOCs the potential to decrease costs and increase access.193 Because
MOOCs are generally “open,” they have the potential to reach exponentially more students in a
much more diverse range of contexts than can conventional online courses granted for credit.
However, because MOOCs usually do not build toward a degree and may or may not be valued
on the labor market, it is less clear what, if any benefits, MOOCs may bring beyond the value of
the educational content they impart.
What has been the effect of MOOC proliferation? Observational research has found that
expectations that MOOCs will “democratize education” have been overblown and that, although
MOOCs have offered the opportunity for many disadvantaged individuals to access high-quality
educational content, enrollment and success rates are highly skewed toward advantaged
populations. MOOCs may even “exacerbate rather than reduce disparities in educational
outcomes related to socioeconomic status.”194 But overall impact is difficult to evaluate. People
may take MOOCs for a wide variety of reasons, from practicing skills for school or work to fun
and personal interest. Because MOOCs broadly speaking lack a clear counterfactual in that there
is no single function they seek to fulfill or institution they attempt to substitute for, no clear
experimental evidence has yet emerged on their overall impact, although this is likely to change
over the next several years given the outpouring of interest. Nonetheless, MOOCs are being
given to millions of students each year, and researchers have begun to delve experimentally into
questions of how MOOC usage can be improved for interested students. In fact, MOOCs lend
themselves well to low-cost RCTs, among other types of data generation and analysis.195

193

Ibid.
Hansen and Reich, 2015.
195
Lamb et al., 2015.
194

75

A growing body of studies has thus evaluated the effects of interventions aimed at
improving MOOC effort, persistence, and completion. For instance, MOOCs face very low
completion rates—“few users actually complete the class”.196 These low rates in themselves do
not necessarily signal a problem—many students enroll with no intention of completing the
course, and students may generally be getting what they wanted or needed from the MOOCs
even if they are only accessing bits and pieces. But low rates may at least in part reflect missed
learning opportunities that could be avoided with modifications to the MOOC platform.197
Interventions aiming to improve student MOOC effort have generally followed the approaches of
the behavioral and mindset interventions discussed in the preceding section. The studies have
typically found improvements, with seven of the nine studies evaluating these interventions
finding positive effects from at least one treatment arm.198
How might students be prompted to increase effort and persistence? One approach
adopted from the behavioral economics literature has been the model of “social comparison”
interventions—programs that inform students of their performance relative to other students. The
behavioral economics literature suggests that social comparisons may drive individuals to try
harder to excel. Two recent RCT studies199 found that social comparison interventions can
improve MOOC performance and completion, although one of these200 found significant effects
only when framed “negatively” (i.e., when target students were informed of how many students
had outperformed them rather than how many students they had outperformed).

196

Banerjee and Duflo, 2014.
Ibid.
198 Banerjee and Duflo, 2014; Davis et al., 2017; Kizilcec et al., 2014; Kizilcec et al., 2017; Lamb et al., 2015; Martinez, 2015A;
Martinez, 2015B; Patterson, 2015; Yeomans and Reich, 2017. Banarjee and Dufo, 2014 and Kizilcec et al., 2014 do not find
positive impacts.
199
Davis et al., 2017; Martínez, 2014.
200
Martinez, 2014.
197

76

Even if fully motivated to succeed in a course, MOOC students may struggle with time
management issues and, in particular, the temptation to procrastinate. Procrastination may be a
particularly acute temptation for MOOC students since they are not being directly observed by
an instructor. One study that attempted to address problems of procrastination found that a
commitment device that encouraged students to commit to limitations on time spent on
distracting internet sites increased the likelihood of completion by 40 percent and grades by 0.29
standard deviations, while treatment arms that reminded students how much time they were
spending on these websites or blocked them while on the course page showed no significant
effect.201 Relatedly, sending MOOC students a “planning prompt” improved course completion
by 29 percent.202
Many educators firmly believe that discussion and interaction is a central component of
education. But because MOOCs have thousands of students who generally access content at
different times, regular discussions of the types that occur in classroom are rarely feasible.
MOOC designers have attempted to at least partially address this problem by building discussion
forums into MOOCs, but participation is often relatively low. Two experimental studies have
evaluated efforts to increase participation in discussion forums. One study found insignificant or
negative impacts from an email prompt (depending on the content of the email),203 while another
found positive impacts on forum participation from asking participants to fill out a selfevaluation about forum participation.204
Another friction preventing efficient and equitable use of MOOCs may be “social threat,”
the tendency of individuals—typically from marginalized social backgrounds—to “suffer from

201

Patterson, 2015.
Yeomans and Reich, 2017.
203
Kizilcec et al., 2014.
204
Lamb et al., 2015.
202

77

the cognitive burden of wrestling with feeling unwelcome while trying to learn and, therefore,
underperform.”205 Social identity threat has been shown to impair learning in a variety of ways.
One recent set of RCTs evaluations tested the effects of writing exercises aimed at reducing
social identity threat and found them to be effective in increasing persistence and completion
among MOOC students from developing countries.206 While this study focused on closing the
gap between students from developed and developing countries, related interventions could also
plausibly reduce social identity threat-driven gaps between advantaged and marginalized
populations within the developed world.

6.3 Looking ahead
The online learning field is changing quickly, and new models that do not easily fit into
the categories discussed here are springing up. For one, websites that offer more independent
standalone modules--which allow for easier picking and choosing of content, and use in
supplementing other classes—are becoming increasingly important. The iconic website in this
category is Khan Academy, which is currently undergoing several evaluations. Also popular in
this space has been BrainPOP, which provides instructors with an expansive library of
educational videos intended to be fun and engaging.
Another new development has been the rise of quasi-formal certification schemes, like
NanoDegrees and MicroMasters, as alluded to above. These are certifications granted for
completing sets of courses that are not formal degrees in the sense of college degrees, but that
programs’ designers hope will increase their legitimacy and acceptance as real skill creators.

205
206

Kizilcec et al., 2017.
Ibid.

78

Whether or not these quasi-formal certifications will be accepted as useful by employers and will
come to take on some kind of labor market premium may become clear over the next few years.
If employers had better ways of assessing skills during the hiring process, these programs could
significantly expand education options. With regard to MOOCs, an important task for the
research agenda will be to hammer out what outcomes should be measured, beyond completion
rates, to judge the success through closer investigations of where specifically they may add value
to the education process. This will in turn require more nuanced study of students’ reasons for
accessing MOOCs, and, more broadly, the role of MOOCs within the broader education field.
6. Table 4
Study

Intervention

Data Source

Sample

Alpert,
Couch, and
Harmon
(2016)

Face-to-face,
blended and
purely online
course content in
a principles of
microeconomics
course

Administrative
data of students'
cumulative final
exam scores from
the course

College students
of a principles of
microeconomics
course taught at a
large public
university in the
Northeast.

Banerjee and
Duflo (2014)

"Deadline Effect"
in the 1473:
Challenges of
Global Poverty
MOOC - are
students who
register late less
likely to do well
or receive a
certificate in the
course?

Enrollment,
performance and
completion data
from the 1473
MOOC

Students
registering within
15 days of
deadline for
1473: Challenges
of Global Poverty
MOOC

79

Findings
(1) Those who
completed the purely
online course had
learning outcomes that
were significantly
worse than those in
the face-to-face
section of the course
(about four to five
points or one-half of a
letter grade) (2) No
difference in outcomes
those who completed
the blended relative to
the face-to-face course
(1) Students who
enrolled one day late
were less likely to get
a certificate (a
reduction of 16.6
percentage points),
and their grades were
10.7 percentage points
lower. (2) Students
whose behavior
suggests that they are
not organized are
significantly less
likely to succeed in a
MOOC, and this is
entirely driven by their
failure to complete

Type

Online

MOOCs

assignments on time,
rather than by their
performance
conditional on
completing them.

Banerjee and
Duflo (2016)

(1) Structured
study time: A
randomly chosen
subset of students
had the option to
commit to a
regular study
time. (2) Selfefficacy
messages:
Students were
randomly
allocated to see
either no message
or one of three
self-efficacy
messages during
the course
entrance survey:
(1) a generic
message (2) a
message related
to females
performing well
in the course, (3)
a message related
to non-native
English speakers
performing well
in the course. (3)
Tutoring: All
students that
enrolled in the
course were
offered the
opportunity to
enter a lottery for
tutoring services
in groups of 20.

Data from the
MOOC platform
on course
retention,
interaction,
completion and
exam grades

19,694 online
course
participants

80

(1) There was no
significant impact of
regular study time,
self-efficacy messages
or of tutoring on eight
outcomes of interest.
(2) Those assigned to
a tutoring group were
more likely to have
any interaction with
staff (with tutor or on
the forum); however,
there was no impact
on other measures of
engagement.

MOOCs

Davis et al.
(2017)

A personalized
feedback system
that facilitates
social comparison
of current
students with
previously
successful
learners.

Deming et al.
(2016)

Resume audit of
fictitious resumes
varied by forprofit v. public,
online v. brickand-mortar, and
more selective
versus nonselective postsecondary
institutions, based
on degrees and
programs in
business and
health

Secondary data
collected from
job vacancies,
and primary data
collected on
"callbacks"

Employers
posting job
vacancies in
business and
health identified
by a nationally
recognized online
job search
website in five of
the largest
metropolitan
labor markets in
the U.S. Chicago,
Los Angeles,
Miami, New
York City and
San Francisco

Goodman,
Melkers, and
Pallais (2016)

The new Online
Master of Science
in Computer
Science
(OMSCS) offered
by the Georgia
Institute of
Technology
(Georgia Tech)
and developed in
partnership with
Udacity and
AT&T

Administrative
data from (1)
Georgia Tech's
Computer
Science
Department on
their applicant
pool and (2) the
National Student
Clearinghouse on
enrollment

Online and in
person applicant
pools for Georgia
Tech's online and
in person
Computer
Science Master’s
program

Data from the
MOOC online
platforms on
student
characteristics,
engagement,
completion and
performance.

Learners across
four MOOCs
provided by the
Delft University
of Technology on
the edX platform

81

Across four
randomized controlled
trials in MOOCs, (1)
the availability of
social comparison
cues significantly
increases completion
rates, (2) this type of
feedback benefits
highly educated
learners, and (3)
learners' cultural
context plays a
significant role in their
course engagement
and achievement.
(1) A business
bachelor's degree from
a for-profit online
institution is 22
percent less likely to
receive a callback than
one from a
nonselective public
institution. (2) For
health jobs, the forprofit credentials
receive fewer
callbacks unless the
job requires an
external quality
indicator such as an
occupational license
(1) Access to this
online option
substantially increases
overall enrollment in
formal education (by
about 20 percentage
points) and satisfies an
unmet demand for
mid-career training.
(2) This opportunity is
estimated to boost
annual production of
American computer
science degrees by
about 7 percent

MOOCs

Online

Online

Joyce et al.
(2015)

Heppen et al.
(2012)

Keefe (2003)

Amount of in
class time on an
introductory
microeconomics
course

Online algebra
courses for credit
recovery

Two studies: (1)
lecture and
interaction online
versus traditional
face-to-face; (2)
interaction versus
regular lecture
experience

Administrative
data from Baruch
college on
student
characteristics,
previous
academic
performance, and
course test scores
and survey data
on student
attitudes.
Administrative
records of credit
recovery course
grades, credit
attainment, math
courses taken in
10th grade and
grades earned
and student
scores on the preACT; student
survey on student
perceptions, and
a selfadministered
post-course
Algebra
assessment

Pre- and postsurveys on
demographics
and
psychological
measures and
content based
post-session
exams

725 college
students at
Baruch College
in
microeconomics
course

Students in the
traditional format
scored 3.2 out of 100
points higher (0.21
SDs) on the midterm
than those in the
compressed format,
but a statistically
insignificant 1.6 points
higher (0.11 SDs) on
the final.

Online

Two cohorts of
students at
Chicago Public
Schools who
failed Algebra I
in 9th grade and
enrolled in
summer recovery
program.

In both cohorts,
students in the online
course earned
significantly lower
grades and were less
likely to recover credit
than students in the
face-to-face course.

Online High
School

Six sections of
118 students in an
Organizational
Behavior course
in Indiana
University
Southeast

(1) Students taking the
course online rated the
course and the
professor less
positively than
students taking the
course face-to-face;
(2) Students taking the
course online did 7.6
percent worse on
exams that students
taking the face-to-face
course

Online

82

Kirabo,
Jackson, and
Makarin 2016

Kizilcec et al.
(2014)

Off-the-shelf
quality lessons
and teacher
support to
promote their use

"Collectivist,"
"individualist" or
"neutral" emails
sent to MOOC
participants to
encourage forum
participation

Administrative
records for
teachers and their
students
including teacher
characteristics
and student
characteristics,
student math
achievement on
the Virginia
Standards of
Learning
assessment,
teacher survey
data on
implementation

Data from the
MOOC platform
on forum
participation

All middle school
teachers in three
school districts in
Virginia

(1) Providing teachers
with online access to
the off-the-shelf
lessons increased
student math
achievement by 0.06
SDs, (2) providing
them with online
access along with
supports to promote
their use increased
students math
achievement by 0.09
SDs.

Online

A subset of
learners who
enrolled in a
MOOC on an
undergraduatelevel computer
science topic at a
major U.S.
university

(1) The intervention
has no significant
effect on learners'
decision to contribute
to the forum, neither
one week after the
intervention, nor ten
weeks. (2) The
number of
contributions made by
learners receiving the
individualist
encouragement and
the collectivist
message are
significantly lower
than those receiving
the neutral message,
both one week and 10
weeks after the start of
the course.

MOOCs

83

Kizilcec et al.
(2017)

Lamb et al.
(2015)

Martinez
(2015A)

Mindset
interventions
addressing social
identity threat
using a "value
relevance
affirmation"
exercise and a
"social-belonging
intervention"

Self-assessment
questions and
aimed to improve
forum
participation for
MOOC students :
(1) a selfparticipation
check, (2)
discussion
priming and (3)
discussion
preview emails
Emails informing
students of their
relative position
in the course: (1)
a "positive" one
telling how many
students
recipients did
better than, and
(2) a "negative"
one stating how
other students
outperformed the
recipient

Two samples: (1)
2286 students
from a Computer
Science MOOC
offered at
Stanford, (2)
1165 students in a
6 week Harvard
MOOC

(1) The interventions
had large effects
consistent with
predictions,
eliminating the global
achievement gap in
both experiments; (2)
In the first experiment,
both interventions
doubled persistence
for learners in LDCs
and didn't affect
persistence for
learners in MDCs; (3)
In the second
experiment, the social
belonging intervention
increases persistence
for LDC learners
without affecting
persistence for MDC
learners, and the
affirmation
experiment reduced
persistence for MDC
learners, but increased
persistence for LDC
learners

MOOCs

Data from the
JusticeX
platform on
forum
participation

MOOC students
in JusticeX a
HarvardX course

Self-assessment
questions about forum
participation
encourage more
students to engage in
forums and increases
the participation of
already active
students.

MOOCs

Data from the
MOOC platform
on quiz
performance

Students
registered for a
Coursera MOOC,
Foundations of
Business Strategy
at UVA

Emails lead to
improved performance
on subsequent quizzes
(2 percentage points
for "positive" emails
and 3 percentage
points for "negative"
emails)

MOOCs

Data from the
online platforms
on each MOOC
on course
persistence

84

Martinez
(2015B)

E-mails on the
negative
correlation
between
procrastination
and achievement

Patterson
(2015)

(1) A
commitment
device where
students precommit to time
limits on
distracting
Internet activities;
(2) a reminder
tool by time spent
on districting
websites; (3) a
focusing tool that
allows students to
block distracting
sites whole on the
course website

Poirier and
Feldman
(2004)

Traditional faceto-face versus
online course

Data from the
MOOC platform
on completion
rates

Data from the
MOOC platform
on student effort
and performance,
including student
characteristics
collected from a
pre-study survey

Primary data on
student
performance on a
proctored exam.

24,122 students
from the third
Foundations of
Business Strategy
(FSB) MOOC at
University of
Virginia and
5,675 from the
fourth FSB
MOOC

(1) Students assigned
to the treatment group
were 16.85 percent
more likely to
complete the course.
(2) Another
randomized control
trial demonstrated that
the effect on the
completion rate cannot
be attributed to the
Hawthorne effect.

MOOCs

657 MOOC
participants in a
Stanford OpenX
course

(1) Commitment
device: 24 percent
more time than control
working on course and
receive course grades
0.29 SDs higher; 40
percent more likely to
complete the course;
reminder and focusing
treatments not
significantly different
from control

MOOCs

Twenty-three
students from a
large state
university who
indicate that
either a face-toface or an online
course was
acceptable

Students in the online
course performed
better on exams and
equally well on paper
assignments compared
to students in the
traditional course.
Results indicate that
students who are
amenable to taking
either an online course
of a traditional course
performed as well in
an online course as
students enrolled in a
large traditional
course.

Online

85

Yeomans and
Reich (2017)

Open-ended
planning prompts
asking students to
describe any
specific plans
they made to
engage course
content and
complete
assignments on
time.

Data from the
MOOC platform
on student
enrollment,
verification and
grades

Students in 3
HarvardX
MOOCs

Zhang (2005)

The interactive eclassroom
component of the
LBA system
versus traditional
face-to-face
classrooms

Student
performance on
content related
post-test and
student
satisfaction
survey

155
undergraduate
students from a
large public
university in the
United States

interactive video,
non-interactive
video and without
video learning
environments

Primary data
collection
including: a
student presurvey on student
characteristics, a
post-test and
student
questionnaire at
the end of each
session

138
undergraduate
students from a
large university
in Southwest
United States

Zhang et al.
(2006)

86

Planning prompts
increased course
completion by 29
percent compared to
the control condition.
This effect size is
similar to the
difference between
students who enrolled
in and completed one
MOOC before, and
students who never
enrolled in a MOOC
Students in the fully
interactive multimedia based elearning environment
achieved better
performance and
higher levels of
satisfaction than those
in a traditional
classroom and those in
a less interactive elearning environment.
(1) Students in the elearning environment
that provided
interactive video
achieved significantly
better learning
performance and a
higher level of learner
satisfaction than those
in other settings. (2)
However, students
who used the elearning environment
that provided noninteractive video did
not improve either.
The findings suggest
that it may be
important to integrate
interactive
instructional video
into e-learning
systems.

MOOCs

Online

Online

7. Conclusion
Technology has transformed large segments of society in ways that were once considered
unimaginable. Education is no exception. Around the world, there is tremendous interest in
leveraging technology to transform how students learn. In the coming years, new uses of ed-tech
will continue to flood the market, providing students, parents, and educators with a seemingly
limitless array of options. And experimental literatures are beginning to emerge in new domains,
including in-class technology like iClickers207 and adult education offered through text messages
and other new platforms.208
Amidst the buzz and sizeable investment in ed-tech, we aim to step back and take stock
of what we currently know from the experimental evidence in this nascent field. This review
hopes to advance the knowledge base by identifying and discussing the most promising uses of
ed-tech to date and highlighting areas that merit further exploration. We categorize the existing
literature into four categories: 1) access to technology, 2) computer-assisted learning, 3)
behavioral interventions, and 4) online courses.
We found that simply providing students with access to technology yields largely mixed
results. At the K-12 level, much of the experimental evidence suggests that giving a child a
computer may have limited impacts on learning outcomes, but generally improves computer
proficiency and other cognitive outcomes. One bright spot that warrants further study is the
provision of technology to students at the post-secondary level, an area with some positive RCT
evidence.
From our review, computer-assisted learning and behavioral interventions emerge as two
areas that show considerable promise. Especially when equipped with a feature of

207
208

Lantz et al., 2013
Aker et al., 2010; Ksoll et al., 2014.

87

personalization, computer-assisted learning can be quite effective in helping students learn,
particularly with math. Two interventions in the United States stand out as being particularly
promising—a fairly low-intensity online program that provides students with immediate
feedback on math homework was found to have an effect size of 0.18 standard deviations, and a
more intensive software-based math curriculum intervention improved seventh and eighth grade
math scores by a remarkable 0.63 and 0.56 standard deviations. These results mirror those from
promising interventions examined in the developing country literature, such as an adaptive
learning software in India found to have large, positive impacts on Math and Hindi. In light of
the promising evidence, more research is needed to understand the mechanisms behind
computer-assisted learning, specifically how software interacts with teachers and current
curriculum.
Like with computer-assisted learning, evaluations of behavioral interventions generally
find positive effects across all stages of the education life cycle, although they are generally
smaller than those found with the most effective computer-assisted learning models. At the same
time, technology-enabled behavioral interventions, such as large-scale text message campaigns,
are often extremely cheap to carry out and hold great promise as a cost-effective approach in
education. Moving forward, researchers should prioritize understanding when technology-based
behavioral nudges are most impactful. With the emergence of new approaches such as machine
learning, additional research can help us understand how innovative technologies may further
enhance behavioral interventions.
Though online learning courses have exploded in popularity over the last decade, there
continues to be limited rigorous research to help us understand their effectiveness. From our
review, we have found that, relative to courses with some degree of face-to-face teaching,

88

students taking online-only courses may experience negative learning outcomes. On the other
hand, the effects of blended learning are generally on-par with those of fully in-person courses.
This suggests that the appropriate combination of online and in-person learning may be costeffective. As the online learning field is constantly evolving, new research is needed to
understand how new models—such as MicroMasters programs and nanocredentials—may
impact or democratize learning.
The ed-tech field is rapidly changing, and innovative tools and programs are frequently
considered out-of-date after only several years. When faced with purchasing decisions, education
administrators often demand research that is timely, relevant, and actionable. The direction of
research and form of the research may need to change to integrate more seamlessly into decisionmaking. New tools have emerged to address some of these challenges, including Mathematica’s
Ed-Tech Rapid-Cycle Evaluation Coach and the EduStar RCT platform. While rapid-cycle
product testing is of course valuable, more research is needed to evaluate how underlying
mechanisms—rather than a specific product—can advance learning. In the end, it should not be
about the most popular product or even necessarily the technology itself, but about the best way
to help students of all ages and levels learn.

89

References
Alpert, William T., Kenneth A. Couch, and Oskar R. Harmon. 2016. “A Randomized Assessment of
Online Learning.” The American Economic Review 106 (5): 378–382.
Anderson, Monica. 2017. “Digital Divide Persists Even as Lower-Income Americans Make Gains in Tech
Adoption.” Fact Tank. March 22. Pew Research Center, http://www.pewresearch.org/facttank/2017/03/22/digital-divide-persists-even-as-lower-income-americans-make-gains-in-tech-adoption/.
Anderson, Monica. 2015. “The Demographics of Device Ownership.” October 29.
http://www.pewinternet.org/2015/10/29/the-demographics-of-device-ownership/.
Angrist, Joshua D., and Jörn-Steffen Pischke. 2008. Most Harmless Econometrics: An Empiricist's
Companion. Princeton: Princeton University Press.
Bai, Yu, Di Mo, Linxiu Zhang, Matthew Boswell, and Scott Rozelle. 2016. “The Impact of Integrating
ICT with Teaching: Evidence from a Randomized Controlled Trial in Rural Schools in China.”
Computers & Education 96 (May): 1–14.
Balu, Rekha, Kristin Porter, and Brad Gunton. 2016. “Can Informing Parents Help High School Students
Show U for School? Results from a Partnership Between New Visions for Public Schools and MDRC.”
MDRC.
Bando, Rosangela, Francisco Gallego, Paul Gertler, and Dario Romero. 2016. “Books or Laptops? The
Cost-Effectiveness of Shifting from Printed to Digital Delivery of Educational Content.” NBER Working
Paper 22928. National Bureau of Economic Research.
Banerjee, Abhijit V., Shawn Cole, Esther Duflo, and Leigh Linden. 2007. “Remedying Education:
Evidence from Two Randomized Experiments in India.” The Quarterly Journal of Economics 122 (3):
1235–64.
Banerjee, Abhijit V., and Esther Duflo. 2014. “(Dis) Organization and Success in an Economics MOOC.”
American Economic Review 104, No. 5: 514–518.
Banerjee and Duflo (2016). “Structured Study Time, Self-Efficacy, and Tutoring.” AEA RCT Registry.
May 31.
Barr, Andrew, Kelli Bird, and Benjamin L. Castleman. 2016. “Prompting Active Choice among HighRisk Borrowers: Evidence from a Student Loan Counseling Experiment.” Ed Policy Works Working
Paper Series No. 41.
Barrera-Osorio, Felipe, and Leigh L. Linden. 2009. “The Use and Misuse of Computers in Education:
Evidence from a Randomized Experiment in Colombia.” Impact Evaluation series; No. IE 29 Policy
Research working paper.
Barrow, Lisa, Lisa Markman, and Cecilia Elena Rouse. 2009. “Technology’s Edge: The Educational
Benefits of Computer-Aided Instruction.” American Economic Journal: Economic Policy 1 (1): 52–74.
BBC. 2013. “India Uttar Pradesh State Gives Away Free Laptops to Students.” March 11.
http://www.bbc.com/news/world-asia-india-21738237.

Beal, Carole, Christopher Harrison, Shandy Hauk, Weiling Li, and Steven A. Schneider. 2013.
“Randomized Controlled Trial (RCT) Evaluation of a Tutoring System for Algebra Readiness.”
Benton Foundation. “ConnectED and Modernizing the FCC's E-rate Program.” 2013.
https://www.benton.org/initiatives/e-rate?page=2%2C1%2C1.
Bergman, Peter. 2015. “Parent-Child Information Frictions and Human Capital Investment: Evidence
from a Field Experiment.” Working Paper.
Bergman, Peter. 2016a. “Technology Adoption in Education: Usage, Spillovers and Student
Achievement.” SSRN Scholarly Paper ID 2866866.
Bergman, Peter. 2016b. “Technology Adoption in Education: Usage, Spillovers and Student
Achievement.” SSRN Scholarly Paper ID 2866866.
Bergman, Peter, and Eric W. Chan. 2017. “Leveraging Technology to Engage Parents at Scale: Evidence
from a Randomized Controlled Trial.” Working Paper.
Bergman, Peter, Jeffrey T. Denning, and Dayanand Manoli. 2016. “Is Information Enough? Evidence
from a Tax Credit Information Experiment with 1,000,000 Students.” Working Paper.
Bergman, Peter, Chana Edmond-Verley, and Nicole Notario-Risk. 2016. “Parent Skills and Information
Asymmetries: Experimental Evidence from Home Visits and Text Messages in Middle and High
Schools.” Working Paper.
Bergman, Peter and Matthew J. Hill. n.d. “The Effects of Making Performance Information Public:
Regression Discontinuity Evidence From Los Angeles Teachers.” Working Paper.
Bergman, Peter, and Todd Rogers. 2016. “Parent Adoption of School Communications Technology: A
12-School Experiment of Default Enrollment Policies.” Society for Research on Educational
Effectiveness. ERIC Number: ED567596.
Bettinger, Eric P., Bridget Terry Long, Philip Oreopoulos, and Lisa Sanbonmatsu. 2012. “The Role of
Application Assistance and Information in College Decisions: Results from the H&R Block FAFSA
Experiment.” The Quarterly Journal of Economics 127 (3): 1205–1242.
Beuermann, Diether W., Julian Cristia, Santiago Cueto, Ofer Malamud, and Yyannu Cruz-Aguayo. 2015.
“One Laptop per Child at Home: Short-Term Impacts from a Randomized Experiment in Peru.” American
Economic Journal: Applied Economics 7 (2): 53–80.
Bird, Kelli A., Benjamin L. Castleman, Joshua Goodman, and Cait Lamberton. 2017. “Nudging at a
National Scale: Experimental Evidence from a FAFSA Completion Campaign.” Ed Policy Works
Working Paper Series No. 54.
Borman, Geoffrey D., James G. Benson, and Laura Overman. 2009. “A Randomized Field Trial of the
Fast ForWord Language Computer-Based Training Program.” Educational Evaluation and Policy
Analysis 31 (1): 82–106.
Bowen, William G., Matthew M. Chingos, Kelly A. Lack, and Thomas I. Nygren. 2014. “Interactive
Learning Online at Public Universities: Evidence from a Six-Campus Randomized Trial.” Journal of
Policy Analysis and Management 33 (1): 94–111.

Bursztyn, Leonardo, and Robert Jensen. 2015. “How Does Peer Pressure Affect Educational
Investments?” The Quarterly Journal of Economics 130 (3): 1329–67. doi:10.1093/qje/qjv021.
Cabalo, Ma, and Jaciw. 2007. “Comparative Effectiveness of Carnegie Learning’s Cognitive Tutor Bridge
to Algebra Curriculum.” Empirical Education Inc. ERIC Number: ED538958.
Campuzano, Larissa, Mark Dynarski, Roberto Agodini, and Kristina Rall. 2009. “Effectiveness of
Reading and Mathematics Software Products: Findings From Two Student Cohorts. NCEE 2009-4041.”
National Center for Education Evaluation and Regional Assistance.
Carrillo, Paul E., Mercedes Onofa, and Juan Ponce. 2011. “Information Technology and Student
Achievement: Evidence from a Randomized Experiment in Ecuador.” IDB Working Paper Series No.
IDB-WP-233
Carter, Susan Payne, Kyle Greenberg, and Michael S. Walker. 2017. “The Impact of Computer Usage on
Academic Performance: Evidence from a Randomized Trial at the United States Military Academy.”
Economics of Education Review 56: 118–132.
Castleman, Benjamin L., Karen Arnold, and Katherine Lynk Wartman. 2012. “Stemming the Tide of
Summer Melt: An Experimental Study of the Effects of Post-High School
Summer Intervention on Low-Income Students’ College Enrollment.” Journal of Research on
Educational Effectiveness, 5:1, 1-17.
Castleman, Benjamin L. and Katherine Meyer. 2016. “Can text message nudges improve academic
outcomes in college? Evidence from a West Virginia Initiative.” Ed Policy Works Working Paper Series
No 43.
Castleman, Benjamin L., and Lindsay C. Page. 2015. “Summer Nudging: Can Personalized Text
Messages and Peer Mentor Outreach Increase College Going among Low-Income High School
Graduates?” Journal of Economic Behavior & Organization 115: 144–160.
Castleman, Benjamin L., and Lindsay C. Page. 2016. “Freshman Year Financial Aid Nudges: An
Experiment to Increase FAFSA Renewal and College Persistence.” Journal of Human Resources 51 (2):
389–415.
Castleman, Benjamin L., and Lindsay C. Page. 2017. “Parental Influences on Postsecondary Decision
Making: Evidence from a Text Messaging Experiment.” Educational Evaluation and Policy Analysis 39
(2): 361-377
Cavalluzzo, Linda, Deborah Lowther, Christine Mokher, and Xitao Fan. 2012. “Effects of the Kentucky
Virtual Schools’ hybrid program for algebra I on grade 9 student math achievement Final Report.” IES
NCEE 2012-4020.
Chande, Raj, Michael Luca, Michael Sanders, Xian-Zhi Soon, Oana Borcan, Netta Barak Corren,
Elizabeth Linos, Elspeth Kirkman, and Sean Robinson. 2015. “Curbing Adult Student Attrition: Evidence
from a Field Experiment.” Harvard Business School NOM Unit Working Paper No. 15-065.
Council of Economic Advisers. 2016. “The Digital Divide and Economic Benefits of Broadband Access.”
https://obamawhitehouse.archives.gov/sites/default/files/page/files/20160308_broadband_cea_issue_brief
.pdf.

Cristia, Julian, Pablo Ibarrarán, Santiago Cueto, Ana Santiago, and Eugenio Severín. 2012. “Technology
and Child Development: Evidence from the One Laptop per Child Program.” IDB Working Paper No.
IDB-WP-304.
Darolia, Rajeev. 2016. “An Experiment on Information Use in College Student Loan Decisions.” SSRN
Scholarly Paper ID 2805857. Rochester, NY: Social Science Research Network. FRB of Philadelphia
Working Paper No. 16-18.
Davis, Dan, Guanliang Chen, Claudia Hauff, Geert-Jan Houben, Ioana Jivet, and René F. Kizilcec. 2017.
“Follow the Successful Crowd: Raising MOOC Completion Rates through Social Comparison at Scale.”
Proceedings of the Seventh International Learning Analytics & Knowledge Conference: 454-463
Deault, Louise, Robert Savage, and Philip Abrami. 2009. “Inattention and Response to the
ABRACADABRA Web-Based Literacy Intervention.” Journal of Research on Educational Effectiveness
2 (3): 250–286.
Deming, David J., Noam Yuchtman, Amira Abulafi, Claudia Goldin, and Lawrence F. Katz. 2016. “The
Value of Postsecondary Credentials in the Labor Market: An Experimental Study.” American Economic
Review 106(3): 778–806.
Doss, Christopher, Erin Fahle, Susanna Loeb, and Ben York. 2016. “Supporting Parenting through
Differentiated and Personalized Text-Messaging: Testing Effects on Learning During Kindergarten.”
CEPA Working Paper No.16-18.
Dynarski, Mark, Roberto Agodini, Sheila Heaviside, Timothy Novak, Nancy Carey, Larissa
Campuzano, Barbara Means, et al. 2007. “Effectiveness of Reading and Mathematics Software Products:
Findings from the First Student Cohort.” Research report – Report number NCEE 2007 – 4005.
Faber, Benjamin, Rosa Sanchis-Guarner, and Felix Weinhardt. 2015. “ICT and Education: Evidence from
Student Home Addresses.” NBER Working Paper 21306. National Bureau of
Fairlie, Robert W. 2012a. “Academic Achievement, Technology and Race: Experimental Evidence.”
Economics of Education Review 31 (5): 663–679.
Fairlie, Robert W. 2012b. “The Effects of Home Access to Technology on Computer Skills: Evidence
from a Field Experiment.” Information Economics and Policy 24 (3–4): 243–53.
Fairlie, Robert W., and Samantha H. Grunberg. 2014. “Access to Technology and the Transfer Function
of Community Colleges: Evidence from a Field Experiment.” Economic Inquiry 52 (3): 1040–1059.
Fairlie, Robert W. 2015. “Do Boys and Girls Use Computers Differently, and Does it Contribute to Why
Boys Do Worse in School than Girls? CESifo Working Paper Series No. 5496.
Fairlie, Robert W., and Peter Riley Bahr. 2017 “The Labor Market Returns to Computer Skills:
Evidence from a Field Experiment and California UI Earnings Records.” Working Paper.
Fairlie, Robert W., and Ariel Kalil. 2017. “The Effects of Computers on Children’s Social Development
and School Participation: Evidence from a Randomized Control Experiment.” Economics of Education
Review. 57: 10-19.

Fairlie, Robert W., and Rebecca A. London. 2012. “The Effects of Home Computers on Educational
Outcomes: Evidence from a Field Experiment with Community College Students.” The Economic
Journal 122 (561): 727–753.
Fairlie, Robert W., and Jonathan Robinson. 2013. “Experimental Evidence on the Effects of Home
Computers on Academic Achievement among Schoolchildren.” American Economic Journal: Applied
Economics 5(3): 211-240.
Figlio, David, Mark Rush, and Lu Yin. 2013. “Is It Live or Is It Internet? Experimental Estimates of the
Effects of Online Instruction on Student Learning.” Journal of Labor Economics 31 (4): 763–784.
Forsyth, Donelson R., Natalie K. Lawrence, Jeni L. Burnette, and Roy F. Baumeister. 2007.
“Attempting to Improve the Academic Performance of Struggling College Students by Bolstering Their
Self–Esteem: An Intervention That Backfired.” Journal of Social and Clinical Psychology 26 (4): 447-59.
Fryer, Roland G. 2016. “Information, Non-Financial Incentives, and Student Achievement: Evidence
from a Text Messaging Experiment.” Journal of Public Economics 144: 109–121.
Good, Catherine, Joshua Aronson, and Michael Inzlicht. 2003. “Improving adolescents’ standardized test
performance: An intervention to reduce the effects of stereotype threat.” Applied Developmental
Psychology 24: 645–662.
Good, Catherine, Joshua Aronson, and Michael Inzlicht. 2003. “Improving adolescents’ standardized test
performance: An intervention to reduce the effects of stereotype threat.” Applied Developmental
Psychology 24: 645 – 662.
Goodman, Joshua, Julia Melkers, and Amanda Pallais. 2016. “Can Online Delivery Increase Access to
Education?” Working Paper 22754. National Bureau of Economic Research.
Goolsbee, Austan, and Jonathan Guryan. 2006. “The Impact of Internet Subsidies in Public Schools.” The
Review of Economics and Statistics 88 (2): 336–347.
Harackiewicz, Judith M., Christopher S. Rozek, Chris S. Hulleman, and Janet S. Hyde. 2012. “Helping
Parents to Motivate Adolescents in Mathematics and Science: An Experimental Test of a Utility-Value
Intervention.” Psychological Science 23 (8): 899–906.
He, F., L. Linden, and M. MacLeod. 2007. “Helping Teach What Teachers Don’t Know: An Assessment
of the Pratham English Language Program Cambridge, MA: Abdul Latif Jameel Poverty Action Lab
(JPAL).”
Hegedus, Stephen J., Sara Dalton, and John R. Tapper. 2015a. “The Impact of Technology-Enhanced
Curriculum on Learning Advanced Algebra in US High School Classrooms.” Educational Technology
Research and Development 63 (2): 203–28.
Heppen, Jessica, Nicholas Sorensen, Elaine Allensworth, Kirk Walters, Suzanne Stachel, and Valerie
Michelman. 2012. “Efficacy of Online Algebra I for Credit Recovery for At-Risk Ninth Graders:
Consistency of Results from Two Cohorts.” Society for Research on Educational Effectiveness. ERIC
Number: ED 562703.
Horrigan, John B. 2015. “The Numbers Behind the Broadband ‘Homework Gap.’” Fact Tank, April 20.
http://www.pewresearch.org/fact-tank/2015/04/20/the-numbers-behind-the-broadband-homework-gap/.

Hurwitz, Lisa B., Alexis R. Lauricella, Ann Hanson, Anthony Raden, and Ellen Wartella. 2015.
“Supporting Head Start Parents: Impact of a Text Message Intervention on Parent–child Activity
Engagement.” Early Child Development and Care 185 (9): 1373–1389.
Joyce, Ted, Sean Crockett, David A. Jaeger, Onur Altindag, and Stephen D. O’Connell. 2015. “Does
Classroom Time Matter?” Economics of Education Review 46: 64–77.
Kang, Cecilia. 2016. “Bridging a Digital Divide That Leaves Schoolchildren Behind.” New York Times,
February 22. https://www.nytimes.com/2016/02/23/technology/fcc-internet-access-school.html.
Karam, Rita, John F. Pane, Beth Ann Griffin, Abby Robyn, Andrea Phillips, and Lindsay Daugherty.
2017. “Examining the Implementation of Technology-Based Blended Algebra I Curriculum at Scale.”
Educational Technology Research and Development 65 (2): 399–425.
Keefe, Thomas J. 2003. “Using Technology to Enhance a Course: The Importance of Interaction.”
Educause Quarterly 1.
Kelly, Kim, Neil Heffernan, Cristina Heffernan, Susan Goldman, James Pellegrino, and Deena Soffer
Goldstein. 2013. “Estimating the Effect of Web-Based Homework.” In International Conference on
Artificial Intelligence in Education, 824–827.
Kirabo, C. Jackson and Alexey Makarin. 2016. “Can Online Off-The-Shelf Lessons Improve Student
Outcomes? Evidence From A Field Experiment.” NBER Working Paper 22398. National Bureau of
Economic Research.
Kizilcec, René F., Emily Schneider, Geoffrey L. Cohen, Daniel A. McFarland. 2014. “Encouraging
Forum Participation in Online Courses with Collectivist, Individualist and Neutral Motivational
Framings.” eLearning Papers 37, 13-22.
Kizilcec, René F., Andrew J. Saltarelli, Justin Reich, Geoffrey L. Cohen. 2017. “Closing global
achievement gaps in MOOCs.” Science 355 (6322): 251-252.
Kraft, Matthew A., and Shaun M. Dougherty. 2013. “The Effect of Teacher–family Communication on
Student Engagement: Evidence from a Randomized Field Experiment.” Journal of Research on
Educational Effectiveness 6 (3): 199–222.
Kraft, Matthew A., and Manuel Monti-Nussbaum. 2017. “Can Schools Empower Parents to Prevent
Summer Learning Loss? A Text Messaging Field Experiment to Promote Literacy Skills.” The ANNALS
of the American Academy of Political and Social Science.
Kraft, Matthew A., and Todd Rogers. 2015. “The Underutilized Potential of Teacher-to-Parent
Communication: Evidence from a Field Experiment.” Economics of Education Review 47: 49–63.
Ksoll, Christopher, Jenny Aker, Danielle Miller, Karla C. Perez, and Susan L. Smalley. 2014. “Learning
without Teachers? A Randomized Experiment of a Mobile Phone-Based Adult Education Program in Los
Angeles.” CGD Working Paper 368. Washington, DC: Center for Global Development.
Lai, Fang, Renfu Luo, Linxiu Zhang, Xinzhe Huang, and Scott Rozelle. 2015. “Does Computer-Assisted
Learning Improve Learning Outcomes? Evidence from a Randomized Experiment in Migrant Schools in
Beijing.” Economics of Education Review 47: 34–48.

Lai, Fang, Linxiu Zhang, Yu Bai, Chengfang Liu, Yaojiang Shi, Fang Chang, and Scott Rozelle. 2016.
“More Is Not Always Better: Evidence from a Randomised Experiment of Computer-Assisted Learning
in Rural Minority Schools in Qinghai.” Journal of Development Effectiveness 8 (4): 449–72.
Lai, Fang, Linxiu Zhang, Xiao Hu, Qinghe Qu, Yaojiang Shi, Yajie Qiao, Matthew Boswell, and Scott
Rozelle. 2013. “Computer Assisted Learning as Extracurricular Tutor? Evidence from a Randomised
Experiment in Rural Boarding Schools in Shaanxi.” Journal of Development Effectiveness 5 (2): 208–
231.
Lamb, Anne, Jascha Smilack, Andrew Ho, and Justin Reich. 2015. “Addressing Common Analytic
Challenges to Randomized Experiments in MOOCs: Attrition and Zero-Inflation.” L@S ‘15 Proceedings
of the Second (2015) ACM Conference on Learning @ Scale: 21-30.
Leuven, Edwin, Mikael Lindahl, Hessel Oosterbeek, and Dinand Webbink. 2007. “The Effect of Extra
Funding for Disadvantaged Pupils on Achievement.” The Review of Economics and Statistics 89 (4):
721–36.
Linden, Leigh L. 2008. Complement or Substitute?: The Effect of Technology on Student Achievement in
India. InfoDev.
Malamud, Ofer, and Cristian Pop-Eleches. 2011. “Home Computer Use and the Development of Human
Capital.” The Quarterly Journal of Economics 126 (2): 987–1027.
Martinez, Ignacio. 2014. “The effects of informational nudges on students' effort and performance:
Lessons from a MOOC.” EdPolicyWorks Working Paper Series No. 19.
Martinez, Ignacio. 2015. “Never Put Off Till Tomorrow?” EdPolicyWorks Working Paper Series No. 28.
Mayer, Susan E., Ariel Kalil, Philip Oreopoulos, and Sebastian Gallegos. 2015. “Using Behavioral
Insights to Increase Parental Engagement: The Parents and Children Together (PACT) Intervention.”
NBER Working Paper 21602. National Bureau of Economic Research.
McGuigan, Martin, Sandra McNally, and Gill Wyness. 2012. Student Awareness of Costs and Benefits of
Educational Decisions: Effects of an Information Campaign. CEE DP 139. Centre for the Economics of
Education.
McLester, Susan. 2012. “One Tablet Per Child?” Last modified May 16.
https://www.districtadministration.com/article/one-tablet-child-0.
Meuwissen, Alyssa, Alison Giovanelli, Madelyn Labella, and Amy Susman-Stillman. n.d. “Text2Learn:
An Early Literacy Texting Intervention by Community Organizations.”
Mitchell, Mary Jane and Barbara J. Fox. 2001. “The Effects of Computer Software for Developing
Phonological Awareness in Low-Progress Readers.” Reading Research and Instruction
Summer 40(4) 325-332.
Mo, Di, Weiming Huang, Yaojiang Shi, Linxiu Zhang, Matthew Boswell, and Scott Rozelle. 2015.
“Computer Technology in Education: Evidence from a Pooled Study of Computer Assisted Learning
Programs among Rural Students in China.” China Economic Review 36: 131–45.

Mo, Di, Johan Swinnen, Linxiu Zhang, Hongmei Yi, Qinghe Qu, Matthew Boswell, and Scott Rozelle.
2013. “Can One-to-One Computing Narrow the Digital Divide and the Educational Gap in China? The
Case of Beijing Migrant Schools.” World Development 46: 14–29.
Mo, Di, Linxiu Zhang, Renfu Luo, Qinghe Qu, Weiming Huang, Jiafu Wang, Yajie Qiao, Matthew
Boswell, and Scott Rozelle. 2014. “Integrating Computer-Assisted Learning into a Regular Curriculum:
Evidence from a Randomised Experiment in Rural Schools in Shaanxi.” Journal of Development
Effectiveness 6 (3): 300–323.
Mo, Di, Linxiu Zhang, Jiafu Wang, Weiming Huang, Yaojiang Shi, Matthew Boswell, and Scott Rozelle.
2014. “The Persistence of Gains in Learning from Computer Assisted Learning (CAL): Evidence from a
Randomized Experiment in Rural Schools in Shaanxi Province in China.” Unpublished Manuscript.
Stanford, CA: Rural Education Action Program (REAP).
Molnar, Michele. 2017. “Ed-Tech Surges Internationally–and Choices for Schools Become More
Confusing.” Ed Week, January 20. https://marketbrief.edweek.org/marketplace-k-12/ed-tech-surgesinternationally-choices-schools-become-confusing/.
Morgan, Pat, and Steven Ritter. 2002. “An Experimental Study of the Effects of Cognitive Tutor Algebra
I on Student Knowledge and Attitude.” Pittsburgh, PA: Carnegie Learning, Inc.
Morisano, Dominique, Jacob B. Hirsh, Jordan B. Peterson, Robert O. Pihl, and Bruce M. Shore. 2010.
“Setting, Elaborating, and Reflecting on Personal Goals Improves Academic Performance.” Journal of
Applied Psychology 95 (2): 255.
Morrison, Nick. 2017. “Google Leapfrogs Rivals to Be Classroom King.” Forbes, May 9.
https://www.forbes.com/sites/nickmorrison/2017/05/09/google-leapfrogs-rivals-to-be-classroomking/#32966ae927a6.
Muralidharan, Karthik, Abhijeet Singh, and Alejandro J. Ganimian. 2016. “Disrupting Education?
Experimental Evidence on Technology-Aided Instruction in India.” NBER Working Paper 22923.
National Bureau of Economic Research.
Naik, Gopal, Chetan Chitre, Manaswini Bhalla, and Jothsna Rajan. 2016. “Can Technology Overcome
Social Disadvantage of School Children’s Learning Outcomes? Evidence from a Large-Scale Experiment
in India.” SSRN Scholarly Paper ID 2775558.
Obama White House Archives. “ConnectED Initiative.”
https://obamawhitehouse.archives.gov/issues/education/k-12/connected.
One Laptop per Child. http://laptop.org/en/vision/mission/.
Oreopoulos, Philip, and Ryan Dunn. 2013. “Information and College Access: Evidence from a
Randomized Field Experiment.” The Scandinavian Journal of Economics 115 (1): 3–26.
Oreopoulos, Philip, and Reuben Ford. 2016. “Keeping College Options Open: A Field Experiment to
Help All High School Seniors Through the College Application Process.” Working Paper 22320. National
Bureau of Economic Research.
Oreopoulos, Philip, and Uros Petronijevic. 2017. “Student Coaching: How Far Can Technology Go?”
Journal of Human Resources, February, 1216–8439R.

Page, Lindsay C., Benjamin Castleman, and Katharine Meyer. 2016. “Customized Nudging to Improve
FAFSA Completion and Income Verification.” SSRN.
Pane, John F., Daniel F. McCaffrey, Mary Ellen Slaughter, Jennifer L. Steele and Gina S. Ikemoto. 2010.
“An Experiment to Evaluate the Efficacy of Cognitive Tutor Geometry.” Journal of Research on
Educational Effectiveness, 3: 254–281.
Pane, John F., Beth Ann Griffin, Daniel F. McCaffrey, and Rita Karam. 2014. “Effectiveness of
Cognitive Tutor Algebra I at Scale.” Educational Evaluation and Policy Analysis 36 (2): 127–144.
Patterson Richard W. 2015. “Can Behavioral Tools Improve Online Student Outcomes?
Experimental Evidence from a Massive Open Online Course.” Working Paper.
Paunesku, David, Gregory M. Walton, Carissa Romero, Eric N. Smith, David S. Yeager, and Carol S.
Dweck. 2015. “Mind-Set Interventions Are a Scalable Treatment for Academic Underachievement.”
Psychological Science, 0956797615571017.
Pew Research Center. 2017. “Internet/Broadband Fact Sheet.” Last modified January 12.
http://www.pewinternet.org/fact-sheet/internet-broadband/.
Piper, Benjamin, Stephanie Simmons Zuilkowski, Dunston Kwayumba, and Carmen Strigel. 2016. “Does
Technology Improve Reading Outcomes? Comparing the Effectiveness and CostEffectiveness of ICT Interventions for Early Grade Reading in Kenya.” International Journal of
Educational Development 49: 204–14.
Poirier, Christopher R., and Robert S. Feldman. 2004. “Teaching in Cyberspace: Online Versus
Traditional Instruction Using a Waiting-List Experimental Design.” Teaching of Psychology 31 (1): 59–
62.
Ragosta, Marjorie, and others. 1982. “Computer-Assisted Instruction and Compensatory Education: The
ETS/LAUSD Study. The Final Report.” http://eric.ed.gov/?id=ED222169.
Ritter, Steven, Jonna Kulikowich, P.-W. Lei, Christy L. McGuire, and Pat Morgan. 2007. “What
Evidence Matters? A Randomized Field Trial of Cognitive Tutor Algebra I.” Frontiers in Artificial
Intelligence and Applications 162: 13.
Rockoff. n.d. “Evaluation Report on the School of One i3 Expansion.” Working Paper.
Rogers, Todd and Avi Feller. 2016. “Reducing Student Absences at Scale.” Working Paper.
Roschelle, Jeremy, Mingyu Feng, Robert F. Murphy, and Craig A. Mason. 2016. “Online Mathematics
Homework Increases Student Achievement.” AERA Open 2 (4): 2332858416673968.
Roschelle, Jeremy, Nicole Shechtman, Deborah Tatar, Stephen Hegedus, Bill Hopkins, Susan Empson,
Jennifer Knudsen, and Lawrence P. Gallagher. 2010. “Integration of Technology, Curriculum, and
Professional Development for Advancing Middle School Mathematics: Three Large-Scale Studies.”
American Educational Research Journal 47 (4): 833–878.

Rouse, Cecilia Elena, and Alan B. Krueger. 2004. “Putting Computerized Instruction to the Test: A
Randomized Evaluation of a ‘scientifically Based’ Reading Program.” Economics of Education Review,
Special Issue In Honor of Lewis C. Solman, 23 (4): 323–38. doi:10.1016/j.econedurev.2003.10.005.
Rutherford, Teomara, George Farkas, Greg Duncan, Margaret Burchinal, Melissa Kibrick, Jeneen
Graham, Lindsey Richland, et al. 2014. “A Randomized Trial of an Elementary School Mathematics
Software Intervention: Spatial-Temporal Math.” Journal of Research on Educational Effectiveness 7 (4):
358–383.
SIIA. 2015. “SIIA Estimates $8.38 Billion US Market for PreK-12 Educational Software and Digital
Content.” Last modified February 24. http://www.siia.net/Press/SIIA-Estimates-838-Billion-Dollars-USMarket-for-PreK-12-Educational-Software-and-Digital-Content.
School Guides. 2014. “Survey Reveals How Much College Students Rely on Technology.” July 13.
http://www.schoolguides.com/College_News/Survey_reveals_how_much_college_students_rely_on_tech
nology_643742.html.
Singh, Ravi, M. Saleem, P. Pradhan, Cristina Heffernan, N. Heffernan, Leena Razzaq, and M. Dailey.
2011. “Improving K-12 Homework with Computers.” In Proceedings of the Artificial Intelligence in
Education Conference, 328–336.
Simhan, T.E. Raja. 2011. “Distribution of Free Laptops to TN Students from Sept 15.” Business Line,
June 21. http://www.thehindubusinessline.com/economy/policy/distribution-of-free-laptops-to-tnstudents-from-sept-15/article2123738.ece.
Snipes, Jason, Chun-Wei Huang, Karina Jaquet, and Neal Finkelstein. 2015. “The effects of the Elevate
Math summer program on math achievement and algebra readiness.” U.S. Department of Education,
Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance,
Regional Educational Laboratory West. REL 2015–096
Tatar, Deborah, Jeremy Roschelle, Jennifer Knudsen, Nicole Shechtman, Jim Kaput, and Bill Hopkins.
2008. “Scaling up Innovative Technology-Based Mathematics.” The Journal of the Learning Sciences 17
(2): 248–286.
Trucano, Michael. 2013. “Big Educational Laptop and Tablet Projects -- Ten Countries to Learn From.”
EduTech, July 31. http://blogs.worldbank.org/edutech/big-educational-laptop-and-tablet-projects-tencountries.
Unkovic, Cait, Maya Sen, and Kevin M. Quinn. 2016. “Does Encouragement Matter in Improving
Gender Imbalances in Technical Fields? Evidence from a Randomized Controlled Trial.” PLOS ONE 11
(4): e0151714.
Van Klaveren, Chris, Sebastiaan Vonk, and Ilja Cornelisz. 2017. “The effect of adaptive versus static
practicing on student learning - evidence from a randomized field experiment.” Economics of Education
Review 58: 175–187.
Wang, Haiwen, and Katrina Woodworth. 2011. “Evaluation of Rocketship Education’s Use of DreamBox
Learning’s Online Mathematics Program.” SRI International Center for Education Policy.
Worcester Polytechnic Institute. 2016. “ASSISTments.” https://www.assistments.org/.

West, Darrell M., and Jack Karsten. 2016. “Rural and Urban America Divided by Broadband Access.”
Tech Tank, July 18. https://www.brookings.edu/blog/techtank/2016/07/18/rural-and-urban-americadivided-by-broadband-access/.
Wijekumar, Kausalai, Bonnie JF Meyer, and Pui-Wa Lei. 2012. “Large-scale randomized controlled trial
with 4th graders using intelligent tutoring of the structure to improve nonfiction reading comprehension.”
Educational Technology Research and Development 60 (6): 987 – 1013.
Wijekumar, Kausalai, Bonnie JF Meyer, Pui-Wa Lei, Yu-Chu Lin, Lori A. Johnson, James A. Spielvogel,
Kathryn M. Shurmatz, Melissa Ray, and Michael Cook. 2014. “Multisite Randomized Controlled Trial
Examining Intelligent Tutoring of Structure Strategy for Fifth-Grade Readers.” Journal of Research on
Educational Effectiveness 7 (4): 331–357.
Yeager, David Scott, Kali H. Trzesniewski, Carol S. Dweck. 2013. “An Implicit Theories of Personality
Intervention Reduces Adolescent Aggression in Response to Victimization and Exclusion.” Child
Development 84(3): 970–988.
Yeager, David Scott, Rebecca Johnson, Brian James Spitzer, Kali H. Trzesniewski, Joseph Powers and
Carol S. Dweck. 2014. “The Far-Reaching Effects of Believing People Can Change: Implicit
Theories of Personality Shape Stress, Health, and Achievement During Adolescence.” Journal of
Personality and Social Psychology 106(6): 867-884.
Yeager, David S., Marlone D. Henderson, David Paunesku, Gregory M. Walton, Sidney D’Mello, Brian
J. Spitzer, and Angela Lee Duckworth. 2014. “Boring but Important: A Self-Transcendent Purpose for
Learning Fosters Academic Self-Regulation.” Journal of Personality and Social Psychology 107(4): 559–
580.
Yeager, David S., Carissa Romero, Dave Paunesku, Christopher S. Hulleman, Barbara Schneider, Cintia
Hinojosa, Hae Yeon Lee, Joseph O’Brien Kate Flint, Alice Roberts, Jill Trott, Daniel Greene, Gregory M.
Walton, and Carol S. Dweck. 2016. “Using Design Thinking to Improve Psychological Interventions: The
Case of the Growth Mindset During the Transition to High School.” Journal of Educational Psychology
108(3): 374–391.
Yeager, David S., Gregory M. Walton, Shannon T. Brady, Ezgi N. Akcinar, David Paunesku, Laura
Keane, Donald Kamentz, Gretchen Ritter, Angela Lee Duckworth, Robert Urstein, Eric M. Gomez, Hazel
Rose Markus, Geoffrey L. Cohen, and Carol S. Dweck. 2016. “Teaching a lay theory before college
narrows achievement gaps at scale.” Proceedings of the National Academy of Sciences of the United
States of America, 113 (24): D3341-E3348.
Yeager, David S., Paul Hanselman, Gregory Walton, Sophia Yang Hooper, Cintia P.
Hinojosa, Elizabeth Tipton, Christopher Hulleman, David Paunesku, Angela Duckworth,
Robert Crosnoe, Chandra Muller, Ronald Ferguson, Barbara Schneider & Carol S. Dweck. 2017. “How
Can We Foster Nations of Learners? An Experiment in a National Probability Sample.” Working Paper.
Yeoman, Michael and Justin Reich. 2017. “Planning Prompts Increase Course Completion in MOOCs.”
Seventh International Learning Analytics and Knowledge Conference. doi:10.1145/12345.67890.
York, Benjamin N., and Susanna Loeb. 2014. “One Step at a Time: The Effects of an Early Literacy Text
Messaging Program for Parents of Preschoolers.” NBER Working Paper 20659. National Bureau of
Economic Research.

Zhang, Dongsong. 2005. “Interactive Multimedia-Based E-Learning: A Study of Effectiveness.” The
American Journal of Distance Education,19(3): 149–162.
Zhang, Dongsong, Lina Zhou, Robert O. Briggs, and Jay F. Nunamaker. 2006. “Instructional Video in ELearning: Assessing the Impact of Interactive Video on Learning Effectiveness.” Information &
Management 43 (1): 15–27.

