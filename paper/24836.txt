NBER WORKING PAPER SERIES

TESTING
Annika B. Bergbauer
Eric A. Hanushek
Ludger Woessmann
Working Paper 24836
http://www.nber.org/papers/w24836

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
July 2018

We gratefully acknowledge comments from participants at the Spring Meeting of Young
Economists, the BGPE Research Workshop, and the center seminar of the ifo Center for the
Economics of Education. This work was supported by the Smith Richardson Foundation. The
views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2018 by Annika B. Bergbauer, Eric A. Hanushek, and Ludger Woessmann. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.

Testing
Annika B. Bergbauer, Eric A. Hanushek, and Ludger Woessmann
NBER Working Paper No. 24836
July 2018
JEL No. H0,I20,J24
ABSTRACT
School systems regularly use student assessments for accountability purposes. But, as highlighted
by our conceptual model, different configurations of assessment usage generate performanceconducive incentives of different strengths for different stakeholders in different school
environments. We build a dataset of over 2 million students in 59 countries observed over 6
waves in the international PISA student achievement test 2000-2015. Our empirical model
exploits the country panel dimension to investigate reforms in assessment systems over time,
where identification comes from taking out country and year fixed effects along with a rich set of
student, school, and country measures. We find that the expansion of standardized external
comparisons, both school-based and student-based, is associated with improvements in student
achievement. The effect of school-based comparison is stronger in countries with initially low
performance. Similarly, standardized monitoring without external comparison has a positive
effect in initially poorly performing countries. By contrast, the introduction of solely internal
testing and internal teacher monitoring including inspectorates does not affect student
achievement. Our findings point out the pitfalls of overly broad generalizations from specific
country testing systems.
Annika B. Bergbauer
ifo Institute
ifo Center for the Economics of Education
Poschingerstr. 5
81679 Munich, Germany
bergbauer@ifo.de
Eric A. Hanushek
Hoover Institution
Stanford University
Stanford, CA 94305-6010
and NBER
hanushek@stanford.edu

Ludger Woessmann
University of Munich
ifo Institute and CESifo
Poschingerstr. 5
81679 Munich, Germany
woessmann@ifo.de

1. Introduction
Use of student assessments for accountability purposes has grown rapidly around the world.
While some have argued that this trend has been damaging to schooling (Hout and Elliott (2011);
Andrews and coauthors (2014)), others have argued that even more student assessment is called
for. In fact, the World Bank (2018), in evaluating the need for improved human capital
development around the world, explicitly calls for expansion of student evaluations and
concludes that “[t]here is too little measurement of learning, not too much” (p. 17). However,
both critics and proponents of international and national testing often fail to differentiate among
alternative forms and uses of testing, leading to a confused debate. For example, in the United
States consideration of testing is mostly restricted to such accountability systems as exemplified
by No Child Left Behind (NCLB). In reality, there are many other dimensions of student
assessments. Testing students in order to provide external comparisons is very different from
evaluating teachers on the basis of student performance or from making selections of which
students should continue on to university. And standardized tests normed to a large population
are very different than teacher-generated tests used to assess the pace of classroom learning.
Understanding the overall impact of student testing requires careful consideration of how the
assessments are used and what incentives they create.
This paper exploits international comparisons to estimate the effects of different types and
dimensions of student assessments on overall levels of student achievement. It places the
evaluation of student assessments into the general analysis of how information is translated into
incentives for the actors and into behavioral results. The conceptual framework of a principalagent model leads us to consider three dimensions of student assessments: varying strengths of
incentives, different stakeholders on whom the incentives are focused, and dependence on
particular school environments.
While there have been previous evaluations of the impact of accountability systems, largely
within the United States (Figlio and Loeb (2011)), it is unclear how to generalize from these.
These policies operate within a specific institutional environment of national school systems; as
such, the evaluations necessarily neglect overall features that are common across a nation.
Moreover, testing policies are often set at the national level, making it difficult to construct an
adequate comparison group for evaluation of policy outcomes. By moving to international
comparisons, it is possible to consider how overall institutional structures interact with the

1

specifics of student assessments and school accountability systems. This cross-country approach
allows us to investigate which aspects of student assessment systems generalize to larger settings
and which do not. Of course, this advantage comes at a cost, because identifying the impact of
various schooling policies across nations offers its own challenges.
Our empirical analysis uses data from the Programme for International Student Assessment
(PISA) to construct a panel of country observations of student performance. Specifically, we
pool the micro data of over two million students across 59 countries participating in six PISA
waves between 2000 and 2015. PISA includes not only assessments of student outcomes, but
also rich background information on both students and schooling institutions in the different
countries. We derive a series of measures of different types of student assessments from these
survey data and from other international data sources.
Because this is a period of rapid change in student assessment policies across countries, we
can link policies to outcomes in fixed-effects panel models. Our identification relies on changes
in student assessment regimes within countries over time. While using the individual student data
for estimation at the micro level, we measure our treatment variables as country aggregates at
each point in time to avoid bias from within-country selection of students into schools.
Conditioning on country and year fixed effects allows us to account for unobserved timeinvariant country characteristics as well as common time-specific shocks. 1
Our analysis shows that some uses of student testing affect student learning, while others
have no discernible impact. We create four categories of test usage that correspond to differing
incentive patterns in our conceptual model. On the one hand, we find that expanded standardized
testing that provides external comparisons is associated with increased performance on the
international tests. This is true for both school-based and student-based forms of external
comparisons and in math, science, and reading. On the other hand, internal testing that simply
informs or monitors progress without external comparability and internal teacher monitoring
including inspectorates have little discernible effect on overall performance. While not being
related to student achievement on average, introducing standardized monitoring without external
comparison has a positive effect in initially poorly performing countries, but not in initially

1
Our analysis expands on the growing literature studying determinants of student achievement in a crosscountry setting (Hanushek and Woessmann (2011); Woessmann (2016)). Methodologically, our approach builds on
the analysis of school autonomy in Hanushek, Link, and Woessmann (2013).

2

highly performing countries. Similarly, the impact of school-based external comparisons differs
across schooling systems with larger impacts being seen in poorer performing systems.
In a placebo test with leads of the assessment variables, we show that new usages of
assessments are not systematically linked to prior outcome conditions. We also show that the
results are not affected by any individual country; that they are robust to subsets of countries, to a
long-difference specification, and to controlling for test exclusion rates; and that changes in
PISA testing procedures are not affecting the results.
Sorting out the implications of alternative testing regimes is increasingly important from a
policy perspective. As testing technologies change, it is becoming easier to expand assessments.
Further, the linkage of accountability systems with ideas of reform and improvement has led to
worldwide increases in testing for accountability purposes. At the same time, backlash to various
applications of testing and monitoring of schools has placed assessment policies into open and
often contentious public debate. Our analysis can inform this debate in a scientific way.
The next section develops a conceptual framework that highlights the achievement effects of
different dimensions of student assessments. Section 3 introduces the data and Section 4 the
empirical model. Section 5 presents our results including analyses of heterogeneous effects.
Section 6 reports a placebo test and Section 7 a series of robustness analyses. Section 8
concludes.

2. An Incentive Framework of Different Dimensions of Assessments
To frame our thinking about potential effects of different uses and displays of student
assessments, we develop a simple conceptual framework that focuses on how assessment
regimes create incentives for teachers and students to focus on raising student achievement. We
start with a basic principal-agent framework, discuss the technology of student assessment, and
then analyze three dimensions of student assessments: different strengths of incentives, different
addressees of incentives, and dependence on school environments.
2.1 Conceptual Framework: Principal-Agent Relationships
Our underlying framework is one in which parents are trying to ensure the welfare of their
children. We take a very simplified view that highlights parental choices over the schooling
investments of their children. Of course, parental choices and the activities of parents and

3

children are much more complicated than the simplified views we express here, but we want to
emphasize strategic choices about child investment and how these are affected by student
assessment systems.
Abstracting from any other factors that enter parental considerations, let us assume that
parents p aim to maximize the following value function V that balances long-run outcomes and
short-run happiness of their child (student) s:
Parents: max 𝑉𝑉𝑝𝑝 = 𝑓𝑓𝑝𝑝 [𝐴𝐴𝑠𝑠 , 𝑅𝑅𝑠𝑠 , 𝐸𝐸𝑠𝑠 ]

(1)

Specifically, parents care about their child’s achievement A of knowledge and skills, which we
believe directly affects their long-run economic outcomes (Card (1999); Hanushek et al. (2015)).
The happiness of the child in the short run depends positively on any short-term reward R for
learning and negatively on the effort E that the child has to put in.
Parents, however, cannot directly choose the elements of this value function but must work
indirectly to achieve their ends. In particular, they may offer short-term rewards for learning R to
their child and try as best as possible to observe and control child effort E. Similarly,
achievement A is only partially controlled by parents but as a general rule relies heavily upon
purchasing the services of schools. This is natural because of economies of scale in producing
knowledge, of the limited ability of parents to provide the full array of school services, and of the
benefits of specialization.
The production of achievement A can thus be described through an educational production
function that we write as
𝐴𝐴𝑠𝑠 = 𝐴𝐴𝑠𝑠 (𝐼𝐼, 𝐸𝐸𝑡𝑡 , 𝐸𝐸𝑠𝑠 )

(2)

For simplicity, child achievement A is a function of inputs I into the teaching process (including
parental inputs, school inputs, and student ability), teacher effort Et, and student effort Es.
As effort levels of teachers and children cannot be perfectly observed or controlled by
parents, this setup gives rise to a tree of standard principal-agent relationships (Laffont and
Martimort (2002)). In particular, parents act as principals that contract the teaching of their
children to schools and teachers as agents. In the process of classroom instruction, teachers also
act as principals themselves who cannot fully observe the learning effort of their students as
agents. Teaching in the classroom and studying at your desk may be viewed as classical
examples of asymmetric information where the respective principal cannot fully monitor the

4

behavior of the respective agent. Parents, teachers, and students each have specific objective
functions that combine with the asymmetric information of the actors. Therefore, one cannot
simply assume that the actions of children and teachers will lead to the optimal result for parents.
Let us assume that teachers maximize the following value function:
Teachers: max 𝑉𝑉𝑡𝑡 = 𝑓𝑓𝑡𝑡 �𝐴𝐴𝑠𝑠 �𝐼𝐼, 𝐸𝐸⏟𝑡𝑡 , 𝐸𝐸𝑠𝑠 � , 𝑅𝑅𝑡𝑡 , 𝐸𝐸⏟𝑡𝑡 �
(+)

(3a)

(−)

Teachers derive value from their students’ achievement A, which is a positive function of their
own effort Et, as well as from other short-term rewards Rt. At the same time, their effort at
teaching Et is costly to them, directly entering their value function negatively.
The value function of students is very similar, except that the focus is their own rewards and
effort:
Students: max 𝑉𝑉𝑠𝑠 = 𝑓𝑓𝑠𝑠 �𝐴𝐴𝑠𝑠 �𝐼𝐼, 𝐸𝐸𝑡𝑡 , 𝐸𝐸⏟𝑠𝑠 � , 𝑅𝑅𝑠𝑠 , 𝐸𝐸⏟𝑠𝑠 �
(+)

(4a)

(−)

Note that the students’ value function has the same arguments as the parents’ value function,
only that, for several reasons, children and parents may put different weights to the short-run and
long-run costs and rewards. For example, children may be less aware of the importance of
achievement A for their long-run well-being than parents. Furthermore, children may be less
willing or able to solve the dynamic optimization problem, leading to behavioral biases that
prevent them from pursuing their own long-run well-being (Lavecchia, Liu, and Oreopoulos
(2016)).
If parents had full information about the effort levels of teachers and students, they could
effectively contract with each to maximize their own value function. However, because of the
incomplete monitoring of effort and the differing value functions, the ensuing principal-agent
problems may lead to suboptimal effort levels by teachers and by students.
2.2 The Technology of Student Assessment
Solving these problems can be accomplished if there is sufficient information about the
effort levels of agents, but actually obtaining and monitoring effort levels is generally costly. The
more common solution is to begin with outside assessments of the outcomes of interest A.
Nonetheless, there are a number of complications with the usage of information about

5

achievement, and these are the subject of many current policy deliberations and controversies.
Because achievement is a function of both teacher and student effort, it is not easily possible to
infer the effort of either with just information on achievement levels.
At a basic level, student assessments provide information on student outcomes. They use a
testing technology τ to transform actual outcomes A into observed outcomes O:
𝑂𝑂𝑠𝑠 = 𝜏𝜏(𝐴𝐴𝑠𝑠 )

(5)

From this information on student outcomes, one can try to infer effort levels. This would then
allow creating incentives that align agents’ behavior more closely with the principals’ objective
function.
Historically, a variety of testing regimes have been developed that are designed to provide
information about achievement levels. For our purposes, however, we have to consider how any
of these assessments can be used to solve the underlying principal-agent problems. In reality, the
emerging policy choices frequently assume specific features of the production function in
arriving at solutions to these problems.
In a general way, we can think of providing rewards R to both teachers and students based
on the outcome levels O observed by the student assessments: 2
Teachers: max 𝑉𝑉𝑡𝑡 = 𝑓𝑓𝑡𝑡 �𝐴𝐴𝑠𝑠 �𝐼𝐼, 𝐸𝐸⏟𝑡𝑡 , 𝐸𝐸𝑠𝑠 � , 𝑅𝑅𝑡𝑡 �𝑂𝑂⏟𝑠𝑠 � , 𝐸𝐸⏟𝑡𝑡 �

(3b)

Students: max 𝑉𝑉𝑠𝑠 = 𝑓𝑓𝑠𝑠 �𝐴𝐴𝑠𝑠 �𝐼𝐼, 𝐸𝐸𝑡𝑡 , 𝐸𝐸⏟𝑠𝑠 � , 𝑅𝑅𝑠𝑠 �𝑂𝑂⏟𝑠𝑠 � , 𝐸𝐸⏟𝑠𝑠 �

(4b)

(+)

(+)

(+)

(+)

(−)

(−)

This effectively alters their value functions and introduces incentives for their behavior.
That is the focus of this paper: By creating outcome information, student assessments
provide a mechanism for developing better incentives to elicit increased effort by teachers and
students, thereby ultimately raising student achievement levels to better approximate the desires
of the parents. We think of the potential rewards R for observed outcomes O in a very general
way, including implicit and explicit rewards, material and non-material rewards, and ranging

2
Throughout, we have taken the simplifying assumption that there is a single teacher whose behavior is
affected by incentive schemes. In reality, the incentive schemes almost certainly have an impact not only on the
effort choices of existing teachers, but also on who becomes a teacher and the long-run supply of teachers.

6

from simple observability of outcomes over parental gratification for students to consequences
for teachers at school.
There are two issues that we have to consider. First, how do we separate the joint effort
levels of teachers and students in order to provide the right incentives? Second, how do we deal
with imperfect technologies that do not provide complete information on A? For expositional
purposes, let us start with the assumption that actual achievement is perfectly observed, i.e.,
Os = As. We will come back to the more realistic assumption that Os is only an imperfect measure
of actual achievement below.
The first issue is a classical identification problem. We want to know when we can infer
effort levels of teachers and students from information on outcomes. If student efforts were
constant over time, we could directly relate changes in achievement in a given classroom to the
teacher and from that infer their effort levels. Alternatively, if we thought teacher effort was
constant, we could attribute different performance of students to their own effort. The first is
roughly the idea behind value-added modelling (Koedel, Mihaly, and Rockoff (2015); Chetty,
Friedman, and Rockoff (2017)). The second is closer to providing consequential exit exams for
student achievement (Bishop (1997)). Of course, in neither case is it realistic to assume constant
effort by the other actor, but the policy choices implicitly assume that one form of effort is much
more important than the other. These issues will be discussed more completely in Section 2.4
below.
The second issue recognizes the fact that no assessment technology τ today provides
complete measurement of the relevant achievement for long-run well-being. Prior discussions of
accountability systems have considered various dimensions of this problem (Figlio and Loeb
(2011)). Perhaps the best-known conceptual discussion is the classic Holmstrom and Milgrom
(1991) paper that considers how imperfect measurement of outcomes distorts incentives (see also
Dixit (2002)). In particular, if there are multiple objectives and only a subset is measured, effort
could be distorted to the observed outcomes to the detriment of unobserved outcomes. But there
is also more general discussion of such topics as teaching to the test (Koretz (2017)), 3 gaming of

3

There are two aspects of teaching to the test. On the one hand, teaching may unduly focus on the form and
character of the test itself, which is not in the interest of parents. Creative and flexible designs of tests are required to
prevent such activity. On the other hand, if the tests accurately sample from the domains of achievement that parents
desire, focusing teaching towards the contents of the test is in fact part of the mechanism of aligning teaching with
the parental value function.

7

tests (e.g., nutritious feeding on testing days, see Figlio and Winicki (2005)), and cheating (Jacob
and Levitt (2003)). Each of these topics includes an element of testing technology and the
accuracy of observed measures and is the subject of a much larger literature. Here, we simply
want to note that the impact of different incentives will be conditioned by elements of the testing
technology. The ultimate effects on achievement thus become an empirical question.
2.3 Assessment Dimension 1: Different Strengths of Incentives
Testing is a ubiquitous component of schooling, but not all tests have the same use or impact
in helping to solve the underlying principal-agent problems. By far the most common type of
testing is teacher-developed tests that are used both to guide instruction and to provide feedback
to students and parents. The key feature of teacher-developed tests is that it is generally difficult
if not impossible to compare results across teachers. Therefore, while these tests may be useful in
providing incentives to students and related information to parents (Os enters positively in Rs in
equation (4b)), they do not solve the principal-agent problem between parents and teachers (Os
effectively does not enter Rt in equation (3b)). One would not expect the results of these tests to
affect teacher effort levels. There is a blurry line between teacher-developed tests and periodic
content testing that generally goes under the heading of formative assessments which may also
be provided by external producers. In both cases, the information provided by the tests is just
used internally by the teacher without parents being able to compare outcomes externally.
At the other end of the continuum of testing are standardized tests that have been normed to
relevant population performance. These tests allow for direct comparisons of student outcomes
in different circumstances and thus suggest the possibility of using them to provide incentives to
teachers in addition to students.
Of course, the strength of any incentives relating to these various tests will depend upon
how they enter into rewards for teachers and students in equations (3b) and (4b). On the one
hand, results of student assessments may just provide information to some or all actors in the
system. 4 On the other hand, performance on any test may also be linked directly to consequences

4
For example, school rankings may be published to the general public (see Koning and van der Wiel (2012),
Burgess, Wilson, and Worth (2013), and Nunes, Reis, and Seabra (2015) for evidence from the Netherlands, Wales,
and Portugal, respectively), and school report cards may provide information to local communities (see Andrab,
Das, and Khwaja (2017) for evidence from a sample of villages in Pakistan).

8

– rewards and punishments to students (including retention and promotion) and teachers. 5 As a
general principle, we would naturally expect attaching consequences to results to produce
stronger incentives and larger behavioral changes.
2.4 Assessment Dimension 2: Different Addressees of Incentives
Previously, we described the overall problem as a tree of principal-agent relationships. We
did that because the problem applies to the behavior and effort levels of a wide variety of actors
in the schooling system. As a canonical description of the tree, we are concerned with the parentchild problem, the parent-teacher problem, and the teacher-child problem. Adding another layer
to the system, parents often look beyond the individual teacher to school administrators at
different levels, including the nation, the region, the school district, and the school. This suggests
that there are parent-administrator problems, administrator-administrator problems, and
administrator-teacher problems that are relevant to incentive design questions.
The optimal design of incentives generally calls for rewarding the results of behavior
directly under the control of the actor and not rewarding results from other sources. The problem
as sketched out above is that most testing includes the results of actions of multiple parties.
While incentives found in various schooling circumstances are often implicitly discussed and
instituted with one of these principal-agent problems in mind, it is easy to see how incentives
may differ across the various actors and how solving one principal-agent problem may leave
others untouched.
For example, centralized exit exams that have consequences for further schooling of
students may have strong incentives for student effort (equation (4b)), but limited impact on
teacher effort (equation (3b)). 6 On the other hand, testing that is directly linked to consequences
for schools such as the NCLB legislation in the US may have limited relevance for students and

5

Apart from systemic consequences, different parents will attach different consequences to their children for
the same performance, likely contributing to achievement differences across socioeconomic groups.
6
By affecting chances to enter specific institutions and fields of higher education as well as the hiring
decisions of potential employers, central exit exams usually have real consequences for students; see Bishop (1997),
Woessmann (2003), Woessmann et al. (2009), Jürges, Schneider, and Büchel (2005), Lüdemann (2011), and
Schwerdt and Woessmann (2017) for further analysis of the effects of central exit exams.

9

their efforts. 7 Similarly, school inspectorates and inspections of teacher lessons may be more
relevant for school and teacher effort than for student effort.
There is much public discussion of the implications of high-stakes testing, but this
frequently is not accurately aligned with incentives for the different actors in the system. For
example, differential rewards to teachers based upon test-score growth are high stakes for the
teachers, but not for the students. At the same time, tests that have no consequences for any of
the actors may be inconsequential for overall performance because nobody may take them
seriously.
2.5 Assessment Dimension 3: Dependence on School Environments
The prior conceptual discussion is framed in terms of a series of individual two-way
interactions. Understanding the implications of various testing schemes and their usage
necessarily involves looking at performance across schools and, in our case, across countries.
When we think in these larger terms, it is difficult to believe that behavior is uniform across
systems even when confronted with the same incentive structure. 8
For example, if we look at a set of high-performing schools, we may think that they know
how to react to achievement signals and different rewards. Therefore, we may expect that any
type of incentive structure created by student assessments has a stronger impact on them than on
an otherwise comparable set of low-performing schools. But at the same time, we might think
that the results are just the opposite: Low-performing schools have more room for improvement
and may be in greater need to have their incentives focused on student outcomes. Highperforming schools, by contrast, may have the capabilities and be subject to overall political and
schooling institutions that already better reflect the desires of parents.

7

For analyses of the effects of NCLB and predecessor reforms, see Hanushek and Raymond (2005), Jacob
(2005), Dee and Jacob (2011), Reback, Rockoff, and Schwartz (2014), and Deming et al. (2016); see Figlio and
Loeb (2011) for a survey.
8
Another dimension of heterogeneity may be across parents within a system, in that different parents have
different value functions (including different discount rates that affect the relative value of short-term and long-term
outcomes) and/or different capacity to drive favorable results. Such differences may lie behind movements such as
parents opting out of state-wide testing in the US, in that some parents may feel that the measured output does not
provide much information about the type of achievement that they care about.

10

3. International Panel Data
For our analysis, we combine the student micro data of all available waves of the PISA
international achievement test with measures of different types of student assessment policies
over a period of 15 years. We describe each of the two components in turn.
3.1 Six Waves of PISA Student Achievement Tests
In 2000, the Organisation for Economic Co-operation and Development (OECD) conducted
the first wave of the international student achievement test called Programme for International
Student Assessment (PISA). Since then, PISA has tested the math, science, and reading
achievement of representative samples of 15-year-old students in all OECD countries and in an
increasing number of non-OECD countries on a three-year cycle (OECD (2016)). 9 PISA makes a
concerted effort to ensure random sampling of schools and students and to monitor testing
conditions in participating countries. Data are not reported for countries that do not meet the
standards. 10 PISA does not follow individual students over time. But the repeated testing of
representative samples of students creates a panel structure of countries observed every three
years.
In our analyses, we consider all countries that have participated in at least three of the six
PISA waves between 2000 and 2015. 11 This yields a sample of 59 countries observed in 303
country-by-wave observations. We perform our analysis at the individual student level,
encompassing a total sample of 2,187,415 students in reading and slightly less in math and
science. The sample, listed in Table 1, includes 35 OECD and 24 non-OECD countries that
encompass a wide range of levels of economic development and student achievement.

9

The target population contains all 15-year-old students irrespective of the educational institution or grade that
they attend. Most countries employ a two-stage sampling design, first drawing a random sample of schools in which
15-year-old students are enrolled (with sampling probabilities proportional to schools’ number of 15-year-old
students) and second randomly sampling 35 students of the 15-year-old students in each school.
10
In particular, due to deviations from the protocol, the data exclude the Netherlands in 2000, the United
Kingdom in 2003, the United States in the reading test 2006, and Argentina, Kazakhstan, and Malaysia in 2015.
11
We include the tests conducted in 2002 and 2010 in which a number of previously non-participating
countries administered the 2000 and 2009 test, respectively. We exclude any country-by-wave observation for which
the whole information of a background questionnaire is missing. This applies to France from 2003-2009 (missing
school questionnaire) and Albania in 2015 (missing student questionnaire). Due to its small size, Liechtenstein was
also dropped.

11

PISA uses a broad set of tasks of varying difficulty to create a comprehensive indicator of
the continuum of students’ competencies in each of the three subjects. Overall testing lasts for up
to two hours. Using item response theory, achievement in each domain is mapped on a scale with
a mean of 500 test-score points and a standard deviation of 100 test-score points for OECDcountry students in the 2000 wave. The test scales are then psychometrically linked over time. 12
Until 2012, PISA employed paper and pencil tests. In 2015, the testing mode was changed to
computer-based testing, a topic we will come back to in our robustness analysis below.
Figure 1 depicts the evolution of math achievement of each country over the 15-year period.
While average achievement across all countries was quite stable between 2000 and 2015,
achievement has moved significantly up in some countries and significantly down in others. In
14 countries, achievement improved by at least 20 percent of a standard deviation compared to
their initial achievement (in decreasing order, Peru, Qatar, Brazil, Luxembourg, Chile, Portugal,
Israel, Poland, Italy, Mexico, Indonesia, Colombia, Latvia, and Germany). On the other hand,
achievement decreased by at least 20 percent of a standard deviation in eleven countries (United
States, Korea, Slovak Republic, Japan, France, Netherlands, Finland, Iceland, United Kingdom,
Australia, and New Zealand).
In student and school background questionnaires, PISA provides a rich array of background
information on the participating students and schools. Students are asked to provide information
on their personal characteristics and family background, and school principals provide
information on the schools’ resources and institutional setting. While some questionnaire items,
such as student gender and age, remain the same across the six PISA assessment cycles, other
information is not available in or directly comparable across all waves. We therefore select a set
of core variables of student characteristics, family backgrounds, and school environments that
are available in each of the six waves and merge them with the test score data into one dataset
comprising all PISA waves.
Our vector of control variables allows us to condition on a rich set of observed
characteristics of students, schools, and countries. The student-level controls include student
gender, age, first- and second-generation immigration status, language spoken at home, parental
education (measured in six categories), parental occupation (four categories), and books at home

12

The math (science) test was re-scaled in 2003 (2006), any effect of which should be captured by the year
fixed effects included in our analysis.

12

(four categories). The school-level controls include school size (number of students), community
location (five categories), share of fully certified teachers, principals’ assessments of the extent
to which learning in their school is hindered by teacher absenteeism (four categories), shortage
of math teachers, private operation, and share of government funding. At the country level, we
include GDP per capita and, considering the results in Hanushek, Link, and Woessmann (2013),
the share of schools with academic-content autonomy and its interaction with initial GDP per
capita. To avoid sample selection bias from non-response in the survey data, we impute missing
values in the student and school background variables by using the respective country-by-wave
mean. 13 To ensure that imputed data are not driving our results, all our regressions include a set
of dummy variables – one for each variable with missing data – that are set to one for imputed
values and zero otherwise.
3.2 Categories of Assessment Usage
From the PISA school background questionnaires and other sources, we derive a series of
measures of different categories of the use of student assessments over the period 2000-2015.
The central insight of our conceptual modeling is that different kinds of tests and different uses
of these tests create varied incentives, and these are likely to show up in different achievement
outcomes. To be useful for the analysis, we need information on different testing practices that is
consistent both across countries and across time. There are several sources that provide relevant
data while meeting these stringent requirements. Obviously, survey designers and organizations
supplying information about assessments have not had our conceptual model in mind when
initiating their work. Thus, we have questions that cover a wide range of narrow aspects of
testing, and for our empirical analysis it is useful to collapse several individual items into more
general categories.
Here we summarize the categories of testing that we construct, while the details of questions
and sources can be found in the Data Appendix. From a combination of the surveys for
principals that accompany the PISA assessments, of the regular publications and data collection
of other parts of the OECD, and from data compiled under the auspices of the European
Commission, we have 13 separate indicators of the use and purpose of testing, each measured at

13

The share of missing values is generally very low, see Appendix Table A1.

13

the country-by-wave level. 14 We combine these into four separate categories that represent quite
different aspects of testing in the schools. They differ by the degree of standardization of the
assessment data and the specific actors – administrators, teachers, and students – most affected.
We construct these aggregate measures because of overlap and correlations among the individual
questions and because of potential measurement error in different individual questions of similar
content areas.
Standardized External Comparisons. The first category relates to four separate data
sources that identify use of standardized assessments constructed outside of the schools and used
explicitly to allow comparisons of student outcomes across schools and students. This category
includes the proportion of schools where (according to the principals of schools participating in
PISA) performance of 15-year-olds is regularly compared through external examinations to
students across the district or the nation (which we term “school-based external comparisons”). It
also includes indicators of whether central examinations affect student placement at lower
secondary level (two sources) and whether central exit exams determine student outcomes at the
end of secondary school (which, together, we term “student-based external comparisons”). 15 This
overall category of exams has strong incentives through the rewards to students but also affects
rewards to administrators and teachers by making external information available to parents and
policy makers. While not fully explicit from the surveys, the items in this category are roughly
ones where consequential outcomes are related to student scores, making for stronger
incentives. 16
Standardized Monitoring. In other instances, standardized assessments are used to monitor
the performance of students, teachers, or schools without necessarily involving any external
comparison or public recording. Three questions in the PISA survey provide information on the
prevalence of different aspects of this usage: standardized testing in the tested grade, monitoring
of teacher practices by assessments, and tracking of achievement data by an administrative
authority. While not always clear, these test usages appear closer to report card systems without
14
Appendix Table A2 provides an overview of the different underlying assessment indicators. Appendix Table
A3 indicates the number of country observations by wave for each indicator.
15
As discussed in the Data Appendix, data on assessments used for student placement are available for only a
subset of countries, largely the OECD countries.
16
In prior work on U.S. accountability, accountability that had consequential impacts on schools were much
more closely related to student performance than accountability that was confined to report card information
(Hanushek and Raymond (2005)).

14

external comparison and imply less powerful incentives than in the previous category of external
comparisons.
Internal Testing. This category would generally cover testing – either standardized or
unstandardized – that is used for general pedagogical management including informing parents
of student progress, public posting of outcomes, and tracking school outcomes across cohorts.
The data come from three separate PISA questions and, in our conceptual framework, represent
low-level incentives because of the lack of comparability across student groups.
Internal Teacher Monitoring. In addition to the general use of internal assessments
covered in the previous category, this final category covers internal assessments that are directly
focused on teachers. Specifically, this category, again derived directly from the principal surveys
in PISA, combines schools’ use of assessments to judge teacher effectiveness and the monitoring
of teacher practice by principals and by external inspectorates. These assessments would have
minimal incentives for students and uncertain but generally small impacts on teacher rewards
because of the lack of comparability across settings.
Aggregation of Separate Indicators. We combine the original 13 separate indicators of
assessment practices into four main categories as the simple average of the observed indicators in
each category. 17 Constructing the aggregate categories serves several purposes. In various
instances the survey items are measuring very similar concepts, so that the aggregation acts to
reduce individual measurement error and to limit multicollinearity at the country level (which is
key in our identification strategy). For example, using our aggregate country-by-wave data, some
individual indicators are correlated above 0.5 even after extracting country and year fixed
effects. Additionally, the aggregation permits including the added information from some more
specialized OECD and EU sources while not forcing elimination of other countries outside these
boundaries.
Some Descriptive Statistics. Table 2 provides descriptive statistics both for the individual
indicators of student assessment and for the four combined assessment categories. The measures
derived from the PISA background questionnaires are shares bounded between 0 and 1, whereas

17

The variables in each category are calculated as proportionate usage in terms of the specific indicators for
each country and wave. Note also that indicator data entirely missing for specific PISA waves are imputed by
country-specific linear interpolation of assessment usages, a procedure that retains the entire country-by-wave
information but that does not influence the estimated impact of the test category because of the inclusion of
imputation dummies in the panel estimates (see Data Appendix for details).

15

the other assessments measures are dummy variables. 18 As is evident, some assessment practices
are more common than others. For example, 89 percent of schools in our country-by-wave
observations use some form of assessment to inform parents, but only 29 percent have national
standardized exams in lower secondary school. Table 1 provides country-by-country statistics of
the initial and final value of three selected measures of standardized external comparison. Of
particular relevance, there is a tendency for increased prevalence of the measures of standardized
external comparison over time.
The important aspect of our test usage data is the amount of variation over time within
individual countries. To give some understanding of the patterns of change, Figure 2 provides a
depiction of the evolution of using standardized assessments for school-based external
comparison from 2000 to 2015 for each country. The increasing use of such external assessments
in many countries is quite evident. For example, in five countries, the share of schools that are
externally compared with student assessments increased by more than 50 percentage points
(Luxembourg, Denmark, Italy, Portugal, and Poland), and in another 18 countries, the share
increased by more than 20 percentage points. In three countries, by contrast, the share decreased
by more than 20 percentage points (Tunisia, Costa Rica, and Croatia). 19

4. Empirical Model
Identifying the impacts of testing in a cross-country analysis is of course challenging.
Assessments are not exogenously distributed across schools and countries. At the student level,
an obvious potential source of bias stems from the selection of otherwise high-performing
students into schools that have specific assessment practices. At the country level, there may also
be reverse causality if poorly performing countries introduce assessment systems in order to

18

In federal countries, the dummy variables capture whether the majority of the student population in a country
is subject to the respective assessment policy.
19
It is beyond the scope of this paper to provide detailed anecdotal narratives of specific policy reforms that
underlie the changes in student assessment measures documented by the PISA school background questionnaires.
However, on a number of occasions, it is straightforward to link major policy reforms directly to the overall pattern
of expanded accountability measures. For example, the strong increase in school-based assessments used for
external comparison in Italy in 2009, clearly visible in Figure 2, coincides with the introduction of the Invalsi
national test (https://it.wikipedia.org/wiki/Test_INVALSI). Similarly, the increased external assessment in Denmark
in 2006 reflects the 2006 Folkeskole Act which introduced a stronger focus on evaluation, assessment, and
accountability including national tests (Shewbridge et al. (2011)). And the strong increase in external assessments in
Luxembourg shows the introduction of standardized national assessments that monitor student outcomes in French,
German, and mathematics (Shewbridge et al. (2012)).

16

improve their students’ achievement. Ultimately, any omitted variable that is associated both
with the existence of student assessments and with student achievement levels will lead to bias in
conventional estimation. In the cross-country setting, for example, unobserved country-level
factors such as culture, the general valuation of educational achievement, or other government
institutions may introduce omitted-variable bias.
In our empirical model, we address leading concerns of bias in cross-country estimation by
formulating a fixed-effects panel model of the following form:
𝐴𝐴𝑖𝑖𝑖𝑖𝑖𝑖 = 𝐼𝐼𝑖𝑖𝑖𝑖𝑖𝑖 𝛼𝛼𝐼𝐼 + 𝑆𝑆𝑖𝑖𝑖𝑖𝑖𝑖 𝛼𝛼𝑆𝑆 + 𝐶𝐶𝑐𝑐𝑐𝑐 𝛼𝛼𝐶𝐶 + 𝛽𝛽𝑋𝑋𝑐𝑐𝑐𝑐 + 𝜇𝜇𝑐𝑐 + 𝜇𝜇𝑡𝑡 + 𝜀𝜀𝑖𝑖𝑖𝑖𝑖𝑖

(6)

In this empirical version of an education production function, achievement A of student i in
country c at time t is expressed as a linearly additive function of vectors of input factors at the
level of students I, schools S, and countries C, as well as the measures of student assessment X.
The parameters μc and μt are country and year fixed effects, respectively, and εict is an individuallevel error term. Because of potential multicollinearity between the four categories of student
assessment, we start by estimating separate models for each assessment category and
subsequently report models that consider all four categories simultaneously.
Our fixed-effects panel model identifies the effect of assessment practices on student
achievement only from country-level within-country variation over time. First, note that the
treatment variable, Xct, is aggregated to the country-by-wave level. By measuring the average
extent of student assessments in a country at any given point in time, this specification avoids
bias from within-country selection of students into schools that use student assessments. This
does not, however, address concerns of bias from unobserved features at the country level.
Therefore, we secondly include country fixed effects μc, which effectively address any
potential omitted variable bias that arises from unobserved time-invariant country characteristics
that may be correlated with both assessments and achievement. The specification exploits the
fact that different countries have reformed their assessment systems at different points in time.
Being identified from country-level variation over time, our parameter of interest β will not be
affected by systematic, time-invariant differences across countries. This implies that countries
that do not change their assessment practices over the observation period will not enter into the
estimation of β.
To avoid bias from the fact that the global trend towards more assessment may coincide with
other trends that are relevant for student achievement, the model also includes time fixed effects
17

μt. These also capture any common shocks that affect testing in a specific PISA wave, as well as
any changes in the testing instruments in a given wave.
The key identifying assumption of our model is the standard assumption of fixed-effects
panel models. Conditional on the rich set of control variables at the student, school, and country
level included in our model, in the absence of reform the change in student achievement in
countries that have introduced or extended assessment practices would have been similar to the
change in student achievement in countries that did not reform at the given point in time. We will
come back to a discussion of potential violations of this identifying assumption and thus
potential remaining bias in the panel estimates in our further analyses below.

5. Results
The conceptual model identified three primary dimensions of the outcome implications of
alternative assessment usage: strength of incentives, addressee of the primary incentives, and
interactions with the overall environment. Here we sequentially consider the estimated impact of
each of these dimensions.
5.1 Strength of Incentives across Usage Categories
We start our discussion of results with the average effects of the different categories of
student assessment in our country sample. Table 3 presents the results for the combined
measures of the four assessment categories, first entered separately (columns 1-4) and then
jointly (columns 5-7). All models are estimated as panel models with country and year fixed
effects, conditioning on the rich set of control variables at the student, school, and country level
indicated above. 20 Regressions are weighted by students’ sampling probabilities within countries,
giving equal weight to each country-by-wave cell across countries and waves. Standard errors
are clustered at the country level throughout.
Overall, the basic impact results displayed in Table 3 suggest that different forms and
dimensions of student assessments have very different effects on student achievement. Among
20

Appendix Table A1 shows the coefficients on all control variables for the specification of the first column in
Table 5. Note that our results confirm the finding of Hanushek, Link, and Woessmann (2013) that the effect of
school autonomy on student achievement is negative in developing countries but positive in developed countries in
this extended setting. With six rather than four PISA waves and with 303 rather than 155 country-by-wave
observations, we show that the previous results about autonomy are robust to the consideration of the effects of
student assessment reforms.

18

the four assessment categories, only standardized testing that is used for external comparisons
has a strong and statistically significant positive effect on student outcomes. The coefficients on
standardized monitoring and internal testing are insignificant and close to zero, whereas there is
quite a sizeable negative coefficient on internal teacher monitoring. These different impacts are
consistent with the predictions on differing strengths of incentives from the conceptual
discussion.
The point estimate for standardized external comparisons suggests that a change from not
used to complete standardized external comparison is related to an increase in math achievement
by more than one quarter of a standard deviation. The point estimates and the statistical
significance of the category impacts are very similar between the regressions that include each
category of test usage individually and the regression that includes all four categories
simultaneously (column 5), indicating that there is enough independent variation in the different
assessment categories for estimation and that the effect of standardized external comparison does
not reflect reforms in other assessment categories. In the inclusive regression, the negative
coefficient on internal teacher monitoring even turns significant in math. With that nuanced
exception, results for science and reading achievement are very similar to those for math
(columns 6 and 7).
Individual results for each of the 13 underlying country-level indicators of student
assessment going into our test usage categories are shown in Appendix Table A4, where each
cell represents a separate regression. 21 Of particular interest, each of the four elements of the
external comparison composite, with one exception, has a significantly positive impact on
student performance in the three subjects. The exception is the use of central exit examinations,
which could simply reflect that student performance measured by PISA at age 15 is not very
responsive to rewards that only occur at the end of secondary school (when students are usually
aged around 18 or 19). While the point estimates are positive in all three subjects, they do not
reach statistical significance. 22 The estimated coefficients for the other three indicators taken
separately are substantially smaller than the combined measure. As noted, this probably reflects

21

In the separate regressions of Appendix Table A4, the number of countries and waves included in each
estimation varies and is determined by the availability of the specific assessment indicator.
22
Consistent with the weaker evidence on central exit exams, constructing the combined measure of
standardized external comparison without the central exit exam measure (i.e., based on the other three underlying
indicators) yields a slightly larger coefficient estimate of 30.926 in the specification of column 5 of Table 3.

19

both a reduction in measurement error for the correlated indicators and the fact that the different
incentives are additive. 23 We return below to a consideration of separate components of external
comparisons.
At the individual indicator level in Appendix Table A4, there is also some evidence of
positive effects of standardized testing in the relevant grade for PISA, and some indication of
impact from the use of assessment to inform parents. None of the other indicators of standardized
monitoring without external comparison, of internal testing, and of internal teacher monitoring is
significantly related to student achievement on average. The individual estimates suggest that the
potential negative impact of the internal monitoring of teachers is driven by the two subjective
components – monitoring by the principal and by external inspectorates. The aggregate
categorical variable is larger than these two subcomponents, potentially again reflecting a
reduction in measurement error and possible additivity.
Overall, the results indicate that, when assessing the effects of student assessments, it is
important to differentiate among alternative forms and dimensions of student assessments.
Across the different measures and subjects, the results for the effects of standardized external
comparisons consistently suggest that introducing such assessments leads to higher achievement.
By contrast, student assessments that are only used for internal testing and inspection do not
seem to matter much for average student achievement. The findings suggest that clearer, more
targeted information creates stronger incentives.
5.2 School-based versus Student-Based External Comparisons
The previous section highlighted the impacts of having standardized examinations that were
used for external comparisons. The category of external comparisons, however, actually
aggregates two quite distinct sets of incentives. One component (from the PISA questionnaires)
considers the general use of standardized assessments for external comparison of schools to
district or national performance. This category mainly indicates incentives to schools, potentially
having its greatest effect on administrators and teachers. The second category combines three

23
A third possibility is that the estimation samples for the separate indicators are varied and smaller than for
the combined indicator. However, we reject this explanation because estimating the combined model in column 5 of
Table 3 just for the smallest sample of countries in the separate indicator models yields a virtually identical
coefficient for external comparisons.

20

different measures of using tests to determine school and career placement decisions for students
with the clear locus of incentives on the students themselves.
Table 4 disaggregates the standardized external comparisons into school-based and studentbased external comparisons (each of which is based on standardized exams that have meaning
across schools). 24 This table presents simultaneous estimates that include the other three
categories. Both school and student incentives are strongly positive and statistically significant,
with estimates for the school-based incentives being somewhat larger than for the individual
student incentives. At the same time, none of the estimates for the remaining categories are
qualitatively affected. The results suggest that focusing incentives on different actors yields
different responses and leads to separate effects on outcomes.
5.3 Environmental Differences in Usage Impact
Results so far were distinguished by the first two assessment dimensions stressed by our
conceptual framework, different strengths of incentives and different addressees of incentives.
This section turns to the third assessment dimension, the extent to which effects vary by different
school environments.
Countries enter our observation period at very different stages of educational development,
and almost certainly with environments that have both different amounts of information about
schools and different degrees of policy interactions among parents, administrators, and teachers.
One straightforward way to parameterize these differences is to explore how incentive effects
vary with a country’s initial level of achievement.
We introduce an interaction term between the specific assessment measure Xct and a
country’s average achievement level when it first participated in PISA, Ac0:
𝐴𝐴𝑖𝑖𝑖𝑖𝑖𝑖 = 𝐼𝐼𝑖𝑖𝑖𝑖𝑖𝑖 𝛼𝛼𝐼𝐼 + 𝑆𝑆𝑖𝑖𝑖𝑖𝑖𝑖 𝛼𝛼𝑆𝑆 + 𝐶𝐶𝑐𝑐𝑐𝑐 𝛼𝛼𝐶𝐶 + 𝛽𝛽1 𝑋𝑋𝑐𝑐𝑐𝑐 + 𝛽𝛽2 (𝑋𝑋𝑐𝑐𝑐𝑐 × 𝐴𝐴𝑐𝑐0 ) + 𝜇𝜇𝑐𝑐 + 𝜇𝜇𝑡𝑡 + 𝜀𝜀𝑖𝑖𝑖𝑖𝑖𝑖

(7)

The parameter β2 indicates whether the assessment effect varies between countries with initially
low or high performance. Note that the initial performance level is a country feature that does not

24
The measure of student-based external comparison is the simple average of the three underlying indicators of
standardized external comparison except for the one on school-based external comparison. Note that the estimates of
Table 4 are based on smaller student samples from fewer countries, because data on student-based external
comparison are available for few countries beyond OECD and European Union countries.

21

vary over time, so that any main effect is captured by the country fixed effects μc included in the
model.
Table 5 presents estimates of the interacted model for the three subjects. The left three
columns provide results for the aggregate category of standardized external comparisons, while
the right three columns divide the external comparisons into school-based and student-based
comparisons. The initial score is centered on 400 PISA points (one standard deviation below the
OECD mean). The precise patterns of estimated effects by initial achievement with confidence
intervals are displayed in Figure 3 for math performance.
In broad generalities, the picture of how the overall achievement environment interacts with
the incentives from different test usage can be summarized as follows. First, the impact of
standardized external comparisons is stronger in lower achieving countries and goes to zero for
the highest achieving countries. In particular, at an initial country level of 400 PISA points the
introduction of standardized external comparison leads to an increase in student achievement of
37.3 percent of a standard deviation in math. With each 100 initial PISA points, this effect is
reduced by 24.6 percent of a standard deviation. Second, standardized monitoring similarly
creates significant incentives in initially low-achieving countries, with effects disappearing for
higher-achieving countries (i.e., those with initial scores of roughly above 490 in all subjects).
Third, the estimate of internal testing is insignificant throughout the initial-achievement support.
Fourth, the estimates for internal teacher monitoring are insignificant for most of the initialachievement distribution and turn negative only at high levels of initial achievement in math
(perhaps reflecting the purely linear interaction). Fifth, when external comparisons are
disaggregated into school-based and student-based components, school-based comparisons
follow essentially the same heterogeneous pattern as overall standardized external comparisons
but go to zero for a somewhat larger set of initially high-achieving countries. By contrast, the
impact of student-based external comparisons does not vary significantly with initial
achievement levels.
The disaggregated underlying individual indicators of standardized external comparison
consistently show the pattern of significantly stronger effects in initially poorly performing

22

countries (Appendix Table A5). 25 Interestingly, the introduction of central exit exams – which
did not show a significant effect on average – also shows the pattern of decreasing effects with
higher initial achievement, in particular in science. Similarly, all three underlying indicators of
standardized monitoring also show the same pattern of significant positive effects at low levels
of achievement and significantly decreasing effects with initial achievement. Thus, the positive
effect of standardized testing in low-achieving countries appears to be quite independent of
whether the standardized tests are used for external comparison or just for monitoring. This
finding supports the World Bank report that focused on low achieving countries: “There is too
little measurement of learning, not too much” (World Bank (2018), p. 17). 26
In contrast to the significant interactions with initial achievement levels, we do not find
evidence of consistent heterogeneities in several other environmental dimensions (not shown). In
particular, the effects of the four assessment categories do not significantly interact with
countries’ initial level of GDP per capita, which contrasts with the heterogeneous effects found
for school autonomy in that dimension in Hanushek, Link, and Woessmann (2013). Similarly,
there are no significant interactions of the assessment categories with the level of school
autonomy in a country. In addition, the use of standardized external comparisons does not
significantly interact with the other three categories of student assessments.
Overall, the heterogeneity analysis suggests that the use of standardized assessments is
particularly fruitful in countries with relatively poor achievement, irrespective of whether they
are used for external comparison or only for internal monitoring.

6. A Placebo Test with Leads of the Assessment Variables
Our fixed-effects panel model identifies the effect of assessment policies on student
achievement from policy changes within countries over time. Bias from non-random within25

There is no significant heterogeneity in the effect of the Eurydice measure of national testing, which is likely
due to the fact that this measure is available only for 18 European countries which do not feature a similarly wide
range of initial achievement levels.
26
An interesting outlier in the individual-indicator analysis is the use of assessments to inform parents, which
shows the opposite type of heterogeneity (significantly so in math and science): The expansion of using assessments
to inform parents about their child’s progress does not have a significant effect at low levels of initial achievement,
but the effect gets significantly more positive at higher levels. Among initially high-performing countries, informing
parents leads to significant increases in student achievement; e.g., at an initial achievement level of 550 PISA points,
there is a significantly positive effect on science achievement of 37.0 percent of a standard deviation. It seems that
addressing assessments at parents is only effective in raising student achievement in environments that already show
a high level of achievement, capacity, and responsiveness of schools.

23

country selection of students into schools is avoided through aggregating the assessment
variables to the country level. Bias from common shocks or specific issues of particular PISA
waves is taken care of through the inclusion of year fixed effects. Bias from any unobserved
country features is taken care of through the inclusion of country fixed effects to the extent that
the country features do not vary systematically over time. The rich set of student, school, and
country background factors considered in our model takes out country-specific variation over
time to the extent that it is observed in these variables.
A leading remaining concern of the fixed-effects model is that reforms may be endogenous,
in the sense that reforming countries may already be on a different trajectory than non-reforming
countries for other reasons, thus violating the usual common-trend assumption of the fixedeffects model.
Our panel setup lends itself to an informative placebo test. In particular, any given reform
should not have a causal effect on the achievement of students in the wave before it is
implemented. But, if the reform were endogenous, we should in fact see an association between
prior achievement and subsequent reform. Therefore, including leads of the assessment measures
– i.e., additional variables that indicate the assessment status in the next PISA wave – provides a
placebo test of this.
Table 6 reports the results of this placebo test. As is evident, none of the lead variables of
the four assessment categories is significantly related to student achievement (i.e., in the wave
before reform implementation). At the same time, the results of the contemporaneous assessment
measures are fully robust to conditioning on the lead variables: The use of standardized external
comparison has a significant positive effect on the math, science, and reading achievement of
students in the year in which it is implemented, but not in the wave in which it is not
implemented yet. Moreover, the estimated coefficients for the usage categories are qualitatively
similar to those in Table 3.
The fact that the leads of the assessment variables are insignificant also indicates that lagged
achievement does not predict assessment reforms. In that sense, the results speak against the
possibility that endogeneity of assessment reforms to how a school system is performing is a
relevant concern for the interpretation of our results.
Estimating the full interacted model with all four assessment categories and their leads
interacted with initial achievement is overly demanding to the data. Nevertheless, focusing just

24

on the main results of Section 5.3, an interacted model that includes just standardized external
comparison, its lead, and their interactions with initial achievement gives confirmatory results:
standardized external comparison is significantly positive, its interaction with initial achievement
is significantly negative, and both the lead variable and its interaction with initial achievement
are statistically insignificant (not shown).
No similar test is possible for the lag of the assessment variables, as lagged assessment
policies may in fact partly capture the effect of previously implemented reforms to the extent that
reforms take time to generate their full effects. In a specification that includes the
contemporaneous, lead, and lagged variable, both the contemporaneous and the lag of the
standardized external comparison variable are statistically significant while the lead remains
insignificant (not shown).
In sum, there is no evidence of the introduction of different test usage regimes in response to
prior educational circumstances.

7. Robustness Analyses
Our results prove robust to a number of interesting alternative specifications. To begin with,
we want to make sure that none of our results are driven by the peculiarity of any specific
country. Therefore, we re-ran all our main models (the simultaneous regressions of columns 5-7
in Table 3 and columns 1-3 in Table 5) excluding one country at a time. The qualitative results
are insensitive to this, with all significant coefficients remaining significant in all regressions
(not shown).
To test whether results differ between developed and less developed countries, we split the
sample into OECD and non-OECD countries. As the first two columns of Table 7 show,
qualitative results are similar in the two subgroups of countries, although the positive effect of
standardized external comparison is larger in OECD countries. Patterns of heterogeneity are less
precisely identified within the two more homogeneous subgroups (Table 8). In the group of
OECD countries, the significant effect of standardized external comparison does not vary
significantly with initial achievement, but the demands of the fully interacted model make
estimation difficult with just the 35-country sample. When we drop the insignificant interactions
(column 2), the point estimate of the use of standardized scores for comparisons is significant.

25

The heterogeneous effect of standardized monitoring is somewhat more pronounced in OECD
countries. But overall, the patterns do not differ substantively between the two country groups.
Our main model is identified from changes that occur from one PISA wave to the next, i.e.,
from three-year changes. To identify from less frequent changes and to gauge the long-run
relevance of the policy reforms, column 3 of Table 7 estimates a model in long differences that
restricts the analysis to the 15-year change from the first to the last PISA wave. Our main finding
is robust in this long-difference specification, with the estimate of the positive effect of
standardized external comparison being even larger when considering only long-run changes and
with the estimates of effects of the other three assessment categories remaining insignificant.
Similarly, while obviously less precise, the pattern of heterogeneity by initial achievement is
evident when the analysis is restricted to the category of standardized external comparisons (see
column 5 of Table 8). 27
While PISA has stringent sampling standards, there is some variation over countries and
time in the extent to which specific schools and students are excluded from the target population.
Main reasons for possible exclusions are inaccessibility in remote regions or very small size at
the school level and intellectual disability or limited test-language proficiency at the student level
(OECD (2016)). The average total exclusion rate is below 3 percent, but it varies from 0 percent
to 9.7 percent across countries and waves. To test whether this variation affects our analysis, the
next columns in Tables 7 and 8 control for the country-by-wave exclusion rates reported in each
PISA wave. As is evident, results are hardly affected.
Finally, in 2015 PISA instituted a number of major changes in testing methodology (OECD
(2016)). Most importantly, PISA changed its assessment mode from paper-based to computerbased testing. In addition, a number of changes in the scaling procedure were undertaken,
including changing from a one-parameter Rasch model to a hybrid of a one- and two-parameter
model and changing the treatment of non-reached testing items. We performed three robustness
tests to check whether these changes in testing methodology affect our results.
First, the simplest test of whether our analysis is affected by the 2015 changes in testing
methodology is to drop the 2015 wave from our regressions. As is evident from column 5 in
Table 7 and column 7 in Table 8, qualitative results do not change when estimating the model

27

Similarly, a model restricted to the category of standardized monitoring yields a significantly positive main
effect and a significantly positive interaction (not shown).

26

just on the PISA waves from 2000 to 2012, indicating that our results cannot be driven by the
indicated changes in testing mode.
Second, to address the changes in the psychometric scaling procedure, PISA recalculated
countries’ mean scores in the three subjects for all PISA waves since 2006 using the new 2015
scaling approach. In the final columns of Tables 7 and 8, we run our models with these rescaled
country mean scores instead of the original individual scores as the dependent variable for the
PISA waves 2006 to 2015. Again, qualitative results do not change, indicating that the changes
in scaling approach do not substantively affect our analysis.
Third, while no similar analysis is possible for the change in testing mode, we analyzed
whether countries’ change in PISA achievement from paper-based testing in 2012 to computerbased testing in 2015 is correlated with a series of indicators of the computer familiarity of
students and schools that we derive from the PISA school and student background questionnaires
in 2012. As indicated by Appendix Table A6, indicators of computer savviness in 2012 do not
predict the change in test scores between 2012 and 2015 across countries. In particular, the
change in countries’ test achievement is uncorrelated with several measures of schools’
endowment with computer hardware, internet connectivity, and software, as well as with several
measures of students’ access to and use of computers, internet, and software at home. The only
exception is that the share of schools’ computers that are connected to the internet is in fact
negatively correlated with a country’s change in science achievement, speaking against an
advantage of computer-savvy countries profiting from the change in testing mode.

8. Conclusions
The extent of student testing and its usage in school operations have become items of heated
debate in many countries, both developed and developing. Some simply declare that high-stakes
tests – meaning assessments that enter into reward and incentive systems for some individuals –
are inappropriate (Koretz (2017)). Others argue that increased use of testing and accountability
systems are essential for the improvement of educational outcomes (World Bank (2018)) and, by
extension, of economic outcomes (Hanushek and Woessmann (2015); Hanushek et al. (2015)).
Many of these discussions, however, fail to distinguish among alternative uses of tests. And,
most applications of expanded student assessments used for accountability purposes have not
been adequately evaluated, largely because they have been introduced in ways that make clear

27

identification of impacts very difficult. Critically, the expansion of national testing programs has
faced a fundamental analytical issue of the lack of suitable comparisons.
Our analysis turns to international comparisons to address the key questions of when student
assessments can be used in ways that promote higher achievement. The conceptual framework
behind the empirical analysis is a principal-agent model that motivates focusing on the strength
of incentives to teachers and students, on the specific addressees of incentives created by
differing test usage, and on environmental factors that affect the country-specific results of
testing regimes.
The empirical analysis employs the increasingly plentiful international student assessment
data that now support identification of causal implications of national testing. 28 Specifically, the
six waves of the PISA assessments of the OECD between 2000 and 2015 permit country-level
panel estimation that relies on within-country over-time analysis of country changes in
assessment practices. We combine data across 59 countries to estimate how varying testing
situations and applications affect student outcomes.
Our results indicate that accountability systems that use standardized tests to compare
outcomes across schools and students produce greater student outcomes. These systems tend to
have consequential implications and produce higher student achievement than those that simply
report the results of standardized tests. They also produce greater achievement results than
systems relying on localized or subjective information that cannot be readily compared across
schools and classrooms, which have little or negative impacts on student achievement.
Moreover, both rewards to schools and rewards to students for better outcomes result in
greater student learning. General comparisons of standardized testing at the school level appear
to lead to somewhat stronger results than direct rewards to students that come through sorting
across educational opportunities and subsequent careers.
Most interestingly from an international perspective is the finding that testing and
accountability systems are more important for school systems that are performing poorly. It
appears that systems that are showing strong results know more about how to boost student
performance and are less in need of strong accountability systems.

28
Interestingly, even the international testing – conducted on a voluntary basis in a low-stakes situation – has
come under attack for potentially harming the educational programs of countries. Recent analysis, however, rejects
this potential problem (Ramirez, Schofer, and Meyer (2018)).

28

Overall, the results from international comparisons of performance suggest that school
systems gain from measuring how their students and schools are doing and where they stand in a
comparative way. Comparative testing appears to create incentives for better performance and
allows rewarding those who are contributing most to educational improvement efforts.

29

References
Andrab, Tahir, Jishnu Das, and Asim Ijaz Khwaja. 2017. "Report cards: The impact of providing
school and child test scores on educational markets." American Economic Review 107, no. 6:
1535-1563.
Andrews, Paul, and coauthors. 2014. "OECD and Pisa tests are damaging education worldwide."
The Guardian: https://www.theguardian.com/education/2014/may/06/oecd-pisa-testsdamaging-education-academics (accessed June 20, 2018).
Bishop, John H. 1997. "The effect of national standards and curriculum-based exams on
achievement." American Economic Review 87, no. 2: 260-264.
Braga, Michela, Daniele Checchi, and Elena Meschi. 2013. "Educational policies in a long-run
perspective." Economic Policy 28, no. 73: 45-100.
Burgess, Simon, Deborah Wilson, and Jack Worth. 2013. "A natural experiment in school
accountability: The impact of school performance information on pupil progress." Journal of
Public Economics 106: 57-67.
Card, David. 1999. "The causal effect of education on earnings." In Handbook of Labor
Economics, Vol. 3A, edited by Orley Ashenfelter and David Card. Amsterdam: NorthHolland: 1801-1863.
Chetty, Raj, John N. Friedman, and Jonah E. Rockoff. 2017. "Measuring the impacts of teachers:
Reply." American Economic Review 107, no. 6: 1685-1717.
Dee, Thomas S., and Brian A. Jacob. 2011. "The impact of No Child Left Behind on student
achievement." Journal of Policy Analysis and Management 30, no. 3: 418-446.
Deming, David J., Sarah Cohodes, Jennifer Jennings, and Christopher Jencks. 2016. "School
accountability, postsecondary attainment, and earnings." Review of Economics and Statistics
98, no. 5: 848-862.
Dixit, Avinash. 2002. "Incentives and organizations in the public sector: An interpretative
review." Journal of Human Resources 37, no. 4: 696-727.
Eurydice. 2009. National testing of pupils in Europe: Objectives, organisation and use of results.
Brussels: European Commission; Education, Audiovisual and Culture Executive Agency
(EACEA), Eurydice.
Eurydice. 2017. Online platform, ec.europa.eu/eurydice. Brussels: Education Audiovisual &
Culture Executive Agency (EACEA), Eurydice Unit.
Figlio, David, and Susanna Loeb. 2011. "School accountability." In Handbook of the Economics
of Education, Vol. 3, edited by Eric A. Hanushek, Stephen Machin, and Ludger Woessmann.
Amsterdam: North Holland: 383-421.
Figlio, David N., and Joshua Winicki. 2005. "Food for thought: The effects of school
accountability plans on school nutrition." Journal of Public Economics 89, no. 2-3: 381-394.
Hanushek, Eric A., Susanne Link, and Ludger Woessmann. 2013. "Does school autonomy make
sense everywhere? Panel estimates from PISA." Journal of Development Economics 104:
212-232.

30

Hanushek, Eric A., and Margaret E. Raymond. 2005. "Does school accountability lead to
improved student performance?" Journal of Policy Analysis and Management 24, no. 2: 297327.
Hanushek, Eric A., Guido Schwerdt, Simon Wiederhold, and Ludger Woessmann. 2015.
"Returns to skills around the world: Evidence from PIAAC." European Economic Review 73:
103-130.
Hanushek, Eric A., and Ludger Woessmann. 2011. "The economics of international differences
in educational achievement." In Handbook of the Economics of Education, Vol. 3, edited by
Eric A. Hanushek, Stephen Machin, and Ludger Woessmann. Amsterdam: North Holland:
89-200.
Hanushek, Eric A., and Ludger Woessmann. 2015. The knowledge capital of nations: Education
and the economics of growth. Cambridge, MA: MIT Press.
Holmstrom, Bengt, and Paul Milgrom. 1991. "Multitask principal-agent analyses: Incentive
contracts, asset ownership, and job design." Journal of Law, Economics and Organization 7:
24-52.
Hout, Michael, and Stuart W. Elliott, eds. 2011. Incentives and test-based accountability in
education. Washington, DC: National Academies Press.
Jacob, Brian A. 2005. "Accountability, incentives and behavior: The impact of high-stakes
testing in the Chicago Public Schools." Journal of Public Economics 89, no. 5-6: 761-796.
Jacob, Brian A., and Steven D. Levitt. 2003. "Rotten apples: An investigation of the prevalence
and predictors of teacher cheating." Quarterly Journal of Economics 118, no. 3: 843-877.
Jürges, Hendrik, Kerstin Schneider, and Felix Büchel. 2005. "The effect of central exit
examinations on student achievement: Quasi-experimental evidence from TIMSS Germany."
Journal of the European Economic Association 3, no. 5: 1134-1155.
Koedel, Cory, Kata Mihaly, and Jonah E. Rockoff. 2015. "Value-added modeling: A review."
Economics of Education Review 47: 180-195.
Koning, Pierre, and Karen van der Wiel. 2012. "School responsiveness to quality rankings: An
empirical analysis of secondary education in the Netherlands." De Economist 160, no. 4: 339355.
Koretz, Daniel. 2017. The testing charade: Pretending to make schools better. Chicago:
University of Chicago Press.
Laffont, Jean-Jacques, and David Martimort. 2002. The theory of incentives: The principal-agent
model. Princeton, NJ: Princeton University Press.
Lavecchia, Adam M., Heidi Liu, and Philip Oreopoulos. 2016. "Behavioral economics of
education: Progress and possibilities." In Handbook of the Economics of Education, Vol. 5,
edited by Eric A. Hanushek, Stephen Machin, and Ludger Woessmann. Amsterdam: North
Holland: 1-74.
Leschnig, Lisa, Guido Schwerdt, and Katarina Zigova. 2017. "Central school exams and adult
skills: Evidence from PIAAC." Unpublished manuscript, University of Konstanz.

31

Lüdemann, Elke. 2011. "Intended and unintended short-run effects of the introduction of central
exit exams: Evidence from Germany." In Elke Lüdemann, Schooling and the formation of
cognitive and non-cognitive outcomes. ifo Beiträge zur Wirtschaftsforschung 39. Munich: ifo
Institute.
Nunes, Luis C., Ana Balcão Reis, and Carmo Seabra. 2015. "The publication of school rankings:
A step toward increased accountability?" Economics of Education Review 49: 15-23.
OECD. 2015. Education at a glance 2015: OECD indicators. Paris: Organisation for Economic
Co-operation and Development.
OECD. 2016. PISA 2015 results (volume I): Excellence and equity in education. Paris:
Organisation for Economic Co-operation and Development.
Ramirez, Francisco O., Evan Schofer, and John W. Meyer. 2018. "International tests, national
assessments, and educational development (1970-2012)." Comparative Education Review 62,
no. 3.
Reback, Randall, Jonah Rockoff, and Heather L. Schwartz. 2014. "Under pressure: Job security,
resource allocation, and productivity in schools under No Child Left Behind." American
Economic Journal: Economic Policy 6, no. 3: 207-241.
Schwerdt, Guido, and Ludger Woessmann. 2017. "The information value of central school
exams." Economics of Education Review 56: 65-79.
Shewbridge, Claire, Melanie Ehren, Paulo Santiago, and Claudia Tamassia. 2012. OECD
Reviews of Evaluation and Assessment in Education: Luxembourg. Paris: OECD.
Shewbridge, Claire, Eunice Jang, Peter Matthews, and Paulo Santiago. 2011. OECD Reviews of
Evaluation and Assessment in Education: Denmark. Paris: OECD.
Woessmann, Ludger. 2003. "Schooling resources, educational institutions, and student
performance: The international evidence." Oxford Bulletin of Economics and Statistics 65,
no. 2: 117-170.
Woessmann, Ludger. 2016. "The importance of school systems: Evidence from international
differences in student achievement." Journal of Economic Perspectives 30, no. 3: 3-32.
Woessmann, Ludger, Elke Luedemann, Gabriela Schuetz, and Martin R. West. 2009. School
accountability, autonomy, and choice around the world. Cheltenham, UK: Edward Elgar.
World Bank. 2018. World Development Report 2018: Learning to realize education's promise.
Washington, DC: World Bank.

32

Data Appendix: Sources and Construction of Assessment Measures
We derive a series of measures of different categories of the use of student assessments over
the period 2000-2015 from the PISA school background questionnaires and other sources.
Information on testing usage is classified into four groups with varying strength of generated
incentives: standardized external comparison, standardized monitoring, internal testing, and
internal teacher monitoring. We aggregate each assessment measure to the country-by-wave
level. Below, we also discuss how we combine the different indicators into an aggregate measure
for each of the four assessment categories. Details on the precise underlying survey questions
and any changes in question wording over time are found in Appendix Table A2.
A.1 Standardized External Comparison
Drawing on four different sources, we combine four separate indicators of standardized
testing usage designed to allow for external comparisons.
First, from the PISA school background questionnaires, we measure the share of schools in
each participating country that is subject to assessments used for external comparison. In
particular, school principals respond to the question, “In your school, are assessments of 15-yearold students used to compare the school to district or national performance?” Figure 2 in the text
provides a depiction of the evolution of this measure from 2000 to 2015 for each country.
Second, in the 2015 version of its Education at a Glance (EAG) publication, the OECD
(2015) published an indicator of the existence of national/central examinations at the lower
secondary level together with the year that is was first established. The data were collected by
experts and institutions working within the framework of the OECD Indicators of Education
Systems (INES) program in a 2014 OECD-INES Survey on Evaluation and Assessment.
National examinations are defined as “standardized student tests that have a formal consequence
for students, such as an impact on a student’s eligibility to progress to a higher level of education
or to complete an officially-recognized degree” (OECD (2015), p. 483). According to this
measure, five of the 37 countries with available data have introduced national standardized
exams in lower secondary school between 2000 and 2015. 29

29

In federal countries, all system-level indicator measures are weighted by population shares in 2000.

A1

Third, following a very similar concept, the Eurydice unit of the Education, Audiovisual and
Culture Executive Agency (EACEA) of the European Commission provides information on the
year of first full implementation of national testing in a historical overview of national testing of
students in Europe (Eurydice (2009); see also Braga, Checchi, and Meschi (2013)). In particular,
they classify national tests for taking decisions about the school career of individual students,
including tests for the award of certificates, promotion at the end of a school year, or streaming
at the end of primary or lower secondary school. We extend their measure to the year 2015
mostly based on information provided in the Eurydice (2017) online platform. During our period
of observation, eight of the 18 European countries introduced national tests for career decisions
and two abolished them.
Fourth, Leschnig, Schwerdt, and Zigova (2017) compile a dataset of the existence of central
exit examinations at the end of secondary school over time for the 31 countries participating in
the Programme for the International Assessment of Adult Competencies (PIAAC). They define
central exit exams as “a written test at the end of secondary school, administered by a central
authority, providing centrally developed and curriculum based test questions and covering core
subjects.” Following Bishop (1997), they do not include commercially prepared tests or
university entrance exams that do not have direct consequences for students passing them.
Central exit exams “can be organized either on a national level or on a regional level and must be
mandatory for all or at least the majority of a cohort of upper secondary school.” We extend their
time period, which usually ends in 2012, to 2015. Five of the 30 countries in our sample
introduced central exit exams over our 15-year period, whereas two countries abandoned them.
A.2 Standardized Monitoring
Beyond externally comparative testing, the PISA school background questionnaire also
provides three additional measures of standardized testing used for different types of monitoring
purposes.
First, school principals answer the question, “Generally, in your school, how often are 15year-old students assessed using standardized tests?” Answer categories start with “never” and
then range from “1-2 times a year” (“yearly” in 2000) to more regular uses. We code a variable
that represents the share of schools in a country that use standardized testing at all (i.e., at least
once a year).

A2

Second, school principals provide indicators on the following battery of items: “During the
last year, have any of the following methods been used to monitor the practice of teachers at your
school?” Apart from a number of non-test-based methods of teacher practice monitoring, one of
the items included in the battery is “tests or assessments of student achievement.” We use this to
code the share of schools in a country that monitors teacher practice by assessments.
Third, school principals are asked, “In your school, are achievement data used in any of the
following accountability procedures?” One consistently recorded item is whether “achievement
data are tracked over time by an administrative authority,” which allows us to construct a
measure of the share of schools in a country for which an administrative authority tracks
achievement data. The reference to over-time tracking by administrations indicates that the
achievement data are standardized to be comparable over time.
A.3 Internal Testing
The PISA school background questionnaire also provides information on three testing
policies where tests are not necessarily standardized and are mostly used for pedagogical
management.
In particular, school principals also report on the use of assessments of 15-year-old students
in their school for purposes other than external comparisons. Our first measure of internal testing
captures whether assessments are used “to inform parents about their child’s progress.” The
second measure covers the use of assessments “to monitor the school’s progress from year to
year.” Each measure is coded as the share of schools in a country using the respective type of
internal assessments.
The question on use of achievement data in accountability procedures referred to above also
includes an item indicating that “achievement data are posted publicly (e.g. in the media).” Our
third measure thus captures the share of schools in a country where achievement data are posted
publicly. In the questionnaire item, the public posting is rather vaguely phrased and is likely to
be understood by school principals to include such practices as posting the school mean of the
grade point average of a graduating cohort, derived from teacher-defined grades rather than any
standardized test, at the school’s blackboard.

A3

A.4 Internal Teacher Monitoring
Finally, the PISA school background questionnaire provides three additional measures of
internal monitoring that are all focused on teachers.
First, again reporting on the use of assessments of 15-year-old students in their school,
school principals report whether assessments are used “to make judgements about teachers’
effectiveness.”
The battery of methods used to monitor teacher practices also includes two types of
assessments based on observations of teacher practices by other persons rather than student
achievement tests. Our second measure in this area captures the share of schools where the
practice of teachers is monitored through “principal or senior staff observations of lessons.” Our
third measure captures whether “observation of classes by inspectors or other persons external to
the school” are used to monitor the practice of teachers.
A.5 Constructing Combined Measures for the Four Assessment Categories
Many of the separate assessment indicators are obviously correlated with each other, in
particular within each of the four groups of assessment categories. For example, the correlation
between the EAG measure of national standardized exams in lower secondary school and the
Eurydice measure of national tests used for career decisions is 0.59 in our pooled dataset (at the
country-by-wave level) and 0.54 after taking out country and year fixed effects (which reflects
the identifying variation in our model). Similarly, the two internal-testing measures of using
assessments to inform parents and using assessments to monitor school progress are correlated at
0.42 in the pooled data and 0.57 after taking out country and year fixed effects (all highly
significant).
While these correlations are high, there is also substantial indicator-specific variation. These
differences may reflect slight differences in the concepts underlying the different indicators and
different measurement error in the different indicators, but also substantive differences in the
measured assessment dimensions. In our main analysis, we combine the individual indicators
into one measure for each of the four assessment categories, but in the appendix tables below we
report results for each indicator separately.
Our construction of the combined measures takes into account that the different indicators
are available for different sets of waves and countries, as indicated in Appendix Table A3.

A4

Before combining the indicators, we therefore impute missing observations in the aggregate
country-by-wave dataset from a linear time prediction within each country. We then construct
the combined measures of the four assessment categories as the simple average of the individual
imputed indicators in each category. To ensure that the imputation does not affect our results, all
our regression analyses include a full set of imputation dummies that equal one for each
underlying indicator that was imputed and zero otherwise.
The combined measures of the four assessment categories are also correlated with each
other. In the pooled dataset of 303 country-by-wave observations, the correlations range from
0.278 between standardized external comparison and internal teacher monitoring to 0.583
between standardized monitoring and internal testing. After taking out country and year fixed
effects, the correlations are lowest between standardized external comparison and all other
categories (all below 0.2), moderate between standardized monitoring and the other categories
(all below 0.3), and largest between internal testing and internal teacher monitoring (0.485).
Because of potential multicollinearity, we first run our analyses for each aggregate assessment
category separately and then report a model that considers all four categories simultaneously.

A5

Table A1: Descriptive statistics and complete model of basic interacted specification
Mean

Descriptive statistics
Std. dev.

Share imputed

Coeff.
37.304***
-0.246***
67.772***
-0.776***
-13.858
0.161
10.432
-0.478*

Basic model
Std. err.
(6.530)
(0.085)
(17.139)
(0.175)
(12.216)
(0.100)
(25.005)
(0.249)

0.504
15.78

0.500
0.295

0.001
0.001

-11.557***
12.284***

(0.946)
(0.921)

0.892
0.054
0.054
0.111

0.221
0.223
0.305

0.034
0.034
0.061

-8.322
-2.772
-15.133***

(4.635)
(2.736)
(2.309)

0.088
0.019
0.062
0.108
0.077
0.265

0.278
0.134
0.238
0.307
0.262
0.435

0.031
0.031
0.031
0.031
0.031
0.031

9.138***
10.814***
20.951***
26.363***
36.135***

(2.228)
(2.421)
(2.984)
(2.559)
(2.538)

0.08
0.088
0.168
0.335

0.265
0.278
0.366
0.464

0.041
0.041
0.041
0.041

8.401***
15.520***
35.601***

(1.153)
(1.108)
(1.552)

0.174
0.478
0.276
0.072

0.374
0.493
0.442
0.255

0.026
0.026
0.026
0.026

30.297***
64.817***
73.718***

(1.908)
(2.426)
(3.433)

Standardized external comparison
X initial score
Standardized monitoring
X initial score
Internal testing
X initial score
Internal teacher monitoring
X initial score
Student and family characteristics
Female
Age (years)
Immigration background
Native student
First generation migrant
Second generation migrant
Other language than test language or
national dialect spoken at home
Parents’ education
None
Primary
Lower secondary
Upper secondary I
Upper secondary II
University
Parents’ occupation
Blue collar low skilled
Blue collar high skilled
White collar low skilled
White collar high skilled
Books at home
0-10 books
11-100 books
101-500 books
More than 500 books
(continued on next page)

Table A1 (continued)

School characteristics
Number of students
Privately operated
Share of government funding
Share of fully certified teachers at school
Shortage of math teachers
Teacher absenteeism
No
A little
Some
A lot
School’s community location
Village or rural area (<3,000)
Town (3,000-15,000)
Large town (15,000-100,000)
City (100,000-1,000,000)
Large city (>1,000,000)
Country characteristics
Academic-content autonomy
Academic-content autonomy x Initial GDP p.c.
GDP per capita (1,000 $)
Country fixed effects; year fixed effects
Student observations
Country observations
Country-by-wave observations
R2

Mean

Descriptive statistics
Std. dev.

Share imputed

Coeff.

Std. err.

849.0
0.193
0.802
0.822
0.202

696.7
0.383
0.289
0.294
0.394

0.093
0.071
0.106
0.274
0.041

0.012***
7.500*
-16.293***
6.662**
-5.488***

(0.002)
(4.396)
(4.596)
(2.793)
(1.031)

0.337
0.484
0.140
0.039

0.427
0.447
0.310
0.173

0.213
0.213
0.213
0.213

-0.325
-6.089***
-7.715***

(1.175)
(1.556)
(2.413)

0.092
0.208
0.311
0.251
0.137

0.281
0.397
0.451
0.422
0.336

0.056
0.056
0.056
0.056
0.056

5.238***
9.935***
14.209***
17.482***

(1.768)
(2.148)
(2.594)
(3.447)

0.597
5.043
27.30

0.248
7.578
20.80

-

-11.666
1.871***
0.009

(8.826)
(0.475)
(0.123)

2,193,026
59
303

Basic model

Yes
2,094,856
59
303
0.393

Notes: Descriptive statistics: Mean: international mean (weighted by sampling probabilities). Std. dev.: international standard deviation. Share imputed: share of
missing values in the original data, imputed in the analysis. Basic model: Full results of the specification reported in first column of Table 5. Dependent variable:
PISA math test score. Least squares regression weighted by students’ sampling probability. Regression includes imputation dummies. Robust standard errors
adjusted for clustering at the country level in parentheses. Significance level: *** 1 percent, ** 5 percent, * 10 percent.

Table A2: Measures of student assessments: Sources and definitions
Source
(1)

Countries
(2)

Standardized external comparison
School-based
PISA school PISA
external
questionnaire sample
comparison
National standar- OECD
dized exams in
(2015)
lower secondary
school

National tests
used for career
decisions

Eurydice
(2009)

Central exit exams Leschnig,
Schwerdt,
and Zigova
(2017)

Waves
(3)

Definition
(4)

2000-2003, In your school, are assessments of 15-year-old students used
2009-2015 for any of the following purposes? To compare the school to
district or national performance.

Deviation in wording in specific waves
(5)
2000: without “for any of the following
purposes”; 2009-2015: “students in <national
modal grade for 15-year-olds>” instead of “15year-old students”; 2015: “standardized tests”
instead of “assessments”.

OECD
EAG
sample

2000-2015 National/central examinations (at the lower secondary level),
which apply to nearly all students, are standardized tests of
what students are expected to know or be able to do that have
a formal consequence for students, such as an impact on a
student’s eligibility to progress to a higher level of education
or to complete an officially recognized degree.
EU
2000-2015 Year of first full implementation of national testing, ISCED
countries
levels 1 and 2: Tests for taking decisions about the school
career of individual pupils, including tests for the award of
certificates, or for promotion at the end of a school year or
streaming at the end of ISCED levels 1 or 2.
PIAAC 2000-2015 Exit examination at the end of secondary school: A central
sample
exam is a written test at the end of secondary school,
administered by a central authority, providing centrally
developed and curriculum based test questions and covering
core subjects. (See text for additional detail.)

Standardized monitoring
Standardized
PISA school PISA
testing in
questionnaire sample
tested grade

2000, 2003, Generally, in your school, how often are 15-year-old students
2009, 2015 assessed using standardized tests? More than “never.”

2009-2015: “students in <national modal grade
for 15-year-olds>” instead of “15-year-old
students”; 2009: “using the following methods:”
“standardized tests”; 2015: “using the following
methods:” “mandatory standardized tests” or
“non-mandatory standardized tests”.

Monitor teacher PISA school
practice by
questionnaire
assessments
Achievement data PISA school
tracked by admini- questionnaire
strative authority

2003,
During the last year, have any of the following methods been
2009-2015 used to monitor the practice of teachers at your school? Tests
or assessments of student achievement.
2006-2015 In your school, are achievement data used in any of the
following accountability procedures? Achievement data are
tracked over time by an administrative authority.

2003 and 2012: “mathematics teachers” instead
of “teachers”; 2009: “<test language> teachers”
instead of “teachers”

(continued on next page)

PISA
sample
PISA
sample

Table A2 (continued)
Source
(1)

Countries
(2)

Internal testing
Assessments used PISA school PISA
to inform parents questionnaire sample

Waves
(3)

Definition
(4)

Deviation in wording in specific waves
(5)

2000-2003, In your school, are assessments of 15-year-old students used
2009-2015 for any of the following purposes? To inform parents about
their child’s progress.

2000: without “for any of the following
purposes”; 2009-2015: “students in <national
modal grade for 15-year-olds>” instead of “15year-old students”; 2015: “standardized tests”
instead of “assessments”.

Assessments used PISA school PISA
to monitor school questionnaire sample
progress

2000-2003, In your school, are assessments of 15-year-old students used
2009-2015 for any of the following purposes? To monitor the school’s
progress from year to year.

2000: without “for any of the following
purposes”; 2009-2015: “students in <national
modal grade for 15-year-olds>” instead of “15year-old students”; 2015: “standardized tests”
instead of “assessments”.

Achievement data PISA school PISA
posted publicly
questionnaire sample

2006-2015 In your school, are achievement data used in any of the
following accountability procedures? Achievement data are
posted publicly (e.g. in the media).

Internal teacher monitoring
Assessments used PISA school PISA
to judge teacher questionnaire sample
effectiveness
Monitor teacher
practice by
school principal
Monitor teacher
practice by
external inspector

PISA school PISA
questionnaire sample
PISA school PISA
questionnaire sample

2000-2003, In your school, are assessments of 15-year-old students used
2009-2015 for any of the following purposes? To make judgements about
teachers’ effectiveness.

2000: without “for any of the following
purposes”; 2009-2015: “students in <national
modal grade for 15-year-olds>” instead of “15year-old students”; 2015: “standardized tests”
instead of “assessments”.

2003,
During the last year, have any of the following methods been
2009-2015 used to monitor the practice of teachers at your school?
Principal or senior staff observations of lessons.
2003,
During the last year, have any of the following methods been
2009-2015 used to monitor the practice of teachers at your school?
Observation of classes by inspectors or other persons external
to the school.

2003 and 2012: “mathematics teachers” instead
of “teachers”; 2009: “<test language> teachers”
instead of “teachers”

Notes: Own depiction based on indicated sources.

2003 and 2012: “mathematics teachers” instead
of “teachers”; 2009: “<test language> teachers”
instead of “teachers”

Table A3: Country observations by wave
2000/02
(1)

2003
(2)

2006
(3)

2009/10
(4)

2012
(5)

2015
(6)

Total
(7)

Standardized external comparison
School-based external comparison
National standardized exams in lower secondary school
National tests used for career decisions
Central exit exams

39
30
17
23

37
29
15
22

–
35
21
28

58
35
21
29

59
36
21
30

55
36
21
30

248
201
116
162

Standardized monitoring
Standardized testing in tested grade
Monitor teacher practice by assessments
Achievement data tracked by administrative authority

38
–
–

35
36
–

–
–
53

58
57
58

–
59
59

51
56
56

182
208
226

Internal testing
Assessments used to inform parents
Assessments used to monitor school progress
Achievement data posted publicly

40
40
–

37
37
–

–
–
53

58
58
58

59
59
59

55
55
56

249
249
226

Internal teacher monitoring
Assessments used to judge teacher effectiveness
Monitor teacher practice by school principal
Monitor teacher practice by external inspector

40
–
–

37
37
37

–
–
–

58
58
58

59
59
59

55
56
56

249
210
210

Notes: Own depiction based on PISA data and other sources. See Data Appendix for details.

Table A4: Baseline model for separate underlying assessment indicators

Standardized external comparison
School-based external comparison
National standardized exams in lower secondary school
National tests used for career decisions
Central exit exams
Standardized monitoring
Standardized testing in tested grade
Monitor teacher practice by assessments
Achievement data tracked by administrative authority
Internal testing
Assessments used to inform parents
Assessments used to monitor school progress
Achievement data posted publicly
Internal teacher monitoring
Assessments used to judge teacher effectiveness
Monitor teacher practice by school principal
Monitor teacher practice by external inspector

Math
(1)

Science
(2)

Reading
(3)

Observations
(4)

Countries
(5)

Waves
(6)

R2
(7)

13.797*
(7.417)
13.400**
(5.508)
15.650***
(1.701)
3.694
(7.041)

13.147*
(6.598)
14.272**
(5.336)
11.144***
(2.377)
8.242
(6.575)

16.058**
(6.227)
14.568**
(5.418)
11.002***
(2.932)
9.806
(6.551)

1,703,142

59

5

0.382

1,517,693

36

6

0.326

676,732

21

6

0.264

1,141,162

30

6

0.308

15.497**
(7.244)
-19.266*
(9.625)
-3.555
(9.266)

11.051
(6.901)
0.305
(9.785)
5.173
(9.578)

19.380***
(7.169)
-10.046
(6.329)
-1.677
(12.787)

1,198,463

59

4

0.386

1,537,802

59

4

0.385

1,713,976

59

4

0.394

7.923
(6.594)
1.480
(5.343)
0.344
(8.371)

14.664**
(6.974)
7.283
(7.630)
0.571
(7.630)

4.234
(7.912)
-1.598
(7.308)
-16.954
(10.165)

1,705,602

59

5

0.385

1,705,602

59

5

0.385

1,713,976

59

4

0.394

-4.065
(8.249)
-19.751
(14.072)
-13.152
(10.038)

3.110
(9.619)
-10.893
(10.793)
-13.524
(8.898)

-1.981
(7.810)
-14.239
(10.062)
-17.553*
(10.306)

1,705,602

59

5

0.385

1,588,962

59

4

0.385

1,588,962

59

4

0.385

Notes: Each cell presents results of a separate regression. Dependent variable: PISA test score. Least squares regression weighted by students’ sampling
probability, including country and year fixed effects. Student assessment measures aggregated to the country level. Sample: student-level observations in six
PISA waves 2000-2015. See Table 3 for included control variables. Number of observations and R2 refer to the math specification. Robust standard errors
adjusted for clustering at the country level in parentheses. Significance level: *** 1 percent, ** 5 percent, * 10 percent.

Table A5: Interacted model for separate underlying assessment indicators
Main effect
(1)
Standardized external comparison
School-based external comparison
National standardized exams in lower secondary school
National tests used for career decisions
Central exit exams
Standardized monitoring
Standardized testing in tested grade
Monitor teacher practice by assessments
Achievement data tracked by administrative authority
Internal testing
Assessments used to inform parents
Assessments used to monitor school progress
Achievement data posted publicly
Internal teacher monitoring
Assessments used to judge teacher effectiveness
Monitor teacher practice by school principal
Monitor teacher practice by external inspector

Math
X initial score
(2)

Science
Main effect
X initial score
(3)
(4)

Reading
Main effect
X initial score
(5)
(6)

39.945***
(10.118)
50.625**
(18.887)
21.890***
(5.524)
24.550
(31.796)

-0.456***
(0.078)
-0.464**
(0.206)
-0.081
(0.077)
-0.254
(0.322)

43.605***
(10.441)
50.720***
(13.905)
11.309
(6.728)
58.473***
(18.255)

-0.484***
(0.117)
-0.434**
(0.162)
-0.002
(0.083)
-0.542***
(0.156)

47.018***
(9.023)
39.186
(31.246)
20.983**
(8.517)
54.899
(46.933)

-0.481***
(0.098)
-0.273
(0.301)
-0.119
(0.102)
-0.540
(0.543)

46.491***
(9.608)
15.863
(14.109)
28.970*
(14.631)

-0.460***
(0.108)
-0.384***
(0.116)
-0.417***
(0.129)

42.679***
(9.829)
44.530***
(14.908)
38.054**
(18.191)

-0.427***
(0.105)
-0.508***
(0.174)
-0.419**
(0.198)

54.278***
(9.918)
25.154*
(12.715)
43.775**
(19.113)

-0.509***
(0.104)
-0.391***
(0.130)
-0.631**
(0.242)

-8.895
(6.714)
6.106
(8.812)
15.898
(15.782)

0.233***
(0.047)
-0.065
(0.115)
-0.197
(0.133)

-10.140
(8.012)
2.356
(13.376)
22.711
(15.355)

0.314***
(0.079)
0.065
(0.177)
-0.264*
(0.144)

-6.900
(10.352)
6.433
(13.825)
-8.159
(19.472)

0.151
(0.103)
-0.115
(0.177)
-0.123
(0.236)

0.387
(14.989)
0.807
(26.483)
18.086
(12.412)

-0.063
(0.153)
-0.239
(0.208)
-0.370**
(0.145)

0.220
(16.015)
31.735
(21.136)
17.783
(17.744)

0.037
(0.202)
-0.514**
(0.201)
-0.365*
(0.207)

1.141
(14.510)
1.358
(20.928)
-6.485
(16.606)

-0.043
(0.163)
-0.186
(0.222)
-0.134
(0.189)

Notes: Two neighboring cells present results of one separate regression, with “main effect” reporting the coefficient on the variable indicated in the left column
and “X initial score” reporting the coefficient on its interaction with the country’s PISA score in the initial year (centered at 400, so that the “main effect”
coefficient shows the effect of assessments on test scores in a country with 400 PISA points in 2000). Dependent variable: PISA test score. Least squares
regression weighted by students’ sampling probability, including country and year fixed effects. Student assessment measures aggregated to the country level.
Sample: student-level observations in six PISA waves 2000-2015. See Table A4 for numbers of observations, countries, and waves and Table 3 for the included
control variables. Robust standard errors adjusted for clustering at the country level in parentheses. Significance level: *** 1 percent, ** 5 percent, * 10 percent.

Table A6: Correlation of computer indicators in 2012 with change in PISA score from 2012 to 2015 at the country level
Math
(1)

Science
(2)

Reading
(3)

-0.015
(0.912)

-0.045
(0.744)

0.091
(0.503)

-0.223*
(0.099)

-0.395***
(0.003)

-0.125
(0.360)

0.000
(0.998)

0.028
(0.837)

-0.029
(0.834)

Lack or inadequacy of Internet connectivity

0.106
(0.438)

0.247*
(0.066)

0.040
(0.771)

Shortage or inadequacy of computer software for instruction

0.091
(0.503)

0.059
(0.666)

0.083
(0.541)

0.034
(0.805)

0.240*
(0.075)

-0.162
(0.233)

Number of computers at home

0.083
(0.544)

-0.043
(0.751)

0.181
(0.182)

Educational software at home

-0.111
(0.414)

0.044
(0.746)

-0.238*
(0.077)

Link to the Internet at home

0.043
(0.752)

0.221
(0.102)

-0.116
(0.394)

Frequency of programming computers at school and outside of school

-0.150
(0.270)

-0.110
(0.419)

-0.003
(0.980)

Weekly time spent repeating and training content from school lessons
by working on a computer

0.095
(0.485)

0.071
(0.604)

0.030
(0.826)

School
Ratio of computers for education to students in respective grade
Share of computers connected to Internet
School’s capacity to provide instruction hindered by:
Shortage or inadequacy of computers for instruction

Student
Computer at home for use for school work

Notes: Correlation between the respective computer indicator (2012) indicated in the first column with the change in PISA test scores (2012-215) in the subject
indicated in the header. Sample: 56 country-level observations of countries participating in the PISA waves 2012 and 2015. p-values in parentheses. Significance
level: *** 1 percent, ** 5 percent, * 10 percent.

Figure 1: PISA math achievement in 2000-2015
Panel A: Countries above initial median achievement

Panel B: Countries below initial median achievement

Notes: Country mean achievement in PISA math test. Country sample split at median of initial achievement level for expositional reasons. Country identifiers are
listed in Table 1. Own depiction based on PISA micro data.

Figure 2: School-based external comparison in 2000-2015

Notes: Country share of schools with use of assessments for external comparison. Country identifiers are listed in Table 1. Own depiction based on PISA micro
data.

Figure 3: Effect of student assessments on math performance by initial achievement levels
Standardized external comparison

Standardized monitoring

Internal testing

Internal teacher monitoring

Notes: Average marginal effects of student assessments on PISA math score by initial country achievement, with 95 percent confidence intervals. See first
column of Table 5 for underlying model.

Table 1: Selected indicators by country
OECD

Albania (ALB) a
Argentina (ARG) a
Australia (AUS)
Austria (AUT)
Belgium (BEL)
Brazil (BRA)
Bulgaria (BGR) a
Canada (CAN)
Chile (CHL) a
Colombia (COL) c
Costa Rica (CRI) e
Croatia (HRV) c
Czech Republic (CZE)
Denmark (DNK)
Estonia (EST) c
Finland (FIN)
France (FRA)
Germany (DEU)
Greece (GRC)
Hong Kong (HKG) a
Hungary (HUN)
Iceland (ISL)
Indonesia (IDN) a
Ireland (IRL)
Israel (ISR) a
Italy (ITA)
Japan (JPN)
Jordan (JOR) c
Korea (KOR)
Latvia (LVA)
(continued on next page)

2015
(1)
0
0
1
1
1
0
0
1
1
0
0
0
1
1
1
1
1
1
1
0
1
1
0
1
1
1
1
0
1
1

PISA math score
2000
(2)
380
387
534
514
515
333
430
533
383
370
410
467
493
514
515
536
518
485
447
560
483
515
366
503
434
459
557
384
548
462

2015
(3)
395
389
494
496
507
377
442
516
423
390
400
463
492
512
519
511
494
505
455
547
477
487
387
504
468
489
533
381
524
482

School-based
external comparison
2000
2015
(4)
(5)
0.70
0.77
0.35
0.22
0.52
0.55
0.08
0.21
0.07
0.42
0.39
0.84
0.64
0.68
0.44
0.81
0.36
0.60
0.63
0.81
0.61
0.33
0.73
0.44
0.44
0.69
0.06
0.72
0.67
0.78
0.57
0.75
0.36
0.50
0.12
0.34
0.12
0.19
0.21
0.57
0.61
0.75
0.78
0.95
0.77
0.69
0.36
0.85
0.45
0.64
0.21
0.82
0.09
0.17
0.77
0.82
0.33
0.69
0.72
0.91

National standardized
exams in lower sec. school
2000
2015
(6)
(7)
.
.
.
.
0
0
0
0
0
0.32
0
0
.
.
0
0
0
0
0
0
.
.
.
.
0
0
1
1
1
1
0
0
1
1
.
.
0
0
.
.
0
0
0
0
.
.
1
1
0
0
1
1
0
0
.
.
0
0
1
1

National tests used
for career decisions
2000
2015
(8)
(9)
.
.
.
.
.
.
.
.
0
0.32
.
.
0
1
.
.
.
.
.
.
.
.
.
.
0
0
1
1
.
.
.
.
.
.
0
1
0
0
.
.
.
.
1
0
.
.
1
1
.
.
0
1
.
.
.
.
.
.
1
1

Table 1 (continued)
OECD

Lithuania (LTU) c
Luxembourg (LUX) b
Macao (MAC)
Mexico (MEX)
Montenegro (MNE) c
Netherlands (NLD) b
New Zealand (NZL)
Norway(NOR)
Peru (PER) a
Poland (POL)
Portugal (PRT)
Qatar (QAT) c
Romania (ROU) a
Russia (RUS)
Serbia (SRB) c
Singapore (SGP) d
Slovak Republic (SVK) b
Slovenia (SVN) c
Spain (ESP)
Sweden (SWE)
Switzerland (CHE)
Taiwan (TWN) c
Thailand (THA) a
Tunisia (TUN) b
Turkey (TUR) b
United Arab Emirates (ARE) e
United Kingdom (GBR)
United States (USA)
Uruguay (URY) b
Country average

2015
(1)
0
1
0
1
0
1
1
1
0
1
1
0
0
0
0
0
1
1
1
1
1
0
0
0
1
0
1
1
0
0.59

PISA math score
2000
(2)
486
446
527
387
399
538
538
499
292
471
453
318
426
478
435
563
499
505
476
510
528
550
433
359
424
421
530
493
422
465

2015
(3)
479
487
543
408
416
513
494
500
386
505
493
402
443
494
449
564
475
510
486
494
520
544
415
365
421
427
492
470
420
469

School-based
external comparison
2000
2015
(4)
(5)
0.55
0.69
0.00
0.94
0.03
0.30
0.55
0.87
0.38
0.46
0.64
0.63
0.94
0.86
0.58
0.68
0.40
0.62
0.39
0.91
0.19
0.73
0.61
0.85
0.60
0.81
0.78
0.95
0.35
0.34
0.93
0.94
0.46
0.64
0.54
0.35
0.20
0.47
0.76
0.88
0.14
0.47
0.47
0.68
0.57
0.94
0.73
0.50
0.59
0.71
0.69
0.87
0.91
0.91
0.92
0.96
0.18
0.24
0.48
0.66

National standardized
exams in lower sec. school
2000
2015
(6)
(7)
.
.
0
0
.
.
0
0
.
.
1
1
0
0
0
1
.
.
0
1
0
1
.
.
.
.
.
.
.
.
.
.
0
0
0
0
0
0
0
0
.
.
.
.
.
.
.
.
1
1
.
.
0
0
0
1
.
.
0.23
0.35

National tests used
for career decisions
2000
2015
(8)
(9)
0
0
1
1
.
.
.
.
.
.
1
1
.
.
0
1
.
.
0
1
0
1
.
.
0
1
.
.
.
.
.
.
.
.
0
0
.
.
1
1
.
.
.
.
.
.
.
.
.
.
.
.
0.87
0
.
.
.
.
0.39
0.67

Notes: PISA data: Country means, based on non-imputed data for each variable, weighted by sampling probabilities. “.” = not available. a-e “2000” PISA data
refer to country’s initial PISA participation in a 2002, b 2003, c 2006, d 2009, e 2010.

Table 2: Descriptive statistics of assessment measures
Mean
(1)

Std. dev.
(2)

Min
(3)

Max
(4)

Countries
(5)

Waves
(6)

Standardized external comparison

0.518

0.271

0.022

0.978

59

6

School-based external comparison
National standardized exams in lower secondary school
National tests used for career decisions
Central exit exams

0.573
0.292
0.601
0.689

0.251
0.452
0.481
0.442

0
0
0
0

0.960
1
1
1

59
37
18
30

5
6
6
6

Standardized monitoring

0.714

0.160

0.219

0.996

59

6

Standardized testing in tested grade
Monitor teacher practice by assessments
Achievement data tracked by administrative authority

0.721
0.750
0.723

0.233
0.191
0.201

0
0.128
0.070

1
1
1

59
59
59

4
4
4

Internal testing

0.684

0.147

0.216

0.963

59

6

Assessments used to inform parents
Assessments used to monitor school progress
Achievement data posted publicly

0.892
0.770
0.393

0.185
0.209
0.239

0.141
0
0.016

1
1
0.927

59
59
59

5
5
4

Internal teacher monitoring

0.553

0.216

0.026

0.971

59

6

Assessments used to judge teacher effectiveness
Monitor teacher practice by school principal
Monitor teacher practice by external inspector

0.532
0.773
0.402

0.261
0.262
0.255

0
0.049
0.006

0.992
1
0.994

59
59
59

5
4
4

Notes: Own depiction based on PISA micro data and other sources. See Data Appendix for details.

Table 3: The effect of different dimensions of student assessments on student achievement: Fixed-effects panel models
Math
(1)
Standardized external comparison

(2)

(3)

(5)

(6)

(7)

28.811***
(6.126)

23.282***
(6.144)

28.424***
(5.911)

-5.469
(14.062)

1.252
(13.950)

-2.036
(13.148)

7.491
(11.646)

17.669
(13.155)

-12.660
(14.736)

-23.478
(14.518)

-35.850**
(15.680)

-27.549*
(14.226)

-25.358
(15.835)

(4)

-4.800
(15.238)

Internal testing

2.093
(10.067)

Internal teacher monitoring

Student observations
Country observations
Country-by-wave observations
R2

Reading

26.365***
(6.058)

Standardized monitoring

Control variables
Country fixed effects
Year fixed effects

Science

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

2,094,856
59
303
0.391

2,094,856
59
303
0.390

2,094,856
59
303
0.390

2,094,856
59
303
0.390

2,094,856
59
303
0.391

2,094,705
59
303
0.348

2,187,415
59
303
0.357

Notes: Dependent variable: PISA test score in subject indicated in the header. Least squares regression weighted by students’ sampling probability, including
country and year fixed effects. Student assessment measures aggregated to the country level. Sample: student-level observations in six PISA waves 2000-2015.
Control variables include: student gender, age, parental occupation, parental education, books at home, immigration status, language spoken at home; school
location, school size, share of fully certified teachers at school, teacher absenteeism, shortage of math teachers, private vs. public school management, share of
government funding at school; country’s GDP per capita, school autonomy, GDP-autonomy interaction; imputation dummies; country fixed effects; year fixed
effects. Robust standard errors adjusted for clustering at the country level in parentheses. Significance level: *** 1 percent, ** 5 percent, * 10 percent.

Table 4: Disaggregation of standardized external comparisons into school-based and student-based comparisons
Math
(1)

Science
(2)

Reading
(3)

School-based external comparison

25.015***
(7.667)

21.317**
(8.246)

23.480***
(7.291)

Student-based external comparison

17.309***
(3.620)

15.198***
(3.883)

14.481***
(3.753)

Standardized monitoring

-4.658
(16.599)

-8.333
(15.007)

-8.400
(14.602)

Internal testing

4.896
(13.686)

13.419
(15.306)

-16.890
(18.616)

Internal teacher monitoring

-35.424**
(15.165)

-27.374
(16.656)

-18.372
(16.373)

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

1,672,041
42
230
0.348

1,671,914
42
230
0.315

1,751,351
42
230
0.321

Control variables
Country fixed effects
Year fixed effects
Student observations
Country observations
Country-by-wave observations
R2

Notes: Dependent variable: PISA test score in subject indicated in the header. Least squares regression weighted by students’ sampling probability, including
country and year fixed effects. Student assessment measures aggregated to the country level. Sample: student-level observations in six PISA waves 2000-2015.
See Table 3 for included control variables. Robust standard errors adjusted for clustering at the country level in parentheses. Significance level: *** 1 percent, ** 5
percent, * 10 percent.

Table 5: Effects of student assessments by initial achievement level: Fixed-effects panel models

Standardized external comparison
X initial score

Math
(1)

Science
(2)

Reading
(3)

37.304***
(6.530)
-0.246***
(0.085)

28.680***
(8.222)
-0.149
(0.101)

47.977***
(9.005)
-0.345***
(0.113)

School-based external comparison
X initial score
Student-based external comparison
X initial score
Standardized monitoring
X initial score
Internal testing
X initial score
Internal teacher monitoring
X initial score
Control variables
Country fixed effects
Year fixed effects
Student observations
Country observations
Country-by-wave observations
R2

67.772***
(17.139)
-0.776***
(0.175)
-13.858
(12.216)
0.161
(0.100)
10.432
(25.005)
-0.478*
(0.249)
Yes
Yes
Yes
2,094,856
59
303
0.393

86.860***
(20.263)
-0.989***
(0.255)
-14.734
(15.155)
0.289**
(0.143)
18.210
(25.338)
-0.407
(0.289)
Yes
Yes
Yes
2,094,705
59
303
0.349

88.701***
(21.396)
-1.026***
(0.260)
-26.214
(17.261)
0.082
(0.185)
-22.463
(32.946)
0.077
(0.317)
Yes
Yes
Yes
2,187,415
59
303
0.359

Math
(4)

Science
(5)

Reading
(6)

45.740***
(15.067)
-0.385**
(0.165)
15.138**
(6.518)
-0.019
(0.105)
72.689***
(26.701)
-0.756***
(0.273)
-14.462
(21.562)
0.159
(0.201)
-0.620
(32.969)
-0.290
(0.355)
Yes
Yes
Yes
1,672,041
42
230
0.350

39.343*
(21.244)
-0.347
(0.229)
7.120
(10.564)
0.079
(0.160)
77.183**
(34.691)
-0.921**
(0.387)
-0.669
(35.177)
0.087
(0.324)
2.077
(42.956)
-0.191
(0.506)
Yes
Yes
Yes
1,671,914
42
230
0.316

49.581**
(21.699)
-0.361
(0.248)
2.535
(5.975)
0.147
(0.091)
116.503***
(31.505)
-1.378***
(0.377)
-44.234
(33.433)
0.219
(0.337)
-42.345
(43.058)
0.421
(0.436)
Yes
Yes
Yes
1,751,351
42
230
0.323

Notes: Dependent variable: PISA test score in subject indicated in the header. Least squares regression weighted by students’ sampling probability, including
country and year fixed effects. Student assessment measures aggregated to the country level. Initial score: country’s PISA score in the initial year (centered at
400, so that main-effect coefficient shows effect of assessments on test scores in a country with 400 PISA points in 2000). Sample: student-level observations in
six PISA waves 2000-2015. See Table 3 for included control variables. Complete model of specification in column 1 displayed in Table A1. Robust standard
errors adjusted for clustering at the country level in parentheses. Significance level: *** 1 percent, ** 5 percent, * 10 percent.

Table 6: Placebo test with leads of assessment reforms
Math
(1)

Science
(2)

Reading
(3)

Control variables
Country fixed effects
Year fixed effects

25.104***
(6.316)
-16.172
(18.139)
14.305
(15.367)
-35.785
(22.833)
12.119
(11.045)
-15.195
(13.881)
6.965
(14.408)
-5.394
(17.088)
Yes
Yes
Yes

24.567***
(5.242)
-3.734
(19.288)
19.522
(21.238)
-38.797*
(19.796)
4.475
(8.506)
-11.138
(16.216)
-7.014
(15.286)
20.922
(18.269)
Yes
Yes
Yes

27.787***
(7.501)
4.660
(18.490)
-17.675
(20.325)
-31.560
(19.079)
5.746
(9.351)
-17.220
(19.718)
5.567
(14.069)
-15.352
(17.759)
Yes
Yes
Yes

Student observations
Country observations
Country-by-wave observations
R2

1,638,149
59
235
0.396

1,638,084
59
235
0.350

1,710,196
59
235
0.361

Standardized external comparison
Standardized monitoring
Internal testing
Internal teacher monitoring
Lead (Standardized external comparison)
Lead (Standardized monitoring)
Lead (Internal testing)
Lead (Internal teacher monitoring)

Notes: Dependent variable: PISA test score in subject indicated in the header. Lead indicates values of test usage category from subsequent period, i.e., before its
later introduction. Least squares regression weighted by students’ sampling probability, including country and year fixed effects. Student assessment measures
aggregated to the country level. Sample: student-level observations in six PISA waves 2000-2015. See Table 3 for included control variables. Robust standard
errors adjusted for clustering at the country level in parentheses. Significance level: *** 1 percent, ** 5 percent, * 10 percent.

Table 7: Robustness tests: Base specification
OECD
countries

Non-OECD
countries

(1)

(2)

(3)

(4)

(5)

(6)

Standardized external comparison

29.303***
(7.471)

16.429*
(8.387)

61.184***
(9.981)

27.431***
(6.160)

31.205***
(5.996)

33.247***
(8.937)

Standardized monitoring

4.671
(15.292)

-10.835
(19.542)

-16.515
(19.191)

-5.817
(13.900)

-10.664
(15.272)

-10.906
(15.499)

Internal testing

1.727
(13.704)

15.001
(14.846)

19.131
(26.395)

5.665
(10.619)

6.381
(16.582)

5.434
(9.393)

Internal teacher monitoring

-25.693
(16.190)

-22.625
(21.114)

-13.438
(23.881)

-35.308**
(15.460)

-46.460**
(20.489)

-29.108
(21.312)

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes
Yes

1,434,355
35
197
0.283

660,501
24
106
0.441

404,344
38
76
0.365

2,045,454
59
289
0.388

1,679,250
59
247
0.399

1,698,971
58
223
n.a.

Control variables
Country fixed effects
Year fixed effects
Student observations
Country observations
Country-by-wave observations
R2

Long difference
Control for Without 2015
(2000+2015 only) exclusion rates

Rescaled
test scale

Notes: Dependent variable: PISA math test score. Least squares regression weighted by students’ sampling probability, including country and year fixed effects.
Student assessment measures aggregated to the country level. Sample: student-level observations in six PISA waves 2000-2015. Rescaled test scale available for
waves 2006-2015 only. See Table 3 for included control variables. Robust standard errors adjusted for clustering at the country level in parentheses. Significance
level: *** 1 percent, ** 5 percent, * 10 percent.

Table 8: Robustness tests: Interacted specification
OECD
countries

Non-OECD
countries

(1)

(2)

(3)

22.346***
(7.479)

Control variables
Country fixed effects
Year fixed effects

51.462
(30.820)
-0.359
(0.326)
58.619*
(32.496)
-0.547*
(0.321)
18.179
(29.982)
-0.134
(0.262)
46.444
(38.979)
-0.733*
(0.385)
Yes
Yes
Yes

61.681
(40.538)
-0.887*
(0.387)
Yes
Yes
Yes

26.378***
(5.872)
-0.374***
(0.106)
20.508
(18.675)
-0.319*
(0.185)
-10.840
(13.040)
0.232**
(0.105)
0.663
(20.416)
-0.342
(0.315)
Yes
Yes
Yes

Student observations
Country observations
Country-by-wave observations
R2

1,434,355 1,434,355
35
35
197
197
0.285
0.285

660,501
24
106
0.443

Standardized external comparison
X initial score
Standardized monitoring
X initial score
Internal testing
X initial score
Internal teacher monitoring
X initial score

64.291*
(34.495)
-0.636*
(0.343)
6.054
(11.613)

Long difference
(2000+2015 only)
(4)

(5)

18.944
69.060***
(24.016) (17.063)
0.211
-0.272
(0.222)
(0.187)
42.848
(31.020)
-0.510
(0.335)
-106.185**
(45.672)
1.119**
(0.473)
72.304
(52.716)
-1.106*
(0.551)
Yes
Yes
Yes
Yes
Yes
Yes
404,344
38
76
0.367

404,344
38
76
0.365

Control for
exclusion rates

Without
2015

Rescaled
test scale

(6)

(7)

(8)

35.439***
(7.362)
-0.217**
(0.096)
61.292***
(20.757)
-0.716***
(0.207)
-11.153
(12.372)
0.126
(0.105)
4.894
(29.938)
-0.402
(0.292)
Yes
Yes
Yes

35.085***
(9.954)
-0.189
(0.125)
55.777***
(19.008)
-0.703***
(0.209)
-1.941
(31.980)
0.020
(0.334)
8.063
(40.220)
-0.681
(0.434)
Yes
Yes
Yes

60.655***
(15.693)
-0.507**
(0.196)
8.894
(30.447)
-0.152
(0.274)
-5.212
(15.369)
0.076
(0.131)
-72.152**
(35.725)
0.666*
(0.359)
Yes
Yes
Yes

2,045,454
59
289
0.389

1,679,250
59
247
0.400

1,698,971
58
223
n.a.

Notes: Dependent variable: PISA math test score. Least squares regression weighted by students’ sampling probability, including country and year fixed effects.
Student assessment measures aggregated to the country level. Initial score: country’s PISA score in the initial year (centered at 400, so that main-effect
coefficient shows effect of assessments on test scores in a country with 400 PISA points in 2000). Sample: student-level observations in six PISA waves 20002015. Rescaled test scale available for waves 2006-2015 only. See Table 3 for included control variables. Robust standard errors adjusted for clustering at the
country level in parentheses. Significance level: *** 1 percent, ** 5 percent, * 10 percent.

