NBER WORKING PAPER SERIES

CLOSED-FORM LIKELIHOOD EXPANSIONS FOR MULTIVARIATE DIFFUSIONS

Yacine Ait-Sahalia

Working Paper 8956
http://www.nber.org/papers/w8956

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
May 2002

This research was partly funded by the NSF under grants SBR-9996023 and SES-0111140. The views
expressed herein are those of the author and not necessarily those of the National Bureau of Economic
Research.

© 2002 by Yacine Ait-Sahalia. All rights reserved. Short sections of text, not to exceed two paragraphs, may
be quoted without explicit permission provided that full credit, including © notice, is given to the source.

Closed-Form Likelihood Expansions for Multivariate Diffusions
Yacine Ait-Sahalia
NBER Working Paper No. 8956
May 2002
JEL No. C32, G12

ABSTRACT
This paper provides closed-form expansions for the transition density and likelihood function of
arbitrary multivariate diffusions. The expansions are based on a Hermite series, whose coefficients are
calculated explicitly by exploiting the special structure afforded by the diffusion hypothesis. Because the
transition function for most diffusion models is not known explicitly, the expansions of this paper can
help make maximum-likelihood a practical estimation method for discretely sampled multivariate
diffusions. Examples of interest in financial econometrics are included.

Yacine Ait-Sahalia
Department of Economics
Princeton University
Princeton, NJ 08544-1021
and NBER
(609) 258-4015
yacine@princeton.edu

1

Introduction

Diﬀusions and more generally continuous-time Markov processes are generally speciÞed in economics and
Þnance by their evolution over inÞnitesimal instants, that is, by writing down the stochastic diﬀerential
equation followed by the state vector. However, for most estimation techniques relying on discrete data, we
need to be able to infer the implications of the inÞnitesimal time evolution of the process for longer time
intervals, for instance the time interval at which the process is actually sampled, say daily or weekly. The
transition function plays a key role in that context. The transition function of a Markov process is the
conditional density for the values of the state variable at a Þxed future date, given the current level of the
state vector. It eﬀectively gives a precise answer to the time aggregation problem inherent in the dichotomy
between the time scale of the model (continuous) and that of the observed data (discrete): if the process
evolves at each instant according to a given inÞnitesimal continuous-time equation, what is the distribution
of the values of the process after a Þnite amount of time has elapsed?
Continuous-time models in Þnance have long been predominantly univariate, whether the variable in an
asset price as in the Black-Scholes and Merton models, or an interest rate as in the Cox, Ingersoll and Ross
or Vasicek models. In recent years, however, the literature has naturally evolved towards the inclusion of
multiple variables in continuous-time diﬀusion models. Typical examples include asset pricing models with
multiple explanatory factors, term structure models with multiple yields or factors, and stochastic volatility
or stochastic mean reversion models (see Sundaresan (2000) for a recent survey).
In response to this trend towards multivariate models, this paper describes the construction of closedform approximations to the transition density of arbitrary multivariate diﬀusions, thereby extending to the
multivariate setting the results of Aït-Sahalia (2002). The form of the likelihood expansions derived here is
based on Hermite polynomials. While writing down a Hermite series can be done for any model, the key
idea is to exploit the speciÞcity aﬀorded by the diﬀusion hypothesis in order to obtain the expressions for
the coeﬃcients of the series fully explicitly, as functions of the state vectors at the present and future dates,
the time interval that separates them and the parameters of the assumed stochastic diﬀerential equation.
Other methods can be used to approximate the transition function, which involve solving numerically the
Fokker-Planck-Kolmogorov equation, simulating the process to Monte Carlo integrate the transition density
or approximating the process with binomial trees (see Aït-Sahalia (2002) for a review of the literature, and
Jensen and Poulsen (2002) for a comparison of the diﬀerent methods). None however produces a closed form
approximation.
The extension from the univariate to the multivariate setting presents many challenges. Through judicious
use of Itô’s Lemma, every univariate diﬀusion can be transformed into one with unit diﬀusion, whose density
can then be approximated around a standard Normal. This is no longer the case for multivariate diﬀusions.
I therefore introduce the concept of reducibility for multivariate diﬀusions, which essentially characterizes
diﬀusions for which such a transformation exists. For reducible multivariate diﬀusions, the ideas introduced in
the univariate setting can be extended, leading to an expansion for the log-likelihood function in the form of a
Taylor series in the time variable, which is a particularly convenient way of gathering the Hermite terms. For
irreducible diﬀusions, however, one must proceed diﬀerently. The situation is more involved, yet still amenable
1

to a closed-form result, but this time in the form of a double Taylor expansion in the time variable and the
state vector. Extensions of the results of Aït-Sahalia (2002) in two diﬀerent univariate directions have also
recently been developed, for time-inhomogenous diﬀusions (Egorov, Li, and Xu (2001)) and for models driven
by Lévy processes other than Brownian motion (Schaumburg (2001)).
Once the expansion is computed for the diﬀusion model at hand, it can be immediately applied to the
estimation of parameters of the discretely sampled diﬀusion by maximum-likelihood, or to a variety of other
estimation methods which require an expression for the transition density of the state variables, such as
Bayesian methods where one wishes to obtain a posterior distribution for the parameters of a stochastic
diﬀerential equation. The method can also be applied to generate simulated data at the desired frequency
from the continuous-time model, or to serve as the instrumental or auxiliary model in indirect inference and
simulated or eﬃcient moments methods. The point is that the explicit nature of the expansion as a function
of all the relevant variables makes these computations, whether maximization of the classical likelihood or
computation of posterior distributions, straightforward and computationally very eﬃcient.
The paper is organized as follows. Section 2 sets up the model, notation and assumptions. In Section
3, I introduce the concept of reducibility of a diﬀusion and provide a necessary and suﬃcient condition for
the reducibility of a multivariate diﬀusion. When diﬀusions are reducible, the coeﬃcients of the expansion
are obtained by a change of variable, which I show in Section 4. When the diﬀusion is not reducible, the
expressions for the coeﬃcients are given in Section 5. Section 6 contains examples of multivariate diﬀusions
relevant for Þnancial econometrics and gives their corresponding likelihood expansions. Finally, Section 7
concludes. All proofs are in the Appendix.

2

Setup and Assumptions

Consider the multivariate diﬀusion
dXt = µ (Xt ; θ) dt + σ (Xt ; θ) dWt

(2.1)

where Xt and µ (Xt ; θ) are m × 1 vectors, σ (Xt ; θ) is an m × m matrix, θ is a p-dimensional parameter and

Wt is an m × 1 vector of independent Brownian motions. Independence of the components is without loss of

generality as arbitrary correlation structures between the shocks to the diﬀerent equations can be modelled

through the inclusion of oﬀ-diagonal terms in the σ matrix. Note that σ need not be symmetric, and if
convenient attention can be restricted to triangular matrices by appropriate rotation of the m−dimensional
Brownian motion.
The objective of this paper is to derive closed-form approximations to the transition function pX (∆, x|x0 ; θ)
of the process X, that is the conditional density of Xt+∆ = x given Xt = x0 induced by the model (2.1).
Assume that we observe the process at dates { t = i∆ | i = 0, . . . , n}, where ∆ > 0 is Þxed. Bayes’ rule

combined with the Markovian nature of (2.1), which the discrete data inherit, imply that the log-likelihood

2

function has the simple form
0n (θ) ≡ n−1

Xn

i=1

¡
¢
lX ∆, Xi∆ |X(i−1)∆ ; θ

(2.2)

where lX ≡ ln pX . In practice, the issue is that for most models of interest, the function pX , hence lX , is not

available in closed-form.

If the sampling interval ∆ is time-varying deterministically, say ∆i is the actual time interval between
the (i − 1)th and ith observations X̃i−1 and X̃i , then it suﬃces to replace ∆ in (2.2) by its actual value ∆i

when evaluating the transition density for the ith pair of observations. If the sampling interval is random
and either drawn independently of the X process or conditionally on X̃i−1 , then one can write down the joint

likelihood function of the pair of observations and ∆i and utilize Bayes’ rule to express it as the product
of the conditional density of X̃i given X̃i−1 and ∆i , times the marginal density d of ∆i given X̃i−1 , that
³
´
³
´
is pX ∆i , X̃i |X̃i−1 ; θ × d ∆i |X̃i−1 ; κ where κ is a parameter vector parametrizing the sampling density

d. Aït-Sahalia and Mykland (2000) study the diﬀerent eﬀects resulting in the likelihood framework from
randomly and discretely spaced observations. In all cases, an expression is needed for lX , which is what this
paper delivers.
I will use the following notation. Let SX , a subset of Rm , denote the domain of the diﬀusion X and Θ ⊂ Rp

the open parameter space. SX can often be taken to be of the form of a product of m intervals with limits xi
¯
and x̄i , where possibly xi = −∞ and/or x̄i = +∞. The intervals are closed at Þnite limits and open at inÞnite
¯
limits. For simplicity, I will assume that Θ is such that SX is identical for each value of the parameter vector θ

T
in Θ. I will use T to denote transposition and, for a function η(x; θ) = (η1 (x; θ), ..., η d (x; θ)) , diﬀerentiable in

x, I will write ∇η(x; θ) for the Jacobian matrix of η, i.e., the matrix ∇η(x; θ) = [∂η i (x; θ)/∂xj ]i=1,...,d;j=1,...,m .
For x ∈ Rm , kxk denotes the usual Euclidean norm. The binomial coeﬃcients will be denoted
µ ¶
k
k!
.
≡
j
j!(k − j)!

(2.3)

If a = [aij ]i,j=1,...,m is a m × m invertible matrix then I write a−1 = [a−1
ij ]i,j=1,...,m for the matrix inverse,

rather than using the tensor notation (note that a−1
ij denotes the element (i, j) of the inverse matrix, not the

inverse of the element (i, j) of the original matrix). Det [a] and tr[a] denote the determinant of a and its trace,
respectively. If a = [ai ]i=1,...,m is a vector, tr[a] denotes the sum of the elements of a. a = diag[ai ]i=1,...,m
denotes the m × m diagonal matrix with diagonal elements ai . When a function η(x; θ) is invertible in x, I
write ηinv (y; θ) for its inverse, i.e., the solution in x of the equation y = η(x; θ) is x = ηinv (y; θ). By the Inverse

Function Theorem (see e.g., Theorem 8.7.8 in Haaser and Sullivan (1991)), η(x; θ) is invertible in x at x = x0
if ∇η(x; θ) has a bounded matrix inverse at x = x0 ; the inverse function ηinv then inherits the smoothness

properties of η.

Let AX denote the inÞnitesimal generator of the process X, which is characterized by its action on functions
f (∆, x, x0 ; θ) in its domain:
AX ·f (∆, x, x0 ; θ) =

m
m m
∂f (∆, x, x0 ; θ) X
∂f (∆, x, x0 ; θ) 1 X X
∂ 2 f (∆, x, x0 ; θ)
+
µi (x; θ)
+
vij (x; θ)
. (2.4)
∂∆
∂xi
2 i=1 j=1
∂xi ∂xj
i=1

3

The domain of AX includes at least functions that, for each (x0 ; θ) ∈ SX × Θ, are once continuously diﬀeren-

tiable in ∆ in R+ , twice continuously diﬀerentiable in x in SX and have compact support.

In some instances, it may be more natural to parametrize directly the inÞnitesimal variance-covariance

matrix of the process
v (x; θ) ≡ σ (x; θ) σ T (x; θ) ,

(2.5)

that σ(x; θ) itself. In that case, σ (x; θ) is deÞned indirectly as the positive deÞnite square root of v (x; θ) ,
σ (x; θ) = v (x; θ)

1/2

.

σ can be obtained by the Cholesky decomposition of the matrix v (x; θ). While it is traditional to parametrize
the process by (µ, σ), every characterization of the process, such as its transition probability, depends in fact
on (µ, v). In particular, it can be shown that, should there exist a continuum of solutions in σ to the equation
(2.5), the transition probability of the process is identical for each one of these σ (see Remark 5.17 and Section
5.3 in Stroock and Varadhan (1979)). This is also quite clear from the deÞnition (2.4) of the inÞnitesimal
generator of the process, which is an equivalent characterization of the process, and depends on v rather than
σ. As this will pay a role in the likelihood expansions, deÞne
Dv (x; θ) ≡

1
ln (Det[v(x; θ)]) .
2

(2.6)

To avoid the issues associated with the multiple σ scenario, I assume from now on that σ is uniquely
determined, either directly as part of the assumed speciÞcation of the model (2.1) or indirectly as the unique
solution of (2.5), in which case the form of v is such that it yields a unique square root matrix. I will assume
that this matrix σ satisÞes the following regularity condition:
Assumption 1. The matrix σ (x; θ) is positive deÞnite for all x in the interior of SX and θ ∈ Θ.
Further assumptions are required to insure the existence and unicity of a solution to (2.1), and to make
the computation of likelihood expansions possible. I will assume the following:
Assumption 2. For each θ ∈ Θ, µ (x; θ) and σ (x; θ) are inÞnitely diﬀerentiable in x on SX .
Assumption 2 insures the unicity of solutions to (2.1). Indeed, Assumption 2 implies in particular that the
coeﬃcients of the stochastic diﬀerential equation are locally Lipschitz under their assumed (once) diﬀerentiability, by applying the mean value theorem. That is, for each C > 0, there exists a constant K > 0 such that
for every x and x0 in SX , kxk ≤ C and kx0 k ≤ C, we have
|µi (x; θ) − µi (x0 ; θ)| ≤ K kx − x0 k

(2.7)

|σij (x; θ) − σ ij (x0 ; θ)| ≤ K kx − x0 k

(2.8)

for i, j = 1, ..., m. This insures that a solution, if it exists, will be unique (see e.g., Theorem 5.2.5 in Karatzas
and Shreve (1991)). The inÞnite diﬀerentiability assumption in x is unnecessary for that purpose, but it allows

4

the computation of expansions of the transition density, which as we will see involve repeated diﬀerentiation
of the coeﬃcient functions µ and σ.
There exist models of interest in Þnance, such as Feller’s square-root diﬀusion used in the Cox, Ingersoll and
Ross model of the term structure, that fail to satisfy (2.8) since they violate the diﬀerentiabilty requirement of
Assumption 2 at a boundary of SX : for instance, σ(x; θ) = σ0 x1/2 is not diﬀerentiable at the left boundary

0 of SX . Fortunately, it is possible to weaken Assumption 2 to cover such cases:

Assumption 3. (Yamada-Watanabe Conditions) Assumption 2 can be replaced by:
1. For each θ ∈ Θ, µ (x; θ) and σ (x; θ) are inÞnitely diﬀerentiable in x on the interior of SX .

2. There exist real-valued, continuous, positive and increasing functions ρ(u) and κ(u) deÞned on [0, C)

for some C > 0 such that ρ(0) = κ(0) = 0, ρ2 (u)u−1 and κ(u) are concave and satisfy
lim+

ε→0

Z

C

ε

lim+

ε→0

Z

u
du = +∞
ρ2 (u)

C

ε

1
du = +∞.
κ(u)

(2.9)
(2.10)

Then
|µi (x; θ) − µi (x0 ; θ)| ≤ κ (kx − x0 k)

(2.11)

|σij (x; θ) − σ ij (x0 ; θ)| ≤ ρ (kx − x0 k)

(2.12)

2
such that kx − x0 k < C and all i, j = 1, ..., m.
for all (x, y) ∈ SX

3. If σ(x; θ) is of the form σ(x; θ) = diag [σ i (xi ; θ)]i=1,..,m (this is always the case if m = 1), condition

(2.9) can be weakened to
lim+

ε→0

Z

ε

C

1
ρ2 (u)

du = +∞

(2.13)

with no concavity requirement.
4. If m = 2 and σ(x; θ) is of the isotropic form σ(x; θ) = diag [s (x; θ)]i=1,2 then condition (2.9) can be
weakened to
lim+

ε→0

Z

ε

C

u ln(1/u)
du = +∞
ρ2 (u)

(2.14)

provided that G(u) = u3 exp(2/u)ρ2 (exp(−1/u)) is concave.
As in the case of Assumption 2, Assumption 3.1 is there for the purpose of computing likelihood expansions.
The fact that Assumption 3.2 insures unicity of the solution follows from Theorem 4 in Watanabe and Yamada
(1971); Assumption 3.3 from Theorem 1 in Yamada and Watanabe (1971); Assumption 3.4 from Theorem
3 in Watanabe and Yamada (1971). Examples of functions ρ that satisfy (2.9) are: ρ(u) = uα with α ≥ 1,

ρ(u) = u(ln(1/u))1/2 . The functions ρ(u) = uα with α ≥ 1/2 satisfy (2.13). The functions ρ(u) = uα with

α ≥ 1/2 and ρ(u) = u ln(1/u) satisfy (2.14). A function σ ij satisfying condition (2.12) with ρ(u) = uα is said

to be Hölder-continuous of order α.

5

Assumption 3.3 with ρ(u) = u1/2 allows us in particular to consider mutivariate Cox, Ingersoll and Ross
i
h
1/2
(see the term structure examples in Section 6.3). The issue with
models where σ(x; θ) = diag ηi xi
i=1,..,m

these aﬃne models (linear µ and v = σσT ) lies in the non-Lipschitz behavior of the σ function rather than
that of the µ function. In that case, Assumption 3 for µ with κ(u) = k.u reduces to the Lipschitz condition
(2.7) for the drift µ.
These conditions are essentially the best possible, in that examples where multiple solutions to the stochastic diﬀerential equation (2.1) arise when they are violated. If m ≥ 3, take any subadditive ρ(u) (i.e.,

ρ(u + v) ≤ ρ(u) + ρ(v)) such that

lim+

ε→0

Z

ε

C

u
ρ2 (u)

du < +∞,

for instance ρ(u) = u1/2 , then the stochastic diﬀerential equation dXt = σ(Xt )dWt , X0 = 0, with isotropic
σ matrix σ(x) = diag [ρ (kxk)]i=1,...,m , has, apart from the solution Xt = 0, other non-zero solutions. Thus
condition (2.9) in Assumption 3.2 is sharp. In dimension m = 1, the famous example of Girsanov, dXt =
|Xt |α dWt , has a unique solution if α ≥ 1/2, namely Xt = 0, but that solution is no longer unique if 0 < α < 1/2;

hence condition (2.13) in Assumption 3.3 is also sharp. In Assumption 3.4 concerning the dimension m = 2,
the restriction that the matrix σ(x; θ) be of the isotropic form cannot be relaxed: a counterexample was
provided recently in Swart (2001). The condition (2.14) is also seen to be sharp, by forming a counterexample
with a subadditive ρ as in dimension m ≥ 3.

The next assumption restricts the growth behavior of the coeﬃcients near the boundaries of the domain:

Assumption 4. The drift and diﬀusion functions satisfy linear growth conditions, that is, for each θ ∈ Θ

there exists a constant K such that for all x ∈ SX , and i, j = 1, ..., m :
|µi (x; θ)| ≤ K (1 + kxk)

(2.15)

|σij (x; θ)| ≤ K (1 + kxk) .

(2.16)

The role of Assumption 4 is to insure existence of a solution to the stochastic diﬀerential equation (2.1)
by preventing explosions of the process in Þnite expected time. While it can be relaxed in speciÞc examples,
it is not possible to do so in full generality as shown by the following counterexamples, illustrating the need
for restricting the growth of both µ and σ. The one-dimensional equation dXt = (1 + Xt2 )dt, X0 = 0,
2

has the exploding solution Xt = tan(t). The three-dimensional equation dXt = (1 + kXt k )dWt explodes

in Þnite time. In dimension one, however, Þner results are available (see the Engelbert-Schmidt criterion in
Theorem 5.5.15 in Karatzas and Shreve (1991)) allowing linear growth to be imposed only when the drift
coeﬃcient pulls the process towards an inÞnity boundary (see Proposition 1 of Aït-Sahalia (2002)). Even in
higher dimensions, the condition can sometimes be reÞned in speciÞc examples (see Section 6.2 below). In all
dimensions, the linear growth condition in Assumption 4 is only an issue near the boundaries of SX . On any

compact set, the growth condition (boundedness, in fact) follows from diﬀerentiability of the functions and

the mean value theorem.
While nothing in this paper hinges upon the stationarity of the process X, it is useful to have a suﬃcient
6

condition that would guarantee it, if need be. From Hasminskii (1980), for given θ ∈ Θ, there exists a unique

stationary distribution for the process X if there exists C > 0 and some positive deÞnite matrix V such that
1
µ(x; θ)V x + tr [v (x; θ) V ] < −1
2

(2.17)

for all x in SX such that kxk > C. Then the stationary density of X is the solution π (x0 ; θ) of the equation
m
m m
X
1 XX
∂
∂2
[µi (x0 ; θ) π (x0 ; θ)] −
[vij (x0 ; θ) π (x0 ; θ)] = 0
∂x0i
2 i=1 j=1 ∂x0i ∂x0j
i=1

(2.18)

that integrates to one. The process X will be stationary provided that the initial random variable X0 is
distributed with density π (x0 ; θ) . Of course, the process may be stationary for some values of θ in Θ and not
others. For example, in an Ornstein-Uhlenbeck process stationarity depends upon the positivity of the real
parts of the eigenvalues of the mean reversion matrix.
If the approximation to the function lX is to be used for maximum-likelihood estimation of the parameters
θ, then care must be taken to insure that all the parameters are identiÞed. The MLE is well-deÞned and
identiÞcation is achieved if we assume:
Assumption 5. For each x ∈ SX , µ (x; θ) and σ (x; θ) are three times continuously diﬀerentiable in θ on Θ,
¡
¢
2
of
and, if there exist (θ, θ 0 ) ∈ Θ2 such that pX (∆, x|x0 ; θ) = pX ∆, x|x0 ; θ 0 on a set of values of (x, x0 ) ∈ SX

non-zero measure, then θ = θ0 .

More primitive conditions, not involving the function pX , can be given in speciÞc examples, see Section 6
below. This paper deals only with the construction of an approximation to lX , which can then be used for
purposes other than maximum likelihood estimation. In that case, there is no reason to assume Assumption
5.
One last remark. The diﬀusion process X is fully deÞned by the speciÞcation of the functions µ and σ
and its behavior at the boundaries of SX . In many examples, the speciÞcation of µ and σ predetermines the

boundary behavior of the process, but this will not be the case for models that represent limiting situations.
For instance, in Cox, Ingersoll and Ross processes with aﬃne µ and v, the behavior at the 0 boundary depends
upon the values of the parameters θ in (µ, σ). When this situation occurs for a particular model, the behavior
of the likelihood expansion near such a boundary will be speciÞed exogneously to match that of the assumed
model.

3

Reducible Diﬀusions

Whenever possible, I will Þrst transform the diﬀusion X into one that is more amenable to the derivation of
an expansion for its transition density. For that purpose, I introduce the following deÞnition:
DeÞnition 1. (Reducibility) The diﬀusion X is said to be reducible to unit diﬀusion (or reducible, in short)
if and if only if there exists a one-to-one transformation of the diﬀusion X into a diﬀusion Y whose diﬀusion
matrix σY is the identity matrix. That is, there exists an invertible function γ (x; θ) , inÞnitely diﬀerentiable in
7

X on SX and three times continuously diﬀerentiable in θ on Θ such that Yt ≡ γ (Xt ; θ) satisÞes the stochastic
diﬀerential equation

dYt = µY (Yt ; θ) dt + dWt

(3.1)

on the domain SY .
To avoid needless complications, I will assume that the domain of the transformed process, SY , is inde-

pendent of the parameter value θ. As discussed for SX already, in typical examples, SX and SY are both

products of intervals with lower limits xi and yi that are either −∞ or 0, and upper limits x̄i and ȳi that are
¯
¯
either 0 or +∞.
By Itô’s Lemma, when the diﬀusion is reducible, the change of variable γ satisÞes
∇γ(Xt ; θ) = σ−1 (x; θ) .

(3.2)

Every scalar (i.e., one-dimensional) diﬀusion is reducible, by means of the transformation
Yt ≡ γ (Xt ; θ) =

Z

Xt

du
σ (u; θ)

(3.3)

and we have by Itô’s Lemma:
¡
¢
¢
µ γ inv (y; θ) ; θ
1 ∂σ ¡ inv
−
γ (y; θ) ; θ .
µY (y; θ) =
inv
σ (γ (y; θ) ; θ)
2 ∂x

This transformation played a critical role in the derivation of closed-form Hermite approximations to the
transition density of univariate diﬀusions in Aït-Sahalia (2002). However, not every multivariate diﬀusion is
reducible. Whether or not a given multivariate diﬀusion is reducible depends on the speciÞcation of its σ
matrix, namely:
Proposition 1. (Necessary and Suﬃcient Condition for Reducibility) The diﬀusion X is reducible if and only
¤
£
if the inverse diﬀusion matrix σ−1 = σ−1
i,j i,j=1,...,m satisÞes on SX × Θ the condition that
∂σ −1
∂σ−1
ij (x; θ)
ik (x; θ)
=
∂xk
∂xj

(3.4)

for each triplet (i, j, k) = 1, ..., m such that k > j.
In the bivariate case m = 2, the state vector is Xt = (X1t , X2t )T and the components of the µ vector and
σ matrix are



dX1t
dX2t





=

and condition (3.4) reduces to

µ1 (Xt ; θ)
µ2 (Xt ; θ)





 dt + 

σ 11 (Xt ; θ) σ 12 (Xt ; θ)
σ 21 (Xt ; θ) σ 22 (Xt ; θ)




∂σ −1
∂σ −1
∂σ−1
∂σ−1
11 (x; θ)
12 (x; θ)
21 (x; θ)
22 (x; θ)
−
=
−
= 0.
∂x2
∂x1
∂x2
∂x1
8

dW1t
dW2t




(3.5)

(3.6)

Example 1. Diagonal Systems: If σ12 = σ21 = 0, then the reducibility condition becomes ∂σ −1
11 /∂x2 =
−1
∂σ−1
22 /∂x1 = 0. Since σ ii = 1/σ ii in the diagonal case, reducibility is equivalent to the fact that σ ii depends

only on xi (and θ) for each i = 1, 2. This is true more generally in dimension m. Note that this is not the
case if oﬀ-diagonal elements are present.
Another example is provided by the class of stochastic volatility models:
Example 2. Stochastic Volatility: If


σ (x; θ) = 

σ11 (x2 ; θ)

0

0

σ 22 (x2 ; θ)




then the process is not reducible in light of the previous example, as this is a diagonal system where σ11 depends
on x2 . However, if


σ (x; θ) = 

a(x1 ; θ) a(x1 ; θ)b(x2 ; θ)
c(x2 ; θ)

0

then the process is reducible as can be seen by applying (3.6).




The situation now is as follows. Whenever a diﬀusion is reducible, an expansion can be computed for
the transition density pX of X by Þrst computing it for the density pY of the reduced process Y and then
transforming Y back into X, proceeding essentially by extending the univariate method: see Section 4. When
a diﬀusion is not reducible, I explain below how to nevertheless derive a closed-form expansion directly for the
transition density pX : this is done in Section 5.

4

Closed-Form Expansion for the Transition Density of a Reducible
Diﬀusion

4.1

Form of the Hermite Series in the Univariate Case

As discussed above, every univariate diﬀusion is reducible. To motivate the approach in the multivariate case,
let me Þrst recall how one proceeds in the univariate case, summarizing brießy the results of Aït-Sahalia (2002).
To understand the construction of the sequence of approximations to the transition function pX , the following
analogy may be helpful. Consider a standardized sum of random variables to which the Central Limit Theorem
(CLT) apply. Often, one is willing to approximate the actual sample size n by inÞnity and use the N (0, 1)
limiting distribution for the properly standardized transformation of the data. If not, higher order terms of
the limiting distribution (for example the classical Edgeworth expansion based on Hermite polynomials) can
be calculated to improve the small sample performance of the approximation. The basic idea is to create an
analogy between this situation and that of approximating the transition density of a diﬀusion. Think of the
sampling interval ∆ as playing the role of the sample size n in the CLT. If we properly standardize the data,
then we can Þnd out the limiting distribution of the standardized data as ∆ tends to 0 (by analogy with what
9

happens in the CLT when n tends to ∞). Properly standardizing the data in the CLT means summing them
and dividing by n1/2 ; here it will involve transforming the original diﬀusion X into another one, called Z

below. In both cases, the appropriate standardization makes N (0, 1) the leading term. I will then reÞne this
N (0, 1) approximation by “correcting” for the fact that ∆ is not 0 (just like in practical applications of the
CLT n is not inÞnity), i.e., by computing the higher order terms. As in the CLT case, it is natural to consider
higher order terms based on Hermite polynomials, which are orthogonal with respect to the leading N (0, 1)
term.
So let pY denote the transition function of the process Y, whose dynamics are given by (3.1). As shown
in Aït-Sahalia (2002), the tails of pY have a Gaussian-like upper bound; but while Y is “closer” to a Normal
variable than X is, it is not practical to expand pY . This is due to the fact that pY gets peaked around
the conditional value y0 when ∆ gets small. And a Dirac mass is not a particularly appealing leading term
for an expansion. For that reason, a further transformation is performed, deÞning the “pseudo-normalized”
increment of Y as
Z∆ ≡ ∆−1/2 (Y∆ − y0 ) .
I then expand the density of Z around a N(0, 1), leading to an expansion for pY of the form:
µ
¶
1
(y − y0 )2 XJ
(J)
exp
−
η(j) (∆, y0 ; θ) Hj (∆−1/2 (y − y0 ))
pùY (∆, y|y0 ; θ) =
1/2
j=0
2∆
(2π∆)

(4.1)

where the Hermite coeﬃcients η(j) (∆, y0 ; θ) are given by
η

(j)

(∆, y0 ; θ) = (1/j!)
= (1/j!)

Z

+∞

Hj (z) pZ (z|y0 , ∆; θ) dz

−∞
Z +∞

Z

Hj (z) ∆1/2 pY

−∞
+∞

³

¯
´
¯
∆1/2 z + y0 ¯ y0 , ∆; θ dz

³
´
Hj ∆−1/2 (y − y0 ) pY (y|y0 , ∆; θ) dy
−∞
³
h
´¯
i
¯
= (1/j!) E Hj ∆−1/2 (Y∆ − y0 ) ¯ Y0 = y0 ; θ .

= (1/j!)

(4.2)

To evaluate the conditional expectation (4.2), I use the Taylor expansion
EY1 [f(∆, Y∆ , Y0 ; θ)|Y0 = y0 ] =

K
X
∆k

k=0

k!

¢
¡
AkY · f (δ, y, y0 ; θ)| y=y0 ,δ=0 + O ∆K+1

(4.3)

where AY is the inÞnitesimal generator of the process, i.e., the operator whose action is deÞned by
AY · f (∆, y, y0 ; θ) =

∂f (∆, y, y0 ; θ)
∂f (∆, y, y0 ; θ) 1 ∂ 2 f (∆, y, y0 ; θ)
+ µY (y, θ0 )
+
.
∂∆
∂y
2
∂y 2

(4.4)

In all cases, this expression is a proper Taylor series; whether the series is analytic at ∆ = 0 is not guaranteed,
although suﬃcient conditions can be given (see Proposition 4 in Aït-Sahalia (2002), who also discusses the
class of functions f, such as polynomials, for which this representation is admissible).
¡
¢
Applying (4.3) to f(∆, Y∆ , Y0 ; θ) = Hj ∆−1/2 (Y∆ − Y0 ) up to order K for the purpose of evaluating ηj
10

(J)

(J,K)

in pùY yields the expansion pùY

. Diﬀerent ways of gathering the terms are available (as in the Central Limit

Theorem, where for example both the Edgeworth and Gram-Charlier expansions are based on a Hermite
expansion). One particularly convenient way of gathering the terms of the expansion consists in grouping
them in powers of ∆. This is then in the same spirit as the “small-time” expansions of Azencott (1984), except
that the expansions obtained here are fully explicit instead of relying of moments of functionals of Brownian
Bridges. Indeed, if we gather all the terms according to increasing powers of ∆ instead of increasing order of
(K)

the Hermite polynomials, and let pY
(K)
pY (∆, y|y0 ; θ)

−1/2

=∆

(∞,K)

≡ pùY

φ

µ

y − y0
∆1/2

(K)

, we obtain an explicit representation of pY , given by:

¶

exp

µZ

y

µY (w; θ) dw

y0

¶X

K
k=0

(k)

cY (y|y0 ; θ)

∆k
k!

(4.5)

(0)

where φ (w) = exp(−w2 /2)/(2π) is the N (0, 1) density function, cY = 1 and for all k > 1:
(k)

cY (y|y0 ; θ) = k (y − y0 )−k

Z

y

y0

n
(k−1)
k−1
λY (w; θ) cY
(w − y0 )
(w|y0 ; θ)
´ o
³
(k−1)
+ ∂ 2 cY
(w|y0 ; θ) /∂w2 /2 dw

(4.6)

with
λY (y; θ) = −

µ
¶
1
∂µY (y; θ)
µ2Y (y; θ) +
.
2
∂y

(4.7)
(0)

Equation (4.6) allows the recursive computation of the coeﬃcients, starting from cY = 1.
When we are interested in computing the logarithm of the transition function, an alternative form of the
Taylor series can be more amenable to the computation of the log-likelihood, and guarantee positivity of
the density. Indeed, the function lY (∆, y|y0 ; θ) can also be expressed directly as a series in ∆, namely by
P
j
(j)
Taylor-expanding ln( Jj=0 cY (y|y0 ; θ) ∆j! ) in ∆. This yields the form
(−1)

(K)

lY

C
1
(∆, y|y0 ; θ) = − ln (2π∆) + Y
2

(y|y0 ; θ) XK
∆k
(k)
+
CY (y|y0 ; θ)
k=0
∆
k!

(4.8)

and, by application of the Jacobian change of variable formula,
¢
1 ¡
(K)
(K)
lX (∆, x|x0 ; θ) = − ln σ2 (x; θ) + lY (∆, γ (x; θ) |γ (x0 ; θ) ; θ) .
2

(4.9)

The coeﬃcients are given by
(−1)

2

(y|y0 ; θ) = − (y − y0 ) /2
Z y
(0)
CY (y|y0 ; θ) =
µY (w; θ) dw

CY

(4.10)
(4.11)

y0

(1)

(1)

CY (y|y0 ; θ) = cY (y|y0 ; θ) = (y − y0 )−1

Z

y

λY (w; θ) dw

(4.12)

y0

Note that
(−1)

CY

(0)

(1)

(y0 |y0 ; θ) = 0, CY (y0 |y0 ; θ) = 0, CY (y0 |y0 ; θ) = λY (y0 ; θ) ,
11

(4.13)

the last equation being a consequence of L’Hôpital’s Rule.
(−1)

The other coeﬃcients are obtained recursively. Given CY

(0)

(k−1)

, CY , ..., CY

is given by:
(k)
CY (y|y0 ; θ)

−k

= k (y − y0 )

Z

y

k−1

(w − y0 )

y0

(

(k)

, the coeﬃcient CY , k ≥ 2,

(k−1)

(w|y0 ; θ)
1 ∂ 2 CY
2
∂w2

)
µ
¶ (h)
(k−1−h)
(w|y0 ; θ)
1 Xk−2 k − 1 ∂CY (w|y0 ; θ) ∂CY
+
dw.
h=1
h
2
∂w
∂w

(4.14)

For consistency with the multivariate case to appear below, note for now that these expressions can also
be written in the form
(k)
CY (y|y0 ; θ)

−k

= k (y − y0 )
= k

Z

1

0

Z

y

y0

(k)

(w − y0 )k−1 GY (w|y0 ; θ) dw

(k)

GY (y0 + u (y − y0 ) |y0 ; θ) uk−1 du

(4.15)

where
(1)
GY (y|y0 ; θ)

" (0)
#2
(0)
(0)
∂CY (y|y0 ; θ) 1 ∂ 2 CY (y|y0 ; θ) 1 ∂CY (y|y0 ; θ)
∂µY (y; θ)
− µY (y; θ)
+
= −
+
∂y
∂y
2
∂y 2
2
∂y
= λY (y; θ)

(4.16)

and for k ≥ 2
(k−1)

(k)

4.2

(k−1)

(y|y0 ; θ) 1 ∂ 2 CY
(y|y0 ; θ)
+
∂y
2
∂y 2
µ
¶
(h)
(k−1−h)
1 Xk−1 k − 1 ∂CY (y|y0 ; θ) ∂CY
(y|y0 ; θ)
+
h=0
h
2
∂y
∂y
µ
¶ (h)
(k−1)
(k−1−h)
(y|y0 ; θ) 1 Xk−2 k − 1 ∂CY (y|y0 ; θ) ∂CY
(y|y0 ; θ)
1 ∂ 2 CY
=
+
h=1
h
2
∂y 2
2
∂y
∂y

GY (y|y0 ; θ) = −µY (y; θ)

∂CY

(4.17)

Determination of the Coeﬃcients in the Multivariate Reducible Case

In the case of a multivariate reducible diﬀusion, I proceed along the same lines. Hermite polynomials are
available in the multivariate case (see e.g., Chapter 5 of McCullagh (1987) or Withers (2000)). Let φ(x)
denote the density of the m−dimensional multivariate Normal distribution with mean zero and covariance
matrix κ = [κij ]i,j=1,..,m . The inverse of κ is κ−1 = [κ−1
ij ]i,j=1,..,m , so that
φ(x; κ) = (2π)−m/2 Det[κ]−1/2 exp(−

Xm Xm
i=1

j=1

κ−1
ij xi xj ).

For each vector h = (h1 , ..., hm )T ∈ Nm , recall that tr[h] = h1 + ... + hm , I will denote by Hh (x) the associated
Hermite polynomials, which are deÞned by

Hh (x; κ) =

(−1)tr[h] ∂ tr[h] φ(x; κ)
φ(x; κ) dxh1 1 ...dxhmm

12

and can be computed explicitly to an arbitrary order tr[h]. The dual Hermite polynomials are
H̃h (x; κ) =

(−1)tr[h] ∂ tr[h] φ(z; κ)
hm
φ(x; κ) dz1h1 ...dzm

at z = κ−1 x. We have that H̃h (x; κ) = Hh (κ−1 x; κ−1 ). The polynomials are orthogonal with respect to their
duals in the sense that
Z

Hh (x; κ)H̃k (x; κ)φ(x; κ)dx = h1 !...hm !

Rm

if h = k and 0 otherwise.
The Hermite series approximation of pY is in the form
³
´X
(J)
pùY (∆, y|y0 ; θ) = ∆−m/2 φ ∆−1/2 (y − y0 ) ; I

h∈Nm :tr[h]≤J

ηh (∆, y0 ; θ) Hh (∆−1/2 (y − y0 ); I)

(4.18)

i.e., with κ = I, and the Hermite coeﬃcients η h (∆, y0 ; θ) can be computed as in the univariate case, by relying
on their orthogonality. Also as in the univariate case, the Hermite expansions can be written directly for the
log-density. The key question addressed in this paper is the computation of the coeﬃcients, and this is where
I rely on the structure aﬀorded by the diﬀusion hypothesis (note of course that I do not assume that the
characteristic function of the process is known).
The inÞnitesimal generator AY corresponding to the reduced diﬀusion Y in (3.1) is
m

AY · f (∆, y, y0 ; θ) =

m

m

∂f (∆, y, y0 ; θ) X
∂f (∆, y, y0 ; θ) 1 X X ∂ 2 f (∆, y, y0 ; θ)
+
µY i (y; θ)
+
.
∂∆
∂yi
2 i=1 j=1
∂yi ∂yj
i=1

(4.19)

Gathering again the coeﬃcients in an expansion in increasing powers of ∆, the form of the expansion
analogous to (4.8) is then
(−1)

(K)

lY

(∆, y|y0 ; θ) = −

C
m
ln (2π∆) + Y
2

(k)

(y|y0 ; θ) XK
∆k
(k)
+
CY (y|y0 ; θ)
k=0
∆
k!

(4.20)

leaving us with the computation of the coeﬃcients CY , k = −1, 0, 1, 2, ..., K. The following result gives an
explicit expression for each one of these coeﬃcients:

(K)

Theorem 1. The coeﬃcients of the log-density Taylor expansion lY

(∆, y|y0 ; θ) are given explicitly by:

1 Xm
(yi − y0i )2
i=1
2
Z 1
Xm
(0)
CY (y|y0 ; θ) =
(yi − y0i )
µY i (y0 + u (y − y0 ) ; θ) du
(−1)

CY

(y|y0 ; θ) = −

i=1

and, for k ≥ 1,
(k)
CY (y|y0 ; θ)

=k

Z

0

1

(4.21)
(4.22)

0

(k)

GY (y0 + u (y − y0 ) |y0 ; θ) uk−1 du

13

(4.23)

where
(0)
Xm ∂µ (y; θ) Xm
∂CY (y|y0 ; θ)
Yi
−
µY i (y; θ)
i=1
i=1
∂y
∂yi
 i
"
#2 
(0)
(0)

2
X
m
1
∂CY (y|y0 ; θ) 
∂ CY (y|y0 ; θ)
+
+
2
i=1 

2
∂yi
∂yi

(1)

GY (y|y0 ; θ) = −

and for k ≥ 2

Xm

(4.24)

(k−1)
(y|y0 ; θ) 1 Xm ∂ 2 CY
(y|y0 ; θ)
+
i=1
i=1
∂yi
2
∂yi2
µ
¶
(h)
(k−1−h)
1 Xm Xk−1 k − 1 ∂CY (y|y0 ; θ) ∂CY
(y|y0 ; θ)
+
.
i=1
h=0
h
2
∂yi
∂yi

(k)

GY (y|y0 ; θ) = −

(k−1)

µY i (y; θ)

∂CY

(4.25)

To obtain an expansion for the density pY instead of the log-density lY , one can either take the exponential
of

(K)
lY ,

yielding
(K)
pY (∆, y|y0 ; θ)

−m/2

= (2π∆)

exp

Ã

(−1)

CY

(y|y0 ; θ) XK
∆k
(k)
+
CY (y|y0 ; θ)
k=0
∆
k!

!

(4.26)

or alternatively, given the coeﬃcients Ck for the log-density, the coeﬃcients ck for the density expansion can
(K)

be obtained by matching the coeﬃcients in the two Taylor expansions lY

4.3

(K)

and pY .

Change of Variable

Given an expansion for the density pY of Y, an expansion for the density pX of X can be obtained by a direct
application of the Jacobian formula. DeÞne the Jacobian matrix ∇γ(x; θ). Then the transition density of X
is related to that of Y by

pX (∆, x|x0 ; θ) = Det [∇γ(x; θ)] pY (∆, γ (x; θ) | γ (x0 ; θ) ; θ) .

(4.27)

Then from (3.2) and (2.6), we have
¤
£
−1/2
.
Det [∇γ(x; θ)] = Det σ −1 (x; θ) = Det [v(x; θ)]
(K)

Then, replacing pY on the right-hand-side of (4.27) by pY

(4.28)
(K)

yields an expansion pX

for pX .

In terms of log-densities, we have
1
lX (∆, x|x0 ; θ) = − ln (Det [v(x; θ)]) + lY (∆, γ (x; θ) |γ (x0 ; θ) ; θ)
2
= −Dv (x; θ) + lY (∆, γ (x; θ) |γ (x0 ; θ) ; θ)

14

(4.29)

(K)

which I mimic at the level of the approximations of order K in ∆, thereby deÞning lX
(K)

(K)

from lY

(K)

lX (∆, x|x0 ; θ) = −Dv (x; θ) + lY (∆, γ (x; θ) |γ (x0 ; θ) ; θ)
m
= − ln (2π∆) − Dv (x; θ)
2
(−1)
CY (γ (x; θ) |γ (x0 ; θ) ; θ) XK
∆k
(k)
+
+
CY (γ (x; θ) |γ (x0 ; θ) ; θ)
k=0
∆
k!

(4.30)

(k)

given in (4.20), using the coeﬃcients CY , k = −1, 0, ..., K given in Theorem 1. This fully describes

the construction of the expansion of lX for a reducible diﬀusion.

4.4

Independent Variables

An important special case occurs when the m variables in (2.1) are independent. In that case, the multivariate
transition density pX is simply the product of the m univariate transition densities, and the log-likelihood lX
is the sum of the univariate ones. The following proposition shows that the expansion shares this feature:

Proposition 2. Suppose that for each i = 1, ..., m, µi (x; θ) and σii (x; θ) depend on xi only, and that σ ij (x; θ) =
0 for j 6= i. Then the diﬀusion is reducible and we have
(K)

lX (∆, x|x0 ; θ) =
(K)

Xm

(K)
l
(∆, xi |x0i ; θ)
i=1 X

(4.31)

where lX (∆, xi |x0i ; θ) is the univariate expansion corresponding to the ith variable, deÞned in (4.9).

5

Closed-Form Expansion for the Transition Density of an Irreducible Diﬀusion

I now turn to the irreducible case. Mimicking the form of the Taylor expansion in ∆ obtained in the reducible
case, namely (4.30), leads to postulating the following form for an expansion of the log likelihood
(−1)

(K)

lX (∆, x|x0 ; θ) = −

C
m
ln (2π∆) − Dv (x; θ) + X
2

(x|x0 ; θ) XK
∆k
(k)
+
.
CX (x|x0 ; θ)
k=0
∆
k!

(5.1)

(k)

The idea now is to derive an explicit Taylor approximation in (x − x0 ) of the coeﬃcients CX (x|x0 ; θ) ,
(k)

k = −1, 0, ..., K. SpeciÞcally, I calculate a Taylor series in (x − x0 ) of each coeﬃcient CX , at order jk in
(j ,k)

(x − x0 ). Such an expansion will be denoted by CX k

. A Taylor series in (x − x0 ) is the form that arises
(J)

directly from the representation of the Hermite series pùX as in the univariate case (4.1), with the order J

of the truncation of the series now representing the order of the polynomial term in (x − x0 ) as opposed to

the order of the Hermite polynomials (which are polynomials in (x − x0 )). In the reducible case, we are able

to expand that series in powers of ∆, gather the terms as the coeﬃcient of the term ∆k in the series, take

the limit of the series as the number of Hermite polynomials increase and obtain an explicit expression for
15

(k)

(∞,k)

CX = CX

(k)

, so that we obtained the coeﬃcients CX with no need to Taylor-expand them in (x − x0 ). This

last step is what’s no longer possible when the diﬀusion is irreducible.

(j ,k)

However, it is still possible to compute the Taylor expansions CX k

explicitly. Before describing how

to compute such a coeﬃcient, one remaining question to solve is the choice of the order jk (in (x − x0 ))
¡
¢
corresponding to a given order k (in ∆). For that purpose, recall that x − x0 = Op ∆1/2 so that
¯
¯
¡
¢
¯ (k)
¯
(j ,k)
¯CX (x|x0 ; θ) ∆k − CX k (x|x0 ; θ) ∆k ¯ = Op (x − x0 )jk ∆k = Op (∆jk /2+k )

(5.2)

and setting jk /2 + k = K, i.e.,

jk = 2(K − k)

(5.3)

for k = −1, 0, ..., K, will therefore provide an approximation error due to the Taylor expansion in (x − x0 ) of

the same order ∆K for each one of the terms in the series (5.1).
The resulting expansion will then be
(j

C −1
m
(K)
˜
lX (∆, x|x0 ; θ) = − ln (2π∆) − Dv (x; θ) + X
2

(x|x0 ; θ) XK
∆k
(j ,k)
+
.
CX k (x|x0 ; θ)
k=0
∆
k!

,−1)

(5.4)

This double Taylor expansion (in ∆ and in (x − x0 )) can be viewed as a Taylor expansion in ∆ only, in light of

(5.2). In general, the function need not be analytic at ∆ = 0, hence the expansion is to be interpreted strictly
as a Taylor expansion.
(j ,k)

What remains to be done is to compute explicitly the Taylor expansion CX k

(k)

of each coeﬃcient CX .
(j

As I will now show, this involves solving a cascade of diﬀerential equations, starting with CX −1
that solution to determine

(j ,0)
CX 0 ,

,−1)

, then use

etc. Fortunately, each one of these diﬀerential problems has a closed-form

solution as we will now see in Section 5.2.

5.1

The Leading Term: Geometric Interpretation
(−1)

While the leading term CX

in the case of a reducible diﬀusion is simply
(−1)

CX
(−1)

with CY

(−1)

(x|x0 ; θ) = CY

(γ (x; θ) |γ (x0 ; θ) ; θ) ,

(y|y0 ; θ) = − 12 ky − y0 k2 (see (4.21) and (4.30)), the situation is more involved when the diﬀusion

X is not reducible.

Consider the set Ω (x|x0 ) of m−dimensional diﬀerentiable paths ω(τ ), starting at x0 at time 0 and ending
at x at time 1. An example of such a path is the straight line ω(τ ) = x0 + τ (x − x0 ). Consider now the

Riemannian metric derived from the coeﬃcients of the matrix v(x; θ)−1 , that is the distance between points

x and x + dx deÞned by
ds =

³Xm

i,j=1

−1
vij
(x; θ) dxi dxj

16

´1/2

.

(5.5)

With this metric, the length of any diﬀerentiable path ω is
d(ω; θ) =

Z

0

1 µXm

dω i (τ) ωj (τ )
vij (ω(τ); θ)
i,j=1
dτ
dτ

¶1/2

dτ .

Varadhan (1967) has shown that
lim −2∆ lX (∆, x|x0 ; θ)

=

∆→0

inf

ω∈Ω(x|x0 )

d(ω; θ)2 .

Since from (5.1)
(K)

(−1)

lim ∆ lX (∆, x|x0 ; θ)

= CX

∆→0

(x|x0 ; θ)

the appropriate leading term of the expansion (5.1) ought to be
(−1)

CX

(x|x0 ; θ) = −

1
inf
d(ω; θ)2
2 ω∈Ω(x|x0 )

(5.6)

that is, minus one half the square of the shortest distance from x to x0 in the metric induced in Rm by the
matrix v(x; θ)−1 .
An important special case occurs when σ, hence ν, is the identity matrix. In this case, the distance (5.5)
reduces to the usual Euclidean distance, the inÞmum in (5.6) is achieved by the straight line, and we have
(−1)

CY

1
1 Xm
(y|y0 ; θ) = − ky − y0 k2 = −
(yi − y0i )2
i=1
2
2

which is the result obtained in the reducible case for the reduced diﬀusion Y : see equation (4.21).
But, for any v(x; θ), the distance (5.6) is invariant under coordinate transformations. This applies in
particular to the transformation from X to Y ≡ γ (X; θ) when the diﬀusion is reducible. In this situation, we
have

(−1)

CX

1
(x|x0 ; θ) = − kγ (x; θ) − γ (x0 ; θ)k2 .
2

In dimension m = 1, where every diﬀusion is reducible, this can be recovered directly. We already know
from the univariate case that
(−1)

CX

(x|x0 ; θ) = −

1
2

µZ

x

x0

1
dw
σ (w; θ)

¶2

.

(5.7)

Now, the only way to move on the real line (including the shortest distance path) is to stay on that straight
line. Suppose, without loss of generality, that x ≥ x0 . With ω(τ) = x0 + τ(x − x0 ), we have
µ
¶
1
dω(τ)
dτ
dτ
0 σ (ω(τ ); θ)
Z 1
1
dτ
= (x − x0 )
σ
(x
+
τ
(x
− x0 ); θ)
0
0
Z x
1
dw
=
σ
(w;
θ)
x0

d(ω; θ) =

Z

1

17

with the last equality resulting from the change of variable τ 7→ w = x0 + τ(x − x0 ). Since γ is given by (3.3)

when m = 1, we indeed recover (5.7) from the general formula (5.6).

5.2

Determination of the Coeﬃcients in the Multivariate Irreducible Case
(j ,k)

I now turn to the determination of a closed-form expression for the Taylor expansions CX k

of the coeﬃcients

(j ,−1)
(k)
. Given
CX . Essentially, the coeﬃcients are determined one by one, starting with the leading term CX −1
(j−1 ,−1)
(j0 ,0)
, the next term CX
is calculated explicitly, and so on. The orders of the Taylor expansions j−1 ,
CX

j0 , etc., are chosen to control the order of the remainder terms, setting each jk according to (5.3). This
means in particular that the highest order term (k = −1) is Taylor-expanded to a higher degree of precision
(j

than the successive terms. This is to be expected, given that CX −1
determining

(j ,0)
CX 0 ,

,−1)

in a input to the diﬀerential equation

and so on.
(j ,k)

In order to state the main result pertaining to the closed-form solutions CX k

, I deÞne the following

functions of the coeﬃcients and their derivatives:
(0)

GX (x|x0 ; θ) =

(1)

(−1)
(−1)
∂CX (x|x0 ; θ) Xm Xm ∂vij (x; θ) ∂CX (x|x0 ; θ)
m Xm
−
µi (x; θ)
+
i=1
i=1
j=1
2
∂xi
∂xi
∂xj
(−1)
2
X
X
m
m
1
∂ CX (x|x0 ; θ)
+
vij (x; θ)
(5.8)
i=1
j=1
2
∂xi ∂xj
(−1)
Xm Xm
∂CX (x|x0 ; θ) ∂Dv (x; θ)
−
vij (x; θ)
,
i=1
j=1
∂xi
∂xj

Xm ∂µ (x; θ) 1 Xm Xm ∂ 2 vij (x; θ)
i
+
i=1
i=1
j=1
∂xi
2
∂xi ∂xj
Ã
!
(0)
Xm
∂CX (x|x0 ; θ) ∂Dv (x; θ)
−
µi (x; θ)
−
i=1
∂xi
∂xi
Ã (0)
!
Xm Xm ∂vij (x; θ) ∂C (x|x0 ; θ) ∂Dv (x; θ)
X
+
−
i=1
j=1
∂xi
∂xj
∂xj
(
(0)
1 Xm Xm
∂ 2 CX (x|x0 ; θ) ∂ 2 Dv (x; θ)
+
vij (x; θ)
−
i=1
j=1
2
∂xi ∂xj
∂xi ∂xj
Ã
!Ã
!)
(0)
(0)
∂CX (x|x0 ; θ) ∂Dv (x; θ)
∂CX (x|x0 ; θ) ∂Dv (x; θ)
+
−
−
∂xi
∂xi
∂xj
∂xj

GX (x|x0 ; θ) = −

18

(5.9)

and for k ≥ 2 :
(k)

GX (x|x0 ; θ) = −

Xm

(x|x0 ; θ) Xm Xm ∂vij (x; θ) ∂CX
(x|x0 ; θ)
+
i=1
j=1
∂xi
∂xi
∂xj

(k−1)

i=1

µi (x; θ)

∂CX

(k−1)

1 Xm Xm
∂ 2 CX
(x|x0 ; θ)
vij (x; θ)
(5.10)
i=1
j=1
2
∂xi ∂xj
( Ã
!
(0)
(k−1)
∂CX (x|x0 ; θ) ∂Dv (x; θ) ∂CX
(x|x0 ; θ)
1 Xm Xm
+
vij (x; θ) 2
−
i=1
j=1
2
∂xi
∂xi
∂xj
)
µ
¶
Xk−2 k − 2 ∂C (h) (x|x0 ; θ) ∂C (k−1−h) (x|x0 ; θ)
X
X
+
.
h=1
h
∂xi
∂xj
(k−1)

+

(k)

Note that the computation of each function GX requires only the ability to diﬀerentiate the previously
(−1)

determined coeﬃcients CX

(k−1)

, ..., CX

denote a vector of integers and

. The same applies to their Taylor expansions. Let i ≡ (i1 , i2 , ..., im )

Ik = {i ≡ (i1 , i2 , ..., im ) ∈ Nm : 0 ≤ tr[i] ≤ jk }
(j ,k)

so that the form of CX k
(j ,k)

CX k

(5.11)

is

(x|x0 ; θ) =

X

(k)

i∈Ik

γ i (x0 ; θ) (x1 − x01 )i1 (x2 − x02 )i2 ... (xm − x0m )im .
(j ,k)

The following theorem can now describe how the coeﬃcients CX k
determined:

(5.12)
(k)

, i.e., the coeﬃcients γ i , i ∈ Ik , are

(k)

Theorem 2. For each k = −1, 0, ..., K, the coeﬃcient CX (x|x0 ; θ) in (5.1) solves the equation
(k−1)

fX

(x|x0 ; θ) = 0

(5.13)

where
(−2)

fX

(−1)

fX

(−1)

(x|x0 ; θ) = −2CX
(x|x0 ; θ) = −

(x|x0 ; θ) −

Xm Xm
i=1

Xm Xm
i=1

(−1)

j=1

vij (x; θ)

(−1)

j=1

vij (x; θ)

∂CX

∂CX

(−1)

(x|x0 ; θ) ∂CX (x|x0 ; θ)
(5.14)
∂xi
∂xj

(0)

(x|x0 ; θ) ∂CX (x|x0 ; θ)
(0)
− GX (x|x0 ; θ) .
∂xi
∂xj

(5.15)

and for k ≥ 1
(k−1)

fX

(k)

(x|x0 ; θ) = CX (x|x0 ; θ) −

Xm Xm
i=1

(k)
−GX (x|x0 ; θ) .

(−1)

j=1

(k)

vij (x; θ)

∂CX

(k)

(x|x0 ; θ) ∂CX (x|x0 ; θ)
∂xi
∂xj
(5.16)

(k)

(h)

where the functions GX , k = 0, 1, ..., K are given above. GX involves only the coeﬃcients CX for h =
−1, ..., k−1, so this system of equation can be utilized to solve recursively for each coeﬃcient at a time, meaning
(−2)

that the equation fX

determines

(0)
CX ;

given

(−1)

= 0 determines CX

(−1)
CX

and

(0)
CX ,

(1)
GX

(−1)

; given CX

(0)

=0

(0)
fX

(1)

becomes known and the equation
19

(−1)

, GX becomes known and the equation fX

= 0 then determines CX ,

etc.
(j ,k)

Each one of these equations can be solved explicitly in the form of the Taylor expansion CX k
coeﬃcient

(k)
CX ,

at order jk in (x − x0 ). The coeﬃcients

the Taylor expansion

(j ,k−1)
fX k

of

(k−1)
fX

(k)
γ i (x0 ; θ) ,

i ∈ Ik of

(j ,k)
CX k

are determined by setting

to zero. The key feature that makes this problem solvable in closed
(k)

form is that the coeﬃcients solve a succession of systems of linear equations: Þrst determine γ i
then

(k)
γi

of the

for tr[i] = 0,

for tr[i] = 1, and all the way to tr[i] = jk .
(−1)

Note in particular, for k = −1 : γ i

= 0 for tr[i] = 0, 1 (i.e., the polynomial has no constant or linear

terms) and the terms corresponding to tr[i] = 2 (with of course j−1 ≥ 2) are:
X

(−1)

i∈I−1 :tr[i]=2

γi

1
(x0 ; θ) (x1 − x01 )i1 (x2 − x02 )i2 ... (xm − x0m )im = − (x − x0 )T v−1 (x0 ; θ)(x − x0 ).
2

which is the anticipated term given the Gaussian limiting behavior of the transition density when ∆ is small.
(−1)

Thus with j−1 ≥ 3, we only need to determine the terms γ i
For k = 0 :

(0)
γi

= 0 for tr[i] = 0, so the polynomial has no constant term. For k ≥ 1, the polynomials have
(k)

a constant term (for k ≥ 1, γ i

5.3

corresponding to tr[i] = 3, ..., j−1 .

6= 0 for tr[i] = 0 in general).

Applying the Irreducible Method to a Reducible Diﬀusion

Theorem 2 is more general than Theorem 1 in that it does not require that the diﬀusion be reducible. In
exchange for that generality, the coeﬃcients are available in closed form only in the form of a Taylor series
expansion in (x − x0 ). The following proposition describes the relationship between these two methods when
Theorem 2 is applied to a diﬀusion that is in fact reducible:

(K)

Proposition 3. Suppose that the diﬀusion X is reducible, and let lX

denote its log-likelihood expansion
(K)

calculated by applying Theorem 1. Suppose now that we also calculate its log-likelihood expansion, l̃X ,
without Þrst transforming X into the unit diﬀusion Y, that is by applying Theorem 2 to X directly. Then
(j ,k)

each coeﬃcient CX k
(k)
CX (x|x0 ; θ)

=

(K)

(x|x0 ; θ) from l̃X

(k)
CY (γ

is a Taylor expansion in (x − x0 ) at order jk of the coeﬃcient
(K)

(x; θ) |γ (x0 ; θ) ; θ) from lX .

In other words, applying the irreducible method to a diﬀusion that is in fact reducible involves replacing
(k)

(needlessly) the exact expression for CX (x|x0 ; θ) by its Taylor series in (x − x0 ). Of course, there is no reason

to do so when the diﬀusion is reducible.

6

Examples

In this section, I apply the results above to three examples of multivariate diﬀusion processes of interest in
Þnancial econometrics.

20

6.1

The Bivariate Ornstein-Uhlenbeck Model

Consider the model


 

σ
β 11 (α1 − X1t ) + β 12 (α2 − X2t )
dX1t
 dt +  11
=

dX2t
σ21
β 21 (α1 − X1t ) + β 22 (α2 − X2t )

σ12
σ22




T
where the parameter vector is θ = (α1 , α2 , β 11 , β 12 , β 21 , β 22 , σ 11 , σ12 , σ21 , σ 22 ) . Let






β 11 β 12
σ11 σ12
α1
, β = 
, σ = 

α=
α2
β 22 β 22
σ21 σ22

dW1t
dW2t




(6.1)

so that dXt = β (α − Xt ) dt + σdWt , and assume that β has full rank (as well as σ, recall Assumption ??.3).
This is the most basic model capturing mean reversion in the state variables.
Consider the matrix equation
βλ + λβ T = σσT

(6.2)

whose solution in the bivariate case is the 2 × 2 symmetric matrix λ given by
λ=

´
³
1
Det [β] σσ T + (β − tr [β]) σσ T (β − tr [β])T .
2tr [β] Det [β]

(6.3)

When the process is stationary, i.e., when the eigenvalues of the matrix β have positive real parts, λ is the
stationary variance-covariance matrix of the process. That is, the stationary density of X is the bivariate
Normal density with mean α and variance-covariance λ.
The transition density of X is the bivariate Normal density
pX (∆, x|x0 ; θ) = (2π)−1 Det[Ω (∆; θ)]−1/2 exp(− (x − m (∆, x0 ; θ))T Ω−1 (∆; θ) (x − m (∆, x0 ; θ)))

(6.4)

where
m (∆, x0 ; θ) = α + exp (−β∆) (x0 − α)

(6.5)
T

Ω (∆; θ) = λ − exp (−β∆) λ exp(−β ∆)

(6.6)

and exp applied to a matrix denotes the matrix exponential (which does not in general reduce to the exponential
of each term of the matrix).
I now discuss the identiÞcation of the continuous time parameters from the discrete data. This presence of
the matrix exponential exp (−β∆) provides a clear insight into the aliasing phenomenon as it applies to this
model. From the form of the transition function (6.4) with conditional mean and variance (6.5)-(6.6), discrete
data sampled at time interval ∆ may not distinguish between two sets of parameters β and β 0 such that
¢
¡
exp (−β∆) = exp −β 0 ∆ . The eigenvalues of β are either both real, or both complex conjugates. If they
are complex, then for any given B, there are countably many solutions in β to the equation exp (−β∆) = B.
This phenomenon was noted by Philips (1973). If the eigenvalues of β are a pair of distinct complex conjugate

21

numbers that do not diﬀer by an integer multiple of 2πi/∆, let β = T ΛT −1 where T and Λ are respectively
the matrices of eigenvectors and eigenvalues of β. Then for any integer g, the matrix β 0 deÞned by
β0 = β +

2πi
T · diag(g, −g) · T −1
∆

¡
¢
satisÞes exp −β 0 ∆ = exp (−β∆) = B. The phenomenon does not occur if the eigenvalues of β are all real

because β 0 would then have complex elements since the eigenvalues of β 0 are Λ + (2πi/∆) diag(g, −g) with
T and T −1 real in that case.

This does not necessarily mean that β is not identiÞed, because the conditional variance (6.6) conveys
identifying information about β. Indeed, while the matrix exp (−β∆) and exp(−β T ∆) = exp(−β∆)T are
identical for β and β 0 , the λ matrix may be diﬀerent and as a result the conditional variances Ω (∆; θ)
corresponding to β and β 0 may be diﬀerent. To lose identiÞcation, we would need to Þnd a pair (β 0 , σ 0 ) which
produce the same (m, Ω) as (β, σ). Let v = σσ T and v 0 = σ0 σ 0T . Identical conditional variances under both
sets of parameters would require that
v0 = v +

¢
2πi ¡
T · diag(g, −g) · T −1 · λ + λ · T · diag(g, −g) · T −1 .
∆

Such a matrix v0 always exist but, as pointed out by Hansen and Sargent (1983), except in degenerate
cases, there is at most a Þnite number of integers g for which v0 is positive deÞnite (which is necessary since
v0 = σ 0 σ0T ). Hence the identiÞcation problem is not as severe as it Þrst seems from looking at the inÞnite
number of solutions to the equation exp (−β∆) = B when β has complex eigenvalues.
But in any event, if we wish to identify the parameters in θ from discrete data sampled at the given time
interval ∆, then we must restrict the set of admissible parameter values Θ. For instance, we may restrict Θ
in such a way that that the mapping β 7→ exp (−β∆) is invertible, for instance by restricting the admissible

parameter matrices β to have real eigenvalues. This will be the case for example if we restrict attention to
matrices β which are triangular (and of course have real elements). For the rest of this discussion, I will
assume that Θ has been restricted in such a way.
By applying Proposition 1, we see that the process X is reducible, and that γ (x; θ) = σ−1 x so
dYt

¢
¡ −1
σ βα − σ−1 βσYt dt + dWt
¢
¡
= σ−1 βσ σ −1 α − Yt dt + dWt
=

≡ κ (γ − Yt ) dt + dWt

(6.7)

where


γ = σ−1 α = 

γ1
γ2





 , κ = σ−1 βσ = 

22

κ11

κ12

κ21

κ22



.

One can therefore apply Theorem 1 which gives:
(−1)

CY

(0)

2

(y|y0 ; θ) = − 12 (y1 − y01 ) − 12 (y2 − y02 )

2

CY (y|y0 ; θ) = − 12 (y1 − y01 ) ((y1 + y01 − 2γ 1 ) κ11 + (y2 + y02 − 2γ 2 ) κ12 )
−

(1)

CY (y|y0 ; θ) =

1
2

1
2

(y2 − y02 ) ((y1 + y01 − 2γ 1 ) κ21 + (y2 + y02 − 2γ 2 ) κ22 )

³
´
κ11 − ((y01 − γ 1 ) κ11 + (y02 − γ 2 ) κ12 )2 + κ22 − ((y01 − γ 1 ) κ21 + (y02 − γ 2 ) κ22 )2
¢
¡
¡
¢
− 12 (y1 − y01 ) (y01 − γ 1 ) κ211 + κ221 + (y02 − γ 2 ) (κ11 κ12 + κ21 κ22 )
¡
¢
1
(y1 − y01 )2 −4κ11 2 + κ12 2 − 2κ12 κ21 − 3κ221
+ 24
¢¢
¡
¡
− 12 (y2 − y02 ) (y01 − γ 1 ) (κ11 κ12 + κ21 κ22 ) + (y02 − γ 2 ) κ212 + κ222
¡
¢
1
(y2 − y02 )2 −4κ222 + κ221 − 2κ12 κ21 − 3κ212
+ 24
−

1
3

(y1 − y01 ) (y2 − y02 ) (κ11 κ12 + κ21 κ22 )

³
´
(2)
1
2κ211 + 2κ222 + (κ12 + κ21 )2
CY (y|y0 ; θ) = − 12
¢¢
¡
¡
+ 16 (y1 − y01 ) (κ12 − κ21 ) (y01 − γ 1 ) (κ11 κ12 + κ21 κ22 ) + (y02 − γ 2 ) κ212 + κ222
+

+
+
+

2
1
12 (y1 − y01 ) (κ12 − κ21 ) (κ11 κ12 + κ21 κ22 )
¢
¡
¡ 2
1
2
6 (y2 − y02 ) (κ21 − κ12 ) (y01 − γ 1 ) κ11 + κ21 + (y02 − γ 2 ) (κ11 κ12
2
1
12 (y2 − y02 ) (κ21 − κ12 ) (κ11 κ12 + κ21 κ22 )
¢
¡ 2
1
2
2
2
12 (y1 − y01 ) (y2 − y02 ) (κ12 − κ21 ) κ22 + κ12 − κ11 + κ21

¢
+ κ21 κ22 )

Because this is one of the few multivariate models with a known closed-form density, the OrnsteinUhlenbeck process can serve as a useful benchmark to examine the accuracy of the expansions. Table 1
reports the results of 1,000 Monte Carlo simulations comparing the distribution of the maximum-likelihood
estimator θ̂

(EXACT )

based on the exact transition density for this model, around the true value of the para-

meters θ0 , to the distribution of the diﬀerence between the exact MLE θ̂
θ̂
θ̂

(2)

and the approximate MLE

based on the expansion with K = 2 terms shown above. The results in the table show that the diﬀerence

(EXACT )

noise.

6.2

(EXACT )

− θ̂

(2)

is several orders of magnitude smaller than the diﬀerence θ̂

(EXACT )

− θ0 due to the sampling

A Stochastic Volatility Model

Consider as a second example the prototypical stochastic volatility model


 



γ 11 exp(X2t ) 0
µ
dW1t
dX1t
 dt + 
=



dX2t
dW2t
κ (α − X2t )
0
γ 22

(6.8)

where X1t plays the role of the log of an asset price and exp(X2t ) is the stochastic volatility variable. While
the term exp(X2t ) violates the linear growth condition, it does not cause explosions due to the mean reverting
nature of the stochastic volatility. This model has no closed-form solution.
The diﬀusion (6.8) is in general not reducible, so I will apply the method of Theorem 2 to derive the

23

(j ,k)

expansion. The expansion at order K = 3 is given by (5.1), with the coeﬃcients CX k
by:
(8,−1)

CX

11

−
−
(6,0)

+
+
−
−
−
(4,1)

(x|x0 ; θ) = −
+
−
+
−
+
+
−

(2,2)

CX

(0,3)

CX

³

´

2 2
(x2 −x02 )2 (6κ+γ 222 )
µ(x1 −x01 )
1 −x01 ) γ 22
02 )
01 )(x2 −x02 )
− µ(x1 −x
+ (x2 − x02 ) 12 + κ(α−x
− (x12e
−
2x02 γ 2
e2x02 γ 211
γ 222
e2x02 γ 211
12γ 222
11
µ(x1 −x01 )3 γ 222
(x1 −x01 )2 (x2 −x02 )γ 222
µ(x1 −x01 )(x2 −x02 )2
− 6e4x02 γ 4
+
3e2x02 γ 211
12e2x02 γ 211
11
µ(x1 −x01 )3 (x2 −x02 )γ 222
(x1 −x01 )2 (x2 −x02 )2 γ 222
7(x1 −x01 )4 γ 422
(x2 −x02 )4
+
−
+ 720e
4x02 γ 4
360
3e4x02 γ 411
45e2x02 γ 211
11
4µ(x1 −x01 )3 (x2 −x02 )2 γ 222
(x1 −x01 )2 (x2 −x02 )3 γ 222
µ(x1 −x01 )5 γ 422
µ(x1 −x01 )(x2 −x02 )4
−
−
+
2
4
2
2x
4x
2x
02
02
02
45e
γ 11
15e
γ 11
180e
γ 11
30e6x02 γ 611
7(x1 −x01 )4 (x2 −x02 )γ 422
4µ(x1 −x01 )3 (x2 −x02 )3 γ 222
(x1 −x01 )2 (x2 −x02 )4 γ 222
(x2 −x02 )6
− 5670 +
+
360e4x02 γ 411
45e4x02 γ 411
315e2x02 γ 211
µ(x1 −x01 )5 (x2 −x02 )γ 422
223(x1 −x01 )4 (x2 −x02 )2 γ 422
71(x1 −x01 )6 γ 622
+
− 45360e6x02 γ 6
10e6x02 γ 611
15120e4x02 γ 411
11

(x|x0 ; θ) =

CX

4 2
2
2
2
2
1 −x01 ) γ 22
1 (x2 −x02 )
01 ) (x2 −x02 )
+ (x1 −x
− (x1 −x6e012x) 02(xγ22−x02 ) + (x24e
4x02 γ 4
2
γ 222
2e2x02 γ 211
11
11
(x1 −x01 )4 (x2 −x02 )γ 222
(x1 −x01 )4 (x2 −x02 )2 γ 222
(x1 −x01 )6 γ 422
(x1 −x01 )2 (x2 −x02 )4
+
+
−
4
2
4
6
4x
2x
4x
6x
12e 02 γ 11
90e 02 γ 11
15e 02 γ 11
180e 02 γ 11
(x1 −x01 )4 (x2 −x02 )3 γ 222
(x1 −x01 )6 (x2 −x02 )γ 422
+
45e4x02 γ 411
60e6x02 γ 611
6
2 4
8 6
(x1 −x01 )4 (x2 −x02 )4 γ 222
(x1 −x01 )2 (x2 −x02 )6
01 ) (x2 −x02 ) γ 22
1 −x01 ) γ 22
−
− 3(x1 −x140e
+ (x
6x02 γ 6
945e2x02 γ 211
630e4x02 γ 411
1120e8x02 γ 811
11
2

1 −x01 )
(x|x0 ; θ) = − 12 (xe2x
−
02 γ 2

−

CX

, k = −1, 0, ..., 3 given

(12e2x02 α2 κ2 γ 211 −24e2x02 ακ2 x02 γ 211 +12e2x02 κ2 x02 γ 211 +12µ2 γ 222 −12e2x02 κγ 211 γ 222 +e2x02 γ 211 γ 422 )

24e2x02 γ 211 γ 222
(x2 −x02 )(−e2x02 ακ2 γ 211 +e2x02 κ2 x02 γ 211 −µ2 γ 222 )
µ(x1 −x01 )γ 222
−
6e2x02 γ 211
2e2x02 γ 211 γ 222
(x1 −x01 )2 (−30e2x02 ακ2 γ 211 +30e2x02 κ2 x02 γ 211 −90µ2 γ 222 −e2x02 γ 211 γ 422 )
µ(x1 −x01 )(x2 −x02 )γ 222
−
6e2x02 γ 211
360e4x02 γ 411
3 4
(x2 −x02 )2 (−60e2x02 κ2 γ 211 −60µ2 γ 222 +e2x02 γ 211 γ 422 )
2µ(x1 −x01 )(x2 −x02 )2 γ 222
1 −x01 ) γ 22
+
− 7µ(x
360e2x02 γ 211 γ 222
45e2x02 γ 211
180e4x02 γ 411
(x1 −x01 )2 (x2 −x02 )(15e2x02 κ2 γ 211 +30e2x02 ακ2 γ 211 −30e2x02 κ2 x02 γ 211 +180µ2 γ 222 +e2x02 γ 211 γ 422 )
360e4x02 γ 411
(x2 −x02 )4 (−42µ2 +e2x02 γ 211 γ 222 )
µ(x1 −x01 )(x2 −x02 )3 γ 222
7µ(x1 −x01 )3 (x2 −x02 )γ 422
+
−
90e2x02 γ 211
90e4x02 γ 411
3780e2x02 γ 211
(x1 −x01 )2 (x2 −x02 )2 (98e2x02 κ2 γ 211 +56e2x02 ακ2 γ 211 −56e2x02 κ2 x02 γ 211 +1008µ2 γ 222 +e2x02 γ 211 γ 422 )
2520e4x02 γ 411
(x1 −x01 )4 γ 222 (42e2x02 κ2 γ 211 +112e2x02 ακ2 γ 211 −112e2x02 κ2 x02 γ 211 +840µ2 γ 222 +5e2x02 γ 211 γ 422 )
10080e6x02 γ 611

(x|x0 ; θ) =

(x|x0 ; θ) =

−30e2x02 κ2 γ 211 −30e2x02 ακ2 γ 211 +30e2x02 κ2 x02 γ 211 −30µ2 γ 222 +e2x02 γ 211 γ 422
180e2x02 γ 211
(x2 −x02 )(e2x02 κ2 γ 211 +2µ2 γ 222 )
µ(x1 −x01 )(30e2x02 ακ2 γ 211 −30e2x02 κ2 x02 γ 211 +30µ2 γ 222 +e2x02 γ 211 γ 422 )
+
−
2
2x
02
12e
γ 11
90e4x02 γ 411
µ(x1 −x01 )(x2 −x02 )(15e2x02 κ2 γ 211 +30e2x02 ακ2 γ 211 −30e2x02 κ2 x02 γ 211 +60µ2 γ 222 +e2x02 γ 211 γ 422 )
+
90e4x02 γ 411
(x1 −x01 )2 γ 222 (−105e2x02 κ2 γ 211 −21e2x02 ακ2 γ 211 +21e2x02 κ2 x02 γ 211 −441µ2 γ 222 +4e2x02 γ 211 γ 422 )
−
3780e4x02 γ 411
(x2 −x02 )2 (−21e2x02 κ2 γ 211 −42e2x02 ακ2 γ 211 +42e2x02 κ2 x02 γ 211 +168µ2 γ 222 +4e2x02 γ 211 γ 422 )
−
3780e2x02 γ 211

1890µ4 γ 422 +126e2x02 µ2 γ 211 γ 222 (30κ2 (α−x02 )+γ 422 )+e4x02 γ 411 (1890κ4 (x02 −α)2 −63κ2 (1−2α+2x02 )γ 422 −16γ 822 )
7560e4x02 γ 411 γ 222

Finally, while in many instance Þnancial econometricians are willing to let X2t denote an observable
volatility variable (option-implied from the underlying asset’s option price, direct observation of volatility
derivatives contracts such as the VIX, or other sources), if the variable X2t is not observable (latent) then

24

the transition density pX cannot be used directly in (2.2). Instead, the latent variable must be integrated out
from the joint likelihood of prices and volatility in order to obtain the likelihood function to be maximized.
Alternatively, Bayesian methods can make use of pX .

6.3

Multivariate Term Structure Models

Aït-Sahalia and Kimmel (2002) apply the method of this paper to the class of aﬃne yield models for the term
structure of interest rates. They derive the likelihood expansions for the nine canonical models of Dai and
Singleton (2000). For instance, in dimension m = 3, the four canonical models are respectively


dX1t


 dX2t

dX3t


dX1t


 dX2t

dX3t


dX1t


 dX2t

dX3t




κ11
 
 = κ21
 
κ31

κ22



κ12


κ11
 
 = κ21
 
κ31


dX1t


 dX2t

dX3t

0
κ32

κ22
κ32






κ11
 
 = κ21
 
κ31

0
κ22
κ32




−X1t
dW1t






0 
 −X2t  dt +  dW2t
κ33
−X3t
dW3t
0



 1/2
θ1 − X1t
X


 1t




κ23   −X2t  dt +  0
κ33
−X3t
0
0



 1/2
θ1 − X1t
X


 1t




0  θ2 − X2t  dt +  0
κ33
−X3t
0


κ11
 
 = κ21
 
κ31

0

κ12
κ22
κ32

0
1

0

1/2

X2t
0



 1/2
θ1 − X1t
X


 1t




κ23  θ2 − X2t  dt +  0
κ33
θ3 − X3t
0
κ13




0

(1 + β 21 X1t ) 2

0





dW1t





dW1t





  dW2t 


1
dW3t
(1 + β 21 X1t ) 2
0

0



  dW2t 


1
dW3t
(1 + β 31 X1t + β 32 X2t ) 2
0

0
1/2

X2t
0

0



dW1t







0 
  dW2t  .
1/2
dW3t
X3t

Likelihood expansions for all these models are given in Aït-Sahalia and Kimmel (2002), as well as a Monte
Carlo investigation of the properties of maximum-likelihood estimators of the parameters derived from these
expansions. They show that error due to replacing the exact transition density (for the models where it is
known) with this paper’s approximation is again several orders of magnitude smaller than the uncertainty in the
parameter estimates due to the sampling noise, and that maximum-likelihood estimates are substantially more
eﬃcient (as expected from standard asymptotic theory and the Cramer-Rao lower bound) than alternative
estimates for these models.

7

Conclusions

This paper provides a method to derive closed-form expansions to the likelihood function of arbitrary multivariate diﬀusions. The multivariate diﬀusion setting presents many challenges, including the fact that not
all diﬀusions are reducible. Nevertheless, the paper provides a method that delivers closed form likelihood
25

expansions whether the diﬀusion is reducible or not. I hope that this will contribute to making maximumlikelihood the method of choice for estimating diﬀusion models with discretely sampled data, as is the case for
other time series models.

26

References
Aït-Sahalia, Y. (2002): “Maximum-Likelihood Estimation of Discretely-Sampled Diﬀusions: A Closed-Form
Approximation Approach,” Econometrica, 70, 223—262.
Aït-Sahalia, Y., and R. Kimmel (2002): “Estimating Aﬃne Multifactor Term Structure Models Using
Closed-Form Likelihood Expansions,” Discussion paper, Princeton University.
Aït-Sahalia, Y., and P. A. Mykland (2000): “The Eﬀects of Random and Discrete Sampling When
Estimating Continuous-Time Diﬀusions,” Discussion paper, Princeton University.
Azencott, R. (1984): “Densité des Diﬀusions en Temps Petit: Développements Asymptotiques, Partie I,”
in Seminar on Probability XVIII, Lecture Notes in Mathematics 1059, pp. 402—498. Springer, Berlin.
Dai, Q., and K. J. Singleton (2000): “SpeciÞcation Analysis of Aﬃne Term Structure Models,” Journal
of Finance, 55, 1943—1978.
Edwards, C. H. (1973): Advanced Calculus of Several Variables. Dover, New York.
Egorov, A., H. Li, and Y. Xu (2001): “Maximum Likelihood Estimation of Time Inhomogeneous Diﬀusions,” Discussion paper, Cornell University.
Haaser, N. B., and J. A. Sullivan (1991): Real Analysis. Dover, New York.
Hansen, L. P., and T. J. Sargent (1983): “The Dimensionality of the Aliasing Problem in Models with
Rational Spectral Densities,” Econometrica, 51, 377—387.
Hasminskii, R. Z. (1980): Stochastic Stability of Diﬀerential Equations. Sijthoﬀ and Noordhoﬀ, The Netherlands.
Jensen, B., and R. Poulsen (2002): “Transition Densities of Diﬀusion Processes: Numerical Comparison
of Approximation Techniques,” Journal of Derivatives, 9, 1—15.
Karatzas, I., and S. E. Shreve (1991): Brownian Motion and Stochastic Calculus. Springer-Verlag, New
York.
McCullagh, P. (1987): Tensor Methods in Statistics. Chapman and Hall, London, U.K.
Philips, P. C. B. (1973): “The Problem of IdentiÞcation in Finite Parameter Continuous Time Models,”
Journal of Econometrics, 1, 351—362.
Schaumburg, E. (2001): “Maximum Likelihood Estimation of Jump Processes with Applications to Finance,”
Ph.D. thesis, Princeton University.
Stroock, D. W., and S. R. S. Varadhan (1979): Multidimensional Diﬀusion Processes. Springer-Verlag,
New York.
Sundaresan, S. M. (2000): “Continuous-Time Finance: A Review and an Assessment,” Journal of Finance,
55, 1569—1622.
Swart, J. M. (2001): “A 2-Dimensional SDE Whose Solutions Are Not Unique,” Electronic Communications
in Probability, 6, 67—71.
Varadhan, S. R. S. (1967): “Diﬀusion Processes in a Small Time Interval,” Communications in Pure and
Applied Mathematics, 20, 659—685.
Watanabe, S., and T. Yamada (1971): “On the Uniqueness of Solutions of Stochastic Diﬀerential Equations
II,” Journal of Mathematics of Kyoto University, 11, 553—563.
Withers, C. S. (2000): “A Simple Expression for the Multivariate Hermite Polynomials,” Statistics and
Probability Letters, 47, 165—169.
27

Yamada, T., and S. Watanabe (1971): “On the Uniqueness of Solutions of Stochastic Diﬀerential Equations,” Journal of Mathematics of Kyoto University, 11, 155—167.

28

Appendix: Proofs
A

Proof of Proposition 1
T

Suppose that a transformation γ (x; θ) = (γ 1 (x; θ) , ..., γ m (x; θ)) exists and deÞne Yt ≡ γ (Xt ; θ). By Itô’s
Lemma, the diﬀusion matrix of Y is
σY (Yt ; θ) = ∇γ(Xt ; θ) σ (Xt ; θ) .
For σY to be Id, it must therefore be that
∇γ(Xt ; θ) = σ−1 (x; θ) .
Thus

∂γ i (x; θ)
,
∂xj

σ −1
ij (x; θ) =
hence

∂σ −1
∂
ij (x; θ)
=
∂xk
∂xk

µ

∂γ i (x; θ)
∂xj

¶

∂
=
∂xj

µ

∂γ i (x; θ)
∂xk

(A.1)
¶

=

∂σ−1
ik (x; θ)
∂xj

for all (i, j, k) = 1, ..., m. Continuity of the second order partial derivatives is required for the order of diﬀerentiation to be interchangeable. Here, we have inÞnite diﬀerentiability.
Conversely, suppose that σ−1 satisÞes (3.4). Then, for each i = 1, ..., m, use row i of the matrix σ −1 ,
¤
£
−1
σi· = σ−1
i,j ,j=1,...,m , to deÞne the diﬀerential 1−form
ωi =

m
X

σ−1
ij dxj

j=1

and calculate its diﬀerential, the diﬀerential 2−form dω i . We have that
(m
)
m
m
X
X
X ∂σ−1
ij
−1
dω i =
d(σij ) ∧ dxj =
dxk ∧ dxj
∂xk
j=1
j=1 k=1
( −1
)
m X
m
X
∂σij
∂σ−1
ik
=
−
dxk ∧ dxj
∂x
∂x
k
j
j=1
k=j+1

since dxj ∧ dxk = −dxk ∧ dxj and dxj ∧ dxj = 0 (for notation and deÞnitions of diﬀerential forms, see e.g.,
Chapter V in Edwards (1973)).
Thus condition (3.4) implies that dω i = 0, that is the diﬀerential 1−form ω i is closed on SX . Note also that
because of its form, the domain SX is open and star-shaped (meaning that there exists a point w in its interior
such that for every x ∈ SX the line segment from x to w is contained in SX ). Therefore by Poincaré’s Lemma
(see e.g., Theorem V.8.1 in Edwards (1973)) the form ω i is exact, i.e., there exists a diﬀerential 0−form γ i
such that dγ i = ω i . In other words, for each row i of the matrix σ−1 there exists a function γ i deÞned by
Z xj
σ −1
γ i (x; θ) =
ij (x; θ) dxj
(the choice of the index j is irrelevant) which satisÞes (A.1), the required diﬀerentiability properties and is
invertible. The function γ is then deÞned by each of its d components γ i , i = 1, ..., m. By construction,
Yt ≡ γ (Xt ; θ) has unit diﬀusion and therefore X is reducible.
29

B

Proof of Theorem 1

The expression for the coeﬃcients is obtained by computing a Taylor expansion using the multivariate generator
(4.19), in eﬀect extending the approach used in the univariate case by Aït-Sahalia (2002). This process establish
the form of the solution. But as is often the case when a diﬀerential operator is involved, it is easier to verify
that a given functional form (in this case established using the generator) is the right solution. Indeed, to show
that (4.20) with the coeﬃcients given in the statement of Theorem 1 represent indeed the Taylor expansion in
∆ of the log-density function lY , at order K − 1, it suﬃces to verify that the diﬀerence between the left and
right hand sides in the Fokker-Planck-Kolmogorov (FPK) forward and backward partial diﬀerential equations
is of order ∆K .
The forward and backward FPK equations for pY are respectively:
∂pY (∆, y|y0 ; θ)
∂∆

= −

m
X
∂
{µY i (y; θ) pY (∆, y|y0 ; θ)}
∂y
i
i=1
m

+
∂pY (∆, y|y0 ; θ)
∂∆

=

m

1 X X ∂2
{vij (y; θ) pY (∆, y|y0 ; θ)}
2 i=1 j=1 ∂yi ∂yj

m
X

m

µY i (y0 ; θ)

i=1

(B.1)

m

∂pY (∆, y|y0 ; θ) 1 X X ∂ 2 pY (∆, y|y0 ; θ)
+
∂y0i
2 i=1 j=1
∂y0i ∂y0j

(B.2)

DeÞne FY (∆, y|y0 ; θ) (resp. BY (∆, y|y0 ; θ)) as the the diﬀerence left and right hand sides of (B.1) (resp.
(K)
(K)
(B.2)), divided by pY (∆, y|y0 ; θ); let FY and BY denote the analogous quantities when pY is replaced by
the expansion
Ã
!
(−1)
CY (y|y0 ; θ) XK
∆k
(K)
(k)
−m/2
pY (∆, y|y0 ; θ) = (2π∆)
+
exp
C (y|y0 ; θ)
(B.3)
k=0 Y
∆
k!
obtained by exponentiation of (4.20).
Starting with the Gaussian leading term (4.21), tedious but otherwise straightforward computations show
that:
XK−1 (k)
¡
¢
∆k
(K)
FY (∆, y|y0 ; θ) =
+ O ∆K
fY (y|y0 ; θ)
k=−1
k!
(with the convention that (−1)! = 0! = 1). The Þrst term is
(−1)

fY

(y|y0 ; θ) = −

Solving the equation

Xm

i=1

(yi − y0i ) µY i (y; θ) +
(−1)

fY
(0)

Xm

i=1

(0)

(yi − y0i )

∂CY (y|y0 ; θ)
.
∂yi

(y|y0 ; θ) = 0
(0)

for CY (y|y0 ; θ) with the boundary condition that CY be Þnite when going through the axes yj = yj0 for all
j = 1, ..., m yields the solution (4.22). The boundary condition serves to set the generic integration constants
(0)
αij in the full solution
(0)

CY (y|y0 ; θ) =
to zero.

Xm

i=1

(yi − y0i )

Z

0

1

µY i (y0 + u (y − y0 ) ; θ) du +

30

Xm

i,j=1, j6=i

(0)

αij

yi − y0i
yj − y0j

The next term is
(0)

(1)

fY (y|y0 ; θ) = CY (y|y0 ; θ) +

Xm

(yi − y0i )

Xm

(yi − y0i )

(1)

i=1

∂CY (y|y0 ; θ)
∂yi

(0)
Xm ∂µ (y; θ) Xm
∂CY (y|y0 ; θ)
Yi
+
µY i (y; θ)
i=1
i=1
∂y
∂yi
 i
"
#2 
(0)
(0)

2
X
m
∂CY (y|y0 ; θ) 
∂ CY (y|y0 ; θ)
1
−
+
2
i=1 

2
∂yi
∂yi

+

(1)

= CY (y|y0 ; θ) +
(1)

(1)

i=1

∂CY (y|y0 ; θ)
(1)
− GY (y|y0 ; θ)
∂yi
(−1)

where GY is given in (4.24) and depends on the previously determined CY

(0)

and CY . Solving the equation

(0)

fY (y|y0 ; θ) = 0
(1)

(1)

for CY , including generic integration constants αij , the explicit solution is
(1)

CY (y|y0 ; θ) =

Z

0

1

(1)

GY (y0 + u (y − y0 ) |y0 ; θ) du +

Xm

(1)

i,j=1, j6=i

αij

yi − y0i

(yj − y0j )2
(0)

which reduces to (4.23) after accounting for the same boundary condition as for CY .
(k−1)
, k ≥ 1, is given by
More generally, the term fY
(k−1)

fY
(k)

where GY
equation

(k)

(y|y0 ; θ) = CY (y|y0 ; θ) +

(k)
∂CY (y|y0 ; θ)
1 Xm
(k)
(yi − y0i )
− GY (y|y0 ; θ)
i=1
k
∂yi
(−1)

is given in (4.25) and depends on the previously determined CY

(0)

(k−1)

, CY , ..., CY

. Solving the

(k)

fY (y|y0 ; θ) = 0
(k)

(0)

(1)

for CY (with the same boundary condition as for CY and CY ) yields the explicit solution (4.23). In this
case, the full solution including generic integration constants αij is
Z 1
Xm
yi − y0i
(k)
(k)
(k)
GY (y0 + u (y − y0 ) |y0 ; θ) uk−1 du +
αij
.
CY (y|y0 ; θ) = k
i,j=1,
j6
=
i
(yj − y0j )k+1
0
(k)

Thus by construction, the solution CY , k = −1, 0, ..., K given in the statement of the theorem is such that
¢
¡
(K)
FY (∆, y|y0 ; θ) = O ∆K

which along with the linearity of (B.1) in pY insures that (4.20) is a Taylor expansion of order K − 1 of lY .
Similar calculations show that
¢
¡
(K)
BY (∆, y|y0 ; θ) = O ∆K .

C

Proof of Proposition 2
(K)

To establish that lX is the sum of the univariate components, it suﬃces to establish that each multivariate
(k)
coeﬃcient CX of the expansion is the sum of the corresponding univariate coeﬃcients. Further, it suﬃces
to establish this for the coeﬃcients CYk , since the reducibility transformation γ (x; θ) involves each component
separately:
γ (x; θ) = (γ 1 (x1 ; θ) , ..., γ m (xi ; θ))T
31

where γ i (xi ; θ) is given from σii (x; θ) by equation (3.3). Therefore, we need to establish that
Xm
(k)
(k)
CY (yi |y0i ; θ)
CY (y|y0 ; θ) =
i=1

(C.1)

for k = −1, 0, ..., K.
From (4.21), it can be seen that (C.1) is always satisÞed for k = −1 (whether the variables are independent
or not). For k = 0, we have from (4.22) that
(0)
CY (y|y0 ; θ)

=

Xm

i=1

=

Xm

=

Xm

(yi − y0i )

Z

1

µY i (y0 + u (y − y0 ) ; θ) du

0

Z

1

(yi − y0i )
µY i (y0i + u (yi − y0i ) ; θ) du
i=1
0
Z
yi
Xm
µY i (w; θ) dw
=
i=1

For k = 1, we have

i=1

y0i

(0)

CY (yi |y0i ; θ) .

(0)
Xm ∂µ (yi ; θ) Xm
∂CY (yi |y0i ; θ)
Yi
−
µY i (y; θ)
i=1
i=1
∂y
∂yi
 i
"
#2 
(0)
(0)
1 Xm  ∂ 2 CY (yi |y0i ; θ)
∂CY (yi |y0i ; θ) 
+
+
i=1 

2
∂yi2
∂yi
Xm
(1)
=
GY (yi |y0i ; θ)

(1)

GY (y|y0 ; θ) = −

i=1

and for k ≥ 2
(k)

GY (y|y0 ; θ) = −

Xm

i=1

(yi |y0i ; θ) 1 Xm ∂ 2 CY
+
i=1
∂yi
2

(k−1)

µY i (yi ; θ)

∂CY

(k−1)

(yi |y0i ; θ)
∂yi2

µ
¶ (h)
(k−1−h)
1 Xm Xk−1 k − 1 ∂CY (yi |y0i ; θ) ∂CY
(yi |y0i ; θ)
+
i=1
h=0
h
2
∂yi
∂yi
Xm
(k)
=
GY (yi |y0i ; θ)
i=1

Therefore, for k ≥ 1, we have
(k)

Z

1

(k)

GY (y0 + u (y − y0 ) |y0 ; θ) uk−1 du
Xm Z 1 (k)
= k
GY (y0i + u (yi − y0i ) |y0i ; θ) uk−1 du
i=1 0
Xm
(k)
CY (yi |y0i ; θ) .
=

CY (y|y0 ; θ) = k

0

i=1

32

D

Proof of Theorem 2

This proof proceed along the same lines as that of Theorem 1. The forward and backward FPK equations for
pX are respectively:
∂pX (∆, x|x0 ; θ)
∂∆

= −

m
X
∂
{µi (x; θ) pX (∆, x|x0 ; θ)}
∂x
i
i=1
m

+
∂pX (∆, x|x0 ; θ)
∂∆

=

m

1 X X ∂2
{vij (x; θ) pX (∆, x|x0 ; θ)}
2 i=1 j=1 ∂xi ∂xj

m
X

m

µi (x0 ; θ)

i=1

(D.1)

m

∂pX (∆, x|x0 ; θ) 1 X X
∂ 2 pX (∆, x|x0 ; θ)
+
vij (x0 ; θ)
∂x0i
2 i=1 j=1
∂x0i ∂x0j

(D.2)

DeÞne FX (∆, x|x0 ; θ) (resp. BX (∆, x|x0 ; θ)) as the the diﬀerence left and right hand sides of (D.1) (resp.
(K)
(K)
(K)
(K)
(D.2)), divided by pX (∆, x|x0 ; θ); let FX and F̃X (resp. BX and B̃X ) and denote the analogous
quantities when pX is replaced by the expansions
Ã
!
(−1)
CX (x|x0 ; θ) XK
∆k
(K)
(k)
−m/2
+
pX (∆, x|x0 ; θ) = (2π∆)
exp − ln Dv (x; θ) +
C (x|x0 ; θ)
(D.3)
k=0 X
∆
k!
and
(K)
p̃X (∆, x|x0 ; θ)

−m/2

= (2π∆)

Ã

(j

exp − ln Dv (x; θ) +

CX −1

(x|x0 ; θ) XK
∆k
(j ,k)
+
CX k (x|x0 ; θ)
k=0
∆
k!

,−1)

respectively, obtained by exponentiation of (5.1) and (5.4) respectively.
We have
XK−1 (k)
¡
¢
∆k
(K)
+ O ∆K
FX (∆, x|x0 ; θ) =
fX (x|x0 ; θ)
k=−2
k!

!

(D.4)

(−2)

(with the convention that (−2)! = 2 and (−1)! = 0! = 1). The highest order term is fX given by (5.14) and
(−1)
(−2)
(0)
to zero. Then we have successively CX determined
the coeﬃcient function CX is such that it sets fX
(−1)
(−1)
(0)
(k−1)
, the expression (5.16) for
by setting fX in (5.15) to zero, and more generally, given CX , CX , ..., CX
(k−1)
(k)
is deÞned and can be set to zero to determine the next coeﬃcient CX .
fX
(k)
(k)
(j ,k)
To determine the Taylor expansions in x − x0 for each coeﬃcient CX , k ≥ −1, replace CX by CX k in
(−2)
in (x − x0 ) to order j−1 .
each equation in turn, starting with (5.14). calculate a Taylor expansion of f˜X
(−1)
(−1)
This determines a system of equations in the unknown coeﬃcients γ i , i ∈ I−1 (which appear when CX
is Taylor expanded as in (5.12)). By construction, there are as many equations as unknowns (both are given
by the number of elements in I−1 ). This system of equation can always be solved explicitly because it has the
following form.
(−1)
First, γ i
= 0 for tr[i] = 0, 1 (i.e., the polynomial has no constant or linear terms) and the terms
corresponding to tr[i] = 2 (with of course j−1 ≥ 2) are:
X
(−1)
γi
(x0 ; θ) (x1 − x01 )i1 (x2 − x02 )i2 ... (xm − x0m )im = −(x − x0 )T v−1 (x0 ; θ)(x − x0 ).
i∈I−1 :tr[i]=2

which is the anticipated term given the Gaussian limiting behavior of the transition density when ∆ is small.
(−1)
Thus with j−1 ≥ 3, we only need to determine the terms γ i
corresponding to tr[i] = 3, ..., j−1 .
Then, the next order coeﬃcients in (x − x0 ) , i.e., the coeﬃcients corresponding to tr[i] = 3, each appear
linearly in a separate equations. That is, we have a system
(−1)

M3

(−1)

(x0 ; θ) · γ 3

(−1)

(x0 ; θ) = b3

33

(x0 ; θ)

(−1)

(−1)

(−1)

whose explicit solution is given by γ 3 (x0 ; θ) = Inv[M3
(x0 ; θ)] · b3 (x0 ; θ) , and so on. Given the
previously determined coeﬃcients corresponding to tr[i] = 0, ..., r, the equations determining the coeﬃcients
for tr[i] = r + 1 are given by a linear system:
(−1)

(−1)

(−1)

Mr+1 (x0 ; θ) · γ r+1 (x0 ; θ) = br+1 (x0 ; θ)
(−1)

(−1)

(−1)

for
where the matrix Mr+1 and the vector br+1 are functions of the previously determined coeﬃcients γ i
tr[i] = 0, ..., r, and of course x0 and the parameters θ of the process.
(0)
The same principle applies to all values of k. For k = 0 : γ i = 0 for tr[i] = 0, so the polynomial has no
(k)
constant term. For k ≥ 1, the polynomials have a constant term (for k ≥ 1, γ i 6= 0 for tr[i] = 0 in general).
(j−1 ,−1)
is determined, a Taylor expansion of (5.15)
The same principle applies to each equation in turn: once CX
(0)
determines the coeﬃcients γ i , i ∈ I0 , etc.
Finally, note that the term Dv (x; θ) which arose in the reducible case from the Jacobian transformation is
(0)
independent of ∆ and so could be built into the CX coeﬃcient. Doing so however would subject it to being
Taylor-expanded in x − x0 , which is unnecessary anyway since Dv (x; θ) is known. If Dv (x; θ) were being
(j ,0)
(K)
Taylor-expanded along with CX 0 in (D.4), we would lose the property that p̃X also solves the backward
FPK equation (D.2) to order K − 1 in ∆. Hence the form of the log-likelihood I adopted in (5.1) with Dv
(0)
kept separate from CX is essential to obtain

(K)

in addition to F̃X

E

¢
¡
(K)
B̃X (∆, x|x0 ; θ) = O ∆K

¢
¡
(∆, x|x0 ; θ) = O ∆K .

Proof of Proposition 3
(k)

(k)

If the diﬀusion X is reducible, then CX (x|x0 ; θ) = CY (γ (x; θ) |γ (x0 ; θ) ; θ) . By construction (see the proof
(j ,k)
(k)
of Theorem 2), the coeﬃcients CX k are Taylor expansions of the coeﬃcients CX (which are the expressions
(k−1)
= 0).
solutions of the equations fX

34

Parameter

θ(T RU E)

θ̂

(MLE)

− θ (T RUE)

θ̂

(MLE)

− θ̂

(2)

Mean

Stnd. Dev.

Mean

Stnd. Dev.

γ1

0

−0.0013

0.069

−0.0000015

0.000035

γ2

0

0.00070

0.033

0.00000012

0.000016

κ11

5

0.52

1.17

0.012

0.0085

κ12

1

−0.066

1.74

0.0087

0.017

κ22

5

0.35

1.50

0.069

0.029

Table 1: Monte-Carlo Simulations for the Bivariate Ornstein-Uhlenbeck Model

This table reports the results of 1,000 Monte Carlo simulations comparing the distribution of the maximum-likelihood
(MLE)
estimator θ̂
based on the exact transition density for this model, around the true value of the parameters θ0 , to
(MLE)
(2)
the distribution of the diﬀerence between the exact MLE θ̂
and the approximate MLE θ̂ based on the expansion
with K = 2 terms, for the process (6.7). To insure full identiÞcation, the oﬀ-diagonal term κ21 is constrained to be zero.
As discussed in the text, this guarantees that the eigenvalues of the mean reversion matrix are both real and avoids the
aliasing problem altogether. The constraints κ11 > 0 and κ22 > 0 are imposed to insure stationarity of the process. The
true values of the parameter vector θ = (γ 1 , γ 2 , κ11 , κ12 , κ22 ) used to generate the data are θ(T RU E) = (0, 0, 5, 1, 5). Each
of the 1,000 samples is a series of n = 500 weekly observations (∆ = 1/52), generated using the exact discretization
of the process. The results in the table show that the diﬀerence θ̂
(MLE)
than the diﬀerence θ̂
− θ(T RU E) due to the sampling noise.

35

(MLE)

− θ̂

(2)

is several orders of magnitude smaller

