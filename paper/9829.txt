NBER WORKING PAPER SERIES

ARE MORE DATA ALWAYS BETTER FOR FACTOR ANALYSIS?
Jean Boivin
Serena Ng
Working Paper 9829
http://www.nber.org/papers/w9829
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
July 2003

The authors would like to thank National Science Foundation for financial support (SES-0136923 and SES0214104). We thank the editor and two anonymous referees for useful comments and Pritha Mitra for useful
research assistance. The views expressed herein are those of the authors and not necessarily those of the National
Bureau of Economic Research
©2003 by Jean Boivin and Serena Ng. All rights reserved. Short sections of text not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit including © notice, is given to the source.

Are More Data Always Better for Factor Analysis?
Jean Boivin and Serena Ng
NBER Working Paper No. 9829
July 2003
JEL No. E37, E47, C3, C53
ABSTRACT
Factors estimated from large macroeconomic panels are being used in an increasing number of
applications. However, little is known about how the size and the composition of the data affect the
factor estimates. In this paper, we question whether it is possible to use more series to extract the factors,
and yet the resulting factors are less useful for forecasting, and the answer is yes. Such a problem tends
to arise when the idiosyncratic errors are cross-correlated. It can also arise if forecasting power is
provided by a factor that is dominant in a small dataset but is a dominated factor in a larger dataset. In
a real time forecasting exercise, we find that factors extracted from as few as 40 pre-screened series
often yield satisfactory or even better results than using all 147 series. Weighting the data by their
properties when constructing the factors also lead to improved forecasts. Our simulation analysis is
unique in that special attention is paid to cross-correlated idiosyncratic errors, and we also allow the
factors to have stronger loadings on some groups of series than others. It thus allows us to better
understand the properties of the principal components estimator in empirical applications.
Jean Boivin
Graduate School of Business
821 Uris Hall
Columbia University
New York, NY
and NBER
jb903@columbia.edu

Serena Ng
Department of Economics
University of Michigan
317 Lorch Hall
Ann Arbor, MI 48109-1022
serena.ng@umich.edu

1

Introduction

Most macroeconomic empirical analysis are based on a handful of variables. For example, a typical
VAR has around six and rarely more than ten variables. While abandoning information in a
large number of series can be justified only under rather restrictive assumptions about the joint
distribution of the data, use of large scale models remains an exception rather than the rule. In part,
this is because of the computation burden involved with large models, and in part, this is because
not every available series can be informative, so that including irrelevant information may also come
at a cost. In recent years, a new strand of research has made it possible to use information from
a large number of variables while keeping the empirical framework small. These studies are based
on the assumption that the data admit a factor structure and thus have a common-idiosyncratic
decomposition. Factor analysis provides a formal way of defining what type of variation is relevant
for the panel of data as a whole.
A factor model with mutually uncorrelated idiosyncratic errors is a ‘strict factor model’, and use
of these models is not new. However, the new generation of ‘large dimensional approximate’ factor
models differ from the classical ones in at least two important ways:- (i) the idiosyncratic errors can
be weakly serially and cross-sectionally correlated, and (ii) the number of observations is large in
both the cross-section (N ) and the time (T ) dimensions. Allowing the errors to be correlated makes
the framework suited for a wider range of economic applications. The large dimensional nature
of the panel makes it possible to exploit more data in the analysis. It also opens the horizon for
consistent estimation of the factors, something that is not possible when the number of cross-section
units is small.
In applications, approximate factor models are typically estimated by the method of principal
components, by which an eigenvalue decomposition of the sample covariance matrix (the static
approach) or the spectral density matrix (the dynamic approach). The results thus far are encouraging is performed. Forni and Lippi (1997) and Forni and Reichlin (1998) found two factors
formed from 450 disaggregated series to be helpful in understanding aggregate dynamics. Stock
and Watson (2002b), and Chan, Stock and Watson (1998) showed that the forecast errors of many
macroeconomic variables can be reduced by extracting three factors from around 150 series. Forni,
Hallin, Lippi and Reichlin (2001a) obtained a similar result using 123 series to estimate two factors.
Bernanke and Boivin (2002), Bernanke, Boivin and Eliasz (2002) used roughly the same data as
Stock and Watson and found that information in the factors is relevant for the empirical modeling of monetary policy. Using as many as 479 series, Giannone, Reichlin and Sala (2002) also
adopted a factor approach to assess the conduct of monetary policy. Stock and Watson (2001)

2

and Forni, Hallin, Lippi and Reichlin (2001b) used factors estimated from around 150 and 400
series respectively, to assess whether financial variables help forecast inflation and real activity.
Forni and Reichlin (1998) used data from 138 regions in Europe to extract country and Europespecific factors, while Cristadoro, Forni, Reichlin and Giovanni (2001) used 447 series to construct
a four-factor core inflation index for the Euro area.
The studies cited above are evidently quite different from the small scale VARs that have
dominated the literature, as each study has used at least 100 cross-section units to estimate the
factors. However, Watson (2000) also found that the marginal gain (in terms of forecast meansquared error) from increasing N beyond 50 appears less substantial. Bai and Ng (2002) found
that in simulations, the number of factors can be quite precisely estimated with N as small as 40
when the errors are iid. This suggests that N does not need to be extremely large for the principal
components estimator to give reasonably precise estimates.
Could it be that increasing N beyond a certain point is not even desirable? This might appear to
be an implausible outcome at first thought, as basic statistical principles suggest more data always
improve statistical efficiency. However, whereas a typical panel is sampled to be representative
of a cross section, with indicators provided by the data releasing agency to reflect the sampling
design, the data used in macroeconomic type factor analysis is not subject to the same scrutiny.
The factors are always defined with respect to a specific set of data, and ‘correctness’ of the dataset
depends very much on the exercise on hand. Every rule used to select the data is in some sense
ad-hoc. By different choices of the data, two researchers using the same estimator can end up with
different factor estimates. The choice of data is thus not innocuous.
The basic intuition for why using more data to estimate the factors might not be desirable is as
follows. The asymptotic theory under which the method of principal components is based assumes
that the cross-correlation in the errors is not too large, and that the variability of the common
component is not too small. In practice, our data are typically drawn from a small number of
broad categories (such as industrial production, prices, interest rates). Think of ordering the
series within a category by the importance of its common component, and put together a dataset
comprising of high ranked series from each category. Now expand this dataset by adding the lower
ranked, or ’noisy’ series. Two things will happen. The average size of the common component will
fall as more series are added, and the possibility of correlated errors will increase as more series
from the same category are included. When enough of the ‘noisy’ series are added, the average
common component will be smaller, and/or the residual cross-correlation will eventually be larger
than that warranted by theory, creating a situation where more data might not be desirable.
The objective of this paper is to provide an empirical assessment of the extent to which the

3

properties of the data affect the factor estimates. To our knowledge, this paper is the first to focus
on the finite sample properties of the principal component estimator in the presence of cross-section
correlation in the idiosyncratic errors, which is a pervasive feature of the data. In the empirical
application considered, the errors of 115 out of the 147 series have correlation coefficients larger
than .5. Section 2 begins by using simple examples to show how and to what extent adding more
data can have adverse effects on the factor estimates. We use monte carlo simulations in Section
3 to document the conditions under which adding more data can be undesirable. In Section 4, we
use 147 series as in Stock and Watson (2002b) to obtain standard (unweighted) factor estimates.
We then consider procedures that weigh or drop some of the data before extracting the principal
components. We find that when used to forecast eight macroeconomic time series, the forecasts
using the weighted estimates generally have smaller errors than the unweighted ones. In some
sense, this result is encouraging, as it indicates that we have not fully exploited the potential of
factor analysis. However, the results also point to a need to develop more efficient estimators as it
is not simply N that determines estimation and forecast efficiency. The information that the data
can convey about the factor structure is also important.
2

The Role of N in Theory

Suppose we are interested in the one-period ahead forecast of a series yt . The model that generates
yt is not known. Given the history of yt , a naive forecast can be obtained using an AR(p) model
ybt+1|yt ,...y1 = α
b0 +

p
X

γ
bj yt−j+1

(1)

j=1

with forecast error variance σ
bp2 , where γ
bj , j = 0, . . . , p are the least squares estimates. Suppose we
also observe N series, Xt = (X1t , . . . XN t )0 , some of which are informative about yt+1 . If N is small
(and smaller than T ), we can consider the forecast
ybt+1|yt ,...y1 ,Xt = ηb0 +

N
X

0
ηb1i
Xit

i=1

+

p
X

γ
bj yt−j+1 .

j=1

However, if N is large, such a forecast will not be efficient because sampling variability will increase
with the number of regressors. When N > T , the forecast is not even feasible.
Now assume Xit admits a factor structure:
0
Xit = λ00
i Ft + eit ≡ χit + eit ,

i = 1, . . . N, t = 1, . . . T.

In the above, Ft0 is a r × 1 vector of factors common to all variables, λ0i is the vector of factor
loadings for series i, χit = λ00
i Ft is the common component of series i, and eit is an idiosyncratic
4

error with E(e2i ) = σi2 . If we observe the factors Ft0 , we can consider the forecast:
ybt+1|yt ,...y1 ,Ft0 = βb0 + βb10 Ft0 +

p
X

γ
bj yt−j+1

(2)

j=1

whose forecast error variance is σ
bεb2,0 . The appeal of (2) is that it allows information in a large
number of observed data Xt to be summarized in a small number of variables, Ft0 . But Ft0 is not
observed. Let Fbt,N be a consistent estimate of Ft0 using data from N series. Then the feasible
factor-augmented forecast, referred to by Stock and Watson (2002a) as a ‘diffusion index’ forecast,
is
ybt+1|yt ,...y1 ,Fbt,N = βb0 + βb10 Fbt,N +
with forecast error

p
X

γ
bj yt−j+1 ,

(3)

j=1

σ
bεb2 .
h

Now the difference between the diffusion index forecast and the naive AR(p)
i h
i
forecast is σ
bεb2 −b
σp2 = σ
bεb2 − σ
bεb2,0 + σ
bεb2,0 − σ
bp2 . If Ft0 was observed, the first error would be irrelevant

and a feasible diffusion forecast can do no worse than the AR(p) forecast. This follows from the fact
that (2) nests (1) and the mean squared error from using the latter for forecasting cannot exceed
the former. But the feasible forecast is based upon (3), which involves the generated regressors
Fbt,N . In finite samples, the desirability of a feasible diffusion index forecast depends crucially on
the estimates of Ft0 . We follow the literature and consider the method of principal components.1
Let ΣX and Σχ be the population variance of the data and of the unobserved common components associated with N observations, respectively. Let Ω be the covariance matrix of the idiosyncratic errors. These can be thought of as N -dimensional sub-matrices of the infinite dimensional
population covariances. A factor model has population covariance structure ΣX = Σχ + Ω. As
Ft0 is common to all variables, Σχ has r non-zero eigenvalues, and they increase with N . A fundamental feature of factor models is that the r largest eigenvalues of ΣX also increase with N .
This suggests that the space spanned by the factors can be estimated using an eigenvalue decomb0 = (λ
bi1 . . . λ
bir ) the estimated loadings,
position of the the sample estimate of ΣX . Denote by λ
i

1 ...F
br )0 be the estimated factors. Let vj = (v1j , . . . vN j )0 be the eigenvector
and let Fbt,N = (Fbt,N
t,N
th
b X . The j th esticorresponding to the j largest eigenvalue of the N ×N sample covariance matrix, Σ
√
P
j
b
mated factor is Fbt,N
N vij .
= √1N N
i=1 Xit vij , and the corresponding loading is estimated as λij =

Stock and Watson (2002a) and Bai and Ng (2002), showed that the factor space can be consistently
estimated as N, T → ∞ if (i) the errors are stationary, (ii) the factors have non-trivial loadings,
and (iii) the idiosyncratic errors have weak correlation both serially and cross-sectionally. The
1

Kapetanios and Marcellino (2002) compared the properties of the static and the dynamic principal components
estimator. As both are aimed at large dimensional panels, the issues to be discussed are relevant to both. Our
discussion follows the simpler static principal components estimator.

5

first condition can be relaxed.2 The second condition is necessary for distinguishing the pervasive
factors from the idiosyncratic noise. Under the third condition, Ω need not be a diagonal matrix
for a given N . But, as N tends to infinity, the non-diagonal elements of Ω should go to zero, and
the diagonal terms should approach the cross-section idiosyncratic variance.3 Bai (2003) further
√
showed that Fbt,N , suitably scaled, is N consistent for Ft0 . The various asymptotic results lead to
the natural presumption that the factor estimates are more efficient the larger is N .
Intuition about the large sample properties of the factor estimates is best seen from a model
with one factor (r = 1), and identical loadings (λ0i = λ ∀i). Given a panel of N cross sections, a
P
decomposition of ΣX (assumed known) would yield Fbt,N = Ft + N1 N
i=1 eit , from which it follows
1 PN
σ2
b
b
that var(Ft,N ) = var( N i=1 eit ). If eit is iid, var(Ft,N ) = N would decrease with N irrespective
of the value of λ. The result is analogous to classical regression analysis when the loadings are
observed. In that case, Ft can be estimated from a cross-section regression of the data at time t on
the N loadings. If the errors are iid, then by the Gauss Markov Theorem, the estimator is efficient,
and the variance of the estimates falls with N .
However, even in a regression setting, the relation between the variance of the least squares
estimates and the sample size is not unambiguous when the iid assumption is relaxed. Consider
estimation of the sample mean. Suppose N1 series are drawn from a population with variance σ12 ,
from which we can compute a sample mean, ȳ. Suppose an additional N2 series are drawn from
a population with variance σ22 , where σ12 < σ22 . With N = N1 + N2 series, we obtain a sample
N1 σ12 +N2 σ22
σ2
(ye)
mean ye. It is easy to see that var
> N11 . Whether or not more data yield more
var(ȳ) > 1 if
N2
efficient estimates depend very much on the properties of the additional series.
Indeed, this intuition extends to factor analysis. Consider the special case when a researcher
unintentionally included N1 series twice. More precisely, there are N = 2N1 series, but N1 pairs
of the idiosyncratic errors are perfectly correlated. When the errors are iid, it can be shown that
2
var(Fbt,N ) = σ which depends on N1 , not the total number of series used, N . Nothing is gained by
N1

adding more data because the duplicated series increase the variation of the common component,
but the variance of the now cross correlated errors is also larger by the same proportion. It is then
not hard to construct cases when some series have errors so strongly correlated with others that
adding them reduces rather than improves the efficiency of the factor estimates.
It is also useful to see why the properties of the errors are important from a different viewpoint.
2
Bai and Ng (2001) showed uniform consistency when the idiosyncratic errors are non-stationary. This means that
even when the individual regressions are spurious, the common factors can be consistently estimated from a large
dimensional panel.
3
Connor and Korajzcyk (1986) provided the first results for this estimator using sequential asymptotics. The
asymptotic properties of the dynamic estimator are analyzed in Forni, Hallin, Lippi and Reichlin (2000) under
similar conditions.

6

In a strict factor model, Ω is assumed to be a diagonal matrix. With N fixed, the maximum
e (a diagonal matrix) are the eigenvectors of Ω
e −1/2 Σ
bXΩ
e −1/2 .4
likelihood estimates of λ for arbitrary Ω
b X , which evidently
The principal components estimates, on the other hand, are the eigenvectors of Σ
correspond to the maximum likelihood estimates only when Ω is a scalar matrix. Accordingly, for
a given N , the principal components estimator can be expected to be less precise the further are
the errors from being homoskedastic and mutually uncorrelated.
3

Monte Carlo Simulations

In this section, we set up two monte carlo experiments to study how the data might affect the
factor estimates. The series to be forecasted in both monte carlos are generated by
yt+1 =

r
X

βj Fjt0 + εt+1 ≡ yF 0 ,t+1|t + εt+1 ,

j=1

where εt ∼ N (0, σε2 ), and σε2 is chosen such that the R2 of the forecasting equation is κy . When
β and Ft0 are observed, we denote the forecast by yF 0 ,t+1|t . The infeasible diffusion index forecast
is ybF 0 ,t+1|t , which only requires estimation of β. The feasible diffusion index forecast is denoted
ybFb,t+1|t , which requires estimation of both the factors and β. The Xit are simulated from a model
with r factors:
Xit =

r
X

λim Fmt + eit .

m=1

The factor loadings vary across i and are assumed to be N(1,1). Assumptions on eit will be made
precise below.
Our monte carlo experiments are designed to focus on the effects of heteroskedasticity and
cross-correlated errors on the factor estimates. This is based on two considerations. First, previous simulation studies on the principal components estimator generally assume iid errors (see
Kapetanios and Marcellino (2002) and Forni et al. (2000), for example). Stock and Watson (2002a)
and Bai and Ng (2002) considered a case where an error is correlated with the one ordered before
and after it. But as we will see, the cross-correlation found in the data is more substantial. Second,
previous simulation studies tend to assume the error variances are constant across i. In the data,
the variation in the errors is more substantial.
3.1

Model 1: Correlated and Noisy Errors

In this monte carlo experiment, the factors are assumed to be iid. The total number of series
available is N = N1 + N2 + N3 . With uit ∼ N (0, 1), i = 1, . . . , N as the building block, we consider
4

See Anderson (1984), p. 589.

7

three types of idiosyncratic errors:
N1: eit = σ1 uit ,
N2: eit = σ2 uit ,
N3: eit = σ3 eeit , eeit = uit +

PC

j=1 ρij ujt

.

The first N1 series are what we call ‘clean’ series as their errors are mutually uncorrelated. The next
N2 series also have mutually uncorrelated errors, but differ from the N1 series because σ22 > σ12 .
Each N3 series is correlated with some C series that belong to the N1 set. The N1 × N3 matrix
Ω13 has C × N3 non-zero elements and is the source of cross-correlation. More precisely, series
i ∈ [N1 + N2 + 1, N1 + N2 + N3 ] is correlated with series j ∈ [1, N1 ] with coefficient ρij . Since two
series in the N3 group can be correlated with the same series in the N1 group, the errors of the N3
type can also be mutually correlated. To isolate cases with cross-correlated errors from cases with
large error variances, we let σ1 = σ3 < σ2 .5 These assumptions on the idiosyncratic errors yield an
Ω with the following property:


σ12 IN1
Ω =  0N2 ×N1
Ω013

0N1 ×N2
σ22 IN2
0N3 ×N2

Ω13



0N2 ×N3  .
Ω33

We consider data generated by up to three factors. For a given r (the true number of factors),
we estimate k = 1, . . . 3 factors. Thus, if k < r, the assumed number of factors is too small. We
let N3 = n3 N1 . We vary n3 and draw ρij from a uniform distribution with a lower bound of .05
and an upper bound of .7. The σi2 are chosen such that the factors explain κi of the variation in
the data, given χit , with κ3 set to .01.6 We consider three N1 values, five N2 values, ten pairs of
(N3 , C), nine sets of (κ1 , κ2 , κy ). Each of the 12,150 configurations is simulated M =1000 times.
Let xit be the standardized data (with mean zero and unit variance) to which the method of
bit ,
principal components is used to extract k factors, where k can be different from r. This yields Fbt , χ
and ebit . Chamberlain and Rothschild (1983) showed that asset prices have an approximate factor
structure if the largest eigenvalue (and hence all of the eigenvalues) of Ω = E(et e0t ) is bounded. But
P
the largest eigenvalue of Ω is bounded by maxi N
j=1 |τij |, where τij = E(eit ejt ). Thus, under the
P
assumptions of an approximate factor model, there should exist a P such that N
j=1 |τij | ≤ P < ∞
for all i and for all N . While P is useful for the development of theory, it does not provide a
P
practical guide of how much cross-correlation is permitted in practice. Consider τbi∗ = N
τij |,
j=1 |b
5

To ensure that the presence of cross correlation does not reduce the importance of the factors, the eeit are
standardized to have unit variance.
var(χ) which can be used to solve for σ 2 , given var(χ) implied by the other parameters.
6
More precisely, κ = var
e
(χ)+σ 2
e

8

where τbij =

1
T

PT

bit ebjt .
t=1 e

We then use τb∗ =

1
N

maxi τbi∗ as an indicator for P/N . This quantity

should be small and decrease with N .
To assess the relative importance of the common component, we consider
R2 =

N
1 X 2
Ri ,
N

2
2
and Rq = R.9N
− R.1N

i=1

PT
χ
b2it
where Ri2 = Pt=1
T
2 indicates the relative importance of the common component in series
t=1 xit
average of Ri2 measures the average importance of the common component in the data as a

i. The
whole.

The cross-section dispersion of the common component is measured by Rq , the difference between
the Ri2 in the 90 and the 10 percentile.
Given Fbt , β is estimated by OLS, and the diffusion index forecast ybFb,t+1|t is obtained. We use
three statistics to gauge the properties of the factor estimates and the forecasts:
tr(F 00 Fb (Fb 0 Fb )−1 Fb0 F 0 )
tr(F 00 F 0 )
PT
yF 0 ,t+1|t − ybFb,t+1|t )2
t=1 (b
= 1−
PT
bF2 0 ,t+1|t
t=1 y
PT
bFb,t+1|t )2
t=1 (yFb,t+1|t − y
=
.
PT
2
y
t=1 b

SF,F 0 =
Sy,y0

Sβ,β
b

F ,t+1|t

Since we can only identify the space spanned by the factors, the second factor need not coincide
with the second estimated factor. We therefore project each of the true factors on all estimated
factors. A small SF,F 0 thus indicates a small discrepancy between the space spanned by the actual
and the estimated factors. Similarly, the larger is Sy,y0 , the closer are the diffusion index forecasts to
those generated by the (infeasible) forecasts based on observed factors. The Sβ,β
b statistic assesses
the feasible diffusion index forecasts relative to the conditional mean. This latter evaluation is
possible (but not in an empirical setting) because the model that generates the data is known in
the simulations.
Some summary statistics of the simulated data are given in the second panel of Table 1. Notably,
the experimental design generates substantial variation in the factor estimates, with SF,F0 ranging
from .04 (almost unpredictable) to .99 (almost perfectly predictable). The mean-squared forecast
errors ranges from .072 to .964, while Sy,y0 ranges from .071 to .99.
Because of the large number of configurations involved, we summarize the results using response
surfaces. We begin with a general specification that includes higher order terms and gradually drop
the statistically insignificant ones. Table 1 reports the estimates, along with the robust standard
9

errors. Recall that the larger is SF,F0 , the more precise are the factor estimates. Increasing N1 by
one increases SF,F0 by less than one basis point, but increases Sy,y0 by more than one basis point.
Not surprisingly, increasing N1 reduces Sβ,β
b , but at a declining rate.
Under-estimating the number of factors (r > k) reduces the precision of both the factor estimates
and the forecasts, while overestimating (r < k) the number of factors has the opposite effect. An
additional factor reduces SF,F0 by about 7 basis points. The mean-squared forecast error is also
higher the larger the number of true factors, as having to estimate more factors inevitably increases
sampling variability.
Adding series with relatively large idiosyncratic variances has first order effects on SF,F0 and
Sy,y0 that are positive, but second order effects that are negative. Thus, adding another series has
efficiency gains when N2 is small, but negative when N2 is sufficiently large. To be precise, the effect
of N2 on SF,F0 becomes negative when N2 is around 22 (1.925/.088), all else equal. Coincidentally,
the threshold of N2 for Sy,y0 is also around 22.
Of special interest to us are two results. First, the common factors are more precisely estimated
when the common component is important, as indicated by the coefficient on the variable labeled
R2 in Table 1. However, the larger the dispersion in the importance of the common component, as
indicated by the effect on Rq , the less precise are the estimates. This suggests that adding data with
large idiosyncratic errors or weak factor loadings need not be desirable. Second, SF,F0 and Sy,y0 are
both decreasing in N3 and C, holding other parameters fixed. This suggests that the forecasts and
the factor estimates are adversely affected by cross correlation in the errors. A summary statistic
for the extent of cross correlation is τb∗ , since it depends on C, N3 , and ρij . Evidently, increasing
τb∗ by .01 reduces SF,F0 and Sy,y0 by .86 and .90 basis points, respectively.7
The present monte carlo exercise highlights the fact that while increasing N1 is desirable from
both an estimation and forecasting standpoint, this is not always the case if we increase data of
the N2 and N3 type. The factor estimates and forecasts are clearly less efficient when the errors
are cross correlated and/or have vastly unequal idiosyncratic error variances.
3.2

Model 2: Oversampling

In empirical work, we almost always work with only a subset of the data available. To understand
if it matters which N series are being used for analysis, we simulate data from a strict factor model
in this subsection. We assume that there are two serially correlated factors driving the data, viz:
Xit = λi1 F1t + λi2F2t + eit , with
Fmt = .5Fmt−1 + umt ,
7

umt ∼ N (0, 1),

m = 1, 2..

The second order effect is positive, but numerically small. Evaluated at τb∗ = .1, the second order effect is 1.81.

10

Two series are to be forecasted and are generated as follows:
A
yt+1
= β A F1t + εA
t+1
B
yt+1
= β B F2t + εB
t+1 ,

with σεA = σεB . There are five types of data in this monte carlo, with sample size Ns , s = 1, . . . 5:
N1 : Xit = .8F1t + eit , σi2 ∼ N (0, 1 − .82 ) ;
N2 : Xit = .6F2t + eit , σi2 ∼ N (0, 1 − .62 );
N3 : Xit = .4F1t + .1F2t + eit , σ 2 ∼ N (0, 1 − .42 − .12 );
N4 : Xit = .1F1t + .4F2t + eit , σ 2 ∼ N (0, 1 − .12 − .42 );
N5 : Xit = eit , σi2 ∼ N (0, 1).
The simulated data have two features. First, some series are driven by one factor, some by two
factors, and some do not obey a factor structure. Second, some series weigh factor 1 more heavily
than factor 2 and vice versa. To fix ideas of the situation that the experiment attempts to mimic,
suppose factor one is real and factor two is nominal. The N1 series might be output and employment
type series, the N2 series might be prices, the N3 series might be interest rate type series, and the
N4 series might be stock market type series. Variations in the N5 series are purely idiosyncratic.
The errors are mutually uncorrelated within and between groups. Cross correlation is not an issue
in this experiment.
The simulation results are reported in Table 2. The main features of the previous monte carlo
are also apparent here when the errors are not cross-correlated. First, under-estimating the number
of factors has large efficiency loss, while over-estimating has little impact on the estimates or the
forecasts. Second, the factor estimates are no less precise when the noisy data are dropped, even
though the nominal sample size is smaller. Remarkably, when the number of assumed factors is at
least as large as that in the underlying data, the space spanned by the factors can be quite precisely
estimated by the method of principal components with as few as 40 series, provided the data are
informative about the factors. With 40 series (case 3), SF,F0 is .944. In none of the remaining
cases with two or more factors estimated was there a noticeable improvement in SF,F0 . With 100
series, SF,F0 improves to only .955 in case 9. Thus as in the previous monte carlo, efficiency of the
factor estimates is determined not simply by whether the sample size is 40 or 100, but also by the
informativeness of the data about the factors.
One motivation for the present monte carlo is to highlight the fact that the factor space being
estimated depends on the choice of data. In case 1 when the N1 series was used, the first principal
11

component estimates the space spanned by F1 . For this reason, extracting one factor given the
N1 dataset is adequate for forecasting y A . Analogously, extracting one factor from the N2 dataset
is adequate for the purpose of forecasting y B . However, if the first factor dominates the variation
of the second, we will need to estimate two factors from N1 + N2 series to forecast y B efficiently.
Analogously, if we had data in which F2 dominates F1 , such as case 4, forecasting y A using one
factor would have been disastrous. We refer to a situation in which the data are more informative
about some factors than the others as ‘oversampling’.
More generally, let m be the true number of factors in the forecasting equation. The foregoing
results suggest that when the data are oversampled, the number of estimated factors that will
efficiently forecast a series that depends on m factors will be larger than m, if the m factors are not
the m most dominant factors in X. A criterion that determines the optimal number of factors in
X can be a misleading indicator of the number of factors needed for forecasting a single series, y.
The problem of oversampling is helpful in understanding why in Table 2, y A is always forecasted
more precisely than y B , even though both series have the same degree of predictability (since
σεA = σεB and β A = β B ). This result arises because efficient forecasts of y B requires inclusion of
more estimated factors than y A . But more estimated factors also induce more sampling variability
into the forecasts. For this reason, forecasts of a series that depend on the less important factors
in X will tend to be inferior to those that depend on the dominant factors in X.
As noted earlier, macroeconomic panels are ‘put together’ by the researcher, and as such, the
factors are always sample dependent. As seen from the results, the forecast error for y B using
N2 + N3 series is larger than using N2 series alone. Likewise, the forecast error for y A from using
N1 + N3 series is larger than using the N1 series alone. This raises the possibility that if we think
the series to be forecasted depends on F1 and F2 , estimating F1 from N1 series and F2 from N2
series could outperform estimating F1 and F2 jointly from a larger dataset comprising of series with
varying factor structures. This alternative will be explored in the next section.
4

The Role of N in Real Time Forecasting

The goal of this section is to see if ybt+1|yt ,...y1 ,Fbt,N depends on N in real-time, 12 month ahead forecasting of a large number of economic time series with special attention to eight commonly studied
economic indicators: industrial production (ip), real personal income less transfers (gmyxspq), real
manufacturing trade and sales (msmtq), number of employees on nonagricultural payrolls (lpnag),
the consumer price index (punew), the personal consumption expediture deflator (gmdc), the CPI
less food and energy (puxx), and the producer price index for finished goods (pwfsa). The logarithms of the four real variables are assumed to be I(1), while the logarithms of the four prices are
12

assumed to be I(2).
Let yt generically denote one of the eight series after logarithmic transformation. Define the
h
h step ahead growth to be yt+h
= 100[yt+h − yt ] and the scaled one period growth to be zt =

100 · h[yt − yt−1 ]. The diffusion index forecasts are obtained from the equation
ybt+h|yt ,...y1 ,Fbt ≡ ybt+h|t = βb0 + βb10 Fbt,N +

p
X

γ
bj zt−j+1 ,

(4)

j=1

where βb0 , βb1 and γ
b are OLS estimates. The univariate forecasts are based on the model that excludes
the factors. Since our primary interest is in the role of N , we fix p to 4 to compare univariate AR(4)
forecasts with those augmented with up to k = 6 factors.8 We then use the BIC as suggested in
Stock and Watson (2002b) to determine the number of factors used in the forecasting equation
with ω set to 0.001.
The base case of Xit is a balanced panel of N =147 monthly series available from 1959:1 to
1998:12. Following Stock and Watson (2002b), the data are standardized and transformed to achieve
stationarity where necessary. The data can roughly be classified into 13 groups:- [1]: real output
and income (series 1-19), [2]: employment and hours (series 20-44), [3]: retail and manufacturing
trade (series 45-53), [4]: consumption (series 54-58), [5]: housing starts and sales (series 59-65), [6]:
inventories (series 66-76), [7]: orders (series 77-92), [8]: stock prices (series 93-99), [9]: exchange
rate (series 100-104), [10]: interest rates (105-120), [11]: money and credit (series 121-127), [12]:
price indexes (series 128-145), [13]: misc (series 146-147). Details are given in Appendix A. The
relative importance of the common component for each series are denoted Ri2 (3) and Ri2 (6), when
three and six factors are being estimated respectively.
The forecasting exercise begins with data from 1959:3-1970:1. A 12 period ahead forecast is
h
for h = 12. The sample is
formed by using values of the regressors at 1970:1 to give y1970:1+h

updated by one period, the factors and the forecasting model are both re-estimated, and a 12
month forecast for 1971:2 is formed. The final forecast is made for 1998:12 in 1998:12-h. The
rolling AR(4) forecasts are likewise constructed. We evaluate the k factor diffusion index forecasts
relative to those of the AR(4) forecasts (i.e. with zero factors). The results, when k is chosen
optimally by the BIC, are reported in the first row of columns 5 to 12 of Table 3. Incidentally, the
BIC usually suggests two factors. An entry less than one indicates that the diffusion index forecast
is superior to the naive AR(4) forecast. The results confirm the findings of Stock and Watson that
the diffusion indices can be useful for real time forecasting, even though the factors have to be
estimated.
8

The main difference between our analysis and that of Stock and Watson is that we did not allow lags of the
factors to enter the forecasting model.

13

But can more efficient factor augmented forecasts be obtained? A look at the properties of
the data reported in Appendix A reveal several features. First, there are more series from some
groups than others, so the problem of oversampling is conceivable. Second, many of the Ri2 s are
very small. For example, three factors explain only .01 of the variation in IPUT (series 15). Even
with six factors, Ri2 is improved to a mere .08, much smaller than series such as PMEMP that has
an Ri2 of .8. The dispersion in the importance of the common component is thus quite large. Our
monte carlo results suggest that such dispersion can have adverse effects on the forecasts.
Third, there is substantial cross correlation in the idiosyncratic errors. To gauge the problem,
we obtain τbij , the correlation coefficient between the residuals for series i and j, obtained from
estimation of a six (and three) factor model over the entire sample, 71:1-97:12. For each series i,
we can identify
τb1∗ (i) = max |b
τij | = τbij 1 .
j

i

That is, ji1 is the series whose idiosyncratic error is most correlated with series i, and the correlation
between series i and ji1 is τb1∗ (i). For example, the IPCD and IPCN errors are both most correlated
with IPC, with correlation coefficients of .66 and .69, respectively. The errors of FSPCOM and
FSNCOM have a correlation coefficient of .99 (see Appendix A). Evidently, the maximum residual
cross correlation in the data is non-trivial. As the maximum correlation coefficient could be an
outlier, we also report the second largest residual cross correlation for each series (see the last two
and three columns of Appendix A). That is, we identify ji2 . Then τb2∗ (i) is the second largest residual
correlation for series i. Many of these correlation coefficients remain quite high.
P
As a final check, the quantity j |τij | should be bounded under the assumptions of the approximate factor model. These are reported in the last column of Appendix A. This series has a
mean of 14.62 and a standard deviation of 5.10. As we have 147 series in the analysis, the average
P
cross-correlation is around .1. In many cases, j |b
τij | is large even though τb1∗ (i) is not. See, for
example, series LPCC and FSPUT. This suggests many of the τij are non-zero. In particular, the
idiosyncratic errors of the data in groups 1 (industrial production) and 7 (manufacturing series) exhibit especially strong correlation. These results suggest that the issues of oversampling, correlated
errors, and noisy data could be relevant to the present forecasting exercise.
4.1

Weighted Principal Components

In classical regression analysis, generalized least squares is more efficient than ordinary least squares
when the errors are non-spherical. This suggests that if we observe Ω, we can consider an efficient
principal components estimator that weighs the data with Ω, by analogy to GLS. The problem
14

b
is that we do not observe Ω. The analogous feasible GLS estimator would be to replace Ω by Ω,
b is a
the sample error covariance matrix from unweighted estimation of a k factor model. But Ω
P
P
N
T
matrix of rank N − k and thus not invertible. Thus, while minimizing V (k) = N1T i=1 t=1 e2it is
PT 0 b −1
suboptimal, minimizing W ∗ (k) = k 1
e Ω et k is infeasible. Another solution is to subtract
NT

t=1 t

b which is a diagonal matrix in classical analysis, from Σ
b X . However, Ω
b is not diagonal in
Ω,
bX − Ω
b in fact span the same space as those of
approximate factor models. The eigenvectors of Σ
b X . There is, to our knowledge, no obvious way to exploit the entire Ω
b matrix to improve efficiency.
Σ
Although optimal weighting is not possible, some form of weighting may still be desirable.
Consider the objective function
W (k) =

N
T
X
1 X
wiT
e2it ,
NT
i=1

t=1

where wiT is chosen to reflect the informativeness of series i. Notice that the objective function
weighs the variance of each series; the covariances are not weighted as in feasible GLS. When N
is large, this can be advantageous because having to estimate N instead of N (N + 1)/2 weights
induces less sampling variability.
As wiT is meant to be data dependent, we rely on a first step estimation of the factor model
to get the estimated residuals. If the number of factors is too small in this step, the errors will be
correlated by construction which could lead to inaccurate weighting. The base case of our first step
b and the weights as
extracts six factors from X, from which the residuals are used to construct Ω
defined below. For robustness check, we also consider using three factors in the first step. In the
second step, a new set of factors is estimated using different weighted criteria. After the factors are
re-estimated, the BIC is used to determine how many of them are used in forecasting. This step is
repeated for each of the eight y series under investigation.
We consider the following sets of weights:
b T , estimated using data up to time T .9
Rule SWa: wiT is the inverse of the ith diagonal element of Ω
P
b
Rule SWb: wiT is the inverse of N1 N
j=1 |ΩT (i, j)|.
Rule 1: Let j 1 = {ji1 } be the set of series whose error is most correlated with some other series. These
series are all dropped. If ji1 = ji10 , i.e. if series i and i0 are most correlated with each other,
series i0 is dropped if Ri20 < Ri2 . Of the 147 series, 73 are dropped, leaving us with 71 series.
Rule 2: From Rule 1, we also drop a series if its error is second most correlated with another series.
This removes 38 series from the 71 series in the previous set, leaving us with 33 series.
9

One can also think of weighting each series by

and j as the one we considered, namely

ω
bj
ω
bi

ω
b −1
PN i −1 .
bi
i=1 ω

.

15

This gives the same relative weight between series i

b T , the full sample estimate of Ω. In real time, this estimate
Rule 1c The j 1 set in Rule 1 is based on Ω
b t to obtain wit .
can be updated continuously. Rule 1c uses Ω
Rule 2c Follows Rule 2 but allows for continuous updating as in Rule 1c.
Rule SWa was also considered in Jones (2001). It is aimed at accounting for heteroskedasticity
in the errors, but not cross-correlation. Rule SWb weights the data by the magnitude of residual
cross-correlation. With Rules SWa and SWb, all 147 series are used to estimate the factors as the
weights are never exactly zero. In contrast, wiT is a binary variable under Rules 1 and 2. A series
is either in or out based on the properties of the residuals over the entire sample. Rules 1c and 2c
provide further flexibility by allowing wiT to be updated as the sample size (in the time dimension)
changes.
The above weighting schemes are aimed at accounting for the properties of the residuals. However, our second monte carlo exercise also suggests that it may be more efficient to estimate the
factors from a small dataset than from one with additional series that does not contain information
about the factor. Inspection of Appendix A reveals that while many series (in particular, the real
variables) are quite well explained by the first three factors, many series (in particular, the price
variables) are better explained by factors three through six. For example, the Ri2 (6) for the price
variables is generally much higher than Ri2 (3). In light of the results in our second monte carlo experiment, the 147 series are reclassified into three categories. The ‘real’ (R) category consists of 60
series from output, employment, retail and manufacturing trade, consumption, and miscellaneous.
The ‘nominal’ (N) category consists of 46 series relating to exchange rate, interest rates, money
and credit, and price indexes. The ‘volatile/leading indicator’ (V) category consists of 41 series of
high volatility, including housing starts, inventories, orders, and stock prices.10 An appeal of this
grouping is that it provides some economic interpretation to the factors. Obviously, this amounts
to having three additional sets of wiT . For example, the real factors are essentially extracted with
wiT such that it is one if a series is real and zero otherwise. After the data are classified, we then
estimate three real factors using data exclusively from the real series, three nominal factors from
the nominal variables, and three volatile factors from the volatile series. These T × 1 vectors are
denoted Fbjk , j = 1, . . . 3, k = R, N, V . It remains to determine the order in which the factors are
added to the forecasting equation. We consider 4 sets of orderings:
Rule A: F1R , F2R , F3R
Rule C: F1V , F2V , F3V

Rule B: F1N , F2N , F3N
Rule D: F1R , F1N , F1V ;

As is evident, Rule A uses only the real factors, Rule B uses only the nominal factors, while Rule
C uses only the volatile factors. Rule D eventually has one factor from each category.
10

The ‘real’ variables are thus from groups 1-4, plus 13, the ‘nominal’ variables are from groups 9-12, and the
volatile group are variables from 5-8.

16

There is undoubtedly a certain ad-hocness in all these rules, but if cross-correlated errors are not
prevalent in the data, or if oversampling of the data is inconsequential, dropping the data should
make the estimates less efficient, not more, since the sample size is smaller. The results using these
weighting schemes are therefore revealing even if they are not optimal.
The results with weights obtained from a first step estimation of 6 and 3 factors are reported in
Table 3 and 4, respectively. The real factors (Rule A) apparently have little predictive power for
the real variables. This is perhaps to be expected since the forecasting equation already has four
lags of each series, which are themselves real variables. However, the nominal factors, extracted
from 46 series (Rule B), are extremely effective for forecasting all real series. It beats the forecasts
from the factors extracted from all 147 series (Rule SW). This result can be explained by the fact
that the first factor in the Stock and Watson data is understood to be a real factor. This means
that the nominal factors are not the most important factors in the panel of 147 series. As suggested
by our second monte carlo, extracting these factors from a large panel is inferior to extracting them
from data in which the factor is dominant, which is the case with Rule B. This would be consistent
with real series being over-sampled in the data set.
Turning now to the inflation series, using factors associated with Rule B are evidently uninformative, as lagged prices have likely encompassed much of the information in the nominal factors.
Adding one real factor or one volatile factor both lead to smaller forecast errors than the AR(4)
forecasts. Although none of the methods considered appear to perform noticeably better than the
base case, the forecasts with 147 series are closely matched by those with factors extracted from 33
series, ie Rule 2.
Overall, Rules 2 and SWb produce results that are comparable, and often times better, than
SW for all eight series considered. As these results are specific to a selected set of series, one might
wonder whether our findings are general. To address this issue, we repeat the forecasting exercise
for each of the 147 series in X and assess the performance of each rule. The results from averaging
over all the real and all the nominal series are reported in Tables 3 and 4. Rule B continues to do
well for the real series, though SWb is not far behind, suggesting that reducing the extent of crosscorrelation in the errors can improve the factor forecasts of the real variables. As for the nominal
variables, the weighting schemes did not improve the SW forecasts, but they did no worse either,
suggesting again that using a smaller number of series to construct the factors could have been
adequate. We also use two graphs to summarize the 147 forecasts. Figure 1 shows the percentage
of series for which a given rule is the best one. While SW is best in about 10 percent of the cases,
Rules SWa, SWb and B are better more often. As none of them systematically perform better than
all the others, it is not clear from Figure 1 alone which rule one should prefer. Therefore in Figure

17

2, we present the percentage of series for which a given rule beats SW. In about 65 percent of the
series, SWb outperforms SW.
While we make no claim that these rules are optimal, two observations are useful to highlight.
First, the fact that Rules 2 and B used fewer than 50 series underscores our main point that use of
more data to extract the factors does not necessarily yield better results. Reducing the sample size
can sometimes help sharpen the factor structure and enables more efficient estimation. As well,
use of more series in the estimation increases the possibility of correlated errors. Both observations
serve as a reminder that the principal components estimator has many desirable properties if certain
regularity conditions are satisfied. The selection of data is not innocuous because it determines
how close are these conditions from being violated in practice.
Second, instead of dropping series with highly correlated errors, we can also downweigh their
influence on the objective function. This is perhaps a more appealing way of dealing with the
intrinsic properties of the data from a statistical perspective because no information is wasted.
This is what rules SWa and SWb attempt to accomplish, and as we can see, with encouraging
outcomes. We can expect more formal weighting schemes than the crude ones used here to be able
to further enhance the properties of principal components estimator.
5

Conclusion

A feature stressed in recent applications of factor models is the use of data from ‘large’ panels.
Because the theory is developed for large N and T , there is a natural tendency for researchers to
use as much data as are available. But in simulations and the empirical examples considered, the
factors extracted from as few as 40 series seem to do no worse, and in many cases, better than the
ones extracted from 147 series.
In applications, the number of series available for analysis can be quite large. Suppose we
have included the all-item CPI in the analysis. Would it be useful to also include CPI ex-food and
energy? Suppose we have a large number of disaggregated series, plus a small number of aggregated
ones. Should we use all series? What are the consequences of oversampling data from particular
groups? Is there a trade-off between the quantity and quality of the data? There is at the moment
no guide to what data should be used in factor analysis. Our results nonetheless suggest that
sample size alone does not determine the properties of the estimates. The quality of the data must
be taken into account. There is room to further exploit the diffusion index forecasting technology
by efficiently incorporating information about the properties of the data in the construction of the
factors.

18

Appendix I: Data
Series
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76

name
IP
IPP
IPF
IPC
IPCD
IPCN
IPE
IPI
IPM
IPMND
IPMFG
IPD
IPN
IPMIN
IPUT
IPXMCA
PMI
PMP
GMYXPQ
LHEL
LHELX
LHEM
LHNAG
LHUR
LHU680
LHU5
LHU14
LHU15
LHU26
LPNAG
LP
LPGD
LPCC
LPEM
LPED
LPEN
LPSP
LPTX
LPFR
LPS
LPGOV
LPHRM
LPMOSA
PMEMP
MSMTQ
MSMQ
MSDQ
MSNQ
WTQ
WTDQ
WTNQ
RTQ
RTNQ
GMCQ
GMCDQ
GMCNQ
GMCSQ
GMCANQ
HSFR
HSNE
HSMW
HSSOU
HSWST
HSBR
HMOB
IVMTQ
IVMFGQ
IVMFDQ
IVMFNQ
IVWRQ
IVRRQ
IVSRQ
IVSRMQ
IVSRWQ
IVSRRQ
PMNV

tcode
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
1
1
1
5
5
5
5
5
1
1
1
1
1
1
5
5
5
5
5
5
5
5
5
5
5
5
1
1
1
5
5
5
5
5
5
5
5
5
5
5
5
5
5
4
4
4
4
4
4
4
5
5
5
5
5
5
2
2
2
2
1

group
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
3
4
4
4
4
4
5
5
5
5
5
5
5
6
6
6
6
6
6
6
6
6
6
6

Ri2 (3)
0.70
0.62
0.55
0.41
0.37
0.14
0.44
0.42
0.51
0.36
0.71
0.62
0.41
0.05
0.01
0.70
0.75
0.68
0.38
0.32
0.45
0.24
0.28
0.49
0.41
0.38
0.53
0.50
0.56
0.72
0.70
0.72
0.22
0.69
0.62
0.44
0.39
0.35
0.17
0.20
0.11
0.20
0.10
0.80
0.62
0.58
0.53
0.23
0.18
0.30
0.04
0.20
0.07
0.18
0.13
0.07
0.06
0.10
0.42
0.36
0.44
0.22
0.22
0.33
0.08
0.43
0.40
0.37
0.12
0.12
0.09
0.67
0.64
0.19
0.12
0.61

Ri2 (6)
0.73
0.68
0.61
0.48
0.45
0.16
0.48
0.45
0.53
0.37
0.75
0.67
0.43
0.05
0.08
0.77
0.77
0.71
0.39
0.36
0.49
0.27
0.30
0.72
0.63
0.82
0.89
0.83
0.87
0.76
0.77
0.77
0.29
0.72
0.65
0.46
0.41
0.39
0.20
0.21
0.21
0.21
0.16
0.80
0.77
0.64
0.59
0.26
0.32
0.34
0.18
0.33
0.16
0.36
0.22
0.19
0.14
0.19
0.61
0.42
0.47
0.59
0.41
0.57
0.34
0.44
0.43
0.39
0.17
0.12
0.11
0.79
0.67
0.31
0.24
0.62

j1
11
3
2
3
4
4
3
11
1
13
1
11
6
9
57
43
77
17
140
21
20
23
22
43
28
27
26
29
28
31
30
34
32
35
34
34
30
37
62
37
37
43
42
17
72
73
46
86
51
49
49
75
56
55
58
53
15
55
64
59
59
59
59
59
62
67
68
67
67
66
75
45
46
49
52
17

j2
9
4
4
2
3
13
2
1
11
36
12
1
10
1
45
42
18
77
144
30
138
33
9
42
29
67
29
25
27
37
32
31
31
32
32
32
31
31
78
30
30
24
24
18
46
47
73
46
74
74
74
54
52
56
54
54
54
54
62
43
63
64
64
62
64
68
66
66
28
74
66
85
47
51
71
78

τ
b1∗
0.89
0.93
0.93
0.81
0.66
0.69
0.55
0.41
0.81
0.61
0.89
0.85
0.66
0.47
0.45
0.63
0.87
0.85
0.27
0.69
0.69
0.90
0.90
0.72
0.78
0.72
0.72
0.88
0.88
0.93
0.93
0.74
0.42
0.92
0.92
0.41
0.71
0.55
0.31
0.45
0.47
0.93
0.93
0.73
0.80
0.91
0.87
0.66
0.85
0.58
0.85
0.79
0.81
0.75
0.86
0.81
0.45
0.86
0.87
0.55
0.54
0.80
0.77
0.87
0.51
0.59
0.89
0.89
0.48
0.47
0.61
0.80
0.91
0.82
0.79
0.52

τ
b2∗
0.81
0.78
0.81
0.78
0.57
0.66
0.47
0.41
0.67
0.27
0.85
0.78
0.61
0.34
0.24
0.63
0.85
0.82
0.25
0.18
0.21
0.27
0.20
0.67
0.60
0.32
0.62
0.78
0.62
0.71
0.67
0.67
0.34
0.74
0.67
0.35
0.57
0.39
0.31
0.33
0.31
0.67
0.72
0.64
0.64
0.87
0.78
0.45
0.82
0.49
0.70
0.61
0.59
0.61
0.75
0.61
0.26
0.57
0.80
0.49
0.37
0.76
0.72
0.76
0.41
0.55
0.59
0.55
0.24
0.41
0.51
0.59
0.78
0.70
0.61
0.39

P

bij |
i |τ
25.53
22.74
20.69
20.52
17.34
14.77
11.33
14.24
17.47
11.89
25.83
22.57
17.42
8.48
10.44
11.69
18.43
15.91
11.97
9.66
11.14
9.25
9.07
14.57
16.86
14.32
19.69
19.59
18.71
16.52
16.28
16.42
13.24
16.34
16.52
11.45
12.74
12.11
10.76
9.79
7.30
12.87
13.16
15.11
20.54
16.96
15.68
11.89
14.06
11.75
11.02
15.50
13.15
16.81
13.41
12.53
7.48
11.06
21.58
13.92
14.81
18.85
15.93
20.62
13.63
16.14
13.12
12.15
8.06
7.93
10.69
21.99
17.49
13.27
16.10
11.88

Notes: tcode is the transformation code, taken from Stock and Watson (2002a). 1=no transformation, 2=first difference, 4=logarithm, 5=first difference of logarithms, 6=second difference. The data can be classified into 13 groups,
1=real output and income, (2)=employment and hours, (3)=retail and manfacturing trade, (4)=consumption,

19

Appendix I: continued
Series
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147

name
PMNO
PMDEL
MOCMQ
MDOQ
MSONDQ
MO
MOWU
MDO
MDUWU
MNO
MNOU
MU
MDU
MNU
MPCON
MPCONQ
FSNCOM
FSPCOM
FSPIN
FSPCAP
FSPUT
FSDXP
FSPXE
EXRUS
EXRGER
EXRSW
EXRJAN
EXRCAN
FYFF
FYGT5
FYGT10
FYAAAC
FYBAAC
FYFHA
FM1
FM2
FM3
FM2DQ
FMFBA
FMRRA
FMRNBC
PMCP
PWFSA
PWFCSA
PSM99Q
PUNEW
PU83
PU84
PU85
PUC
PUCD
PUS
PUXF
PUXHS
PUXM
GMDC
GMDCD
GMDCN
GMDCS
LEHCC
LEHM
SFYCP90
SFYGM3
SFYGM6
SFYGT1
SFYGT5
SFYGT10
SFYAAAC
SFYBAAC
SFYFHA
HHSNTN

tcode
1
1
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
2
2
5
5
5
5
5
2
2
2
2
2
2
6
6
6
5
6
6
6
1
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
1
1
1
1
1
1
1
1
1
1

group
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
8
8
8
8
8
8
8
9
9
9
9
9
10
10
10
10
10
10
11
11
11
11
11
11
11
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
13
13
10
10
10
10
10
10
10
10
10
10

Ri2 (3)
0.66
0.52
0.55
0.50
0.12
0.63
0.44
0.50
0.39
0.36
0.18
0.41
0.39
0.22
0.09
0.08
0.28
0.28
0.27
0.22
0.24
0.30
0.18
0.10
0.08
0.08
0.06
0.03
0.23
0.26
0.23
0.28
0.32
0.22
0.05
0.04
0.02
0.24
0.03
0.03
0.05
0.47
0.03
0.03
0.02
0.08
0.03
0.04
0.00
0.08
0.01
0.02
0.04
0.09
0.08
0.09
0.00
0.10
0.01
0.02
0.01
0.23
0.51
0.53
0.52
0.67
0.69
0.70
0.74
0.70
0.38

Ri2 (6)
0.71
0.57
0.58
0.61
0.21
0.73
0.55
0.61
0.50
0.39
0.21
0.47
0.46
0.25
0.17
0.16
0.64
0.64
0.62
0.54
0.45
0.64
0.39
0.20
0.15
0.15
0.15
0.10
0.26
0.45
0.43
0.49
0.50
0.33
0.10
0.08
0.04
0.29
0.03
0.03
0.08
0.50
0.28
0.30
0.03
0.68
0.09
0.31
0.01
0.69
0.04
0.05
0.33
0.66
0.62
0.67
0.04
0.71
0.10
0.02
0.02
0.75
0.83
0.86
0.80
0.86
0.86
0.83
0.85
0.86
0.58

j1
17
17
47
84
91
84
85
80
83
48
86
89
88
87
92
91
94
95
94
95
93
95
94
101
102
101
100
19
106
107
106
107
108
107
112
113
112
62
116
115
116
147
120
119
97
131
124
123
128
134
133
129
119
126
122
135
127
132
132
137
35
140
140
139
140
143
142
145
144
143
118

j2
18
76
46
85
92
80
84
85
84
87
90
85
85
134
81
81
95
93
93
94
94
94
95
103
100
100
102
100
94
108
108
109
107
106
113
111
111
16
117
117
115
141
129
129
107
83
120
129
117
130
134
134
128
82
128
134
132
126
126
33
136
141
141
141
142
145
144
143
143
144
25

τ
b1∗
0.87
0.63
0.51
0.99
0.83
0.90
0.98
0.99
0.98
0.66
0.52
0.99
0.99
0.42
1.00
1.00
0.98
0.99
0.99
0.87
0.33
0.77
0.58
0.84
0.87
0.87
0.83
0.19
0.30
0.94
0.94
0.83
0.82
0.60
0.62
0.64
0.64
0.36
0.67
0.67
0.49
0.55
0.88
0.88
0.20
0.33
0.26
0.26
0.19
0.36
0.28
0.32
0.33
0.26
0.33
0.66
0.28
0.36
0.66
0.26
0.31
0.40
0.83
0.83
0.77
0.96
0.96
0.92
0.92
0.86
0.55

τ
b2∗
0.82
0.39
0.42
0.91
0.83
0.88
0.90
0.92
0.92
0.52
0.42
0.43
0.44
0.21
0.83
0.83
0.96
0.98
0.96
0.84
0.32
0.77
0.57
0.83
0.84
0.81
0.57
0.17
0.28
0.77
0.83
0.82
0.69
0.60
0.45
0.62
0.45
0.35
0.32
0.49
0.32
0.38
0.33
0.30
0.20
0.26
0.20
0.25
0.12
0.26
0.24
0.32
0.32
0.21
0.22
0.36
0.23
0.36
0.25
0.24
0.26
0.32
0.46
0.77
0.62
0.83
0.91
0.91
0.88
0.83
0.40

P

bij |
i |τ
16.53
14.84
13.21
24.79
17.44
26.15
24.69
24.99
24.38
14.52
9.66
17.21
16.84
10.36
16.70
16.64
22.10
22.23
21.45
18.44
14.01
18.72
16.42
13.71
12.87
12.95
10.87
7.57
11.08
17.68
17.65
16.92
15.40
14.32
7.69
9.15
7.73
11.99
7.71
8.71
7.91
11.81
9.12
9.23
6.99
12.84
5.24
8.67
4.96
13.52
7.32
8.10
9.71
10.95
12.62
12.45
5.41
13.96
7.30
8.46
8.81
12.95
16.57
22.09
19.82
25.60
25.90
24.65
23.35
23.29
10.65

(5)-housing starts and sales, (6)= inventories, (7)=orders, (8)=stock prices, (9)=exchange rate, (10)=interest
rates, (11)=money and credit, (12)=prices, (13)=misc. Ri2 (3) and Ri2 (6) are the fraction of variation in series i
explained by three and six factors, respectively. j1 and j2 are the series whose errors are most correlated and second
most correlated with series i. The corresponding correlation coefficients are τb1∗ and τb2∗ , respectively.

20

N1
N12
(r − k) > 0
(k − r) > 0
r
N2
N22
R2
R22
Rq
Rq2
N3
N32
C
C2
τb∗
[b
τ ∗ ]2
cons
R2
SF,F0
Sy,y0
msey

Table 1: Response Surface for
Estimates
100 × SF,F0 100 × Sy,y0 100 × Sβ,β
b
0.874
1.172
-1.214
(18.38)
(20.05)
(25.33)
-0.006
-0.007
0.007
(9.16)
(10.08)
(12.08)
-21.873
-2.195
1.888
(100.92)
(7.69)
(8.37)
14.388
6.882
-8.244
(77.40)
(32.08)
(44.16)
-7.058
2.176
2.635
(28.80)
(8.35)
(12.36)
1.925
1.983
-1.862
(21.70)
(18.27)
(21.09)
-0.088
-0.089
0.083
(13.70)
(11.15)
(12.72)
145.383
171.275
-187.590
(32.37)
(29.45)
(37.57)
-49.465
-105.62
94.508
(8.86)
(14.74)
(15.07)
-12.137
18.773
-32.184
(3.46)
(4.49)
(8.93)
-22.023
-62.641
68.285
(5.66)
(13.43)
(16.69)
-0.483
-0.663
0.689
(19.01)
(20.31)
(24.94)
0.007
0.008
-0.009
(15.60)
(15.52)
(19.46)
-0.333
-0.655
0.649
(16.77)
(25.83)
(30.06)
0.001
0.004
-0.003
(3.79)
(10.72)
(11.84)
-86.512
-90.361
134.357
(9.30)
(7.22)
(13.15)
180.718
358.357
-309.556
(8.73)
(12.25)
(13.04)
34.884
14.159
(27.23)
(8.62)
.8449
.6232
.7184

Monte Carlo 1
Summary Statistics of Simulated Data
mean
s.d.
min
max
40
16.33
20
60

.4444

.684

0

2

.4444

.684

0

2

2

.816

1

3

5

5.773

0

15

.350

.143

.071

.721

.462

.187

.089

.850

15.2

16.237

1

60

24

22.301

0

75

.140

.058

.068

.452

.603
.740
.458

.292
.23
.222

.04
.071
.072

.991
.99
.964

We considered a total of 12150 configurations of Model 1, each simulated 1000 times. The parameters that
we vary between configurations are N1 , N2 , N3 , C, r, k, κ1 , κ2 and κy . We use R2 , Rq2 and τ ∗ to summarize the
properties of the data. Summary statistics for the simulated data are given in the second panel of Table 1. For each
configuration, forecast performance is measured using three statistics: SF,F 0 , Sy,y0 , and M SE. We use a response
surface to summarize the sensitivity of forecast performance to the properties of the factor model. The coefficients
are reported in columns 2, 3, and 4.

21

1

2

3

4

5

6

7

8

9

10

11

12

13

14

N1

Table 2: Monte Carlo 2 with r = 2 factors:
N2 N3 N4 N5 N k SF,F0 S A
b

20
20
20
0
0
0
20
20
20
0
0
0
20
20
20
0
0
0
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20

0
0
0
20
20
20
20
20
20
20
20
20
0
0
0
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
0
0
0
20
20
20
20
20
20
20
20
20

β,β

0
0
0
0
0
0
0
0
0
20
20
20
20
20
20
0
0
0
20
20
20
0
0
0
20
20
20
0
0
0
20
20
20
20
20
20
0
0
0
20
20
20

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
40
40
40
0
0
0
40
40
40
40
40
40
0
0
0
0
0
0
0
0
0
40
40
40
40
40
40

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
40
40
40
40
40
40
40
40
40
40
40
40
40
40
40

20
20
20
20
20
20
40
40
40
40
40
40
40
40
40
60
60
60
60
60
60
80
80
80
100
100
100
80
80
80
80
80
80
100
100
100
120
120
120
140
140
140

22

1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3

0.488
0.491
0.493
0.464
0.467
0.470
0.499
0.944
0.944
0.471
0.870
0.871
0.487
0.506
0.519
0.477
0.492
0.504
0.494
0.943
0.944
0.501
0.955
0.955
0.502
0.955
0.955
0.499
0.942
0.942
0.487
0.492
0.497
0.494
0.942
0.942
0.500
0.954
0.954
0.502
0.954
0.954

0.173
0.188
0.203
0.991
0.995
1.005
0.222
0.185
0.194
0.957
0.414
0.419
0.182
0.192
0.202
0.976
0.976
0.976
0.221
0.184
0.196
0.825
0.187
0.197
0.594
0.186
0.196
0.223
0.187
0.200
0.183
0.198
0.210
0.221
0.185
0.201
0.825
0.188
0.202
0.594
0.186
0.200

M SE B
b

β,β

1.084
1.090
1.096
0.284
0.299
0.305
1.070
0.300
0.311
0.356
0.299
0.307
1.079
1.063
1.053
0.273
0.283
0.291
1.066
0.296
0.304
0.579
0.253
0.262
0.838
0.251
0.262
1.070
0.304
0.313
1.079
1.079
1.085
1.066
0.300
0.309
0.579
0.254
0.262
0.838
0.252
0.261

rule
SW
1
2
1c
2c
SWa
SWb
A
B
C
D
AR(4)

N
147
71
33
71
33
147
147
60
46
41
60

Table 3: Forecast
ip
gmyxspq
0.632
0.906
0.756
1.006
0.615
0.835
0.661
0.819
0.715
0.801
0.625
0.813
0.594
0.921
1.009
1.008
0.585
0.753
1.019
1.055
0.612
0.750
0.050
0.027

Errors Relative to AR(4), 6 Factors: 71:1-97:12
msmtq lpnag Real punew gmdc puxx
0.580 0.919 0.759 0.734 0.832 0.843
0.694 0.986 0.860 0.745 0.833 0.827
0.576 0.867 0.723 0.740 0.843 0.844
0.620 0.872 0.743 0.771 0.874 0.880
0.640 0.853 0.752 0.794 0.869 0.876
0.569 0.618 0.656 0.831 0.912 0.934
0.564 0.797 0.719 0.743 0.848 0.845
1.012 0.983 1.003 0.789 0.863 0.853
0.543 0.645 0.632 1.024 1.075 1.100
1.035 0.963 1.018 0.800 0.893 0.841
0.530 0.813 0.676 0.789 0.863 0.890
0.046 0.017 0.035 0.021 0.016 0.019

pwfsa
0.825
0.837
0.818
0.840
0.864
0.908
0.810
0.868
0.983
0.882
0.868
0.035

Nominal
0.808
0.810
0.811
0.841
0.851
0.896
0.811
0.843
1.045
0.854
0.852
0.023

The number of factors is selected using the information criterion proposed by Stock and Watson (2002b), with
ω = 0.001. SW is the base case using 147 series. Let τij be cross-correlation between the residuals of series i and j
from full sample estimation of a six factor model. Define ji∗1 = maxj |τij | to the series most correlated with series
i. Rule 1 removes all series in {ji∗ }. For each i, we find the series with the second largest |τij |. Rule 2 additionally
removes those series. Rules 1c and 2c are based on rolling estimation of the correlation matrix, so the series that are
dropped can change as we roll the sample. Rules A, B, and C extract three real, three nominal, and three volatile
factors factors from subsets of the data. Rule D uses one real, one nominal, and one volatile factor in the forecasting
exercise.

rule
SW
1
2
1c
2c
SWa
SWb
A
B
C
D
AR(4)

N
147
71
33
71
33
147
147
60
46
41
60

Table 4: Forecast
ip
gmyxspq
0.632
0.906
0.702
0.940
0.918
1.084
0.701
0.890
0.650
0.897
0.624
0.836
0.628
0.833
1.009
1.008
0.585
0.753
1.019
1.055
0.612
0.750
0.050
0.027

Errors Relative to AR(4), 3 Factors: 71:1-97:12
msmtq lpnag Real punew gmdc puxx
0.580 0.919 0.759 0.734 0.832 0.843
0.626 0.947 0.804 0.721 0.816 0.806
0.798 0.948 0.937 0.743 0.859 0.822
0.612 0.901 0.776 0.752 0.834 0.860
0.585 0.877 0.752 0.770 0.851 0.903
0.590 0.778 0.707 0.808 0.913 0.867
0.594 0.916 0.743 0.741 0.837 0.830
1.012 0.983 1.003 0.789 0.863 0.853
0.543 0.645 0.632 1.024 1.075 1.100
1.035 0.963 1.018 0.800 0.893 0.841
0.530 0.813 0.676 0.789 0.863 0.890
0.046 0.017 0.035 0.021 0.016 0.019

pwfsa
0.825
0.819
0.852
0.845
0.851
0.861
0.833
0.868
0.983
0.882
0.868
0.035

Nominal
0.808
0.790
0.819
0.822
0.844
0.862
0.810
0.843
1.045
0.854
0.852
0.023

The number of factors is selected using the information criterion proposed by Stock and Watson (2002b), with
ω = 0.001. SW is the base case using 147 series. Let τij be cross-correlation between the residuals of series i and
j from full sample estimation of a three factor model. Define ji∗1 = maxj |τij | to the series most correlated with
series i. Rule 1 removes all series in {ji∗ }. Rule 1 removes all series in {ji∗ }. For each i, we find the series with the
second largest |τij |. Rule 2 additionally removes those series. Rules 1c and 2c are based on rolling estimation of the
correlation matrix, so the series that are dropped can change as we roll the sample. Rules A, B, and C extract three
real, three nominal, and three volatile factors factors from subsets of the data. Rule D uses one real, one nominal,
and one volatile factor in the forecasting exercise.

23

References
Anderson, T. W. (1984), An Introduction to Multivariate Statistical Analysis, Wiley, New York.
Bai, J. and Ng, S. (2001), A PANIC Attack on Unit Roots and Cointegration, mimeo, Boston
College.
Bai, J. and Ng, S. (2002), Determining the Number of Factors in Approximate Factor Models,
Econometrica 70:1, 191–221.
Bai, J. S. (2003), Inference on Factor Models of Large Dimensions, Econometrica 71:1, 135–172.
Bernanke, B. and Boivin, J. (2002), Monetary Policy in a Data Rich Environment, Journal of
Monetary Economics.
Bernanke, B., Boivin, J. and Eliasz, P. (2002), Factor Augmented Vector Autoregressions (FVARs)
and the Analysis of Monetary Policy, mimeo, Columbia University.
Chamberlain, G. and Rothschild, M. (1983), Arbitrage, Factor Structure and Mean-Variance Analysis in Large Asset Markets, Econometrica 51, 1305–1324.
Chan, Y., Stock, J. and Watson, M. W. (1998), A Dynamic Factor Model Framework for Forecast
Combinations, mimeo, Princeton University.
Connor, G. and Korajzcyk, R. (1986), Performance Measurement with the Arbitrage Pricing Theory: A New Framework for Analysis, Journal of Financial Economics 15, 373–394.
Cristadoro, R., Forni, M., Reichlin, L. and Giovanni, V. (2001), A Core Inflation Index for the
Euro Area, manuscript, www.dynfactor.org.
Forni, M. and Lippi, M. (1997), Aggregation and the Microfoundations of Dynamic Macroeconomics,
Oxford University Press, Oxford, U.K.
Forni, M. and Reichlin, L. (1998), Let’s Get Real: a Factor-Analytic Approach to Disaggregated
Business Cycle Dynamics, Review of Economic Studies 65, 453–473.
Forni, M., Hallin, M., Lippi, M. and Reichlin, L. (2000), The Generalized Dynamic Factor Model:
Identification and Estimation, Review of Economics and Statistics 82:4, 540–554.
Forni, M., Hallin, M., Lippi, M. and Reichlin, L. (2001a), Cooincident and Leading Indicators for
the Euro Area, Economic Journal 111, C82–85.
Forni, M., Hallin, M., Lippi, M. and Reichlin, L. (2001b), Do Financial Variables Help in Forecasting
Inflation and Real Activity in the Euro Area, manuscript, www.dynfactor.org.
Giannone, D., Reichlin, L. and Sala, L. (2002), Tracking Greenspan: Systematic and Unsystematic
Monetary Policy Revisited, manuscript, www.dynfactor.org.
Jones, C. (2001), Extracting Factors from Heteroskedsatic Asset Returns, Journal of Financial
Economics 62:2, 293–325.
Kapetanios, G. and Marcellino, M. (2002), A Comparison of Estimation Methods for Dynamic
Factor Models of Large Dimensions, draft, Bocconi University.
Stock, J. H. and Watson, M. W. (2001), Forecasting Output and Inflation: the Role of Asset Prices,
Journal of Economic Literature 47:1, 1–48.
24

Stock, J. H. and Watson, M. W. (2002a), Diffusion Indexes, Journal of the American Statistical
Association 97:460, 1167–1179.
Stock, J. H. and Watson, M. W. (2002b), Macroeconomic Forecasting Using Diffusion Indexes,
Journal of Business and Economic Statistics 20:2, 147–162.
Watson, M. W. (2000), Macroeconomic Forecasting Using Many Predictors, mimeo, Princeton
University.

25

Figure 1:
Fraction of series (over all 147) a rule outperforms all the others. Selection based on 6 factors
0.25
0.2
0.15
0.1
0.05
0

SW

1

2

1c

2c

SWa

SWb

A

B

C

D

Figure 2:
Fraction of series (over all 147) a given rule outperforms SW. Selection based on 6 factors.
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1

2

1c

2c

SWa

SWb

A

B

C

D

