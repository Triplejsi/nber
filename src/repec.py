import argparse
import os
import pandas as pd
import requests
import xml.etree.ElementTree as et
from bs4 import BeautifulSoup
from datetime import datetime
from random import uniform
from time import sleep


class RePEc:
    '''
    This class processes the XML API into a tabular DataFrame and save the output to .csv format.
    '''

    def __init__(self, nber_id):
        self.nber_id = nber_id
        self.url = 'http://citec.repec.org/api/plain/RePEc:nbr:nberwo:'
        self.file_name = './data/repec.csv'

    def string_id(self):
        '''
        Returns the string format for the NBER ID. For NBER ID above 1000 it will return itself.
        '''

        if self.nber_id < 10: return f'000{self.nber_id}'
        elif 10 <= self.nber_id < 100: return f'00{self.nber_id}'
        elif 100 <= self.nber_id < 1000: return f'0{self.nber_id}'
        else: return str(self.nber_id)

    def citation(self, xml_find):
        '''
        Returns either numbers of citations or numbers of being cited if any, otherwise returns nothing.
        '''
        status_code = None
        while status_code != 200:
            try:
                url = f'{self.url}{self.string_id()}'
                response = requests.get(url)
                status_code = response.status_code
                xml = et.fromstring(response.text)
                # either cites or citedBy
                xml = int(xml.find(xml_find).text)
                if status_code == 200:
                    try: return xml
                    except UnboundLocalError: return None
            except Exception as err:
                print(f'{err}: {self.nber_id}')
                continue

    def reference(self):
        '''
        Returns a list of references for the corresponding NBER paper.
        '''
        status_code = None
        while status_code != 200:
            try:
                url = f'http://citec.repec.org/api/amf/RePEc:nbr:nberwo:{self.string_id()}'
                response = requests.get(url)
                status_code = response.status_code
                parser = et.XMLParser(encoding='utf-8')
                xml = et.fromstring(response.text, parser=parser)
                text = list(xml)[0]
                reference = [list(x)[0].text for x in text if 'isreferencedby' not in x.tag]
                if status_code == 200:
                    try:
                        return reference
                    except UnboundLocalError:
                        return None
            except Exception as err:
                print(f'{err}: {self.nber_id}')
                continue

    def create(self):
        '''
        Returns a dataframe that consists of columns generated by the previous methods.
        '''

        return pd.DataFrame([{
            'id': self.nber_id,
            'cites': self.citation('cites'),
            'cited_by': self.citation('citedBy'),
            'reference': self.reference()
        }])

    def existing(self):
        '''
        Concatenate existing dataset with the new row. Duplicated rows are dropped. Returns a DataFrame sorted by ID in an ascending order.
        '''

        existing = pd.read_csv(self.file_name)
        df = pd.concat([self.create(), existing], sort=False)
        df = df.drop_duplicates(subset=['id'])

        return df.sort_values(by='id', ascending=True)
    
    def save(self):
        '''
        Save file into .csv format.
        '''

        output = self.file_name
        self.existing().to_csv(output, index=False) if os.path.exists(output) else self.create().to_csv(output, index=False)

class AverageTime:
    '''
    This class calculates the average scraping time per paper.
    '''

    def current_timestamp(self):
        '''
        Returns a timestamp in UTC.
        '''

        return datetime.utcnow()

    def subtract(self, start, end):
        '''
        Subtract the end timestamp with the start timestamp.
        '''

        return (end - start).total_seconds()
    
    def result(self, timestamp):
        '''
        Prints the average scraping time for each paper in second.
        '''

        timestamp = round((sum(timestamp) / len(timestamp)), 3) if timestamp != [] else 0
        print(f'On average, the operation takes {timestamp} second(s).')

def main(start, end, interval, output, existing):
    avg = AverageTime()
    timestamp = []
    while start < end:
        repec = RePEc(start)
        _interval = round(uniform(interval, interval+1), 3)
        if start not in existing:
            print(f'[DOWNLOAD \U0001F4BE]: {repec.url}{repec.string_id()}')
            start_timestamp = avg.current_timestamp()
            repec.save()
            print(f'[SUCCEED \U00002705]: {repec.url}{repec.string_id()}.\n[SLEEP \U0001F634]: {_interval} seconds')
            end_timestamp = avg.current_timestamp()
            timestamp.append(avg.subtract(start_timestamp, end_timestamp))
            sleep(_interval)
        else:
            print(f'[IGNORE \U0001F4C1]: {repec.url}{repec.string_id()}')
        start += 1

    return avg.result(timestamp)

if __name__ == '__main__':
    PARSER = argparse.ArgumentParser()
    PARSER.add_argument('-s', '--start', type=int, default=0, help='Starting NBER ID', metavar='')
    PARSER.add_argument('-e', '--end', type=int, default=5, help='Ending NBER ID', metavar='')
    PARSER.add_argument('-i', '--interval', type=float, default=1, help='Time interval between iteration (in second)', metavar='')
    ARGS = PARSER.parse_args()
    START = ARGS.start
    END = ARGS.end
    INTERVAL = ARGS.interval
    OUTPUT = 'data/repec.csv'
    EXISTING = pd.read_csv(OUTPUT).id.to_list() if os.path.exists(OUTPUT) else []
    main(start=START, end=END, interval=INTERVAL, output=OUTPUT, existing=EXISTING)
